{
    "meta_info": {
        "title": "ROLAND: Graph Learning Framework for Dynamic Graphs",
        "abstract": "Graph Neural Networks (GNNs) have been successfully applied to many\nreal-world static graphs. However, the success of static graphs has not fully\ntranslated to dynamic graphs due to the limitations in model design, evaluation\nsettings, and training strategies. Concretely, existing dynamic GNNs do not\nincorporate state-of-the-art designs from static GNNs, which limits their\nperformance. Current evaluation settings for dynamic GNNs do not fully reflect\nthe evolving nature of dynamic graphs. Finally, commonly used training methods\nfor dynamic GNNs are not scalable. Here we propose ROLAND, an effective graph\nrepresentation learning framework for real-world dynamic graphs. At its core,\nthe ROLAND framework can help researchers easily repurpose any static GNN to\ndynamic graphs. Our insight is to view the node embeddings at different GNN\nlayers as hierarchical node states and then recurrently update them over time.\nWe then introduce a live-update evaluation setting for dynamic graphs that\nmimics real-world use cases, where GNNs are making predictions and being\nupdated on a rolling basis. Finally, we propose a scalable and efficient\ntraining approach for dynamic GNNs via incremental training and meta-learning.\nWe conduct experiments over eight different dynamic graph datasets on future\nlink prediction tasks. Models built using the ROLAND framework achieve on\naverage 62.7% relative mean reciprocal rank (MRR) improvement over\nstate-of-the-art baselines under the standard evaluation settings on three\ndatasets. We find state-of-the-art baselines experience out-of-memory errors\nfor larger datasets, while ROLAND can easily scale to dynamic graphs with 56\nmillion edges. After re-implementing these baselines using the ROLAND training\nstrategy, ROLAND models still achieve on average 15.5% relative MRR improvement\nover the baselines.",
        "author": "Jiaxuan You, Tianyu Du, Jure Leskovec",
        "link": "http://arxiv.org/abs/2208.07239v1",
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.SI"
        ],
        "additionl_info": "Published in SIGKDD 2022 (Research Track)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction and Related Work",
                "content": "\n\n\nThe problem of learning from dynamic networks arises in many application domains, such as fraud detection \\cite{khazane2019deeptrax,Li2019ClassifyingAU}, anti-money laundering \\cite{weber2019anti}, and recommender systems \\cite{you2019hierarchical}.\nGraph Neural Networks (GNNs) are a general class of models that can perform various learning tasks on graphs.\nGNNs have gained tremendous success in learning from static graphs \\cite{battaglia2018relational,hamilton2017inductive,zhang2018link,you2019position,you2021identity}. Although various GNNs have been proposed for dynamic graphs \\cite{chen2018gc,li2019predicting,li2017diffusion,pareja2020evolvegcn,peng2020spatial,seo2018structured,taheri2019learning,wang2020traffic,yu2017spatio,zhao2019t} \nthese approaches have limitations of \\emph{model design}, \\emph{evaluation settings} and \\emph{training strategies}.\nOvercoming these limitations is crucial for real-world dynamic graph applications.\n\n\n\n\n\\xhdr{Limitations of model design}\nExisting approaches fail to transfer successful static GNN designs/architectures to dynamic graph applications.\nMany existing works treat a GNN as a feature encoder and then build a sequence model on top of the GNN \\cite{peng2020spatial, wang2020traffic, yu2017spatio}.\n% \\eg, Recurrent Neural Networks (RNNs) or Transformer\nOther works take Recurrent Neural Networks (RNNs), then replace the linear layers in the RNN cells with graph convolution layers \\cite{li2018dcrnn_traffic, seo2018structured, zhao2019t}.\nAlthough these approaches are intuitive, they do not incorporate state-of-the-art designs from static GNNs.\nFor instance, the incorporation of skip-connections \\cite{dwivedi2020benchmarking,he2016deep,li2021deepgcns}, batch normalization \\cite{dwivedi2020benchmarking,ioffe2015batch,you2020design}, edge embedding \\cite{gilmer2017neural,you2020handling} has been beneficial for GNN message passing, but has not been explored for dynamic GNNs.\nTo avoid re-exploring these design choices for dynamic GNNs, instead of building dynamic GNNs from scratch, a better design principle would be to start from a mature static GNN design and adapt it for dynamic graphs.\n\n\n\n\\xhdr{Limitations of evaluation settings}\nWhen evaluating dynamic GNNs, existing literature usually ignores the evolving nature of data and models.\nConcretely, dynamic graph datasets are often deterministically split by time, \\eg, if ten months of data are available, the first eight months of data will be used as training, one month as validation, and the last month as the test set \\cite{pareja2020evolvegcn,zhao2019t}.\nSuch protocol evaluates the model only on edges from the last month of the available dataset; therefore, it tends to overestimate the model performance given the presence of long-term pattern changes.\nAdditionally, in most literature, a non-updated model is used for prediction within the time span at evaluation time, which means the model gets stale over time \\cite{li2017diffusion, wang2020traffic, zhao2019t}. \nHowever, in real applications, users can update their models based on new data and then make future predictions.\n\n\n\n\n\\xhdr{Limitations of training strategies}\nThe common training methods for dynamic GNNs can be improved in terms of scalability and efficiency \\cite{seo2018structured,sharma2020forecasting,zhao2019t}.\nFirst, most existing training methods for dynamic GNNs usually require keeping the entire or a large portion of the graph in GPU memory \\cite{seo2018structured,sharma2020forecasting,zhao2019t}, since all or a large portion of historical edges are used for message passing.\nTherefore, dynamic GNNs are often evaluated on small networks with only a few hundred nodes \\cite{diao2019dynamic, yu2017spatio} or small transaction graphs with fewer than 2 million edges \\cite{pareja2020evolvegcn, sharma2020forecasting}.\nMoreover, there is little research on making dynamic GNNs generalize and quickly adapt to new data.\n\n\n\n\n\n\\xhdr{Present work}\nHere we propose \\name, an effective graph representation learning framework for real-world dynamic graphs. We focus on the snapshot-based representation of dynamic graphs where nodes and edges are arriving in batches (\\eg, daily, weekly).\n\\name framework can help researchers re-purpose any static GNN to a dynamic graph learning task; consequently, we can adapt state-of-the-art designs from static GNNs and significantly lower the barrier to learning from dynamic graphs.\n\n\\xhdr{ROLAND model}\nOur insight is that any GNN for static graphs can be extended for dynamic graph use cases. \nWe offer a new viewpoint for static GNNs, where the node embeddings at different GNN layers are viewed as \\emph{hierarchical node states}.\nTo generalize a static GNN to a dynamic setting, we only need to define how to update these hierarchical nodes states based on newly observed nodes and edges.\nWe explore a variety of update-modules, including moving average, Multi-layer Perceptron (MLP), and Gated Recurrent Unit (GRU) \\cite{chung2014empirical}.\nThis way of building dynamic GNNs is simple and effective, and more importantly, this approach can keep the design of a given static GNN and use it on a dynamic graph.\n\n\\xhdr{ROLAND evaluation}\nWe then introduce a live-update evaluation setting for dynamic graphs, where GNNs are used to make predictions on a rolling basis (\\eg, daily, weekly). When making each prediction, we make sure the model has only been trained using historical data so that there is no information leakage from the future to the past. We additionally allow the model to be fine-tuned using the new graph snapshot, mimicking real-world use cases, in which the model is constantly updated to fit evolving data.\n\n\\xhdr{ROLAND training}\nFinally, we propose a scalable and efficient training approach for dynamic GNNs.\nWe propose an incremental training strategy that does a truncated version of back-propagation-through-time (BPTT) \\cite{williams1990efficient}, which significantly saves GPU memory cost.\nConcretely, we only keep the incoming new graph snapshot and historical node states in GPU memory.\nUsing this technique, we are able to train GNNs on a dynamic transaction network with 56 million edges, which is about 13 times larger than existing benchmarks in terms of edges per snapshot.\nFurthermore, we formulate prediction tasks over dynamic graphs as a \\emph{meta-learning problem}, where we treat making predictions in different periods (\\eg, every day) as different tasks arriving sequentially.\nWe find a meta-model, which serves as a good initialization that is used to derive a specialized model for future unseen prediction tasks quickly.\n\n\\xhdr{Key results}\nWe conduct experiments over eight different dynamic graph datasets, with up to 56 million edges and 733 graph snapshots.\nWe first compare \\name to six baseline models under the evaluation setting in the existing literature on three datasets, \\name achieves 62.7\\% performance gain over the best baseline on average, demonstrating the effectiveness of \\name design framework.\nWe then evaluate our models and baselines on the proposed live-update setting, which provides a more realistic evaluation setting for dynamic graphs.\nWe find that existing dynamic GNN training methods based on BPTT fail to scale to large datasets.\nTo get meaningful comparisons, we re-implement baselines using \\name training, which greatly reduces GPU memory cost.\n\\name still achieves on average 15.5\\% performance gain over the \\name version of baselines.\n\n\n\\xhdr{Contributions}\nWe summarize ROLAND's innovation as follows.\n\\begin{enumerate}\n    \\item \\emph{Model design}: ROLAND describes how to repurpose a static GNN for dynamic settings effectively.\n    Furthermore, ROLAND shows that successful static GNN designs lead to significant performance gain on dynamic prediction tasks.\n    \\item \\emph{Training}: ROLAND can scale to dynamic graphs with 56 million edges, which is at least 13 times larger than existing benchmarks.\n    In addition, we innovatively formulate predicting over dynamic graphs as a meta-learning problem to achieve fast model adaptation. \n    \\item \\emph{Evaluation}: Compared to the common deterministic dataset split, ROLANDâ€™s live-update evaluation can reflect the evolving nature of data and model.\n\\end{enumerate}\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\n\n\n\n\n\n\\label{subsec:preliminaries}\n\n\\xhdr{Graphs and dynamic graphs}\nA graph can be represented as $G = (V,E)$, where $V = \\{v_1, ..., v_n\\}$ is the node set and $E \\subseteq V \\times V$ is the edge multi-set. Nodes can be paired with features $X = \\{\\mb{x}_v \\mid v \\in V\\}$, and edges can also have features $F = \\{\\mb{f}_{uv} \\mid (u,v) \\in E\\}$. \nFor dynamic graphs, each node $v$ further has a timestamp $\\tau_v$ and edge $e$ has a timestamp $\\tau_e$.\nWe focus on the snapshot-based representation for dynamic graphs: a dynamic graph $\\mathcal{G}=\\{G_t\\}_{t=1}^T$ can be represented as a sequence of graph snapshots, where each snapshot is a static graph $G_t = (V_t, E_t)$ with $V_t = \\{v\\in V \\mid \\tau_v = t\\}$ and $E_t = \\{e \\in E \\mid \\tau_e = t\\}$.\nBy modeling over graph snapshots, ROLAND can naturally handle node addition/deletion since different graph snapshots could have different sets of nodes.\n\n\\xhdr{Graph Neural Networks}\nThe goal of a GNN is to learn node embeddings based on an iterative aggregation of messages from the local network neighborhood. We use embedding matrix $H^{(L)} = \\{\\mb{h}_v^{(L)}\\}_{v \\in V}$ to denote the embedding for all the nodes after applying an $L$-layer GNN. The $l$-th layer of a GNN, $H^{(l)} = \\textsc{Gnn}^{(l)}(H^{(l-1)})$, can be written as:\n\\begin{equation}\n    \\begin{aligned}\n    &\\mb{m}_{u \\to v}^{(l)} = \\textsc{Msg}^{(l)}(\\mb{h}_u^{(l-1)}, \\mb{h}_{v}^{(l-1)}) \\\\\n    &\\mb{h}_v^{(l)} = \\textsc{Agg}^{(l)}\\big(\\{\\mb{m}_{u \\to v}^{(l)} \\mid u \\in \\mathcal{N}(v)\\}, \\mb{h}_v^{(l-1)}\\big)\n    \\end{aligned}\n    \\label{eq:gnn}\n\\end{equation}\nwhere $\\mb{h}_v^{(l)}$ is the node embedding for $v \\in V$ after passing through $l$ layers , $\\mb{h}_v^{(0)}=\\mb{x}_v$, $\\mb{m}_v^{(l)}$ is the message embedding, and $\\mathcal{N}(v)$ is the direct neighbors of $v$. Different GNNs can have have various definitions of message-passing functions $\\textsc{Msg}^{(l)}(\\cdot)$ and aggregation functions $\\textsc{Agg}^{(l)}(\\cdot)$.\nWe refer to GNNs defined in Equation (\\ref{eq:gnn}) as static GNNs, since they are designed to learn from static graphs and do not capture the dynamic information of the graph.\n\n\n\n\n\n"
            },
            "section 3": {
                "name": "Proposed ROLAND Framework",
                "content": "\n\n\n",
                "subsection 3.1": {
                    "name": "From Static GNNs to Dynamic GNNs",
                    "content": "\nHere we propose the \\name framework that generalizes static GNNs to a dynamic setting, illustrated in Figure \\ref{fig:overview}. We summarize the forward computation of the ROLAND model in Algorithm \\ref{alg:gnn-forward}.\n\n\n\\xhdr{Revisit static GNNs}\nA static GNN often consists of a stack of $L$ GNN layers, where each layer follows the general formulation in Equation (\\ref{eq:gnn}).\nWe denote the node embedding matrix after the $l$-th GNN layer as \n$H^{(l)} = \\{\\mb{h}_v^{(l)}\\}_{v \\in V}$.\nThe node embeddings computed in all layers characterize a given node in a hierarchical way; concretely, $H^{(l)}$ summarizes the information from the neighboring nodes that are $l$ hops away from a given node. We use $H=\\{H^{(1)},...,H^{(L)}\\}$ to represent the embeddings in all GNN layers.\n\n\\xhdr{From static embeddings to dynamic states}\nWe propose to extend the semantics of $H$ from static embeddings to dynamic node states. Building on previous discussions, we view $H_t$ as the \\emph{hierarchical node state} at time $t$, where each $H^{(l)}$ captures multi-hop node neighbor information.\nSuppose the input graph $G$ has dynamically changed, then the computed embedding $H$ will also change. \nTherefore, we argue that the key to generalizing any static GNN to a dynamic graph relies on how to update the hierarchical node states $H$ over time.\n\n\\xhdr{Differences with prior works}\nIn the common approach where a sequence model is built on top of a GNN, only the top-level node state $H^{(L)}$ is being kept and updated \\cite{peng2020spatial, wang2020traffic, yu2017spatio}. All the lower-level node states $H^{(1)}_{t}, ..., H^{(l)}_{t}$ are always recomputed from scratch based on the new graph snapshot $G_t$. Here we propose to keep and update the entire hierarchical node state $H^{(1)}_{t}, ..., H^{(l)}_{t}$ at all levels. \n\n\n\n\\begin{algorithm}[t]\n\\caption{ROLAND GNN forward computation}\n\\label{alg:gnn-forward}\n\\begin{flushleft}\n\\textbf{Input:} Dynamic graph snapshot $G_t$, hierarchical node state $H_{t-1}$\n\\textbf{Output:} Prediction $y_t$, updated node state $H_t$\n\\end{flushleft}\n\n\n\\begin{algorithmic}[1]\n\\STATE $H_t^{(0)} \\leftarrow X_t$ \\hfill \\COMMENT{Initialize embedding from $G_t$}\n\\FOR{$l = 1,\\dots,L$}\n\\STATE $\\tilde{H}_t^{(l)} = \\textsc{Gnn}^{(l)}(H_t^{(l-1)})$ \\hfill\\COMMENT{Implemented as Equation (\\ref{eq:gnn_roland})}\n\\STATE $H^{(l)}_{t} = \\textsc{Update}^{(l)}(H^{(l)}_{t-1}, \\tilde{H}_t^{(l)})$ \\hfill\\COMMENT{Equation (\\ref{eq:update})}\n\\ENDFOR\n\\STATE $y_t=\\textsc{Mlp}(\\textsc{Concat}(\\mb{h}_{u, t}^{(L)}, \\mb{h}_{v, t}^{(L)})), \\forall (u, v) \\in E$ \\hfill\\COMMENT{Equation (\\ref{eq:pred_head})}\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\\xhdr{Hierarchically update modules in dynamic GNNs}\nIn ROLAND, we propose an \\emph{update-module} that updates node embeddings hierarchically and dynamically. The update-module can be inserted to any static GNN. The new level $l$ node state $H^{(l)}_{t}$ depends on both lower layer node state $\\tilde{H}_t^{(l)}$ and historical node state $H_{t-1}^{(l)}$.\n\\begin{align}\n\\label{eq:update}\n    \\hspace{-2mm} H^{(l)}_{t} = \\textsc{Update}^{(l)}(H^{(l)}_{t-1},\\tilde{H}_{t}^{(l)}),\\hspace{2mm} \\tilde{H}_{t}^{(l)} = \\textsc{Gnn}^{(l)}(H_{t}^{(l-1)})\n\\end{align}\nWe examine three simple yet effective embedding update methods.\n(1) \\emph{Moving Average}: embedding of node $v$ at time $t$ is updated by\n$H^{(l)}_{t,v} = \\kappa_{t, v} H^{(l)}_{t-1, v} + (1-\\kappa_{t,v}) \\tilde{H}^{(l)}_{t,v}$. Moving average can naturally capture dynamics of embedding and is free from trainable parameters.\nWe define the moving average weight $\\kappa_{t,v}$ as\n\\begin{align}\n    \\kappa_{t,v} = \n    \\frac{\\sum_{\\tau = 1}^{t-1} |E_\\tau|}{\\sum_{\\tau = 1}^{t-1} |E_\\tau| + |E_t|} \\in [0, 1]\n    \\label{eq:keep-ratio}\n\\end{align}\n(2) \\emph{MLP}: node embeddings are updated by a 2-layer MLP, $H^{(l)}_{t} = \\textsc{Mlp}(\\textsc{Concat}(H^{(l)}_{t-1} ,\\tilde{H}_t^{(l-1)}))$. \n(3) \\emph{GRU}: node embeddings are updated by a GRU cell \\cite{chung2014empirical}, $H^{(l)}_{t} = \\textsc{Gru}(H^{(l)}_{t-1}, \\tilde{H}_t^{(l)})$, where $H^{(l)}_{t-1}$ is the hidden state and $\\tilde{H}_t^{(l)}$ is the input for the GRU cell.\n\n\n\n\\xhdr{GNN architecture design in ROLAND}\nIn ROLAND, we extend the GNN layer definition in Equation (\\ref{eq:gnn}) to incorporate successful GNN designs in static GNNs. Specifically, we have\n\\begin{equation}\n    \\begin{aligned}\n    & \\mb{m}_{u\\to v}^{(l)} = \\mb{W}^{(l)}\\textsc{Concat}\\big(\\mb{h}_u^{(l-1)}, \\mb{h}_v^{(l-1)}, \\mb{f}_{uv}\\big), \\\\\n    & \\mb{h}_v^{(l)} = \\textsc{Agg}^{(l)}\\big(\\{\\mb{m}_{u\\to v}^{(l)} \\mid u \\in \\mathcal{N}(v)\\}\\big) + \\mb{h}_v^{(l-1)}\n    \\end{aligned}\n    \\label{eq:gnn_roland}\n\\end{equation}\nHere, we consider edge features $\\mb{f}_{uv}$ in message computation $\\textsc{Msg}^{(l)}(\\cdot)$, since dynamic graphs usually have timestamps as edge features; additionally, we include $\\mb{h}_v^{(l-1)}$ in the message passing computation, so that bidirectional information flow can be modeled; we explore different aggregation functions $\\textsc{Agg}$ including summation $\\textsc{Sum}$, maximum $\\textsc{Max}$, and average $\\textsc{Mean}$; finally, we add skip-connections \\cite{he2016deep,li2021deepgcns} when stacking GNN layers by adding $\\mb{h}_v^{(l-1)}$ after message aggregation. We show in Section \\ref{subsec:ablation} that these designs lead to performance boost.\nBased on the computed node embeddings, \\name predicts the probability of a future edge from node $u$ to $v$ via an MLP prediction head\n\\begin{equation}\n\\label{eq:pred_head}\ny_t=\\textsc{Mlp}(\\textsc{Concat}(\\mb{h}_{u, t}^{(L)}, \\mb{h}_{v, t}^{(L)})), \\forall (u, v) \\in V \\times V\n\\end{equation}\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Live-update Evaluation",
                    "content": "\n\\vspace{-1mm}\n\n\\xhdr{Standard fixed split evaluation setting}\nThe evaluation procedure used in prior works constructs training and testing sets by splitting available graph snapshots sequentially. For example, the first 90\\% of snapshots for training and cross-validation, then models are evaluated using all edges from the last 10\\% of snapshots \\cite{pareja2020evolvegcn}.\nHowever, the data distribution of dynamic graphs is constantly evolving in the real world. For example, the number of purchase transactions made in the holiday season is higher than usual.\nTherefore, evaluating models solely based on edges from the last 10\\% of snapshots can provide misleading results.\n\n\n\n\\xhdr{Live-update evaluation setting}\nTo utilize all snapshots to evaluate the model, we propose a live-update evaluation procedure (Figure \\ref{fig:eval} and Algorithm \\ref{alg:live-update-evaluation}), which consists of the following key steps:\n(1) Fine-tune GNN model using newly observed data\n(2) evaluate predictions using new data,\n(3) making predictions using historical information,\nSpecifically, ROLAND first collects link prediction labels $y_{t-1}$ at time $t$ (predict future links), which are split into $y_{t-1}^{(train)}$ and $y_{t-1}^{(val)}$. Then, ROLAND fine-tunes $\\textsc{Gnn}$ on training labels $y_{t-1}^{(train)}$, while performance $\\textsc{MRR}_{t-1}^{(val)}$ on validation labels $y_{t-1}^{(val)}$ are used as the early stopping criterion. We use Mean reciprocal rank (MRR) instead of ROC AUC, since negative labels significantly outnumber positive labels in a link prediction task, and MMR has been adopted in prior work as well \\cite{pareja2020evolvegcn}.\nFinally, after obtaining edge labels $y_{t}$ at time $t+1$, we report ROLAND's performance $\\textsc{MRR}_{t}$ based on historical state $H_{t-1}$ and current snapshot $G_{t}$. This live-update evaluation is free from information leakage, since no future information is used during both training and evaluation.\nWe report the average $\\textsc{MRR}_{t}$ over all prediction steps as the final model performance.\n\n\n\\begin{algorithm}[t]\n\\caption{ROLAND live-update evaluation}\n\\label{alg:live-update-evaluation}\n\\begin{flushleft}\n\\textbf{Input:} Dynamic graph $\\mathcal{G}=\\{G_1,\\dots,G_T\\}$, link prediction labels $y_1,\\dots,y_T$, number of snapshots $T$, $\\textsc{Gnn}(\\cdot)$ defined in Algorithm \\ref{alg:gnn-forward}\\\\\n\\textbf{Output:} Performance $\\textsc{MRR}$, model $\\textsc{Gnn}$\n\\end{flushleft}\n\n\n\\begin{algorithmic}[1]\n\\STATE Initialize hierarchical node state $H_0$\n\\FOR{$t = 2,\\dots,T$}\n\\STATE Collect link prediction labels $y_{t-1} = y_{t-1}^{(train)} \\cup y_{t-1}^{(val)}$, $y_{t}$\n\\WHILE{$\\textsc{MRR}_{t-1}^{(val)}$ is increasing}\n\\STATE $H_{t-1}, \\hat{y}_{t-1} \\leftarrow \\textsc{Gnn}(G_{t-1}, H_{t-2})$, $\\hat{y}_{t-1} = \\hat{y}_{t-1}^{(train)} \\cup \\hat{y}_{t-1}^{(val)}$\n\\STATE Update $\\textsc{Gnn}$ via backprop based on $\\hat{y}_{t-1}^{(train)}$, $y_{t-1}^{(train)}$\n\\STATE $\\textsc{MRR}_{t-1}^{(val)} \\leftarrow \\textsc{Evaluate}(\\hat{y}_{t-1}^{(val)}, y_{t-1}^{(val)})$\n\\ENDWHILE\n\\STATE $H_{t}, \\hat{y}_{t} \\leftarrow \\textsc{Gnn}(G_{t}, H_{t-1})$\n\\STATE $\\textsc{MRR}_{t} \\leftarrow \\textsc{Evaluate}(\\hat{y}_{t}, y_{t})$\n\\ENDFOR\n\\STATE $\\textsc{MRR}=\\sum_{t=2}^{T} \\textsc{MRR}_t/(T-1)$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Training Strategies",
                    "content": "\n\n\\xhdr{Scalability: Incremental training}\nWe propose a scalable and efficient training approach for dynamic GNNs that borrows the idea of the truncated version of back-propagation-through-time, which has been widely used for training RNNs \\cite{williams1990efficient}.\nConcretely, we only keep the model $\\textsc{Gnn}$, the incoming new graph snapshot $G_t$, and historical node states $H_{t-1}$ into GPU memory.\nSince the historical node states $H_{t-1}$ has already encoded information up to time $t-1$, instead of training $\\textsc{Gnn}$ to predict $y_t$ from the whole sequence $\\{G_1, \\dots, G_{t-1}\\}$, the model only needs to learn from $\\{H_{t-1}$, and the newly observed graph snapshot $G_{t}\\}$. With this incremental training strategy, ROLAND's memory complexity is agnostic to the number of graph snapshots, and we can train GNNs on dynamic graphs with 56 million edges and 733 graph snapshots.\n\n\\begin{algorithm}[t]\n\\caption{ROLAND training algorithm}\n\\label{alg:train}\n\\begin{flushleft}\n\\textbf{Input:} Graph snapshot $G_t$, link prediction label $y_t$, hierarchical node state $H_{t-1}$, smoothing factor $\\alpha$, meta-model $\\textsc{Gnn}^{(meta)}$\\\\\n\\textbf{Output:} Model $\\textsc{Gnn}$, updated meta-model $\\textsc{Gnn}^{(meta)}$\n\\end{flushleft}\n\n\\begin{algorithmic}[1]\n\\STATE $\\textsc{Gnn} \\leftarrow \\textsc{Gnn}^{(meta)}$\n\\STATE Move $\\textsc{Gnn}, G_{t}, H_{t-1}$ to GPU \n\\WHILE{$\\textsc{MRR}_t^{(val)}$ is increasing}\n\\STATE $H_t, \\hat{y}_t \\leftarrow \\textsc{Gnn}(G_t, H_{t-1})$, $\\hat{y}_t = \\hat{y}_t^{(train)} \\cup \\hat{y}_t^{(val)}$\n\\STATE Update $\\textsc{Gnn}$ via backprop based on $\\hat{y}_t^{(train)}$, $y_t^{(train)}$\n\\STATE $\\textsc{MRR}_t^{(val)} \\leftarrow \\textsc{Evaluate}(\\hat{y}_t^{(val)}, y_t^{(val)})$\n\\ENDWHILE\n\\STATE Remove $\\textsc{Gnn}, G_{t}, H_{t-1}$ from GPU\n\\STATE $\\textsc{Gnn}^{(meta)} \\leftarrow  (1-\\alpha)\\textsc{Gnn}^{(meta)} + \\alpha\\textsc{Gnn}$\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\\xhdr{Fast adaptation: Meta-training}\nWe formulate prediction tasks over dynamic graphs as a meta-learning problem, where we treat making predictions in different periods (\\eg, every day) as different tasks.\nThe standard approach of updating $\\textsc{Gnn}$ is simply fine-tuning the model using new information.\nHowever, always keeping the prior model for the next future prediction task is not necessarily optimal.\nFor example, for dynamic graphs with weekly patterns, the optimal model on Friday may be drastically different from the optimal model on Saturday; therefore, simply fine-tuning the previous model could lead to inferior models.\nInstead, our proposal is to find a \\emph{meta-model} $\\text{GNN}^{(meta)}$, which serves as a good initialization that is used to derive a specialized model for future unseen prediction tasks quickly. In principle, any meta-learning algorithm can be used to update $\\text{GNN}^{(meta)}$; we follow the Reptile algorithm \\cite{nichol2018first} which is simple and effective. As is shown in Algorithm \\ref{alg:train}, at each time $t$, we first initialize the $\\text{GNN}$ model using $\\text{GNN}^{(meta)}$ and then use back-propagation with early stopping to fine-tune the model for the next prediction task.\nThen, we updates the meta-model $\\text{GNN}^{(meta)}$ by computing a moving average of the trained models $(1-\\alpha)\\textsc{Gnn}^{(meta)} + \\alpha\\textsc{Gnn}$, where $\\alpha \\in [0, 1]$ is the smoothing factor.\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\n\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n\n\\xhdr{Datasets}\nWe perform experiments using eight different datasets.\n\\textbf{(1)} BSI-ZK and \\textbf{(2)} BSI-SVT consist of financial transactions among companies using two different systems of the Bank of Slovenia, and each node has five categorical features; each edge has transaction amount as the feature.\n\\textbf{(3)} Bitcoin-OTC and Bitcoin-Alpha contain who-trusts-whom networks of people who trade on the OTC and Alpha platforms \\cite{kumar2018rev2, kumar2016edge}.\n\\textbf{(4)}  The Autonomous systems AS-733 dataset of traffic flows among routers comprising the Internet \\cite{leskovec2005graphs}.\n\\textbf{(5)} UCI-Message dataset consists of private messages sent on an online social network system among students \\cite{panzarasa2009patterns}.\n\\textbf{(6)} Reddit-Title and \\textbf{(7)} Reddit-Body are networks of hyperlinks in titles and bodies of Reddit posts, respectively. Each hyperlink represents a directed edge between two subreddits \\cite{kumar2018community}.\nTable \\ref{tab:datasets} provides summary statistics of the above-mentioned datasets.\n\n\n\n\\xhdr{Task}\nWe evaluate the ROLAND framework over the future link prediction task.\nAt each time $t$, the model utilizes information accumulated up to time $t$ to predict edges in snapshot $t+1$.\nWe use mean reciprocal rank (MRR) to evaluate candidate models. For each node $u$ with positive edge $(u, v)$ at $t+1$, we randomly sample 1000\\footnote{We only sampled 100 negative edges in the BSI-ZK dataset due to memory constraints.} negative edges emitting from $u$ and identify the rank of edge $(u, v)$'s prediction score among all other negative edges.\nMRR score is the mean of reciprocal ranks over all nodes $u$.\nWe consider two different train-test split methods in this paper.\n(1) \\emph{Fixed-split} evaluates models using all edges from the last 10\\% of snapshots.\nHowever, the transaction pattern is constantly evolving in the real world; evaluating models solely based on edges from the last few weeks provides misleading results.\n(2) \\emph{Live-update} evaluates model performance over all the available snapshots. We randomly choose 10\\% of edges in each snapshot to determine the early-stopping condition.\n\\cut{by randomly sample 10\\% of edges from each snapshot as the test edge. Our method leads to a similar amount of test edges but they are distributed across the whole span of the dataset available instead of just the last few snapshots.}\n\n\n\\xhdr{Baselines}\nWe compare our \\name models to 6 state-of-the-art dynamic GNNs.\n\\textbf{(1)} EvolveGCN-H and \\textbf{(2)} EvolveGCN-O\nutilizes an RNN to dynamically update weights of internal GNNs, which allows the GNN model to change during the test time \\cite{pareja2020evolvegcn}.\n\\textbf{(3)} T-GCN\ninternalizes a GNN into the GRU cell by replacing linear transformations in GRU with graph convolution operators \\cite{zhao2019t}.\n\\textbf{(4)} GCRN-GRU and \\textbf{(5)} GCRN-LSTM\ngeneralize TGCN by capturing temporal information using either GRU or LSTM. Instead of GCNs, GCRN uses ChebNet \\cite{defferrard2016convolutional} for spatial information.\nBesides, GCRN uses separate GNNs to compute different gates of RNNs. Hence GCRN models have much more parameters compared with other models \\cite{seo2018structured}.\n\\textbf{(6)} GCRN-Baseline GCRN baseline firstly constructs node features using a Chebyshev spectral graph convolution layer to capture spatial information; afterward, node features are fed into an LSTM cell for temporal information\n\\cite{seo2018structured}.\n\n\\xhdr{ROLAND architecture} \\name is designed to re-purpose any static GNN into a dynamic one. Thus the hyper-parameter space of the ROLAND model is similar to the hyper-parameter space of the underlying static GNN.\nWe use 128 hidden dimensions for node states, GNN layers with skip-connection, sum aggregation, and batch-normalization. We allow for at most 100 epochs for each live-update in each time step before early stopping. In addition, for each type of update-module and dataset, we search the hyper-parameters over (1) numbers of pre-processing, message-passing, and post-processing layers (1 layer to 5 layers); (2) learning rate for live-update (0.001 to 0.01); (3) whether to use single or bidirectional messages; and (4) the $\\alpha$ level for meta-learning to determine the best configuration. We report the test set MRR when the best validation MRR is achieved. We use \nNVIDIA RTX 8000 GPU in the experiments.\n% and NVIDIA RTX 2080Ti GPU in the experiments. \nWe implement ROLAND\\footnote{The source code of ROLAND is available at \\url{https://github.com/snap-stanford/roland}} with the GraphGym library \\cite{you2020design}.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Results in the Standard Evaluation Settings",
                    "content": "\nWe firstly compare our models with baselines under the standard fixed split setting.\nIn these experiments, we follow the experimental setting in the EvolveGCN paper \\cite{pareja2020evolvegcn}; specifically, we use the same snapshot frequency, set of test snapshots, and the method to compute MRR to ensure fair comparisons.\nTable \\ref{tab:fixed_split_compare} shows that ROLAND with GRU update-module consistently outperforms baseline models and achieves on average 62.69\\% performance gain.\nThis agrees with the fact that the GRU update-module is more expressive than simple moving average or MLP update-modules. \nThe significant performance gain demonstrates that ROLAND can effectively transfer successful static GNN designs to dynamic graph tasks. In Section \\ref{subsec:ablation}, we provide comprehensive ablation studies to explain the success of ROLAND further.\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Results in the Live-update Settings",
                    "content": "\n\n\\xhdr{Baselines with BPTT training vs. ROLAND}\nBy default, the baseline models are trained with back-propagation-through-time (BPTT), which requires storing all the historical node embeddings in GPU memory. This training strategy cannot scale to large graphs.\nThe top part of Table \\ref{tab:combined} summarizes the experiments on training the baseline models using BPTT.\nOur experiment results indicate that the default BPTT training fails to scale to large datasets, such as BSI-ZK and AS-733, and fails to work with large models such as GCRNs.\nFor datasets with a moderate number of edges and snapshots like BSI-SVT, BPTT still leads to OOM for large models like GCRNs and TGCN.\nEven though this training pipeline works on small datasets, the best model trained from this procedure still underperforms compared to baseline models trained using \\name incremental training by almost 10\\%.\n\n\n\\xhdr{Baselines with ROLAND training vs. ROLAND}\nTo quantitatively compare our models with baselines, we re-implemented baseline models and adapted them to be trained using our \\name training.\nThe middle and bottom parts of Table \\ref{tab:combined} contrast the performance of our models and baselines on all eight datasets.\nAll baseline models are successfully trained using our \\name training, demonstrating the flexibility of \\name\nMoreover, ROLAND with GRU update-module outperforms the best baseline model by 15.48\\% on average.\nExcept for the AS-733 dataset, our model consistently outperforms baseline models trained using the ROLAND pipeline. The performance gain of using our models ranges from 2.40\\% on BSI-ZK to 44.22\\% on the Reddit-Body dataset.\nThe performance gain is the largest on datasets with rich edge features such as Reddit hyperlink graphs, on which \\name brings 38.21\\% and 44.22\\% performance gains, respectively. These results demonstrate that \\name can utilize edge features effectively. We include more analysis of the results in the Appendix.\n\n\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\n\\label{subsec:ablation}\n\n\n\n\\xhdr{Effectiveness of ROLAND architecture design}\nAs described in Section 2.2, we introduce several successful GNN designs in static GNNs to ROLAND, so that they can work with dynamic graphs.\nHere, we conduct experiments to show that the ROLAND architecture design is effective in boosting performance.\n\nConcretely, we examine the ROLAND framework on the BSI-SVT dataset under the proposed live-update setting. The setting is similar to Table 3 in the main paper.\nWe found that Batch Normalization, skip-connection, and max aggregation are desirable for GNN architectural design, leading to a significant performance boost. For example, the introduction of skip-connection leads to more than 20\\% performance gain.\n\n\n\n\n\n\\xhdr{Effectiveness of meta-learning}\nWe examine the effectiveness of meta-learning.\nSpecifically, for each of three embedding update methods and each dataset, we run 10 experiments with $\\alpha \\in \\{$0.1, 0.2, $\\dots$, 1.0$\\}$ to study the effect of $\\alpha$ on performance\\footnote{For BSI-ZK dataset, we run 6 experiments for each update method with $\\alpha \\in \\{0.1,0.3,0.5,0.7,0.9,1.0\\}$.}. Here, $\\alpha=1$ corresponds to the baseline that directly uses the previous model to initialize the training of $\\textsc{Gnn}$.\nTable \\ref{tab:meta} reports the performance gains in MRR from meta-learning.\nEnabling meta-learning leads to various levels of performance gain depending on the dataset and model; for different embedding update designs, the average performance gain ranges from 2.84\\% to 13.19\\%.\nBesides the performance gain in MRRs, we find incorporating meta-learning, in general, improves the stability of performance, as suggested by a lower standard deviation of MRRs. We include the complete results in Appendix.\n\n\n\\xhdr{Effectiveness of model retraining}\nWe investigate the effectiveness of model retraining in Figure \\ref{fig:non-retrain}.\nAs is shown in Figure \\ref{fig:non-retrain}(top), the transaction pattern, measured by the number of transactions, significantly varies over time; in the meantime, ROLAND can automatically retrain itself to fit the data (Algorithm \\ref{alg:live-update-evaluation}).\n% ROLAND is designed to automatically retraining \nThanks to the meta-training, ROLAND only requires periodic retraining for a few epochs, and in most snapshots, as is shown in Figure \\ref{fig:non-retrain}(middle).\nWe further compare our strategy with a baseline that stops retraining after training the first 25\\% of snapshots. We found that this baseline performs significantly worse than retraining for all the snapshots.\n\n\n\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Additional Related Work",
                "content": "\n\n\n\n\\xhdr{Dynamic GNNs}\nWhile many works combine GNNs with recurrent models (\\eg, GRU cells), we want to emphasize that our ROLAND framework is quite different.\nMost existing works take a recurrent model (RNN, Transformer) as the backbone architecture, then (1) replace either the feature encoder \\cite{peng2020spatial} with a GNN, or (2) replace the linear layers in the RNN cells with GNN layers \\cite{li2018dcrnn_traffic, seo2018structured, zhao2019t}. The first line of approaches ignores the evolution of lower-level node embeddings, while the second approach only utilizes a given GNN layer rather than building upon an established and successful static GNN architecture.\nEvolveGCN \\cite{pareja2020evolvegcn} proposes to recurrently updates the GNN weights; in contrast, ROLAND recurrently updates hierarchical node embeddings. While the EvolveGCN approach is memory efficient, the explicit historical information is lost (e.g., past transaction information). Our experiments show that EvolveGCN delivers undesirable performance when the number of graph snapshots is large.\nMoreover, few existing dynamic GNN works have explored state-of-the-art static GNN designs, such as the inclusion of edge features, batch normalization, skip-connections, etc.\nBy utilizing mature static GNN designs, we show that ROLAND can significantly outperform existing dynamic GNNs.\n\n\\xhdr{Scalable training}\nMost existing approaches do not consider scalable training since the current benchmark dynamic graphs are small; thus, the entire graph can often fit into the GPU memory.\nAdditionally, some dynamic GNN architectures can hardly scale; for example, when using Transformer as the base sequence model requires keeping the entire historical graph in the GPU to compute the attention over time \\cite{sankar2020dysat}.\nSome existing approaches have used heuristics to scale training. For example, they would limit the number of historical graph snapshots used to predict the current snapshot \\cite{zhao2019t}.\nIn contrast, our proposed incremental training keeps the entire historical information in the node hidden states while only choosing to back-propagate within the given snapshot.\n\n\\xhdr{Non-snapshot dynamic graph representation}\nInstead of using snapshot-based representation, one can also represent a dynamic graph as a single graph where nodes and edges have time stamps \\cite{kumar2019predicting,ma2020streaming,rossi2020temporal}. \nAs is described in Section \\ref{subsec:preliminaries}, we can easily convert that representation to graph snapshots by grouping all the nodes/edges within a given period into a graph snapshot, and then apply our ROLAND framework.\n\n\\xhdr{Other learning methods for dynamic graphs}\nBesides GNNs, researchers have explored other learning methods for dynamic graphs, such as matrix factorization based methods \\cite{li2017attributed}, random walk based models \\cite{nguyen2018continuous,yu2018netwalk}, point process based approaches \\cite{trivedi2017know,trivedi2018representation,zhou2018dynamic}. We focus on designing GNN-based methods in this paper due to their performance and inductive learning capabilities.\n\n"
            },
            "section 6": {
                "name": "Acknowledgements",
                "content": "\nWe thank Rok Sosic, Daniel Borrajo, Vamsi Potluru, Naren Chittar, Pranjal Patil, Hao Yang, Yuhang Wu, Das Mahashweta for their helpful discussions on the paper. \nWe also gratefully acknowledge the support of\nStanford Data Science Initiative and JPMorgan Chase.\n\n"
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\nWe propose \\name, an effective graph representation learning system for building, training, and evaluating dynamic GNNs.\n\\name helps researchers re-purpose any static GNN for a dynamic graph while keeping effective designs from static GNNs.\n\\name comes with a live-update pipeline that mimics real-world usage by allowing the model to be dynamically updated while being evaluated.\nOur experiments show that dynamic GNNs built using \\name framework successfully scale to large datasets and outperform existing state-of-the-art models.\nWe hope \\name can enable more large-scale real-world applications over dynamic graphs.\n\n\\bibliography{bibli}\n\\bibliographystyle{abbrv}\n\n\n"
            }
        },
        "figures": {
            "fig:overview": "\\begin{figure*}[t]\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{fig/overview.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{ROLAND model design principle}. \\textbf{(a)} Example static GNN with modern architecture designs, including intra-layer designs such as BatchNorm and attention, and inter-layer designs such as skip-connections. \\textbf{(b)} ROLAND can easily extend any static GNN to dynamic graphs, by inserting embedding update modules that update hierarchical node states $H_t$ over time. }\n\\label{fig:overview}\n\\end{figure*}",
            "fig:eval": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.98\\linewidth]{fig/eval.pdf} \n\\vspace{-1mm}\n\\caption{\\textbf{ROLAND live-update evaluation}. ROLAND fine-tunes $\\textsc{Gnn}$ with $y_{t-1}$ and updates node embeddings $H_{t-1}$ for the next prediction task. After obtaining labels $y_{t}$, ROLAND evaluates $\\textsc{Gnn}$'s predictive performance based on historical state $H_{t-1}$ and current snapshot $G_{t}$. \n}\n\\label{fig:eval}\n\\end{figure}",
            "fig:roland-design-compare": "\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\linewidth]{fig/ablation_search.png} \n\\vspace{-4mm}\n\\caption{\\textbf{Effectiveness of ROLAND architectural design}. We run experiments with different GNN models and analyze the effect of differ design dimensions. We show the MRR distribution of all the models under different design options. Results show that batch normalization, skip-connection and max aggregation are desirable for GNN architectural design.}\n% \\vspace{-2mm}\n\\label{fig:roland-design-compare}\n\\end{figure}",
            "fig:non-retrain": "\\begin{figure*}[h]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{fig/test_mrr.png} \n\\vspace{-1mm}\n\\caption{\\textbf{Effectiveness of ROLAND model retraining} on BSI-ZK dataset. Top: number of transactions in each daily graph snapshot. Middle: number of training epochs before early stopping. Bottom: ROLAND's performance on the test set. We compare the option of retraining for all snapshots (blue curve), with a baseline that stopping retraining after training the first 25\\% snapshots (organ curve). }\n\\label{fig:non-retrain}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\begin{aligned}\n    &\\mb{m}_{u \\to v}^{(l)} = \\textsc{Msg}^{(l)}(\\mb{h}_u^{(l-1)}, \\mb{h}_{v}^{(l-1)}) \\\\\n    &\\mb{h}_v^{(l)} = \\textsc{Agg}^{(l)}\\big(\\{\\mb{m}_{u \\to v}^{(l)} \\mid u \\in \\mathcal{N}(v)\\}, \\mb{h}_v^{(l-1)}\\big)\n    \\end{aligned}\n    \\label{eq:gnn}\n\\end{equation}",
            "eq:2": "\\begin{align}\n\\label{eq:update}\n    \\hspace{-2mm} H^{(l)}_{t} = \\textsc{Update}^{(l)}(H^{(l)}_{t-1},\\tilde{H}_{t}^{(l)}),\\hspace{2mm} \\tilde{H}_{t}^{(l)} = \\textsc{Gnn}^{(l)}(H_{t}^{(l-1)})\n\\end{align}",
            "eq:3": "\\begin{align}\n    \\kappa_{t,v} = \n    \\frac{\\sum_{\\tau = 1}^{t-1} |E_\\tau|}{\\sum_{\\tau = 1}^{t-1} |E_\\tau| + |E_t|} \\in [0, 1]\n    \\label{eq:keep-ratio}\n\\end{align}",
            "eq:4": "\\begin{equation}\n    \\begin{aligned}\n    & \\mb{m}_{u\\to v}^{(l)} = \\mb{W}^{(l)}\\textsc{Concat}\\big(\\mb{h}_u^{(l-1)}, \\mb{h}_v^{(l-1)}, \\mb{f}_{uv}\\big), \\\\\n    & \\mb{h}_v^{(l)} = \\textsc{Agg}^{(l)}\\big(\\{\\mb{m}_{u\\to v}^{(l)} \\mid u \\in \\mathcal{N}(v)\\}\\big) + \\mb{h}_v^{(l-1)}\n    \\end{aligned}\n    \\label{eq:gnn_roland}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq:pred_head}\ny_t=\\textsc{Mlp}(\\textsc{Concat}(\\mb{h}_{u, t}^{(L)}, \\mb{h}_{v, t}^{(L)})), \\forall (u, v) \\in V \\times V\n\\end{equation}"
        },
        "git_link": "https://github.com/snap-stanford/roland"
    }
}