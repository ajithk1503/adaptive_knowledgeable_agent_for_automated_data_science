{
    "meta_info": {
        "title": "iTransformer: Inverted Transformers Are Effective for Time Series  Forecasting",
        "abstract": "The recent boom of linear forecasting models questions the ongoing passion\nfor architectural modifications of Transformer-based forecasters. These\nforecasters leverage Transformers to model the global dependencies over\ntemporal tokens of time series, with each token formed by multiple variates of\nthe same timestamp. However, Transformers are challenged in forecasting series\nwith larger lookback windows due to performance degradation and computation\nexplosion. Besides, the embedding for each temporal token fuses multiple\nvariates that represent potential delayed events and distinct physical\nmeasurements, which may fail in learning variate-centric representations and\nresult in meaningless attention maps. In this work, we reflect on the competent\nduties of Transformer components and repurpose the Transformer architecture\nwithout any modification to the basic components. We propose iTransformer that\nsimply applies the attention and feed-forward network on the inverted\ndimensions. Specifically, the time points of individual series are embedded\ninto variate tokens which are utilized by the attention mechanism to capture\nmultivariate correlations; meanwhile, the feed-forward network is applied for\neach variate token to learn nonlinear representations. The iTransformer model\nachieves state-of-the-art on challenging real-world datasets, which further\nempowers the Transformer family with promoted performance, generalization\nability across different variates, and better utilization of arbitrary lookback\nwindows, making it a nice alternative as the fundamental backbone of time\nseries forecasting. Code is available at this repository:\nhttps://github.com/thuml/iTransformer.",
        "author": "Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long",
        "link": "http://arxiv.org/abs/2310.06625v4",
        "category": [
            "cs.LG"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\n\n\nTransformer~\\citep{Transformer} has achieved tremendous success in natural language processing~\\citep{brown2020language} and computer vision~\\citep{dosovitskiy2020image}, growing into the foundation model that follows the scaling law~\\citep{kaplan2020scaling}. Inspired by the immense success in extensive fields, Transformer with strong capabilities of depicting pairwise dependencies and extracting multi-level representations in sequences is emerging in time series forecasting~\\citep{Autoformer, PatchTST}. \n\n\nHowever, researchers have recently begun to question the validity of Transformer-based forecasters, which typically embed multiple variates of the same timestamp into indistinguishable channels and apply attention on these \\emph{temporal tokens} to capture temporal dependencies. Considering the numerical but less semantic relationship among time points, researchers find that simple linear layers, which can be traced back to statistical forecasters~\\citep{box1968some}, have exceeded complicated Transformers on both performance and efficiency~\\citep{DLinear, das2023long}. Meanwhile, ensuring the independence of variate and utilizing mutual information is ever more highlighted by recent research that explicitly models multivariate correlations to achieve accurate forecasting~\\citep{Crossformer, TSMixer}, but this goal can be hardly achieved without subverting the vanilla Transformer architecture.\n\n\nConsidering the disputes of Transformer-based forecasters, we reflect on why Transformers perform even worse than linear models in time series forecasting while acting predominantly in many other fields. We notice that the existing structure of Transformer-based forecasters may be not suitable for multivariate time series forecasting. As shown on the top of Figure~\\ref{fig:motivation}, it is notable that the points of the same time step that basically represent completely different physical meanings recorded by inconsistent measurements are embedded into one token with wiped-out multivariate correlations. And the token formed by a single time step can struggle to reveal beneficial information due to excessively local receptive field and \\update{time-unaligned events represented by simultaneous time points}. Besides, while series variations can be greatly influenced by the sequence order, permutation-invariant attention mechanisms are improperly adopted on the temporal dimension~\\citep{DLinear}. Consequently, Transformer is weakened to capture essential series representations and portray multivariate correlations, limiting its capacity and generalization ability on diverse time series data.\n\n\n\\update{Concerning the potential risks of embedding multivariate points of a timestamp as a (temporal) token}, we take an \\emph{inverted view} on time series and embed the whole time series of each variate independently into a (variate) token, the extreme case of Patching~\\citep{PatchTST} that enlarges local receptive field. By inverting, the embedded token aggregates the global representations of series that can be more variate-centric and better leveraged by booming attention mechanisms for multivariate correlating. Meanwhile, the feed-forward network can be proficient enough to learn generalizable representations for distinct variates encoded from arbitrary lookback series and decoded to predict future series.\n\nBased on the above motivations, we believe it is not that Transformer is ineffective for time series forecasting, but rather it is improperly used. In this paper, we revisit the structure of Transformer and advocate \\emph{iTransformer} as a fundamental backbone for time series forecasting. Technically, we embed each time series as \\emph{variate tokens}, adopt the attention for multivariate correlations, and employ the feed-forward network for series representations. Experimentally, the proposed iTransformer achieves state-of-the-art performance on real-world forecasting benchmarks shown in Figure~\\ref{fig:radar} and surprisingly tackles the pain points of  Transformer-based forecasters. Our contributions lie in three aspects:\n\n\n\n\n\\begin{itemize}\n  \\item We reflect on the architecture of Transformer and refine that the competent capability of native Transformer components on multivariate time series is underexplored.\n  \\item We propose iTransformer that regards independent time series as tokens to capture multivariate correlations by self-attention and utilize layer normalization and feed-forward network modules to learn better series-global representations for time series forecasting.\n  \\item Experimentally, iTransformer achieves comprehensive state-of-the-art on real-world benchmarks. We extensively analyze the inverted modules and architecture choices, indicating a promising direction for the future improvement of Transformer-based forecasters.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\nWith the progressive breakthrough made in natural language processing and computer vision areas, elaboratively designed Transformer variants are proposed to tackle ubiquitous time series forecasting applications. Going beyond contemporaneous TCNs~\\citep{bai1803empirical, SCINet} and RNN-based forecasters~\\citep{LSTMnetwork, RNN1, salinas2020deepar}, Transformer has exhibited powerful sequence modeling capability and promising model scalability, leading to the trend of passionate modifications adapted for time series forecasting.\n\nThrough a systematical review of Transformer-based forecasters, we conclude that existing modifications can be divided into four categories by whether to modify the component and architecture. As shown in Figure~\\ref{fig:category}, the first category~\\citep{Autoformer, Informer, fedformer}, which is the most common practice, mainly concerns the component adaptation, especially the attention module for the temporal dependency modeling and the complexity optimization on long sequences. Nevertheless, with the rapid emergence of linear forecasters~\\citep{Nbeats, DLinear, das2023long, liu2023koopa}, the impressive performance and efficiency continuously challenge this direction. Soon afterward, the second category attempts to fully utilize Transformer. It pays more attention to the inherent processing of time series, such as Stationarization~\\citep{Stationary}, Channel Independence, and Patching~\\citep{PatchTST}, which bring about consistently improved performance. Moreover, faced with the increasing significance of the independence and mutual interactions of multiple variates, the third category refurbishes Transformer in both aspects of component and architecture. Representative~\\citep{Crossformer} explicitly captures the cross-time and cross-variate dependencies by the renovated attention mechanism and architecture.\n\n\nUnlike previous works, iTransformer modifies none of the native components of Transformer. Instead, \n% the duties of components are totally \n\\update{we adopt the components on the inverted dimensions with the altered architecture}, as the only one that belongs to the fourth category to our best knowledge. We believe the capabilities of the components have stood the test extensively, the truth is that the architecture of Transformer is improperly adopted.\n\n\n\n\n"
            },
            "section 3": {
                "name": "iTransformer",
                "content": "\n\nIn multivariate time series forecasting, given historical observations $\\mathbf{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_T\\}\\in\\mathbb{R}^{T\\times N}$ with $T$ time steps and $N$ variates, we predict the future $S$ time steps $\\mathbf{Y}=\\{\\mathbf{x}_{T+1},\\ldots,\\mathbf{x}_{T+S}\\}\\in\\mathbb{R}^{S\\times N}$. For convenience, we denote $\\mathbf{X}_{t,:}$ as the simultaneously recorded time points at the step $t$, and $\\mathbf{X}_{:,n}$ as the whole time series of each variate indexed by $n$. It is notable that $\\mathbf{X}_{t,:}$ may not contain time points that \\update{essentially reflect the same event} in real-world scenarios because of the systematical \\update{time lags among variates in the dataset}. Besides, the elements of $\\mathbf{X}_{t,:}$ can be distinct from each other in physical measurements and statistical distributions, for which a variate $\\mathbf{X}_{:, n}$ generally shares.\n\n",
                "subsection 3.1": {
                    "name": "Structure Overview",
                    "content": "\nOur proposed \\emph{iTransformer} illustrated in Figure~\\ref{fig:arch} adopts the \\emph{encoder-only} architecture of Transformer~\\citep{Transformer}, including the embedding, projection, and Transformer blocks.\n\n\n\n\\paragraph{Embedding the whole series as the token}\n\nMost Transformer-based forecasters typically regard multiple variates of the same time as the (temporal) token and follow the generative formulation of forecasting tasks. However, we find the approach on the numerical modality can be less instructive for learning attention maps, which is supported by increasing applications of Patching~\\citep{dosovitskiy2020image, PatchTST} that broadens the respective field. Meanwhile, the triumph of linear forecasters also challenges the necessity of adopting a heavy encoder-decoder Transformer for generating tokens. Instead, our proposed encoder-only iTransformer focuses on representation learning and adaptive correlating of multivariate series. Each time series driven by the underlying complicated process is firstly tokenized to describe the properties of the variate, applied by self-attention for mutual interactions, and individually processed by feed-forward networks for series representations. Notably, the task to generate the predicted series is essentially delivered to linear layers, which has been proven competent by previous work~\\citep{das2023long} and we provide a detailed analysis in the next section.\n\nBased on the above considerations, in iTransformer, the process of predicting future series of each specific variate $\\hat{\\mathbf{Y}}_{:, n}$ based on the lookback series $\\mathbf{X}_{:, n}$ is simply formulated as follows:\n\n \\begin{equation}\n \\begin{aligned}\n     \\mathbf{h}^{0}_n &= \\operatorname{Embedding}(\\mathbf{X}_{:,n}), \\\\\n    \\mathbf{H}^{l+1} &= \\operatorname{TrmBlock}(\\mathbf{H}^{l}),\\ l=0,\\dots, L-1, \\\\\n     \\hat{\\mathbf{Y}}_{:,n} &= \\operatorname{Projection}(\\mathbf{h}^{L}_n), \\\\\n\\end{aligned}\n\\end{equation}\n\nwhere $\\mathbf{H}=\\{\\mathbf{h}_1,\\dots,\\mathbf{h}_N\\}\\in\\mathbb{R}^{N\\times D}$ contains $N$ embedded tokens of dimension ${D}$ and the superscript denotes the layer index. $\\operatorname{Embedding}: \\mathbb{R}^T \\mapsto \\mathbb{R}^D$ and $\\operatorname{Projection}: \\mathbb{R}^D \\mapsto \\mathbb{R}^S$ are both implemented by multi-layer perceptron (MLP). The obtained variate tokens interact with each other by self-attention and are independently processed by the shared feed-forward network in each $\\operatorname{TrmBlock}$. Specifically, as the order of sequence is implicitly stored in the neuron permutation of the feed-forward network, the position embedding in the vanilla Transformer is no longer needed here.\n\n\\paragraph{iTransformers} The architecture essentially presupposes no more specific requirements on Transformer variants, other than the attention is applicable for multivariate correlation. Thus, a bundle of efficient attention mechanisms~\\citep{Informer, wu2022flowformer, dao2022flashattention} can be the plugins, reducing the complexity when the variate number grows large. \\update{Besides, with the input flexibility of attention, the token number can vary from training to inference, and the model is allowed to be trained on arbitrary numbers of variates}. The inverted Transformers, named \\emph{iTransformers}, are extensively evaluated in experiments of Section~\\ref{sec:framework_generality} and demonstrate advantages on time series forecasting.\n\n"
                },
                "subsection 3.2": {
                    "name": "Inverted Transformer Components",
                    "content": "\n\nWe organize a stack of $L$ blocks composed of the layer normalization, feed-forward network, and self-attention modules. But their duties on the inverted dimension are carefully reconsidered.\n\n\\paragraph{Layer normalization} Layer normalization \\citep{LayerNorm} is originally proposed to increase the convergence and training stability of deep networks. In typical Transformer-based forecasters, the module normalizes the multivariate representation of the same timestamp, gradually fusing the variates with each other. Once the collected time points \\update{do not represent the same event}, the operation will also introduce interaction noises between noncausal or delayed processes. In our inverted version, the normalization is applied to the series representation of individual variate as Equation~\\ref{equ:norm}, which has been studied and proved effective in tackling non-stationary problems~\\citep{kim2021reversible, Stationary}. Besides, since all series as (variate) tokens are normalized to a Gaussian distribution, the discrepancies caused by inconsistent measurements can be diminished. By contrast, in previous architecture, different tokens of time steps will be normalized, leading to oversmooth time series.\n\n\\begin{equation}\\label{equ:norm}\n    \\operatorname{LayerNorm}(\\mathbf{H}) = \\left\\{\\frac{\\mathbf{h}_n - \\operatorname{Mean}(\\mathbf{h}_n)}{ \\sqrt{\\operatorname{Var}(\\mathbf{h}_n)}} \\bigg| \\ n=1,\\dots,N \\right\\}\n\\end{equation}\n\n\\paragraph{Feed-forward network}\nTransformer adopts the feed-forward network (FFN) as the basic building block for encoding token representation and it is identically applied to each token. As aforementioned, in the vanilla Transformer, multiple variates of the same timestamp that form the token can be malpositioned and too localized to reveal enough information for predictions. In the inverted version, FFN is leveraged on the series representation of each variate token. By the universal approximation theorem~\\citep{hornik1991approximation}, they can extract complicated representations to describe a time series. With the stacking of inverted blocks, they are devoted to encoding the observed time series and decoding the representations for future series using dense non-linear connections, which work effectively as the recent works completely built on MLPs~\\citep{tolstikhin2021mlp, das2023long}.\n\nMore interestingly, the identical linear operation on independent time series, which serves as the combination of the recent linear forecasters~\\citep{DLinear} and Channel Independence~\\citep{PatchTST}, can be instructive for us to understand the series representations. Recent revisiting on linear forecasters~\\citep{li2023revisiting} highlights that temporal features extracted by MLPs are supposed to be shared within distinct time series. We propose a rational explanation that the neurons of MLP are taught to portray the intrinsic properties of any time series, such as the amplitude, periodicity, and even frequency spectrums (neuron as a filter), serving as a more advantageous predictive representation learner than the self-attention applied on time points. Experimentally, we validate that the division of labor helps enjoy the benefits of linear layers in Section~\\ref{sec:analysis}, such as the promoted performance if providing enlarged lookback series, and the generalization ability on unseen variates.\n\n\n\\paragraph{Self-attention} While the attention mechanism is generally adopted for facilitating the temporal dependencies modeling in previous forecasters, the inverted model regards the whole series of one variate as an independent process. Concretely, with comprehensively extracted representations of each time series $\\mathbf{H}=\\{\\mathbf{h}_0,\\ldots,\\mathbf{h}_{N}\\}\\in\\mathbb{R}^{N\\times D}$, the self-attention module adopts linear projections to get queries, keys, and values $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N\\times d_k}$, where $d_k$ is the projected dimension.\n\nWith denotation of $\\mathbf{q}_i, \\mathbf{k}_j\\in\\mathbb{R}^{d_k}$ as the specific query and key of one (variate) token, we notice that each entry of the pre-Softmax scores is formulated as $\\mathbf{A}_{i, j}=(\\mathbf{Q}\\mathbf{K}^\\top / \\sqrt{d_k})_{i, j} \\propto \\mathbf{q}_i^{\\top}\\mathbf{k}_j$. Since each token is previously normalized on its feature dimension, the entries can somewhat reveal the variate-wise correlation, and the whole score map $\\mathbf{A} \\in \\mathbb{R}^{N\\times N}$ exhibits the multivariate correlations between paired variate tokens. Consequently, highly correlated variate will be more weighted for the next representation interaction with values $\\mathbf{V}$. Based on this intuition, the proposed mechanism is believed to be more natural and interpretable for multivariate series forecasting. We further provide the visualization analysis of the score map in Section~\\ref{sec:analysis} \\update{and Appendix~\\ref{sec:app_vis}}.\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\nWe thoroughly evaluate the proposed iTransformer on various time series forecasting applications, validate the generality of the proposed framework and further dive into the effectiveness of \\update{applying the Transformer components on the inverted dimensions of time series}.\n\n\\paragraph{Datasets} \nWe extensively include 7 real-world datasets in our experiments, including ECL, \\update{ETT (4 subsets)}, \\update{Exchange}, Traffic, Weather used by Autoformer~\\citep{Autoformer}, Solar-Energy datasets proposed in LSTNet~\\citep{LSTNet}, and \\update{PEMS (4 subsets)} evaluated in SCINet~\\citep{SCINet}. We also provide the experiments on Market (6 subsets) in Appendix~\\ref{sec:full_results}. It records the minute-sampled server load of Alipay \n% an \nonline transaction application with hundreds of variates, where we consistently outperform other baselines. Detailed dataset descriptions are provided in Appendix~\\ref{sec:dataset}.\n\n",
                "subsection 4.1": {
                    "name": "Forecasting Results",
                    "content": "\nIn this section, we conduct extensive experiments to evaluate the forecasting performance of our proposed model together with advanced deep forecasters. \n\n\\paragraph{Baselines} We carefully choose 10 well-acknowledged forecasting models as our benchmark, including (1) Transformer-based methods: Autoformer~\\citep{Autoformer}, FEDformer~\\citep{fedformer}, Stationary~\\citep{Stationary}, Crossformer~\\citep{Crossformer}, PatchTST~\\citep{PatchTST}; (2) Linear-based methods: DLinear~\\citep{DLinear}, TiDE~\\citep{das2023long}, \\update{RLinear~\\citep{li2023revisiting}}; and (3) TCN-based methods: SCINet~\\citep{SCINet}, TimesNet~\\citep{Timesnet}.\n\n\n\\paragraph{Main results} Comprehensive forecasting results are listed in Table~\\ref{tab:main_result} with the best in \\boldres{red} and the second \\secondres{underlined}. The lower MSE/MAE indicates the more accurate prediction result. \\update{Compared with other forecasters, iTransformer is particularly good at forecasting high-dimensional time series}. Besides, PatchTST as the previous state-of-the-art, fails in many cases of PEMS, which can stem from the extremely fluctuating series of the dataset, and the patching mechanism of PatchTST may lose focus on specific locality to handle rapid fluctuation. By contrast, the proposed model aggregating the whole series variations for series representations can better cope with this situation. Notably, as the representative that explicitly captures multivariate correlations, the performance of Crossformer is still subpar to iTransformer, indicating the interaction of time-unaligned patches from different multivariate will bring about unnecessary noise for forecasting. Therefore, the native Transformer components are competent for temporal modeling and multivariate correlating, and the proposed inverted architecture can effectively tackle real-world time series forecasting scenarios.\n\n\\vspace{-15pt}\n\n\\vspace{-10pt}\n\n"
                },
                "subsection 4.2": {
                    "name": "iTransformers Generality",
                    "content": "\\label{sec:framework_generality}\nIn this section, we evaluate \\emph{iTransformers} by applying our framework to Transformer and its variants, which generally address the quadratic complexity of the self-attention mechanism, including Reformer~\\citep{kitaev2020reformer}, Informer~\\citep{Informer}, \nFlowformer~\\citep{wu2022flowformer} and FlashAttention~\\citep{dao2022flashattention}. Surprising and promising discoveries are exhibited, indicating the simple inverted perspective can enhance Transformer-based forecasters with promoted performance with efficiency, generalization on unseen variates, and better utilization of historical observations.\n\n\\paragraph{Performance promotion} We evaluate Transformers and the corresponding iTransformers with the reported performance promotions in Table~\\ref{tab:forecasting_promotion}. It is notable that the framework consistently improves various Transformers. Overall, it achieves averaged \\textbf{38.9\\%} promotion on Transformer, \\textbf{36.1\\%} on Reformer,  \\textbf{28.5\\%} on Informer, \n% \\textbf{25.0\\%} on Pyraformer\n\\textbf{16.8\\%} on Flowformer and \\textbf{32.2\\%} on Flashformer, revealing the previous improper usage of the Transformer architecture on time series forecasting. Moreover, since the attention mechanism is adopted on the variate dimension in our inverted structure, the introduction of efficient attentions with linear complexity essentially addresses the computational problem due to numerous variates, which is prevalent in real-world applications but can be resource-consuming for Channel Independence~\\citep{PatchTST}. Therefore, the idea of iTransformer can be widely practiced on Transformer-based forecasters to take advantage of booming efficient attention mechanisms.\n\n\\vspace{-9pt}\n\n\n\\paragraph{Variate generalization} By inverting vanilla Transformers, it is notable that the models are empowered with the generalization capability on unseen variates. Firstly, benefiting from the flexibility of the number of input tokens, the amount of variate channels is no longer restricted and thus feasible to vary from training and inference. Besides, feed-forward networks are identically applied on independent variate tokens in iTransformer. As aforementioned, the neurons as filters learn the intrinsic patterns of any time series, which are inclined to be shared and transferable among distinct variates.\n\nTo verify the hypothesis, we compare inverting with another generalizing strategy: Channel Independence, training a shared backbone to forecast all variates. \\update{We partition the variates of each dataset into five folders, train models with only $20\\%$ of variates of one folder}, and directly forecast all variates without fine-tuning. We compare the performance in Figure~\\ref{fig:variate_generalization} and each bar presents the averaged results of all folders to avoid the randomness of partition. CI-Transformers take a long time to predict each variate one by one during inference while iTransformers directly predict all variates and generally present smaller increases, \\update{indicating FFN is competent to learn transferable time series representations. It leaves a potential direction to build a foundation model upon iTransformer, where diverse multivariate time series with different numbers of variates can be feasibly trained together.}\n\n\\vspace{-4pt}\n\n\\vspace{-12pt}\n\n\\paragraph{Increasing lookback length} Previous works have witnessed the phenomenon that the forecasting performance does not necessarily improve with the increase of lookback length on Transformers~\\citep{PatchTST, DLinear}, which can be attributed to the distracted attention on the growing input. However, the desired performance improvement is generally held on linear forecasts, theoretically supported by statistical methods~\\citep{box1968some} with enlarged historical information to be utilized. As the working dimensions of attention and feed-forward network are inverted, we evaluate the performance of Transformers and iTransformer in Figure~\\ref{fig:increasing_lookback} with increased lookback length. The results surprisingly verify the rationality of leveraging MLPs on the temporal dimension such that Transformers can benefit from the extended lookback window for more precise predictions.\n\n\n\\vspace{-10pt}\n\n"
                },
                "subsection 4.3": {
                    "name": "Model Analysis",
                    "content": "\\label{sec:analysis}\n\n\\paragraph{Ablation study} To verify the rational business of Transformer components, we provide detailed ablations covering both replacing components (Replace) and removing components (w/o) experiments. The results are listed in Table \\ref{tab:ablation}. iTransformer that utilizes attention on the variate dimension and feed-forward on the temporal dimension generally achieves the best performance. Notably, the performance of vanilla Transformer (the third row) performs the worst among these designs, \\update{revealing the potential risks of the conventional architecture, which we describe in detail in Appendix~\\ref{sec:risks}}.\n\n\\vspace{-10pt}\n\n\n\\paragraph{Analysis of series representations} To further validate the claim that feed-forward networks are more favored to extract the series representations. We conduct representation analysis based on the centered kernel alignment (CKA) similarity~\\citep{Kornblith2019SimilarityON}. A higher CKA indicates more similar representations. For Transformer variants and iTransformers, we calculate the CKA between the output features of the first and the last block. Notably, previous works have demonstrated that time series forecasting, as a low-level generative task, prefers the higher CKA similarity~\\citep{Timesnet, dong2023simmtm} for the better performance. As shown in Figure~\\ref{fig:analysis}, a clear division line is exhibited, implying that iTransformers have learned more appropriate series representations by inverting the dimension and thus achieve more accurate predictions. The results also advocate inverting Transformer deserves a fundamental renovation of the forecasting backbone.\n\n\\paragraph{Analysis of multivariate correlations} By assigning the duty of multivariate correlation to the attention mechanism, the learned map enjoys enhanced interpretability. We present the case visualization on series from Solar-Energy in Figure~\\ref{fig:analysis}, which has distinct correlations in the lookback and future windows. It can be observed that in the shallow attention layer, the learned map shares lots of similarities to the correlations of raw input series. As it dives into deeper layers, the learned map become gradually alike to the correlations of future series, which validates the inverted operation empowers interpretable attention for correlating, and the processes of encoding the past and decoding for the future are essentially conducted in series representations during feed-forwarding.\n\n\\vspace{-4pt}\n\n\\vspace{-10pt}\n\n\\paragraph{Efficient training strategy} Due to the quadratic complexity of self-attention, it can be overwhelming for training on numerous variates, which is very common in real-world scenarios. In addition to efficient attention mechanisms, we propose a novel training strategy for high-dimensional multivariate series by taking advantage of previously demonstrated variate generation capability. Concretely, we randomly choose part of the variates in each batch and only train the model with selected variates. Since the number of variate channels is flexible because of our inverting, the model can predict all the variates for predictions. As shown in Figure~\\ref{fig:efficient_strategy}, the performance of our proposed strategy is still comparable with full-variate training, while the memory footprint can be reduced significantly.\n\n\\vspace{-5pt}\n\n\\vspace{-10pt}\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion and Future Work",
                "content": "\n\\update{Considering the characteristics of multivariate time series, we propose iTransformer that inverts the structure of Transformer without modifying any native modules. iTransformer regards independent series as variate tokens to capture multivariate correlations by attention and utilize layer normalization and feed-forward networks to learn series representations}. Experimentally, iTransformer achieves state-of-the-art performance and exhibits remarkable framework generality supported by promising analysis. In the future, we will explore \\update{large-scale pre-training} and more time series analysis tasks.\n\n"
            },
            "section 6": {
                "name": "Ethics Statement",
                "content": "\nOur work only focuses on the time series forecasting problem, so there is no potential ethical risk.\n\n"
            },
            "section 7": {
                "name": "Reproducibility Statement",
                "content": "\nIn the main text, we have strictly formalized the model architecture with equations. All the implementation details are included in the Appendix, including dataset descriptions, metrics, model, and experiment configurations. The code will be made public once the paper is accepted.\n\n"
            },
            "section 8": {
                "name": "Acknowledgments",
                "content": "\nThis work was supported by the National Key Research and Development Plan (2021YFB1715200), the National Natural Science Foundation of China (U2342217 and 62022050), the BNRist Innovation Fund (BNR2024RC01010), Ant Group through CCF-Ant Research Fund, and the National Engineering Research Center for Big Data Software.\n\n\\bibliography{iclr2024_conference}\n\\bibliographystyle{iclr2024_conference}\n\n\\vfill\n\n\\appendix\n\n\\clearpage\n\n"
            },
            "section 9": {
                "name": "Implementation Details",
                "content": " \n\n",
                "subsection 9.1": {
                    "name": "Dataset Descriptions",
                    "content": "\\label{sec:dataset}\n\nWe conduct experiments on \\update{7 real-world datasets} to evaluate the performance of the proposed iTransformer including (1) ETT~\\citep{Informer} contains 7 factors of electricity transformer from July 2016 to July 2018. \\update{There are four subsets where ETTh1 and ETTh2 are recorded every hour, and ETTm1 and ETTm2 are recorded every 15 minutes}. (2) \\update{Exchange~\\citep{Autoformer} collects the panel data of daily exchange rates from 8 countries from 1990 to 2016}. (3) Weather~\\citep{Autoformer} includes 21 meteorological factors collected every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in 2020. (4) ECL~\\citep{Autoformer} records the hourly electricity consumption data of 321 clients. (5) Traffic~\\citep{Autoformer} collects hourly road occupancy rates measured by 862 sensors of San Francisco Bay area freeways from January 2015 to December 2016. (6) Solar-Energy~\\citep{LSTNet} records the solar power production of 137\nPV plants in 2006, which are sampled every 10 minutes. (7) \\update{PEMS contains the public traffic network data in California collected by 5-minute windows. We use the same four public subsets (PEMS03, PEMS04, PEMS07, PEMS08) adopted in SCINet~\\citep{SCINet}}.\n\nApart from the public datasets widely used as forecasting benchmarks, we also collect a set of Market datasets of a real-world application, which records the minute-sampled server load of Alipay online transactions between January 30th, 2023, and April 9th, 2023 with the number of variates varied from 285 to 759. It includes 6 sub-datasets, which are divided according to diverse transaction domains.\n\n\\update{We follow the same data processing and train-validation-test set split protocol used in TimesNet~\\citep{Timesnet}, where the train, validation, and test datasets are strictly divided according to chronological order to make sure there are no data leakage issues. As for the forecasting settings, we fix the length of the lookback series as $96$ in ETT, Weather, ECL, Solar-Energy, PEMS, and Traffic, and the prediction length varies in $\\{96,192,336,720\\}$. For the PEMS dataset, the prediction length varies in $\\{12,24,36,48\\}$, which is the same as  SCINet, the previous state-of-the-art on this dataset. For the Market dataset, the lookback contains the past one day observations with $144$ time points and the forecasting length varies in $\\{12,24,72,144\\}$. The details of datasets are provided in Table \\ref{tab:dataset}}.\n\n\n\n\n"
                },
                "subsection 9.2": {
                    "name": "Implementation Details",
                    "content": " \n\n\\begin{algorithm}[htbp]\n  \\setstretch{1.5}\n  \\caption{iTransformer - Overall Architecture.}\\label{algo:iTransformer}\n  \\begin{algorithmic}[1]\n  \\Require  \n  Input lookback time series $\\mathbf{X}\\in\\mathbb{R}^{T\\times N}$; input Length $T$; predicted length $S$; variates number $N$; token dimension $D$; iTransformer block number $L$.\n\n    \\State $\\mathbf{X}=\\mathbf{X}.\\texttt{transpose}$ \\Comment{$\\mathbf{X}\\in\\mathbb{R}^{N\\times T}$}\n\n    \\State $\\triangleright \\ $ Multi-layer Perceptron works on the last dimension to embed series into variate tokens.\n    \\State $\\mathbf{H}^{0}=\\texttt{MLP}(\\mathbf{X})$ \\Comment{$\\mathbf{H}^{0}\\in\\mathbb{R}^{N\\times D}$}\n\n    \\State $\\textbf{for}\\ l\\ \\textbf{in}\\ \\{1,\\dots,L\\}\\textbf{:}$\\Comment{Run through iTransformer blocks.}\n\n    \\State $\\textbf{\\textcolor{white}{for}}$ $\\triangleright \\ $Self-attention layer is applied on variate tokens.\n    \\State $\\textbf{\\textcolor{white}{for}}\\ \\mathbf{H}^{l-1} = \\texttt{LayerNorm}\\big(\\mathbf{H}^{l-1} + \\texttt{Self-Attn}(\\mathbf{H}^{l-1})\\big)$\n    \\Comment{$\\mathbf{H}^{l-1}\\in\\mathbb{R}^{N\\times D}$}\n    \n     \\State $\\textbf{\\textcolor{white}{for}}$ $\\triangleright \\ $Feed-forward network is utilized for series representations, broadcasting to each token.\n    \\State $\\textbf{\\textcolor{white}{for}}\\ \\mathbf{H}^{l} = \\texttt{LayerNorm}\\big(\\mathbf{H}^{l-1} + \\texttt{Feed-Forward}(\\mathbf{H}^{l-1})\\big)$\n    \\Comment{$\\mathbf{H}^{l}\\in\\mathbb{R}^{N\\times D}$}\n    \\State $\\textbf{\\textcolor{white}{for}}$ $\\triangleright \\ $ LayerNorm is adopted on series representations to reduce variates discrepancies.\n    \\State $\\textbf{End for}$\n\n    \\State $\\mathbf{\\hat{Y}}=\\texttt{MLP}(\\mathbf{H}^{L})$ \\Comment{Project tokens back to predicted series, $\\mathbf{\\hat{Y}}\\in\\mathbb{R}^{N\\times S}$}\n\n    \\State $\\mathbf{\\hat{Y}}=\\mathbf{\\hat{Y}}.\\texttt{transpose}$ \\Comment{$\\mathbf{\\hat{Y}}\\in\\mathbb{R}^{S\\times N}$}\n    \n    \\State $\\textbf{Return}\\ \\mathbf{\\hat{Y}}$ \\Comment{Return the prediction result $\\mathbf{\\hat{Y}}$}\n  \\end{algorithmic} \n\\end{algorithm} \n\nAll the experiments are implemented in PyTorch~\\citep{Pytorch} and conducted on a single NVIDIA P100 16GB GPU. We utilize ADAM~\\citep{Adam} with an initial learning rate in $\\{10^{-3}, 5\\times 10^{-4}, 10^{-4}\\}$ and L2 loss for the model optimization. The batch size is uniformly set to 32 and the number of training epochs is fixed to $10$. We set the number of inverted Transformer blocks in our proposed model $L\\in\\{2, 3, 4\\}$. The dimension of series representations $D$ is set from $\\{256, 512\\}$. \\update{All the compared baseline models that we reproduced are implemented based on the benchmark of TimesNet~\\citep{Timesnet} Repository, which is fairly built on the configurations provided by each model\u2019s original paper or official code}. We provide the pseudo-code of iTransformer in  Algorithm~\\ref{algo:iTransformer}. \\update{We also report the standard deviation of iTransformer performance under five runs with different random seeds in Table~\\ref{tab:std}, which exhibits that the performance of iTransformer is stable}.\n\n\n\n\n"
                }
            },
            "section 10": {
                "name": "Ablation Studies",
                "content": "\n\nTo elaborate on the rational business of Transformer components, we conduct detailed ablations covering replacing components (Replace) and removing components (w/o). Since the average results are listed in Table~\\ref{tab:ablation} due to the paper limit, we provide detailed results and analysis here.\n\n\\update{As shown in Table~\\ref{tab:full_ablation}, among various architectural designs, iTransformer generally exhibits superior performance, which learns multivariate correlations by self-attention and encodes series representations by FFN. Nevertheless, the arrangement of the vanilla Transformer can lead to degenerated performance, indicating the misuse of Transformer components on the time series modality. Based on the relatively poor results of the second (both attentions) and the third (the vanilla Transformer) designs, one of the reasons for that may lie in the attention module over the temporal tokens of the lagged time series, which we elaborate more with the datasets support in Section~\\ref{sec:risks}.}\n\n\\update{It is also notable that applying FFN on both dimensions can also lead to fair performance on datasets with small variate numbers (such as Weather with 21 variates). Still, with the increasing of variate numbers in challenging multivariate forecasting tasks, the importance of capturing multivariate correlations is ever more highlighted. We note that the heterogeneity of variates can be hardly considered by the vanilla Transformer. During embedding, the variates are projected into indistinguishable channels, which ignores the inconsistent physical measurements and thus fails to maintain the independence of variates, let alone capture and utilize the multivariate correlation. Consequently, by incorporating the advanced attention module for the variate correlating, the first (iTransformer) and the fifth (attention on variates) designs perform more effectively in challenging multivariate datasets.}\n\n\\update{In a nutshell, both temporal dependencies and multivariate correlations are of importance for multivariate time series forecasting. The proposed iTransformer employing the self-attention module to disentangle the correlations between variate tokens proves to be more powerful and interpretable than feed-forward networks, thereby further boosting the performance on challenging multivariate datasets and enhancing the model capacity.}\n\n\n\n"
            },
            "section 11": {
                "name": "Hyperparameter Sensitivity",
                "content": "\n\n\\update{We evaluate the hyperparameter sensitivity of iTransformer with respect to the following factors: the learning rate $lr$, the number of Transformer blocks $L$, and the hidden dimension $D$ of variate tokens. The results are shown in Figure~\\ref{fig:hyper}. We find that the learning rate, as the most common influencing factor, should be carefully selected when the number of variates is large (ECL, Traffic). The block number and hidden dimension are not essentially favored to be as large as possible in iTransformer.}\n\n\n\n"
            },
            "section 12": {
                "name": "Model Efficiency",
                "content": "\\label{sec:efficiency_full}\n\n\n\\update{We comprehensively compare the forecasting performance, training speed, and memory footprint of the following models: iTransformer, iTransformer with our efficient training strategy and iTransformer with the efficient flow attention module~\\citep{wu2022flowformer}; linear models: DLinear~\\citep{DLinear} and TiDE~\\citep{das2023long}; Transformers: Transformer~\\citep{Transformer}, PatchTST~\\citep{PatchTST}, and Crossformer~\\citep{Crossformer}. The results are recorded with the official model configuration and the same batch size. In Figure~\\ref{fig:eff}, we compare the efficiency under two representative datasets (21 variates in Weather and 862 in Traffic) with $96$ time steps for lookback.}\n\n\\update{In a nutshell, the efficiency of iTransformer exceeds other Transformers in datasets with a relatively small number of variates (Weather). In datasets with numerous variates (Traffic), the memory footprints are basically the same as Transformers variates, but iTransformer can be trained faster. Based on the complexity of $\\mathcal{O}(N^2)$ of the attention module, where $N$ is the number of tokens, Transformer surpasses iTransformer on efficiency in this case because of $N=96$ for the temporal token and $N=862$ for the variate token. Meanwhile, iTransformer achieves better performance on numerous variates, since the multivariate correlations can be explicitly utilized. By adopting a linear-complexity attention~\\citep{wu2022flowformer} or the proposed efficient training strategy as mentioned in Figure~\\ref{fig:efficient_strategy} (trained on 20\\% variates and forecast all variates), iTransformer can enjoy a comparable speed and memory footprint with linear models. Also, the two strategies can be adopted together.}\n\n"
            },
            "section 13": {
                "name": "Showcases",
                "content": "\n\n",
                "subsection 13.1": {
                    "name": "Visualization of Multivariate Correlations",
                    "content": "\\label{sec:app_vis}\n\\update{By using the attention mechanism on variate tokens, the resulting learned map becomes more interpretable. To present an intuitive understanding of the multivariate correlations, we provide three randomly chosen case visualizations of the time series from Solar-Energy in Figure~\\ref{fig:multivariate_corr}. We provide the Pearson Correlation coefficients of each variate of the raw series by the following equation:}\n\n\\begin{equation*}\n    \\rho_{xy} = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i (x_i - \\bar{x})^2} \\sqrt{\\sum_i (y_i - \\bar{y})^2}},\n\\end{equation*}\n\n\\update{where $x_i, y_i\\in\\mathbb{R}$ run through all time points of the paired variates to be correlated. All the cases have distinct multivariate correlations in the lookback and forecast window because the dataset exhibits obvious seasonal changes in the daytime and night. On the second row of each case, we provide the learned pre-Softmax maps of the self-attention module in both the first and the last layers. As we observe in the shallow attention layer (left), we find that the learned map is similar to the correlations of the raw lookback series. As we go deeper into the layers (right), the learned map gradually becomes more similar to the correlations of the future series to be predicted. This demonstrates that the inverted operation allows for interpretable attention in correlating, and that encoding of the past and decoding for the future are conducted through series representations during layer stacking.}\n\n\n\n\n\\update{We present another interesting observation in Figure~\\ref{fig:multivariate_corr_grouped} to show that the attention module of iTransformer has enhanced interpretability. We provide randomly chosen multivariate time series from Market. In this dataset, each variate represents the monitored values of a service interface of a kind, and the service can be further grouped into refined application categories. We divide these variates into corresponding applications (as listed on the top bar \\emph{App}), such that adjacent variates belong to the same application and we reveal the application index by the top bar.}\n\n\\update{We visualize the time series of the variates and plot the learned multivariate correlations with the marks of specific correlations between variates. On the one hand, we observe clear partitioning in the multivariate correlations map, indicating the grouping of variates. On the one hand, the marked correlation values can reflect the correlation of the raw series, where the similarity of variates from the same application becomes closer than the pairs from the different groups. Therefore, highly correlated variate will be leveraged for the next interaction and thus benefit for multivariate forecasting.}\n\n\n\n"
                },
                "subsection 13.2": {
                    "name": "Visualization of Prediction Results",
                    "content": "\nTo provide a clear comparison among different models, we list supplementary prediction showcases of four representative datasets in Figures~\\ref{fig:showcase1}-~\\ref{fig:showcase4}, which are given by the following models: iTransfomrer, PatchTST~\\citep{PatchTST}, DLinear~\\citep{DLinear}, Crossformer~\\citep{Crossformer}, Autoformer~\\citep{Autoformer}, Transformer~\\citep{Transformer}. Among the various models, iTransformer predicts the most precise future series variations and exhibits superior performance.\n\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 13.3": {
                    "name": "Risks of Embedding Multivariate Points of A Timestamp",
                    "content": "\\label{sec:risks}\n\n\\update{As aforementioned, the embedding approach of the previous Transformer fuses multiple variates representing potentially delayed events and distinct physical measurements, which may fail to learn variate-centric representations and result in meaningless attention maps. We provide the visualization case of Traffic~\\citep{SCINet}, which is collected from sensors on Los Angeles city roads in different areas. As shown in Figure~\\ref{fig:showcase_traffic}, we can observe a strong correlation between the multivariate time series of the dataset, while they also exhibit obvious phase offset, which is due to the systematical time lags in the road occupancy that each series describes. Since the sensors are installed in different areas of the highway, an event (such as a traffic jam) can affect road occupancy with different delays.}\n\n\n\n\\update{Besides, we observe the significantly declined performance on the second and third designs of Traffic in Table~\\ref{tab:full_ablation}, which apply attention to temporal tokens. In our opinion, capturing temporal dependencies by attention is not a big problem. But it is based on the fact that the time points of each timestamp essentially reflect the same event to enclose a semantic representation. Since there are inherent delays between the time points, the performance can degrade a lot because of the meaningless attention map, unless the model has an enlarged respective field to learn about the decay or causal process.}\n\n\\update{Other risks can be aroused from the distinct variate measurements, such as organizing together different meteorological indicators (the temperature and rainfall) in the Weather dataset~\\citep{Autoformer}, and the quantity and proportion of the same observation in ILI~\\citep{Timesnet}. Given these potential risks, iTransformer proposes a new paradigm that embeds the whole series as the variate token, which can be more robust to extensive real-world scenarios, such as delayed events, inconsistent measurements, irregular (unevenly spaced) time series, systematical delay of monitors, and the time interval of generating and recording different time series. \n}\n\n\n\n"
                }
            },
            "section 14": {
                "name": "Full Results",
                "content": "\n",
                "subsection 14.1": {
                    "name": "Full Promotion Results",
                    "content": "\n\\update{We compare the performance of Transformer and iTransformer on all datasets in Table~\\ref{tab:trans_forecasting_promotion}. Consistent and great promotions can be achieved, indicating that the attention and feed-forward network on the inverted dimensions greatly empower Transformers in multivariate time series forecasting, leaving an instructive direction to build up the foundation model of extensive time series data.}\n\n\n\n"
                },
                "subsection 14.2": {
                    "name": "Full Framework Generality Results",
                    "content": "\\label{sec:full_prompt_results}\nWe apply the proposed inverting framework to Transformer and its variants: Transformer~\\citep{Transformer}, Reformer~\\citep{kitaev2020reformer}, Informer~\\citep{Informer}, Flowformer~\\citep{wu2022flowformer}, Flashformer~\\citep{dao2022flashattention}. The averaged results are shown in Table~\\ref{tab:forecasting_promotion} due to the limited pages. We provide the supplementary forecasting results in Table~\\ref{tab:full_forecasting_promotion}. The results demonstrate that our iTransformers framework can consistently promote these Transformer variants, and take advantage of the booming efficient attention mechanisms.\n\n\n\n"
                },
                "subsection 14.3": {
                    "name": "Full Results of Variate Generalization",
                    "content": "\\label{sec:full_gen_results}\n\\update{\n We divide the variates of each dataset into five folders, train models with only 20\\% of variates of one folder, and directly forecast all variates without fine-tuning. We adopt two strategies for Transformers to generalize on unseen variates: (1) \\textbf{CI-Transformers}~\\citep{PatchTST}: Channel Independence regards each variate of time series as independent channels, and trains with a shared backbone. During inference, the model predicts variates one by one, but the procedure can be time-consuming. (2) \\textbf{iTransformers}: with the flexibility of the attention mechanism that the number of input tokens can be dynamically changeable, the amount of variates as tokens is no longer restricted and thus feasible to vary from training and inference, and can even allow the model to be trained on arbitrary variates.}\n\n\\update{As shown in Table~\\ref{fig:full_variate_generalization}, iTransformers can be naturally trained with 20\\% variates and accomplish forecast on all variates with the ability to learn transferable representations.}\n\n\n\n\n"
                },
                "subsection 14.4": {
                    "name": "Full Forecasting Results",
                    "content": "\\label{sec:full_results}\nThe full multivariate forecasting results are provided in the following section due to the space limitation of the main text. We extensively evaluate competitive counterparts on challenging forecasting tasks. \\update{Table~\\ref{tab:full_baseline_results_pems} contains the forecasting results on the four public subsets from PEMS~\\citep{SCINet}. Table~\\ref{tab:full_baseline_results} contains the detailed results of all prediction lengths of the nine well-acknowledged forecasting benchmarks}. And Table~\\ref{tab:full_market_results} records the Market results for Alipay server load forecasting. The proposed model achieves comprehensive state-of-the-art in real-world forecasting applications.\n\n\n\n\n\n\n\n\n\n"
                }
            },
            "section 15": {
                "name": "Discussions and Further Improvement",
                "content": "\n\n",
                "subsection 15.1": {
                    "name": "Discussions on Architecture-free Methods",
                    "content": "\n\n\\update{Channel Independence (CI)~\\citep{PatchTST}, regarding variates of time series independently and adopting the shared backbone, have gained increasing popularity in forecasting with performance promotions as an architecture-free method. Recent works~\\citep{han2023capacity, li2023revisiting} found that while Channel Dependence (CD) benefits from a higher capacity ideally, CI can greatly boost the performance because of sample scarcity, since most of the current forecasting benchmarks are not large enough. We think it is essential to make variates independent, especially when there are potential risks of embedding as mentioned in Appendix~\\ref{sec:risks}, inducing the ideal model capacity of CD limited by the excessively localized receptive field. However, the essence of CI, regarding multivariate time series univariately, can lead to time-consuming training and inference and become an obstacle to scalability. Still, multivariate correlations can not be explicitly utilized. Perpendicular to these works, iTransformer repurposes an architecture with the native Transformer modules to tackle the issues.}\n\n\\update{RevIN~\\citep{kim2021reversible} and Stationarization~\\citep{Stationary} have been widely applied for the distribution shift (non-stationarity) as architecture-free techniques. These works strive to reveal the temporal dependency better. This is accomplished by layer normalization in iTransformer and still leaves further improvement for us to tackle the distribution shift.}\n\n"
                },
                "subsection 15.2": {
                    "name": "Discussions on Linear Forecasters",
                    "content": "\n\\update{Linear forecasters have natural advantages in modeling temporal dependencies. The dense weighting~\\citep{DLinear, li2023revisiting} can reveal measurement-free relationships among the time points of the same variate. More advanced linear forecasters focus on structural point-wise modeling~\\citep{Nbeats, SCINet, liu2023koopa}. By contrast, iTransformer is particularly good at forecasting high-dimensional time series (numerous variates with complicated correlations, which can be common and realistic for practitioners in real forecasting applications). For variate correlating, the embedding keeps the variate independent and the attention module can be applied to dig it out. Under univariate scenarios, iTransformer actually becomes a stackable linear forecaster (attention degradation), which leaves further enhancement to exploit the temporal dependency better.}\n\n"
                },
                "subsection 15.3": {
                    "name": "Discussions on Transformers",
                    "content": "\n\\update{We emphasize that iTransformer actually proposes a new perspective to think about the multivariate time series modality, specifically, how to consider the variates and the tokenization. We list several representatives in Figure~\\ref{fig:tokenization}. Transformer treats time series as the natural language but the time-aligned embedding may bring about risks in multi-dimensional series. The problem can be alleviated by expanding the receptive field. Although it is believed that Patching~\\citep{Crossformer, PatchTST} can be more fine-grained, it also brings higher computational complexity and the potential interaction noise between time-unaligned patches. If the current embedding (implemented by MLP) is enhanced with more inductive bias (such as TCN), it may handle more robust cases with the variate token paradigm and enjoy the flexibility of Transformer with changeable numbers of tokens.}\n\n\\update{We believe the capability and scalability of Transformer have stood the test by extensive fields, but there is still improvement room to elaborately design components based on the inverted architecture, such as efficient attention for multivariate correlation, structural temporal dependency modeling under distribution shift, fine-grained variate tokenization and well-designed embedding mechanisms.}\n\n\n\n\n\n"
                }
            }
        },
        "tables": {
            "tab:forecasting_promotion": "\\begin{table}[htbp]\n  \\caption{Performance promotion obtained by our inverted framework. Flashformer means Transformer equipped with hardware-accelerated FlashAttention~\\citep{dao2022flashattention}. We report the average performance and the relative MSE reduction (Promotion). Full results can be found in Appendix~\\ref{sec:full_prompt_results}.}\\label{tab:forecasting_promotion}\n  \\vskip 0.05in\n  \\centering\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{3.8pt}\n  \\begin{tabular}{c|c|cc|cc|cc|cc|cc}\n    \\toprule\n    \\multicolumn{2}{c|}{\\multirow{2}{*}{{Models}}} & \n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Transformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Reformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Informer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Flowformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Flashformer}}} \\\\\n    \\multicolumn{2}{c|}{}\n    &\\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{Transformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{kitaev2020reformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{Informer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{wu2022flowformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{dao2022flashattention}}} \\\\\n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10} \\cmidrule(lr){11-12}  \n    \\multicolumn{2}{c|}{Metric}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE} & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}\\\\\n    \\toprule\n    \\multirow{3}{*}{\\scalebox{1.0}{ECL}} & \\scalebox{1.0}{Original} & \\scalebox{1.0}{0.277} & \\scalebox{1.0}{0.372}& \\scalebox{1.0}{0.338} & \\scalebox{1.0}{0.422} & \\scalebox{1.0}{0.311} & \\scalebox{1.0}{0.397}  & \\scalebox{1.0}{0.267} & \\scalebox{1.0}{0.359} & \\scalebox{1.0}{0.285} & \\scalebox{1.0}{0.377}\\\\\n    &\\scalebox{1.0}{\\textbf{+Inverted}} & \\textbf{\\scalebox{1.0}{0.178}} & \\textbf{\\scalebox{1.0}{0.270}} & \\textbf{\\scalebox{1.0}{0.208}} & \\textbf{\\scalebox{1.0}{0.301}} & \\textbf{\\scalebox{1.0}{0.216}} & \\textbf{\\scalebox{1.0}{0.311}} & \\textbf{\\scalebox{1.0}{0.210}} & \\textbf{\\scalebox{1.0}{0.293}} & \\textbf{\\scalebox{1.0}{0.206}} & \\textbf{\\scalebox{1.0}{0.291}}\\\\\n    \\cmidrule(lr){2-12}\n    &\\scalebox{1.0}{Promotion} & \\scalebox{1.0}{35.6\\%} & 27.4\\% & \\scalebox{1.0}{38.4\\%} & \\scalebox{1.0}{28.7\\%}  & \\scalebox{1.0}{30.5\\%} & \\scalebox{1.0}{21.6\\%} & \\scalebox{1.0}{21.3\\%} & \\scalebox{1.0}{18.6\\%} & \\scalebox{1.0}{27.8\\%} & \\scalebox{1.0}{22.9\\%}\\\\\n    \\midrule\n    \\multirow{3}{*}{\\scalebox{1.0}{Traffic}} & \\scalebox{1.0}{Original} & \\scalebox{1.0}{0.665} & \\scalebox{1.0}{0.363} & \\scalebox{1.0}{0.741} & \\scalebox{1.0}{0.422} & \\scalebox{1.0}{0.764} & \\scalebox{1.0}{0.416} & \\scalebox{1.0}{0.750} & \\scalebox{1.0}{0.421} & \\scalebox{1.0}{0.658} & \\scalebox{1.0}{0.356}\\\\\n    &\\scalebox{1.0}{\\textbf{+Inverted}} & \\textbf{\\scalebox{1.0}{0.428}} & \\textbf{\\scalebox{1.0}{0.282}} & \\textbf{\\scalebox{1.0}{0.647}} & \\textbf{\\scalebox{1.0}{0.370}} & \\textbf{\\scalebox{1.0}{0.662}} & \\textbf{\\scalebox{1.0}{0.380}} & \\textbf{\\scalebox{1.0}{0.524}} & \\textbf{\\scalebox{1.0}{0.355}} & \\textbf{\\scalebox{1.0}{0.492}} & \\textbf{\\scalebox{1.0}{0.333}}\\\\\n    \\cmidrule(lr){2-12}\n    &\\scalebox{1.0}{Promotion} & 35.6\\% & 22.3\\% & \\scalebox{1.0}{12.7\\%} & \\scalebox{1.0}{12.3\\%} & \\scalebox{1.0}{13.3\\%} & \\scalebox{1.0}{8.6\\%} & \\scalebox{1.0}{30.1\\%} & \\scalebox{1.0}{15.6\\%} & \\scalebox{1.0}{25.2\\%} & \\scalebox{1.0}{6.4\\%}\\\\\n    \\midrule\n    \\multirow{3}{*}{\\scalebox{1.0}{Weather}} & \\scalebox{1.0}{Original} & \\scalebox{1.0}{0.657} & \\scalebox{1.0}{0.572} & \\scalebox{1.0}{0.803} & \\scalebox{1.0}{0.656} & \\scalebox{1.0}{0.634} & \\scalebox{1.0}{0.548} & \\scalebox{1.0}{0.286} & \\scalebox{1.0}{0.308} & \\scalebox{1.0}{0.659} & \\scalebox{1.0}{0.574}\\\\\n    &\\scalebox{1.0}{\\textbf{+Inverted}} & \\textbf{\\scalebox{1.0}{0.258}} & \\textbf{\\scalebox{1.0}{0.279}} & \\textbf{\\scalebox{1.0}{0.248}} & \\textbf{\\scalebox{1.0}{0.292}} & \\textbf{\\scalebox{1.0}{0.271}} & \\textbf{\\scalebox{1.0}{0.330}} & \\textbf{\\scalebox{1.0}{0.266}} & \\textbf{\\scalebox{1.0}{0.285}} & \\textbf{\\scalebox{1.0}{0.262}} & \\textbf{\\scalebox{1.0}{0.282}}\\\\\n    \\cmidrule(lr){2-12}\n    &\\scalebox{1.0}{Promotion} & 60.2\\% &  50.8\\% & \\scalebox{1.0}{69.2\\%} & \\scalebox{1.0}{55.5\\%} & \\scalebox{1.0}{57.3\\%} & \\scalebox{1.0}{39.8\\%} & \\scalebox{1.0}{7.2\\%} & \\scalebox{1.0}{7.7\\%} & \\scalebox{1.0}{60.2\\%} & \\scalebox{1.0}{50.8\\%}\\\\\n    \\bottomrule\n  \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n\\end{table}",
            "tab:ablation": "\\begin{table}[htbp]\n\\caption{Ablations on iTransformer. We replace different components on the respective dimension to learn multivariate correlations (Variate) and series representations (Temporal), in addition to component removal. The average results of all predicted lengths are listed here.}\n\\label{tab:ablation}\n\\vspace{5pt}\n\\centering\n\\begin{small}\n    \\renewcommand{\\multirowsetup}{\\centering}\n    \\setlength{\\tabcolsep}{5.3pt}\n    \\begin{tabular}{c|c|c|cc|cc|cc|cc}\n    \\toprule\n    \\multirow{2}{*}{Design} &\\multirow{2}{*}{Variate} & \\multirow{2}{*}{Temporal} & \\multicolumn{2}{c|}{ECL} & \\multicolumn{2}{c|}{Traffic} & \\multicolumn{2}{c|}{Weather} & \\multicolumn{2}{c}{Solar-Energy}\\\\\n    \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}  \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \n    & & & MSE & MAE & MSE & MAE  & MSE & MAE & MSE & MAE \\\\\n    \\midrule\n     \\textbf{iTransformer} & \\textbf{Attention} & \\textbf{FFN} & \\textbf{0.178} & \\textbf{0.270} & \\textbf{0.428} & \\textbf{0.282} & 0.258 & 0.278 & \\textbf{0.233} & \\textbf{0.262}\\\\\n     \\midrule\n     \\multirow{3}{*}{Replace}& Attention & Attention & 0.193 & 0.293 & 0.913 & 0.500 & 0.255 & 0.280 & 0.261 & 0.291\\\\\n     & FFN & Attention & 0.202 & 0.300 & 0.863 & 0.499 & 0.258 & 0.283 & 0.285 & 0.317 \\\\\n     & FFN & FFN & 0.182 & 0.287 & 0.599 & 0.348 & \\textbf{0.248} & \\textbf{0.274} & 0.269 & 0.287\\\\\n    \\midrule\n    \\multirow{2}{*}{w/o} & Attention & w/o & 0.189 & 0.278 & 0.456 & 0.306 & 0.261 & 0.281 & 0.258 & 0.289\\\\\n     & w/o & FFN & 0.193 & 0.276 & 0.461 & 0.294 & 0.265 & 0.283 & 0.261 & 0.283 \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{small}\n\\end{table}",
            "tab:dataset": "\\begin{table}[thbp]\n  \\vspace{0pt}\n  \\caption{\\update{Detailed dataset descriptions}. \\emph{Dim} denotes the variate number of each dataset. \\emph{Dataset Size} denotes the total number of time points in (Train, Validation, Test) split respectively. \\emph{Prediction Length} denotes the future time points to be predicted and four prediction settings are included in each dataset. \\emph{Frequency} denotes the sampling interval of time points.}\\label{tab:dataset}\n  \\vskip 0.05in\n  \\centering\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{6.5pt}\n  \\begin{tabular}{l|c|c|c|c|c}\n    \\toprule\n    Dataset & Dim & Prediction Length & Dataset Size & Frequency& Information \\\\\n    \\toprule\n     \\update{ETTh1, ETTh2} & 7 & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (8545, 2881, 2881) & Hourly & Electricity\\\\\n     \\midrule\n     \\update{ETTm1, ETTm2} & 7 & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (34465, 11521, 11521) & 15min & Electricity\\\\\n    \\midrule\n    \\update{Exchange} & 8 & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (5120, 665, 1422) & Daily & Economy \\\\\n    \\midrule\n    Weather & 21 & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (36792, 5271, 10540) & 10min & Weather\\\\\n    \\midrule\n    ECL & 321 & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (18317, 2633, 5261) & Hourly & Electricity \\\\\n    \\midrule\n    Traffic & 862 & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (12185, 1757, 3509) & Hourly & Transportation \\\\\n    \\midrule\n    Solar-Energy & 137  & \\scalebox{0.8}{\\{96, 192, 336, 720\\}} & (36601, 5161, 10417) & 10min & Energy \\\\\n    \\midrule\n    \\update{PEMS03} & 358 & \\scalebox{0.8}{\\{12, 24, 48, 96\\}} & (15617, 5135, 5135) & 5min & Transportation\\\\\n    \\midrule\n    \\update{PEMS04} & 307 & \\scalebox{0.8}{\\{12, 24, 48, 96\\}} & (10172, 3375, 3375) & 5min & Transportation\\\\\n    \\midrule\n    \\update{PEMS07} & 883 & \\scalebox{0.8}{\\{12, 24, 48, 96\\}} & (16911, 5622, 5622) & 5min & Transportation\\\\\n    \\midrule\n    \\update{PEMS08} & 170 & \\scalebox{0.8}{\\{12, 24, 48, 96\\}} & (10690, 3548, 3548) & 5min & Transportation\\\\\n    \\midrule\n    Market-Merchant & 285 & \\scalebox{0.8}{\\{12, 24, 72, 144\\}} & (7045, 1429, 1429) & 10min & Transaction\\\\\n    \\midrule\n    Market-Wealth & 485 & \\scalebox{0.8}{\\{12, 24, 72, 144\\}}& (7045, 1429, 1429) & 10min & Transaction\\\\\n    \\midrule\n    Market-Finance & 405 & \\scalebox{0.8}{\\{12, 24, 72, 144\\}}& (7045, 1429, 1429) & 10min & Transaction\\\\\n    \\midrule\n    Market-Terminal & 307 & \\scalebox{0.8}{\\{12, 24, 72, 144\\}}& (7045, 1429, 1429) & 10min & Transaction\\\\\n    \\midrule\n    Market-Payment & 759 & \\scalebox{0.8}{\\{12, 24, 72, 144\\}}& (7045, 1429, 1429) & 10min & Transaction\\\\\n    \\midrule\n    Market-Customer & 395 & \\scalebox{0.8}{\\{12, 24, 72, 144\\}}& (7045, 1429, 1429) & 10min & Transaction\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n  \\vspace{-5pt}\n\\end{table}",
            "tab:std": "\\begin{table}[htbp]\n  \\caption{\\update{Robustness of iTransformer performance. The results are obtained from five random seeds}.}\n  \\vspace{3pt}\n  \\label{tab:std}\n  \\centering\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{7pt}\n  \\begin{tabular}{c|cc|cc|ccc}\n    \\toprule\n    Dataset & \\multicolumn{2}{c}{ECL} & \\multicolumn{2}{c}{ETTh2} & \\multicolumn{2}{c}{Exchange}   \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\\cmidrule(lr){6-7}\n    Horizon & MSE & MAE & MSE & MAE & MSE & MAE  \\\\\n    \\toprule\n    $96$ & 0.148\\scalebox{0.9}{$\\pm$0.000} & 0.240\\scalebox{0.9}{$\\pm$0.000} & 0.297\\scalebox{0.9}{$\\pm$0.002} & 0.349\\scalebox{0.9}{$\\pm$0.001} & 0.088\\scalebox{0.9}{$\\pm$0.001} & 0.209\\scalebox{0.9}{$\\pm$0.001}  \\\\\n    $192$ & 0.162\\scalebox{0.9}{$\\pm$0.002} & 0.253\\scalebox{0.9}{$\\pm$0.002} & 0.380\\scalebox{0.9}{$\\pm$0.001} & 0.400\\scalebox{0.9}{$\\pm$0.001} & 0.181\\scalebox{0.9}{$\\pm$0.001} & 0.304\\scalebox{0.9}{$\\pm$0.001}    \\\\\n    $336$ & 0.178\\scalebox{0.9}{$\\pm$0.000} & 0.269\\scalebox{0.9}{$\\pm$0.001} & 0.428\\scalebox{0.9}{$\\pm$0.002} & 0.432\\scalebox{0.9}{$\\pm$0.001} & 0.334\\scalebox{0.9}{$\\pm$0.001} & 0.419\\scalebox{0.9}{$\\pm$0.001}    \\\\\n    $720$ & 0.225\\scalebox{0.9}{$\\pm$0.006} & 0.317\\scalebox{0.9}{$\\pm$0.007} & 0.427\\scalebox{0.9}{$\\pm$0.004} & 0.445\\scalebox{0.9}{$\\pm$0.002} & 0.829\\scalebox{0.9}{$\\pm$0.012} & 0.691\\scalebox{0.9}{$\\pm$0.005}    \\\\\n    \\midrule\n    Dataset & \\multicolumn{2}{c}{Solar-Energy} & \\multicolumn{2}{c}{Traffic}   & \\multicolumn{2}{c}{Weather}\\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\\cmidrule(lr){6-7}\n    Horizon & MSE & MAE & MSE & MAE & MSE & MAE  \\\\\n    $96$ & 0.203\\scalebox{0.9}{$\\pm$0.002} & 0.237\\scalebox{0.9}{$\\pm$0.002} & 0.395\\scalebox{0.9}{$\\pm$0.001} & 0.268\\scalebox{0.9}{$\\pm$0.001} & 0.174\\scalebox{0.9}{$\\pm$0.000} & 0.214\\scalebox{0.9}{$\\pm$0.000}    \\\\\n    $192$ & 0.233\\scalebox{0.9}{$\\pm$0.002} & 0.261\\scalebox{0.9}{$\\pm$0.001} & 0.417\\scalebox{0.9}{$\\pm$0.002} & 0.276\\scalebox{0.9}{$\\pm$0.001} & 0.221\\scalebox{0.9}{$\\pm$0.002} & 0.254\\scalebox{0.9}{$\\pm$0.001}    \\\\\n    $336$ & 0.248\\scalebox{0.9}{$\\pm$0.000} & 0.273\\scalebox{0.9}{$\\pm$0.000} & 0.433\\scalebox{0.9}{$\\pm$0.004} & 0.283\\scalebox{0.9}{$\\pm$0.000} & 0.278\\scalebox{0.9}{$\\pm$0.002} & 0.296\\scalebox{0.9}{$\\pm$0.001}    \\\\\n    $720$ & 0.249\\scalebox{0.9}{$\\pm$0.001} & 0.275\\scalebox{0.9}{$\\pm$0.000} & 0.467\\scalebox{0.9}{$\\pm$0.003} & 0.302\\scalebox{0.9}{$\\pm$0.000} & 0.358\\scalebox{0.9}{$\\pm$0.000} & 0.349\\scalebox{0.9}{$\\pm$0.000}    \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\end{small}\n  \\end{threeparttable}\n\\end{table}",
            "tab:full_ablation": "\\begin{table}[htbp]\n\\caption{Full results of the ablation on iTransformer. We apply different components on the respective dimension to learn multivariate correlations (Variate) and series representations (Temporal), in addition to removing the specific component of Transformer.}\n\\vspace{3pt}\n\\label{tab:full_ablation}\n\\centering\n%\\resizebox{1\\columnwidth}{!}{\n\\begin{small}\n    \\renewcommand{\\multirowsetup}{\\centering}\n    \\setlength{\\tabcolsep}{3.0pt}\n    \\begin{tabular}{c|c|c|c|cc|cc|cc|cc}\n    \\toprule\n    \\multirow{2}{*}{Design} &\\multirow{2}{*}{Variate} & \\multirow{2}{*}{Temporal} & Prediction & \\multicolumn{2}{c|}{ECL} & \\multicolumn{2}{c|}{Traffic} & \\multicolumn{2}{c|}{Weather} & \\multicolumn{2}{c}{Solar-Energy}\\\\\n    \\cmidrule(lr){5-6} \\cmidrule(lr){7-8}  \\cmidrule(lr){9-10} \\cmidrule(lr){11-12} \n    & & &Lengths & MSE & MAE & MSE & MAE  & MSE & MAE & MSE & MAE \\\\\n    \\midrule\n    \\multirow{5}{*}{\\textbf{iTransformer}}& \\multirow{5}{*}{\\textbf{Attention}} & \\multirow{5}{*}{\\textbf{FFN}} & 96 & 0.148 & 0.240 & 0.395 & 0.268 & 0.174 & 0.214 & 0.203 & 0.237\\\\\n    & & & 192 & 0.162 & 0.253 & 0.417 & 0.276 & 0.221 & 0.254 & 0.233 & 0.261\\\\\n    & & & 336 & 0.178 & 0.269 & 0.433 & 0.283 & 0.278 & 0.296 & 0.248 & 0.273\\\\\n    & & & 720 & 0.225 & 0.317 & 0.467 & 0.302 & 0.358 & 0.349 & 0.249 & 0.275\\\\\n    \\cmidrule(lr){4-12}\n     & & & Avg & \\textbf{0.178} & \\textbf{0.270} & \\textbf{0.428} & \\textbf{0.282} & 0.258 & 0.279 & \\textbf{0.233} & \\textbf{0.262}\\\\\n     \\midrule\n     \\multirow{15}{*}{Replace}& \\multirow{5}{*}{Attention} & \\multirow{5}{*}{Attention} & 96 & 0.161 & 0.263 & 1.021 & 0.581 & 0.168 & 0.213 & 0.227 & 0.270\\\\\n     & & & 192 & 0.180 & 0.280 & 0.834 & 0.447 & 0.217 & 0.256 & 0.255 & 0.292\\\\\n     & & & 336 & 0.194 & 0.296 & 0.906 & 0.493 & 0.277 & 0.299 & 0.279 & 0.301\\\\\n     & & & 720 & 0.238 & 0.331 & 0.892 & 0.477 & 0.356 & 0.351 & 0.283 & 0.300\\\\\n     \\cmidrule(lr){4-12}\n     & & & Avg & 0.193 & 0.293 & 0.913 & 0.500 & 0.255 & 0.280 & 0.261 & 0.291\\\\\n     \\cmidrule(lr){2-12}\n     & \\multirow{5}{*}{FFN} & \\multirow{5}{*}{Attention} & 96 & 0.169 & 0.270 & 0.907 & 0.540 & 0.176 & 0.221 & 0.247 & 0.299\\\\\n     & & & 192 & 0.189 & 0.292 & 0.839 & 0.489 & 0.224 & 0.261 & 0.275 & 0.305 \\\\\n     & & & 336 & 0.204 & 0.304 & 0.248 & 0.364 & 0.279 & 0.301 & 0.317 & 0.337\\\\\n     & & & 720 & 0.245 & 0.335 & 1.059 & 0.606 & 0.354 & 0.347 & 0.301 & 0.329\\\\\n     \\cmidrule(lr){4-12}\n     & & & Avg & 0.202 & 0.300 & 0.863 & 0.499 & 0.258 & 0.283 & 0.285 & 0.317 \\\\\n     \\cmidrule(lr){2-12}\n     & \\multirow{5}{*}{FFN} & \\multirow{5}{*}{FFN} & 96 & 0.159 & 0.261 & 0.606 & 0.342 & 0.162 & 0.207 & 0.237 & 0.277\\\\\n     & & & 192 & 0.171 & 0.271 & 0.559 & 0.342 & 0.211 & 0.252 & 0.273 & 0.293\\\\\n     & & & 336 & 0.187 & 0.287 & 0.569 & 0.348 & 0.270 & 0.293 & 0.284 & 0.287\\\\\n     & & & 720 & 0.211 & 0.307 & 0.664 & 0.359 & 0.349 & 0.345 & 0.284 & 0.289\\\\\n     \\cmidrule(lr){4-12}\n     & & & Avg & 0.182 & 0.287 & 0.599 & 0.348 & \\textbf{0.248} & \\textbf{0.274} & 0.269 & 0.287\\\\\n    \\midrule\n    \\multirow{10}{*}{w/o} & \\multirow{5}{*}{Attention} & \\multirow{5}{*}{w/o} & 96 & 0.163 & 0.254 & 0.427 & 0.296 & 0.177 & 0.219 & 0.226 & 0.266\\\\\n    & & & 192 & 0.174 & 0.263 & 0.446 & 0.300 & 0.226 & 0.259 & 0.255 & 0.288\\\\\n     & & & 336 & 0.191 & 0.280 & 0.459 & 0.306 & 0.281 & 0.298 & 0.275 & 0.301\\\\\n     & & & 720 & 0.228 & 0.315 & 0.492 & 0.324 & 0.359 & 0.249 & 0.275 & 0.301\\\\\n     \\cmidrule(lr){4-12}\n     & & & Avg & 0.189 & 0.278 & 0.456 & 0.306 & 0.261 & 0.281 & 0.258 & 0.289\\\\\n     \\cmidrule(lr){2-12}\n     \n     & \\multirow{5}{*}{w/o} & \\multirow{5}{*}{FFN} & 96 & 0.169 & 0.253 & 0.437 & 0.283 & 0.183 & 0.220 & 0.228 & 0.263\\\\\n     & & & 192 & 0.177 & 0.261 & 0.449 & 0.287 & 0.231 & 0.262 & 0.261 & 0.283\\\\\n     & & & 336 & 0.194 & 0.278 & 0.464 & 0.294 & 0.285 & 0.300 & 0.279 & 0.294\\\\\n     & & & 720 & 0.233 & 0.311 & 0.496 & 0.313 & 0.362 & 0.350 & 0.276 & 0.291\\\\\n     \\cmidrule(lr){4-12}\n     & & & Avg & 0.193 & 0.276 & 0.461 & 0.294 & 0.265 & 0.283 & 0.261 & 0.283 \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{small}\n%}\n\\end{table}",
            "tab:trans_forecasting_promotion": "\\begin{table}[htbp]\n  \\caption{\\update{Full performance comparison between the vanilla Transformer and the proposed iTransformer. The results are averaged from all four prediction lengths.}}\\label{tab:trans_forecasting_promotion}\n  \\vskip 0.05in\n  \\centering\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{2.7pt}\n  \\begin{tabular}{c|cc|cc|cc|cc|cc|cc}\n    \\toprule\n    \\multirow{2}{*}{{Datasets}} & \n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{ETT}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{ECL}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{PEMS}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Solar-Energy}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Traffic}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Weather}}} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11}  \\cmidrule(lr){12-13}  \n    Metric & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE} & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE} & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE} \\\\\n    \\toprule\n    \\scalebox{1.0}{Transformer} & 2.750 & 1.375 & 0.277 & 0.372 & 0.157 & 0.263 & 0.256 & 0.276 & 0.665 & 0.363 & 0.657 & 0.572\\\\\n    \\scalebox{1.0}{\\textbf{iTransformer}} & \\textbf{0.383} & \\textbf{0.407} & \\textbf{0.178} & \\textbf{0.270} & \\textbf{0.113} & \\textbf{0.221} & \\textbf{0.233} & \\textbf{0.262} & \\textbf{0.428} & \\textbf{0.282} & \\textbf{0.258} & \\textbf{0.279}\\\\\n    \\cmidrule(lr){1-13}\n    \\scalebox{1.0}{Promotion} & 86.1\\% & 70.4\\% & 35.6\\% & 27.4\\% & 28.0\\% & 16.0\\% & 9.0\\% & 5.1\\% & 35.6\\% & 22.3\\% & 60.2\\% & 50.8\\% \\\\\n    \\bottomrule\n  \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n\\end{table}",
            "tab:full_forecasting_promotion": "\\begin{table}[htbp]\n\\vspace{-5pt}\n  \\caption{Full results of Transformers with our inverted framework. Flashformer means Transformer equipped with the hardware-accelerated FlashAttention~\\citep{dao2022flashattention}.}\\label{tab:full_forecasting_promotion}\n  \\vskip 0.05in\n  \\centering\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{4.1pt}\n  \\begin{tabular}{c|c|c|cc|cc|cc|cc|cc}\n    \\toprule\n    \\multicolumn{3}{c|}{\\multirow{2}{*}{{Models}}} & \n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Transformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Reformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Informer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Flowformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{1.0}{Flashformer}}} \\\\\n    \\multicolumn{3}{c|}{}\n    &\\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{Transformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{kitaev2020reformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{Informer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{wu2022flowformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{1.0}{\\citeyearpar{dao2022flashattention}}} \\\\\n    \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13}  \n    \\multicolumn{3}{c|}{Metric}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}  & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE} & \\scalebox{1.0}{MSE} & \\scalebox{1.0}{MAE}\\\\\n    \\toprule\n    \\multirow{10}{*}{\\scalebox{1.0}{ECL}} & \\multirow{5}{*}{Original} & 96 & 0.260 & 0.358 & 0.312 & 0.402 & 0.274 & 0.368 & 0.215 & 0.320 & 0.259 & 0.357\\\\\n    & & 192 & 0.266 & 0.367 & 0.348 & 0.433 & 0.296 & 0.386 & 0.259 & 0.355 & 0.274 & 0.374\\\\\n    & & 336 & 0.280 & 0.375 & 0.350 & 0.433 & 0.300 & 0.394 & 0.296 & 0.383 & 0.310 & 0.396\\\\\n    & & 720 & 0.302 & 0.386 & 0.340 & 0.420 & 0.373 & 0.439 & 0.296 & 0.380 & 0.298 & 0.383\\\\\n    \\cmidrule(lr){3-13}\n    & & Avg  & \\scalebox{1.0}{0.277} & \\scalebox{1.0}{0.372}& \\scalebox{1.0}{0.338} & \\scalebox{1.0}{0.422} & \\scalebox{1.0}{0.311} & \\scalebox{1.0}{0.397}  & \\scalebox{1.0}{0.267} & \\scalebox{1.0}{0.359} & \\scalebox{1.0}{0.285} & \\scalebox{1.0}{0.377}\\\\\n    \\cmidrule(lr){2-13}\n    & \\multirow{5}{*}{+Inverted} & 96 & 0.148 & 0.240 & 0.182 & 0.275 & 0.190 & 0.286 & 0.183 & 0.267 & 0.178 & 0.265\\\\\n    & & 192 & 0.162 & 0.253 & 0.192 & 0.286 & 0.201 & 0.297 & 0.192 & 0.277 & 0.189 & 0.276\\\\\n    & & 336 & 0.178 & 0.269 & 0.210 & 0.304 & 0.218 & 0.315 & 0.210 & 0.295 & 0.207 & 0.294\\\\\n    & & 720 & 0.225 & 0.317 & 0.249 & 0.339 & 0.255 & 0.347 & 0.255 & 0.332 & 0.251 & 0.329\\\\\n    \\cmidrule(lr){3-13}\n    & & Avg  & \\textbf{\\scalebox{1.0}{0.178}} & \\textbf{\\scalebox{1.0}{0.270}} & \\textbf{\\scalebox{1.0}{0.208}} & \\textbf{\\scalebox{1.0}{0.301}} & \\textbf{\\scalebox{1.0}{0.216}} & \\textbf{\\scalebox{1.0}{0.311}} & \\textbf{\\scalebox{1.0}{0.210}} & \\textbf{\\scalebox{1.0}{0.293}} & \\textbf{\\scalebox{1.0}{0.206}} & \\textbf{\\scalebox{1.0}{0.291}}\\\\\n    \\midrule\n    \\multirow{10}{*}{\\scalebox{1.0}{Traffic}} & \\multirow{5}{*}{Original} & 96 & 0.647 & 0.357 & 0.732 & 0.423 & 0.719 & 0.391 & 0.691 & 0.393 & 0.641 & 0.348\\\\\n    & & 192 & 0.649 & 0.356 & 0.733 & 0.420 & 0.696 & 0.379 & 0.729 & 0.419 & 0.648 & 0.358\\\\\n    & & 336 & 0.667 & 0.364 & 0.742 & 0.420 & 0.777 & 0.420 & 0.756 & 0.423 & 0.670 & 0.364\\\\\n    & & 720 & 0.697 & 0.376 & 0.755 & 0.432 & 0.864 & 0.472 & 0.825 & 0.449 & 0.673 & 0.354\\\\\n    \\cmidrule(lr){3-13}\n    & & Avg  & \\scalebox{1.0}{0.665} & \\scalebox{1.0}{0.363} & \\scalebox{1.0}{0.741} & \\scalebox{1.0}{0.422} & \\scalebox{1.0}{0.764} & \\scalebox{1.0}{0.416} & \\scalebox{1.0}{0.750} & \\scalebox{1.0}{0.421} & \\scalebox{1.0}{0.658} & \\scalebox{1.0}{0.356}\\\\\n    \\cmidrule(lr){2-13}\n    & \\multirow{5}{*}{+Inverted} & 96 & 0.395 & 0.268 & 0.617 & 0.356 & 0.632 & 0.367 & 0.493 & 0.339 & 0.464 & 0.320\\\\\n    & & 192 & 0.417 & 0.276 & 0.629 & 0.361 & 0.641 & 0.370 & 0.506 & 0.345 & 0.479 & 0.326\\\\\n    & & 336 & 0.433 & 0.283 & 0.648 & 0.370 & 0.663 & 0.379 & 0.526 & 0.355 & 0.501 & 0.337\\\\\n    & & 720 & 0.467 & 0.302 & 0.694 & 0.394 & 0.713 & 0.405 & 0.572 & 0.381 & 0.524 & 0.350\\\\\n    \\cmidrule(lr){3-13}\n    & & Avg  & \\textbf{\\scalebox{1.0}{0.428}} & \\textbf{\\scalebox{1.0}{0.282}} & \\textbf{\\scalebox{1.0}{0.647}} & \\textbf{\\scalebox{1.0}{0.370}} & \\textbf{\\scalebox{1.0}{0.662}} & \\textbf{\\scalebox{1.0}{0.380}} & \\textbf{\\scalebox{1.0}{0.524}} & \\textbf{\\scalebox{1.0}{0.355}} & \\textbf{\\scalebox{1.0}{0.492}} & \\textbf{\\scalebox{1.0}{0.333}}\\\\\n    \\midrule\n    \\multirow{10}{*}{\\scalebox{1.0}{Weather}} & \\multirow{5}{*}{Original} & 96 & 0.395 & 0.427 & 0.689 & 0.596 & 0.300 & 0.384 & 0.182 & 0.233 & 0.388 & 0.425\\\\\n    & & 192  & 0.619 & 0.560 & 0.752 & 0.638 & 0.598 & 0.544 & 0.250 & 0.288 & 0.619 & 0.560\\\\\n    & & 336 & 0.689 & 0.594 & 0.639 & 0.596 & 0.578 & 0.523 & 0.309 & 0.329 & 0.698 & 0.600\\\\\n    & & 720 & 0.926 & 0.710 & 1.130 & 0.792 & 1.059 & 0.741 & 0.404 & 0.385 & 0.930 & 0.711\\\\\n    \\cmidrule(lr){3-13}\n    & & Avg  &  \\scalebox{1.0}{0.657} & \\scalebox{1.0}{0.572} & \\scalebox{1.0}{0.803} & \\scalebox{1.0}{0.656} & \\scalebox{1.0}{0.634} & \\scalebox{1.0}{0.548} & \\scalebox{1.0}{0.286} & \\scalebox{1.0}{0.308} & \\scalebox{1.0}{0.659} & \\scalebox{1.0}{0.574}\\\\\n    \\cmidrule(lr){2-13}\n    & \\multirow{5}{*}{+Inverted} & 96 & 0.174 & 0.214 & 0.169 & 0.225 & 0.180 & 0.251 & 0.183 & 0.223 & 0.177 & 0.218\\\\\n    & & 192 & 0.221 & 0.254 & 0.213 & 0.265 & 0.244 & 0.318 & 0.231 & 0.262& 0.229 & 0.261\\\\\n    & & 336 & 0.278 & 0.296 & 0.268 & 0.317 & 0.282 & 0.343 & 0.286 & 0.301 & 0.283 & 0.300\\\\\n    & & 720 & 0.358 & 0.349 & 0.340 & 0.361 & 0.377 & 0.409 & 0.363 & 0.352 & 0.359 & 0.251\\\\\n    \\cmidrule(lr){3-13}\n    & & Avg  &\\textbf{\\scalebox{1.0}{0.258}} & \\textbf{\\scalebox{1.0}{0.279}} & \\textbf{\\scalebox{1.0}{0.248}} & \\textbf{\\scalebox{1.0}{0.292}} & \\textbf{\\scalebox{1.0}{0.271}} & \\textbf{\\scalebox{1.0}{0.330}} & \\textbf{\\scalebox{1.0}{0.266}} & \\textbf{\\scalebox{1.0}{0.285}} & \\textbf{\\scalebox{1.0}{0.262}} & \\textbf{\\scalebox{1.0}{0.282}}\\\\\n    \\bottomrule\n  \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n  \\vspace{-5pt}\n\\end{table}",
            "tab:full_baseline_results_pems": "\\begin{table}[htbp]\n  \\caption{\\update{Full results of the PEMS forecasting task}. We compare extensive competitive models under different prediction lengths following the setting of SCINet~\\citeyearpar{SCINet}. The input length is set to 96 for all baselines. \\emph{Avg} means the average results from all four prediction lengths.\n  }\\label{tab:full_baseline_results_pems}\n  \\vskip -0.0in\n  \\vspace{3pt}\n  \\renewcommand{\\arraystretch}{0.85} \n  \\centering\n  \\resizebox{1\\columnwidth}{!}{\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{1pt}\n  \\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n    \\toprule\n    \\multicolumn{2}{c}{\\multirow{2}{*}{Models}} & \n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{\\textbf{iTransformer}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{\\update{RLinear}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{PatchTST}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Crossformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{TiDE}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{{TimesNet}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{DLinear}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{SCINet}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{FEDformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Stationary}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Autoformer}}} \\\\% &\\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Informer}}} \\\\\n    \\multicolumn{2}{c}{} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\textbf{(Ours)}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{li2023revisiting}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{PatchTST}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Crossformer}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{das2023long}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Timesnet}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{DLinear}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{SCINet}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{fedformer}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Stationary}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Autoformer}}} \\\\         %& \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Informer}}} \\\\\n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\\cmidrule(lr){11-12}\\cmidrule(lr){13-14} \\cmidrule(lr){15-16} \\cmidrule(lr){17-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-22} \\cmidrule(lr){23-24}\n    \\multicolumn{2}{c}{Metric}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} \\\\\n    \\toprule\n    \n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{PEMS03}}}\n    &  \\scalebox{0.78}{12} & \\secondres{\\scalebox{0.78}{0.071}} &\\secondres{\\scalebox{0.78}{0.174}} &\\scalebox{0.78}{0.126} &\\scalebox{0.78}{0.236} &\\scalebox{0.78}{0.099} &\\scalebox{0.78}{0.216} &\\scalebox{0.78}{0.090} &\\scalebox{0.78}{0.203} & \\scalebox{0.78}{0.178} & \\scalebox{0.78}{0.305} &\\scalebox{0.78}{0.085} &\\scalebox{0.78}{0.192} &\\scalebox{0.78}{0.122} &\\scalebox{0.78}{0.243} &\\boldres{\\scalebox{0.78}{0.066}} &\\boldres{\\scalebox{0.78}{0.172}} &\\scalebox{0.78}{0.126} &\\scalebox{0.78}{0.251} &\\scalebox{0.78}{0.081} &\\scalebox{0.78}{0.188} &\\scalebox{0.78}{0.272} &\\scalebox{0.78}{0.385}    \\\\% &\\scalebox{0.78}{0.126} &\\scalebox{0.78}{0.233}\\\\\n    & \\scalebox{0.78}{24} &\\secondres{\\scalebox{0.78}{0.093}} &\\secondres{\\scalebox{0.78}{0.201}} &\\scalebox{0.78}{0.246} &\\scalebox{0.78}{0.334} &\\scalebox{0.78}{0.142} &\\scalebox{0.78}{0.259} &\\scalebox{0.78}{0.121} &\\scalebox{0.78}{0.240} & \\scalebox{0.78}{0.257} & \\scalebox{0.78}{0.371} &\\scalebox{0.78}{0.118} &\\scalebox{0.78}{0.223} &\\scalebox{0.78}{0.201} &\\scalebox{0.78}{0.317} &\\boldres{\\scalebox{0.78}{0.085}} &\\boldres{\\scalebox{0.78}{0.198}} &\\scalebox{0.78}{0.149} &\\scalebox{0.78}{0.275} &\\scalebox{0.78}{0.105} &\\scalebox{0.78}{0.214} &\\scalebox{0.78}{0.334} &\\scalebox{0.78}{0.440} \\\\%&\\scalebox{0.78}{0.139} &\\scalebox{0.78}{0.250}\\\\\n    & \\scalebox{0.78}{48} &\\boldres{\\scalebox{0.78}{0.125}} &\\boldres{\\scalebox{0.78}{0.236}} &\\scalebox{0.78}{0.551} &\\scalebox{0.78}{0.529} &\\scalebox{0.78}{0.211} &\\scalebox{0.78}{0.319}  &\\scalebox{0.78}{0.202} &\\scalebox{0.78}{0.317} & \\scalebox{0.78}{0.379}& \\scalebox{0.78}{0.463} &\\scalebox{0.78}{0.155} &\\scalebox{0.78}{0.260} &\\scalebox{0.78}{0.333} &\\scalebox{0.78}{0.425} &\\secondres{\\scalebox{0.78}{0.127}} &\\secondres{\\scalebox{0.78}{0.238}} &\\scalebox{0.78}{0.227} &\\scalebox{0.78}{0.348} &\\scalebox{0.78}{0.154} &\\scalebox{0.78}{0.257} &\\scalebox{0.78}{1.032} &\\scalebox{0.78}{0.782} \\\\% &\\scalebox{0.78}{0.186} &\\scalebox{0.78}{0.289}\\\\\n    & \\scalebox{0.78}{96} &\\boldres{\\scalebox{0.78}{0.164}} &\\boldres{\\scalebox{0.78}{0.275}} &\\scalebox{0.78}{1.057} &\\scalebox{0.78}{0.787} &\\scalebox{0.78}{0.269} &\\scalebox{0.78}{0.370} &\\scalebox{0.78}{0.262} &\\scalebox{0.78}{0.367} & \\scalebox{0.78}{0.490}& \\scalebox{0.78}{0.539} &\\scalebox{0.78}{0.228} &\\scalebox{0.78}{0.317} &\\scalebox{0.78}{0.457} &\\scalebox{0.78}{0.515} &\\secondres{\\scalebox{0.78}{0.178}} &\\secondres{\\scalebox{0.78}{0.287}} &\\scalebox{0.78}{0.348} &\\scalebox{0.78}{0.434} &\\scalebox{0.78}{0.247} &\\scalebox{0.78}{0.336} &\\scalebox{0.78}{1.031} &\\scalebox{0.78}{0.796} \\\\% &\\scalebox{0.78}{0.233} &\\scalebox{0.78}{0.323}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} &\\boldres{\\scalebox{0.78}{0.113}} &\\boldres{\\scalebox{0.78}{0.221}} &\\scalebox{0.78}{0.495} &\\scalebox{0.78}{0.472} &\\scalebox{0.78}{0.180} &\\scalebox{0.78}{0.291} &\\scalebox{0.78}{0.169} &\\scalebox{0.78}{0.281} & \\scalebox{0.78}{0.326}& \\scalebox{0.78}{0.419} &\\scalebox{0.78}{0.147} &\\scalebox{0.78}{0.248} &\\scalebox{0.78}{0.278} &\\scalebox{0.78}{0.375} &\\secondres{\\scalebox{0.78}{0.114}} &\\secondres{\\scalebox{0.78}{0.224}} &\\scalebox{0.78}{0.213} &\\scalebox{0.78}{0.327} &\\scalebox{0.78}{0.147} &\\scalebox{0.78}{0.249} &\\scalebox{0.78}{0.667} &\\scalebox{0.78}{0.601} \\\\% &\\scalebox{0.78}{0.171} &\\scalebox{0.78}{0.274}\\\\\n    \n    \\midrule\n    \\multirow{5}{*}{\\update{\\rotatebox{90}{\\scalebox{0.95}{PEMS04}}}} \n    &  \\scalebox{0.78}{12} &\\secondres{\\scalebox{0.78}{0.078}} &\\secondres{\\scalebox{0.78}{0.183}} &\\scalebox{0.78}{0.138} &\\scalebox{0.78}{0.252} &\\scalebox{0.78}{0.105} &\\scalebox{0.78}{0.224} &\\scalebox{0.78}{0.098} &\\scalebox{0.78}{0.218} & \\scalebox{0.78}{0.219}& \\scalebox{0.78}{0.340} &\\scalebox{0.78}{0.087} &\\scalebox{0.78}{0.195} &\\scalebox{0.78}{0.148} &\\scalebox{0.78}{0.272} &\\boldres{\\scalebox{0.78}{0.073}} &\\boldres{\\scalebox{0.78}{0.177}} &\\scalebox{0.78}{0.138} &\\scalebox{0.78}{0.262} &\\scalebox{0.78}{0.088} &\\scalebox{0.78}{0.196} &\\scalebox{0.78}{0.424} &\\scalebox{0.78}{0.491} \\\\% &\\scalebox{0.78}{0.112} &\\scalebox{0.78}{0.222}\\\\\n    & \\scalebox{0.78}{24} &\\secondres{\\scalebox{0.78}{0.095}} &\\secondres{\\scalebox{0.78}{0.205}} &\\scalebox{0.78}{0.258} &\\scalebox{0.78}{0.348} &\\scalebox{0.78}{0.153} &\\scalebox{0.78}{0.275} &\\scalebox{0.78}{0.131} &\\scalebox{0.78}{0.256} & \\scalebox{0.78}{0.292}& \\scalebox{0.78}{0.398} &\\scalebox{0.78}{0.103} &\\scalebox{0.78}{0.215} &\\scalebox{0.78}{0.224} &\\scalebox{0.78}{0.340} &\\boldres{\\scalebox{0.78}{0.084}} &\\boldres{\\scalebox{0.78}{0.193}} &\\scalebox{0.78}{0.177} &\\scalebox{0.78}{0.293} &\\scalebox{0.78}{0.104} &\\scalebox{0.78}{0.216} &\\scalebox{0.78}{0.459} &\\scalebox{0.78}{0.509} \\\\% &\\scalebox{0.78}{0.117} &\\scalebox{0.78}{0.227}\\\\\n    & \\scalebox{0.78}{48} &\\secondres{\\scalebox{0.78}{0.120}} &\\secondres{\\scalebox{0.78}{0.233}} &\\scalebox{0.78}{0.572} &\\scalebox{0.78}{0.544} &\\scalebox{0.78}{0.229} &\\scalebox{0.78}{0.339} &\\scalebox{0.78}{0.205} &\\scalebox{0.78}{0.326} & \\scalebox{0.78}{0.409}& \\scalebox{0.78}{0.478} &\\scalebox{0.78}{0.136} &\\scalebox{0.78}{0.250} &\\scalebox{0.78}{0.355} &\\scalebox{0.78}{0.437} &\\boldres{\\scalebox{0.78}{0.099}} &\\boldres{\\scalebox{0.78}{0.211}} &\\scalebox{0.78}{0.270} &\\scalebox{0.78}{0.368} &\\scalebox{0.78}{0.137} &\\scalebox{0.78}{0.251} &\\scalebox{0.78}{0.646} &\\scalebox{0.78}{0.610} \\\\% &\\scalebox{0.78}{0.126} &\\scalebox{0.78}{0.239}\\\\\n    & \\scalebox{0.78}{96} &\\secondres{\\scalebox{0.78}{0.150}} &\\secondres{\\scalebox{0.78}{0.262}} &\\scalebox{0.78}{1.137} &\\scalebox{0.78}{0.820} &\\scalebox{0.78}{0.291} &\\scalebox{0.78}{0.389} &\\scalebox{0.78}{0.402} &\\scalebox{0.78}{0.457} & \\scalebox{0.78}{0.492}& \\scalebox{0.78}{0.532} &\\scalebox{0.78}{0.190} &\\scalebox{0.78}{0.303} &\\scalebox{0.78}{0.452} &\\scalebox{0.78}{0.504} &\\boldres{\\scalebox{0.78}{0.114}} &\\boldres{\\scalebox{0.78}{0.227}} &\\scalebox{0.78}{0.341} &\\scalebox{0.78}{0.427} &\\scalebox{0.78}{0.186} &\\scalebox{0.78}{0.297} &\\scalebox{0.78}{0.912} &\\scalebox{0.78}{0.748} \\\\% &\\scalebox{0.78}{0.128} &\\scalebox{0.78}{0.242}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} &\\secondres{\\scalebox{0.78}{0.111}} &\\secondres{\\scalebox{0.78}{0.221}} &\\scalebox{0.78}{0.526} &\\scalebox{0.78}{0.491} &\\scalebox{0.78}{0.195} &\\scalebox{0.78}{0.307} &\\scalebox{0.78}{0.209} &\\scalebox{0.78}{0.314} & \\scalebox{0.78}{0.353}& \\scalebox{0.78}{0.437} &\\scalebox{0.78}{0.129} &\\scalebox{0.78}{0.241} &\\scalebox{0.78}{0.295} &\\scalebox{0.78}{0.388} &\\boldres{\\scalebox{0.78}{0.092}} &\\boldres{\\scalebox{0.78}{0.202}} &\\scalebox{0.78}{0.231} &\\scalebox{0.78}{0.337} &\\scalebox{0.78}{0.127} &\\scalebox{0.78}{0.240} &\\scalebox{0.78}{0.610} &\\scalebox{0.78}{0.590} \\\\% &\\scalebox{0.78}{0.121} &\\scalebox{0.78}{0.232}\\\\\n\n    \\midrule\n    \\multirow{5}{*}{\\update{\\rotatebox{90}{\\scalebox{0.95}{PEMS07}}}}\n    &  \\scalebox{0.78}{12} &\\boldres{\\scalebox{0.78}{0.067}} &\\boldres{\\scalebox{0.78}{0.165}} &\\scalebox{0.78}{0.118} &\\scalebox{0.78}{0.235} &\\scalebox{0.78}{0.095} &\\scalebox{0.78}{0.207} &\\scalebox{0.78}{0.094} &\\scalebox{0.78}{0.200} & \\scalebox{0.78}{0.173}& \\scalebox{0.78}{0.304} &\\scalebox{0.78}{0.082} &\\scalebox{0.78}{0.181} &\\scalebox{0.78}{0.115} &\\scalebox{0.78}{0.242} &\\secondres{\\scalebox{0.78}{0.068}} &\\secondres{\\scalebox{0.78}{0.171}} &\\scalebox{0.78}{0.109} &\\scalebox{0.78}{0.225} &\\scalebox{0.78}{0.083} &\\scalebox{0.78}{0.185} &\\scalebox{0.78}{0.199} &\\scalebox{0.78}{0.336} \\\\% &\\scalebox{0.78}{0.173} &\\scalebox{0.78}{0.243} \\\\\n    & \\scalebox{0.78}{24} &\\boldres{\\scalebox{0.78}{0.088}} &\\boldres{\\scalebox{0.78}{0.190}} &\\scalebox{0.78}{0.242} &\\scalebox{0.78}{0.341} &\\scalebox{0.78}{0.150} &\\scalebox{0.78}{0.262} &\\scalebox{0.78}{0.139} &\\scalebox{0.78}{0.247} & \\scalebox{0.78}{0.271}& \\scalebox{0.78}{0.383} &\\scalebox{0.78}{0.101} &\\scalebox{0.78}{0.204} &\\scalebox{0.78}{0.210} &\\scalebox{0.78}{0.329} &\\secondres{\\scalebox{0.78}{0.119}} &\\secondres{\\scalebox{0.78}{0.225}} &\\scalebox{0.78}{0.125} &\\scalebox{0.78}{0.244} &\\scalebox{0.78}{0.102} &\\scalebox{0.78}{0.207} &\\scalebox{0.78}{0.323} &\\scalebox{0.78}{0.420} \\\\% &\\scalebox{0.78}{0.178} &\\scalebox{0.78}{0.247}\\\\\n    & \\scalebox{0.78}{48} &\\boldres{\\scalebox{0.78}{0.110}} &\\boldres{\\scalebox{0.78}{0.215}} &\\scalebox{0.78}{0.562} &\\scalebox{0.78}{0.541} &\\scalebox{0.78}{0.253} &\\scalebox{0.78}{0.340} &\\scalebox{0.78}{0.311} &\\scalebox{0.78}{0.369} & \\scalebox{0.78}{0.446}& \\scalebox{0.78}{0.495} &\\scalebox{0.78}{0.134} &\\scalebox{0.78}{0.238} &\\scalebox{0.78}{0.398} &\\scalebox{0.78}{0.458} &\\secondres{\\scalebox{0.78}{0.149}} &\\secondres{\\scalebox{0.78}{0.237}} &\\scalebox{0.78}{0.165} &\\scalebox{0.78}{0.288} &\\scalebox{0.78}{0.136} &\\scalebox{0.78}{0.240} &\\scalebox{0.78}{0.390} &\\scalebox{0.78}{0.470} \\\\% &\\scalebox{0.78}{0.185} &\\scalebox{0.78}{0.256}\\\\\n    & \\scalebox{0.78}{96} &\\boldres{\\scalebox{0.78}{0.139}} &\\boldres{\\scalebox{0.78}{0.245}} &\\scalebox{0.78}{1.096} &\\scalebox{0.78}{0.795} &\\scalebox{0.78}{0.346} &\\scalebox{0.78}{0.404} &\\scalebox{0.78}{0.396} &\\scalebox{0.78}{0.442} & \\scalebox{0.78}{0.628}& \\scalebox{0.78}{0.577} &\\scalebox{0.78}{0.181} &\\scalebox{0.78}{0.279} &\\scalebox{0.78}{0.594} &\\scalebox{0.78}{0.553} &\\secondres{\\scalebox{0.78}{0.141}} &\\secondres{\\scalebox{0.78}{0.234}} &\\scalebox{0.78}{0.262} &\\scalebox{0.78}{0.376} &\\scalebox{0.78}{0.187} &\\scalebox{0.78}{0.287} &\\scalebox{0.78}{0.554} &\\scalebox{0.78}{0.578} \\\\% &\\scalebox{0.78}{0.195} &\\scalebox{0.78}{0.269}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} &\\boldres{\\scalebox{0.78}{0.101}} &\\boldres{\\scalebox{0.78}{0.204}} &\\scalebox{0.78}{0.504} &\\scalebox{0.78}{0.478} &\\scalebox{0.78}{0.211} &\\scalebox{0.78}{0.303} &\\scalebox{0.78}{0.235} &\\scalebox{0.78}{0.315} & \\scalebox{0.78}{0.380}& \\scalebox{0.78}{0.440} &\\scalebox{0.78}{0.124} &\\scalebox{0.78}{0.225} &\\scalebox{0.78}{0.329} &\\scalebox{0.78}{0.395} &\\secondres{\\scalebox{0.78}{0.119}} &\\secondres{\\scalebox{0.78}{0.234}} &\\scalebox{0.78}{0.165} &\\scalebox{0.78}{0.283} &\\scalebox{0.78}{0.127} &\\scalebox{0.78}{0.230} &\\scalebox{0.78}{0.367} &\\scalebox{0.78}{0.451} \\\\% &\\scalebox{0.78}{0.183} &\\scalebox{0.78}{0.254}\\\\\n\n    \\midrule\n    \\multirow{5}{*}{\\update{\\rotatebox{90}{\\scalebox{0.95}{PEMS08}}}}\n    &  \\scalebox{0.78}{12} &\\boldres{\\scalebox{0.78}{0.079}} &\\boldres{\\scalebox{0.78}{0.182}} &\\scalebox{0.78}{0.133} &\\scalebox{0.78}{0.247} &\\scalebox{0.78}{0.168} &\\scalebox{0.78}{0.232} &\\scalebox{0.78}{0.165} &\\scalebox{0.78}{0.214} & \\scalebox{0.78}{0.227}& \\scalebox{0.78}{0.343} &\\scalebox{0.78}{0.112} &\\scalebox{0.78}{0.212} &\\scalebox{0.78}{0.154} &\\scalebox{0.78}{0.276} &\\secondres{\\scalebox{0.78}{0.087}} &\\secondres{\\scalebox{0.78}{0.184}} &\\scalebox{0.78}{0.173} &\\scalebox{0.78}{0.273} &\\scalebox{0.78}{0.109} &\\scalebox{0.78}{0.207} &\\scalebox{0.78}{0.436} &\\scalebox{0.78}{0.485} \\\\% &\\scalebox{0.78}{0.296} &\\scalebox{0.78}{0.312}\\\\\n    & \\scalebox{0.78}{24} &\\boldres{\\scalebox{0.78}{0.115}} &\\boldres{\\scalebox{0.78}{0.219}} &\\scalebox{0.78}{0.249} &\\scalebox{0.78}{0.343} &\\scalebox{0.78}{0.224} &\\scalebox{0.78}{0.281} &\\scalebox{0.78}{0.215} &\\scalebox{0.78}{0.260} & \\scalebox{0.78}{0.318}& \\scalebox{0.78}{0.409} &\\scalebox{0.78}{0.141} &\\scalebox{0.78}{0.238} &\\scalebox{0.78}{0.248} &\\scalebox{0.78}{0.353} &\\secondres{\\scalebox{0.78}{0.122}} &\\secondres{\\scalebox{0.78}{0.221}} &\\scalebox{0.78}{0.210} &\\scalebox{0.78}{0.301} &\\scalebox{0.78}{0.140} &\\scalebox{0.78}{0.236} &\\scalebox{0.78}{0.467} &\\scalebox{0.78}{0.502} \\\\% &\\scalebox{0.78}{0.327} &\\scalebox{0.78}{0.318}\\\\\n    & \\scalebox{0.78}{48} &\\boldres{\\scalebox{0.78}{0.186}} &\\boldres{\\scalebox{0.78}{0.235}} &\\scalebox{0.78}{0.569} &\\scalebox{0.78}{0.544} &\\scalebox{0.78}{0.321} &\\scalebox{0.78}{0.354} &\\scalebox{0.78}{0.315} &\\scalebox{0.78}{0.355} & \\scalebox{0.78}{0.497}& \\scalebox{0.78}{0.510} &\\scalebox{0.78}{0.198} &\\scalebox{0.78}{0.283} &\\scalebox{0.78}{0.440} &\\scalebox{0.78}{0.470} &\\secondres{\\scalebox{0.78}{0.189}} &\\secondres{\\scalebox{0.78}{0.270}} &\\scalebox{0.78}{0.320} &\\scalebox{0.78}{0.394} &\\scalebox{0.78}{0.211} &\\scalebox{0.78}{0.294} &\\scalebox{0.78}{0.966} &\\scalebox{0.78}{0.733} \\\\% &\\scalebox{0.78}{0.387} &\\scalebox{0.78}{0.365}\\\\\n    & \\scalebox{0.78}{96} &\\boldres{\\scalebox{0.78}{0.221}} &\\boldres{\\scalebox{0.78}{0.267}} &\\scalebox{0.78}{1.166} &\\scalebox{0.78}{0.814} &\\scalebox{0.78}{0.408} &\\scalebox{0.78}{0.417} &\\scalebox{0.78}{0.377} &\\scalebox{0.78}{0.397} & \\scalebox{0.78}{0.721}& \\scalebox{0.78}{0.592} &\\scalebox{0.78}{0.320} &\\scalebox{0.78}{0.351} &\\scalebox{0.78}{0.674} &\\scalebox{0.78}{0.565} &\\secondres{\\scalebox{0.78}{0.236}} &\\secondres{\\scalebox{0.78}{0.300}} &\\scalebox{0.78}{0.442} &\\scalebox{0.78}{0.465} &\\scalebox{0.78}{0.345} &\\scalebox{0.78}{0.367} &\\scalebox{0.78}{1.385} &\\scalebox{0.78}{0.915} \\\\% &\\scalebox{0.78}{0.455} &\\scalebox{0.78}{0.407}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} &\\boldres{\\scalebox{0.78}{0.150}} &\\boldres{\\scalebox{0.78}{0.226}} &\\scalebox{0.78}{0.529} &\\scalebox{0.78}{0.487} &\\scalebox{0.78}{0.280} &\\scalebox{0.78}{0.321} &\\scalebox{0.78}{0.268} &\\scalebox{0.78}{0.307} & \\scalebox{0.78}{0.441}& \\scalebox{0.78}{0.464} &\\scalebox{0.78}{0.193} &\\scalebox{0.78}{0.271} &\\scalebox{0.78}{0.379} &\\scalebox{0.78}{0.416} &\\secondres{\\scalebox{0.78}{0.158}} &\\secondres{\\scalebox{0.78}{0.244}} &\\scalebox{0.78}{0.286} &\\scalebox{0.78}{0.358} &\\scalebox{0.78}{0.201} &\\scalebox{0.78}{0.276} &\\scalebox{0.78}{0.814} &\\scalebox{0.78}{0.659} \\\\% &\\scalebox{0.78}{0.366} &\\scalebox{0.78}{0.350}\\\\\n    \\midrule\n     \\multicolumn{2}{c|}{\\scalebox{0.78}{{$1^{\\text{st}}$ Count}}} & \\scalebox{0.78}{\\boldres{13}} & \\scalebox{0.78}{\\boldres{13}} & \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{\\secondres{7}} & \\scalebox{0.78}{\\secondres{7}} & \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0} \\\\% & \\scalebox{0.78}{0}& \\scalebox{0.78}{0}\\\\\n        \\bottomrule\n      \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n}\n\\end{table}",
            "tab:full_baseline_results": "\\begin{table}[htbp]\n  \\caption{\\update{Full results of the long-term forecasting task}. We compare extensive competitive models under different prediction lengths following the setting of TimesNet~\\citeyearpar{Timesnet}. The input sequence length is set to 96 for all baselines. \\emph{Avg} means the average results from all four prediction lengths.}\\label{tab:full_baseline_results}\n  \\vskip -0.0in\n  \\vspace{3pt}\n  \\renewcommand{\\arraystretch}{0.85} \n  \\centering\n  \\resizebox{1\\columnwidth}{!}{\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{1pt}\n  \\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n    \\toprule\n    \\multicolumn{2}{c}{\\multirow{2}{*}{Models}} & \n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{\\textbf{iTransformer}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{\\update{RLinear}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{PatchTST}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Crossformer}}}  &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{TiDE}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{{TimesNet}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{DLinear}}}&\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{SCINet}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{FEDformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Stationary}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Autoformer}}} \\\\\n    %&\\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Informer}}} \n    \\multicolumn{2}{c}{} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\textbf{(Ours)}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{li2023revisiting}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{PatchTST}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Crossformer}}}  & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{das2023long}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Timesnet}}} & \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{DLinear}}}& \n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{SCINet}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{fedformer}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Stationary}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Autoformer}}} \\\\\n    %&\\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Informer}}} \n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\\cmidrule(lr){11-12}\\cmidrule(lr){13-14} \\cmidrule(lr){15-16} \\cmidrule(lr){17-18} \\cmidrule(lr){19-20} \\cmidrule(lr){21-22} \\cmidrule(lr){23-24}\n    \\multicolumn{2}{c}{Metric}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}  & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} \\\\\n    \\toprule\n    \n    \\multirow{5}{*}{\\update{\\rotatebox{90}{\\scalebox{0.95}{ETTm1}}}}\n    &  \\scalebox{0.78}{96} & \\secondres{\\scalebox{0.78}{0.334}} & \\secondres{\\scalebox{0.78}{0.368}} & \\scalebox{0.78}{0.355} & \\scalebox{0.78}{0.376} & \\boldres{\\scalebox{0.78}{0.329}} & \\boldres{\\scalebox{0.78}{0.367}} & \\scalebox{0.78}{0.404} & \\scalebox{0.78}{0.426} & \\scalebox{0.78}{0.364} & \\scalebox{0.78}{0.387} &{\\scalebox{0.78}{0.338}} &{\\scalebox{0.78}{0.375}} &{\\scalebox{0.78}{0.345}} &{\\scalebox{0.78}{0.372}} & \\scalebox{0.78}{0.418} & \\scalebox{0.78}{0.438} &\\scalebox{0.78}{0.379} &\\scalebox{0.78}{0.419} &\\scalebox{0.78}{0.386} &\\scalebox{0.78}{0.398} &\\scalebox{0.78}{0.505} &\\scalebox{0.78}{0.475} \\\\ %&\\scalebox{0.78}{0.672} &\\scalebox{0.78}{0.571} \\\\\n    & \\scalebox{0.78}{192} & \\scalebox{0.78}{0.377} & \\scalebox{0.78}{0.391} & \\scalebox{0.78}{0.391} & \\scalebox{0.78}{0.392} & \\boldres{\\scalebox{0.78}{0.367}} & \\boldres{\\scalebox{0.78}{0.385}} & \\scalebox{0.78}{0.450} & \\scalebox{0.78}{0.451} &\\scalebox{0.78}{0.398} & \\scalebox{0.78}{0.404} &\\secondres{\\scalebox{0.78}{0.374}} &\\secondres{\\scalebox{0.78}{0.387}}  &{\\scalebox{0.78}{0.380}} &{\\scalebox{0.78}{0.389}} & \\scalebox{0.78}{0.439} & \\scalebox{0.78}{0.450}  &\\scalebox{0.78}{0.426} &\\scalebox{0.78}{0.441} &\\scalebox{0.78}{0.459} &\\scalebox{0.78}{0.444} &\\scalebox{0.78}{0.553} &\\scalebox{0.78}{0.496} \\\\ %&\\scalebox{0.78}{0.795} &\\scalebox{0.78}{0.669}\\\\\n    & \\scalebox{0.78}{336} & \\scalebox{0.78}{0.426} & \\scalebox{0.78}{0.420} & \\scalebox{0.78}{0.424} & \\scalebox{0.78}{0.415} & \\boldres{\\scalebox{0.78}{0.399}} & \\boldres{\\scalebox{0.78}{0.410}} & \\scalebox{0.78}{0.532}  &\\scalebox{0.78}{0.515} & \\scalebox{0.78}{0.428} & \\scalebox{0.78}{0.425} &\\secondres{\\scalebox{0.78}{0.410}} &\\secondres{\\scalebox{0.78}{0.411}}  &{\\scalebox{0.78}{0.413}} &{\\scalebox{0.78}{0.413}} & \\scalebox{0.78}{0.490} & \\scalebox{0.78}{0.485}  &\\scalebox{0.78}{0.445} &\\scalebox{0.78}{0.459} &\\scalebox{0.78}{0.495} &\\scalebox{0.78}{0.464} &\\scalebox{0.78}{0.621} &\\scalebox{0.78}{0.537} \\\\ %&\\scalebox{0.78}{1.212} &\\scalebox{0.78}{0.871} \\\\\n    & \\scalebox{0.78}{720} & \\scalebox{0.78}{0.491} & \\scalebox{0.78}{0.459} & \\scalebox{0.78}{0.487} & \\scalebox{0.78}{0.450} & \\boldres{\\scalebox{0.78}{0.454}} & \\boldres{\\scalebox{0.78}{0.439}} & \\scalebox{0.78}{0.666} & \\scalebox{0.78}{0.589} & \\scalebox{0.78}{0.487} & \\scalebox{0.78}{0.461} &{\\scalebox{0.78}{0.478}} &\\secondres{\\scalebox{0.78}{0.450}} &\\secondres{\\scalebox{0.78}{0.474}} &{\\scalebox{0.78}{0.453}} & \\scalebox{0.78}{0.595} & \\scalebox{0.78}{0.550}  &\\scalebox{0.78}{0.543} &\\scalebox{0.78}{0.490} &\\scalebox{0.78}{0.585} &\\scalebox{0.78}{0.516} &\\scalebox{0.78}{0.671} &\\scalebox{0.78}{0.561} \\\\ %&\\scalebox{0.78}{1.166} &\\scalebox{0.78}{0.823} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\scalebox{0.78}{0.407} & \\scalebox{0.78}{0.410} & \\scalebox{0.78}{0.414} & \\scalebox{0.78}{0.407} & \\boldres{\\scalebox{0.78}{0.387}} & \\boldres{\\scalebox{0.78}{0.400}} & \\scalebox{0.78}{0.513} & \\scalebox{0.78}{0.496} & \\scalebox{0.78}{0.419} & \\scalebox{0.78}{0.419} &\\secondres{\\scalebox{0.78}{0.400}} &\\secondres{\\scalebox{0.78}{0.406}}  &{\\scalebox{0.78}{0.403}} &{\\scalebox{0.78}{0.407}} & \\scalebox{0.78}{0.485} & \\scalebox{0.78}{0.481}  &\\scalebox{0.78}{0.448} &\\scalebox{0.78}{0.452} &\\scalebox{0.78}{0.481} &\\scalebox{0.78}{0.456} &\\scalebox{0.78}{0.588} &\\scalebox{0.78}{0.517} \\\\ %&\\scalebox{0.78}{0.961} &\\scalebox{0.78}{0.734} \\\\\n    \\midrule\n    \n    \\multirow{5}{*}{\\update{\\rotatebox{90}{\\scalebox{0.95}{ETTm2}}}}\n    &  \\scalebox{0.78}{96} & \\secondres{\\scalebox{0.78}{0.180}} & \\secondres{\\scalebox{0.78}{0.264}} & \\scalebox{0.78}{0.182} & \\scalebox{0.78}{0.265} & \\boldres{\\scalebox{0.78}{0.175}} & \\boldres{\\scalebox{0.78}{0.259}} & \\scalebox{0.78}{0.287} & \\scalebox{0.78}{0.366} & \\scalebox{0.78}{0.207} & \\scalebox{0.78}{0.305} &{\\scalebox{0.78}{0.187}} &\\scalebox{0.78}{0.267} &\\scalebox{0.78}{0.193} &\\scalebox{0.78}{0.292} & \\scalebox{0.78}{0.286} & \\scalebox{0.78}{0.377} &\\scalebox{0.78}{0.203} &\\scalebox{0.78}{0.287} &{\\scalebox{0.78}{0.192}} &\\scalebox{0.78}{0.274} &\\scalebox{0.78}{0.255} &\\scalebox{0.78}{0.339} \\\\ % &\\scalebox{0.78}{0.365} &\\scalebox{0.78}{0.453} \\\\\n    & \\scalebox{0.78}{192} & \\scalebox{0.78}{0.250} & {\\scalebox{0.78}{0.309}} & \\secondres{\\scalebox{0.78}{0.246}} & \\secondres{\\scalebox{0.78}{0.304}} & \\boldres{\\scalebox{0.78}{0.241}} & \\boldres{\\scalebox{0.78}{0.302}} & \\scalebox{0.78}{0.414} & \\scalebox{0.78}{0.492} & \\scalebox{0.78}{0.290} & \\scalebox{0.78}{0.364} &{\\scalebox{0.78}{0.249}} &{\\scalebox{0.78}{0.309}} &\\scalebox{0.78}{0.284} &\\scalebox{0.78}{0.362} & \\scalebox{0.78}{0.399} & \\scalebox{0.78}{0.445} &\\scalebox{0.78}{0.269} &\\scalebox{0.78}{0.328} &\\scalebox{0.78}{0.280} &\\scalebox{0.78}{0.339} &\\scalebox{0.78}{0.281} &\\scalebox{0.78}{0.340} \\\\ %&\\scalebox{0.78}{0.533} &\\scalebox{0.78}{0.563} \\\\\n    & \\scalebox{0.78}{336} & {\\scalebox{0.78}{0.311}} & {\\scalebox{0.78}{0.348}} & \\secondres{\\scalebox{0.78}{0.307}} & \\boldres{\\scalebox{0.78}{0.342}} & \\boldres{\\scalebox{0.78}{0.305}} & \\secondres{\\scalebox{0.78}{0.343}}  & \\scalebox{0.78}{0.597} & \\scalebox{0.78}{0.542}  & \\scalebox{0.78}{0.377} & \\scalebox{0.78}{0.422} &{\\scalebox{0.78}{0.321}} &{\\scalebox{0.78}{0.351}} &\\scalebox{0.78}{0.369} &\\scalebox{0.78}{0.427} & \\scalebox{0.78}{0.637} & \\scalebox{0.78}{0.591} &\\scalebox{0.78}{0.325} &\\scalebox{0.78}{0.366} &\\scalebox{0.78}{0.334} &\\scalebox{0.78}{0.361} &\\scalebox{0.78}{0.339} &\\scalebox{0.78}{0.372} \\\\ %&\\scalebox{0.78}{1.363} &\\scalebox{0.78}{0.887} \\\\\n    & \\scalebox{0.78}{720} & \\scalebox{0.78}{0.412} & \\scalebox{0.78}{0.407} & \\secondres{\\scalebox{0.78}{0.407}} & \\boldres{\\scalebox{0.78}{0.398}} & \\boldres{\\scalebox{0.78}{0.402}} & \\secondres{\\scalebox{0.78}{0.400}} & \\scalebox{0.78}{1.730} & \\scalebox{0.78}{1.042} & \\scalebox{0.78}{0.558} & \\scalebox{0.78}{0.524} &{\\scalebox{0.78}{0.408}} &{\\scalebox{0.78}{0.403}} &\\scalebox{0.78}{0.554} &\\scalebox{0.78}{0.522} & \\scalebox{0.78}{0.960} & \\scalebox{0.78}{0.735} &\\scalebox{0.78}{0.421} &\\scalebox{0.78}{0.415} &\\scalebox{0.78}{0.417} &\\scalebox{0.78}{0.413} &\\scalebox{0.78}{0.433} &\\scalebox{0.78}{0.432} \\\\ %&\\scalebox{0.78}{3.379} &\\scalebox{0.78}{1.338} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & {\\scalebox{0.78}{0.288}} & {\\scalebox{0.78}{0.332}} & \\secondres{\\scalebox{0.78}{0.286}} & \\secondres{\\scalebox{0.78}{0.327}} & \\boldres{\\scalebox{0.78}{0.281}} & \\boldres{\\scalebox{0.78}{0.326}} & \\scalebox{0.78}{0.757} & \\scalebox{0.78}{0.610} & \\scalebox{0.78}{0.358} & \\scalebox{0.78}{0.404} &{\\scalebox{0.78}{0.291}} &{\\scalebox{0.78}{0.333}} &\\scalebox{0.78}{0.350} &\\scalebox{0.78}{0.401} & \\scalebox{0.78}{0.571} & \\scalebox{0.78}{0.537} &\\scalebox{0.78}{0.305} &\\scalebox{0.78}{0.349} &\\scalebox{0.78}{0.306} &\\scalebox{0.78}{0.347} &\\scalebox{0.78}{0.327} &\\scalebox{0.78}{0.371} \\\\ %&\\scalebox{0.78}{1.410} &\\scalebox{0.78}{0.810} \\\\\n    \\midrule\n    \n    \\multirow{5}{*}{\\rotatebox{90}{\\update{\\scalebox{0.95}{ETTh1}}}}\n    &  \\scalebox{0.78}{96} & {\\scalebox{0.78}{0.386}} & {\\scalebox{0.78}{0.405}} & \\scalebox{0.78}{0.386} & \\boldres{\\scalebox{0.78}{0.395}} & \\scalebox{0.78}{0.414} & \\scalebox{0.78}{0.419} & \\scalebox{0.78}{0.423} & \\scalebox{0.78}{0.448} & \\scalebox{0.78}{0.479}& \\scalebox{0.78}{0.464}  &\\secondres{\\scalebox{0.78}{0.384}} &{\\scalebox{0.78}{0.402}} & \\scalebox{0.78}{0.386} &\\secondres{\\scalebox{0.78}{0.400}} & \\scalebox{0.78}{0.654} & \\scalebox{0.78}{0.599} &\\boldres{\\scalebox{0.78}{0.376}} &\\scalebox{0.78}{0.419} &\\scalebox{0.78}{0.513} &\\scalebox{0.78}{0.491} &\\scalebox{0.78}{0.449} &\\scalebox{0.78}{0.459}  \\\\ %&\\scalebox{0.78}{0.865} &\\scalebox{0.78}{0.713} \\\\\n    & \\scalebox{0.78}{192} & \\scalebox{0.78}{0.441} & \\scalebox{0.78}{0.436} & {\\scalebox{0.78}{0.437}} & \\boldres{\\scalebox{0.78}{0.424}} & \\scalebox{0.78}{0.460} & \\scalebox{0.78}{0.445} & \\scalebox{0.78}{0.471} & \\scalebox{0.78}{0.474}  & \\scalebox{0.78}{0.525} & \\scalebox{0.78}{0.492} &\\secondres{\\scalebox{0.78}{0.436}} &\\secondres{\\scalebox{0.78}{0.429}}  &{\\scalebox{0.78}{0.437}} &{\\scalebox{0.78}{0.432}} & \\scalebox{0.78}{0.719} & \\scalebox{0.78}{0.631} &\\boldres{\\scalebox{0.78}{0.420}} &\\scalebox{0.78}{0.448} &\\scalebox{0.78}{0.534} &\\scalebox{0.78}{0.504} &\\scalebox{0.78}{0.500} &\\scalebox{0.78}{0.482} \\\\ %&\\scalebox{0.78}{1.008} &\\scalebox{0.78}{0.792} \\\\\n    & \\scalebox{0.78}{336} & {\\scalebox{0.78}{0.487}} & \\secondres{\\scalebox{0.78}{0.458}} & \\secondres{\\scalebox{0.78}{0.479}} & \\boldres{\\scalebox{0.78}{0.446}} & \\scalebox{0.78}{0.501} & \\scalebox{0.78}{0.466} & \\scalebox{0.78}{0.570} & \\scalebox{0.78}{0.546} & \\scalebox{0.78}{0.565} & \\scalebox{0.78}{0.515} &\\scalebox{0.78}{0.491} &\\scalebox{0.78}{0.469} &{\\scalebox{0.78}{0.481}} & {\\scalebox{0.78}{0.459}} & \\scalebox{0.78}{0.778} & \\scalebox{0.78}{0.659} &\\boldres{\\scalebox{0.78}{0.459}} &{\\scalebox{0.78}{0.465}} &\\scalebox{0.78}{0.588} &\\scalebox{0.78}{0.535} &\\scalebox{0.78}{0.521} &\\scalebox{0.78}{0.496} \\\\ %&\\scalebox{0.78}{1.107} &\\scalebox{0.78}{0.809} \\\\\n    & \\scalebox{0.78}{720} & {\\scalebox{0.78}{0.503}} & {\\scalebox{0.78}{0.491}} & \\boldres{\\scalebox{0.78}{0.481}} & \n    \\boldres{\\scalebox{0.78}{0.470}} & \\secondres{\\scalebox{0.78}{0.500}} & \\secondres{\\scalebox{0.78}{0.488}} & \\scalebox{0.78}{0.653} & \\scalebox{0.78}{0.621} & \\scalebox{0.78}{0.594} & \\scalebox{0.78}{0.558} &\\scalebox{0.78}{0.521} &{\\scalebox{0.78}{0.500}} &\\scalebox{0.78}{0.519} &\\scalebox{0.78}{0.516} & \\scalebox{0.78}{0.836} & \\scalebox{0.78}{0.699} &{\\scalebox{0.78}{0.506}} &{\\scalebox{0.78}{0.507}} &\\scalebox{0.78}{0.643} &\\scalebox{0.78}{0.616} &{\\scalebox{0.78}{0.514}} &\\scalebox{0.78}{0.512}  \\\\ %&\\scalebox{0.78}{1.181} &\\scalebox{0.78}{0.865} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & {\\scalebox{0.78}{0.454}} & \\secondres{\\scalebox{0.78}{0.447}} & \\secondres{\\scalebox{0.78}{0.446}} & \\boldres{\\scalebox{0.78}{0.434}} & \\scalebox{0.78}{0.469} & \\scalebox{0.78}{0.454} & \\scalebox{0.78}{0.529} & \\scalebox{0.78}{0.522} & \\scalebox{0.78}{0.541} & \\scalebox{0.78}{0.507} &\\scalebox{0.78}{0.458} &{\\scalebox{0.78}{0.450}} &{\\scalebox{0.78}{0.456}} &{\\scalebox{0.78}{0.452}} & \\scalebox{0.78}{0.747} & \\scalebox{0.78}{0.647} &\\boldres{\\scalebox{0.78}{0.440}} &\\scalebox{0.78}{0.460} &\\scalebox{0.78}{0.570} &\\scalebox{0.78}{0.537} &\\scalebox{0.78}{0.496} &\\scalebox{0.78}{0.487}  \\\\ %&\\scalebox{0.78}{1.040} &\\scalebox{0.78}{0.795} \\\\\n    \\midrule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{ETTh2}}}\n    &  \\scalebox{0.78}{96} & \\secondres{\\scalebox{0.78}{0.297}} & {\\scalebox{0.78}{0.349}} & \\boldres{\\scalebox{0.78}{0.288}} & \\boldres{\\scalebox{0.78}{0.338}} & {\\scalebox{0.78}{0.302}} & \\secondres{\\scalebox{0.78}{0.348}} & \\scalebox{0.78}{0.745} & \\scalebox{0.78}{0.584} &\\scalebox{0.78}{0.400} & \\scalebox{0.78}{0.440}  & {\\scalebox{0.78}{0.340}} & {\\scalebox{0.78}{0.374}} &{\\scalebox{0.78}{0.333}} &{\\scalebox{0.78}{0.387}} & \\scalebox{0.78}{0.707} & \\scalebox{0.78}{0.621}  &\\scalebox{0.78}{0.358} &\\scalebox{0.78}{0.397} &\\scalebox{0.78}{0.476} &\\scalebox{0.78}{0.458} &\\scalebox{0.78}{0.346} &\\scalebox{0.78}{0.388} \\\\ %&\\scalebox{0.78}{3.755} &\\scalebox{0.78}{1.525} \\\\\n    & \\scalebox{0.78}{192} & \\secondres{\\scalebox{0.78}{0.380}} & \\secondres{\\scalebox{0.78}{0.400}} & \\boldres{\\scalebox{0.78}{0.374}} & \\boldres{\\scalebox{0.78}{0.390}} &{\\scalebox{0.78}{0.388}} & {\\scalebox{0.78}{0.400}} & \\scalebox{0.78}{0.877} & \\scalebox{0.78}{0.656} & \\scalebox{0.78}{0.528} & \\scalebox{0.78}{0.509} & {\\scalebox{0.78}{0.402}} & {\\scalebox{0.78}{0.414}} &\\scalebox{0.78}{0.477} &\\scalebox{0.78}{0.476} & \\scalebox{0.78}{0.860} & \\scalebox{0.78}{0.689} &{\\scalebox{0.78}{0.429}} &{\\scalebox{0.78}{0.439}} &\\scalebox{0.78}{0.512} &\\scalebox{0.78}{0.493} &\\scalebox{0.78}{0.456} &\\scalebox{0.78}{0.452} \\\\ %&\\scalebox{0.78}{5.602} &\\scalebox{0.78}{1.931} \\\\\n    & \\scalebox{0.78}{336} & {\\scalebox{0.78}{0.428}} & \\secondres{\\scalebox{0.78}{0.432}} & \\boldres{\\scalebox{0.78}{0.415}} & \\boldres{\\scalebox{0.78}{0.426}} & \\secondres{\\scalebox{0.78}{0.426}} & {\\scalebox{0.78}{0.433}}& \\scalebox{0.78}{1.043} & \\scalebox{0.78}{0.731} & \\scalebox{0.78}{0.643} & \\scalebox{0.78}{0.571}  & {\\scalebox{0.78}{0.452}} & {\\scalebox{0.78}{0.452}} &\\scalebox{0.78}{0.594} &\\scalebox{0.78}{0.541} & \\scalebox{0.78}{1.000} &\\scalebox{0.78}{0.744} &\\scalebox{0.78}{0.496} &\\scalebox{0.78}{0.487} &\\scalebox{0.78}{0.552} &\\scalebox{0.78}{0.551} &{\\scalebox{0.78}{0.482}} &\\scalebox{0.78}{0.486}\\\\ % &\\scalebox{0.78}{4.721} &\\scalebox{0.78}{1.835} \\\\\n    & \\scalebox{0.78}{720} & \\secondres{\\scalebox{0.78}{0.427}} & \\secondres{\\scalebox{0.78}{0.445}} & \\boldres{\\scalebox{0.78}{0.420}} & \\boldres{\\scalebox{0.78}{0.440}} & {\\scalebox{0.78}{0.431}} & {\\scalebox{0.78}{0.446}} & \\scalebox{0.78}{1.104} & \\scalebox{0.78}{0.763} & \\scalebox{0.78}{0.874} & \\scalebox{0.78}{0.679} & {\\scalebox{0.78}{0.462}} & {\\scalebox{0.78}{0.468}} &\\scalebox{0.78}{0.831} &\\scalebox{0.78}{0.657} & \\scalebox{0.78}{1.249} & \\scalebox{0.78}{0.838} &{\\scalebox{0.78}{0.463}} &{\\scalebox{0.78}{0.474}} &\\scalebox{0.78}{0.562} &\\scalebox{0.78}{0.560} &\\scalebox{0.78}{0.515} &\\scalebox{0.78}{0.511} \\\\ %&\\scalebox{0.78}{3.647} &\\scalebox{0.78}{1.625} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\secondres{\\scalebox{0.78}{0.383}} & \\secondres{\\scalebox{0.78}{0.407}} & \\boldres{\\scalebox{0.78}{0.374}} & \\boldres{\\scalebox{0.78}{0.398}} & {\\scalebox{0.78}{0.387}} & {\\scalebox{0.78}{0.407}} & \\scalebox{0.78}{0.942} & \\scalebox{0.78}{0.684} & \\scalebox{0.78}{0.611} & \\scalebox{0.78}{0.550}  &{\\scalebox{0.78}{0.414}} &{\\scalebox{0.78}{0.427}} &\\scalebox{0.78}{0.559} &\\scalebox{0.78}{0.515} & \\scalebox{0.78}{0.954} & \\scalebox{0.78}{0.723} &\\scalebox{0.78}{{0.437}} &\\scalebox{0.78}{{0.449}} &\\scalebox{0.78}{0.526} &\\scalebox{0.78}{0.516} &\\scalebox{0.78}{0.450} &\\scalebox{0.78}{0.459} \\\\ %&\\scalebox{0.78}{4.431} &\\scalebox{0.78}{1.729} \\\\\n    \\midrule\n    \n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{ECL}}} \n    &  \\scalebox{0.78}{96} & \\boldres{\\scalebox{0.78}{0.148}} & \\boldres{\\scalebox{0.78}{0.240}} & \\scalebox{0.78}{0.201} & \\scalebox{0.78}{0.281} & \\scalebox{0.78}{0.181} & \\secondres{\\scalebox{0.78}{0.270}} & \\scalebox{0.78}{0.219} & \\scalebox{0.78}{0.314} & \\scalebox{0.78}{0.237} & \\scalebox{0.78}{0.329} &\\secondres{\\scalebox{0.78}{0.168}} &\\scalebox{0.78}{0.272} &\\scalebox{0.78}{0.197} &\\scalebox{0.78}{0.282} & \\scalebox{0.78}{0.247} & \\scalebox{0.78}{0.345} &\\scalebox{0.78}{0.193} &\\scalebox{0.78}{0.308} &{\\scalebox{0.78}{0.169}} &{\\scalebox{0.78}{0.273}} &\\scalebox{0.78}{0.201} &\\scalebox{0.78}{0.317}  \\\\ %&\\scalebox{0.78}{0.274} &\\scalebox{0.78}{0.368} \\\\\n    & \\scalebox{0.78}{192} & \\boldres{\\scalebox{0.78}{0.162}} & \\boldres{\\scalebox{0.78}{0.253}} & \\scalebox{0.78}{0.201} & \\scalebox{0.78}{0.283} & \\scalebox{0.78}{0.188} & \\secondres{\\scalebox{0.78}{0.274}} & \\scalebox{0.78}{0.231} & \\scalebox{0.78}{0.322} & \\scalebox{0.78}{0.236} & \\scalebox{0.78}{0.330} &{\\scalebox{0.78}{0.184}} &\\scalebox{0.78}{0.289} &\\scalebox{0.78}{0.196} &{\\scalebox{0.78}{0.285}} & \\scalebox{0.78}{0.257} & \\scalebox{0.78}{0.355} &\\scalebox{0.78}{0.201} &\\scalebox{0.78}{0.315} &\\secondres{\\scalebox{0.78}{0.182}} &\\scalebox{0.78}{0.286} &\\scalebox{0.78}{0.222} &\\scalebox{0.78}{0.334} \\\\ %&\\scalebox{0.78}{0.296} &\\scalebox{0.78}{0.386} \\\\\n    & \\scalebox{0.78}{336} & \\boldres{\\scalebox{0.78}{0.178}} & \\boldres{\\scalebox{0.78}{0.269}} & \\scalebox{0.78}{0.215} & \\scalebox{0.78}{0.298} & \\scalebox{0.78}{0.204} & \\secondres{\\scalebox{0.78}{0.293}} & \\scalebox{0.78}{0.246} & \\scalebox{0.78}{0.337} & \\scalebox{0.78}{0.249} & \\scalebox{0.78}{0.344} &\\secondres{\\scalebox{0.78}{0.198}} &{\\scalebox{0.78}{0.300}} &\\scalebox{0.78}{0.209} &{\\scalebox{0.78}{0.301}} & \\scalebox{0.78}{0.269} & \\scalebox{0.78}{0.369} &\\scalebox{0.78}{0.214} &\\scalebox{0.78}{0.329} &{\\scalebox{0.78}{0.200}} &\\scalebox{0.78}{0.304} &\\scalebox{0.78}{0.231} &\\scalebox{0.78}{0.338}  \\\\ %&\\scalebox{0.78}{0.300} &\\scalebox{0.78}{0.394} \\\\\n    & \\scalebox{0.78}{720} & \\secondres{\\scalebox{0.78}{0.225}} & \\boldres{\\scalebox{0.78}{0.317}} & \\scalebox{0.78}{0.257} & \\scalebox{0.78}{0.331} & \\scalebox{0.78}{0.246} & \\scalebox{0.78}{0.324} & \\scalebox{0.78}{0.280} & \\scalebox{0.78}{0.363} & \\scalebox{0.78}{0.284} & \\scalebox{0.78}{0.373} &\\boldres{\\scalebox{0.78}{0.220}} &\\secondres{\\scalebox{0.78}{0.320}} &\\scalebox{0.78}{0.245} &\\scalebox{0.78}{0.333} & \\scalebox{0.78}{0.299} & \\scalebox{0.78}{0.390} &\\scalebox{0.78}{0.246} &\\scalebox{0.78}{0.355} &{\\scalebox{0.78}{0.222}} &{\\scalebox{0.78}{0.321}} &\\scalebox{0.78}{0.254} &\\scalebox{0.78}{0.361} \\\\ %&\\scalebox{0.78}{0.373} &\\scalebox{0.78}{0.439} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.178}} & \\boldres{\\scalebox{0.78}{0.270}} & \\scalebox{0.78}{0.219} & \\scalebox{0.78}{0.298} & \\scalebox{0.78}{0.205} & \\secondres{\\scalebox{0.78}{0.290}} & \\scalebox{0.78}{0.244} & \\scalebox{0.78}{0.334} & \\scalebox{0.78}{0.251} & \\scalebox{0.78}{0.344} &\\secondres{\\scalebox{0.78}{0.192}} &\\scalebox{0.78}{0.295} &\\scalebox{0.78}{0.212} &\\scalebox{0.78}{0.300} & \\scalebox{0.78}{0.268} & \\scalebox{0.78}{0.365} &\\scalebox{0.78}{0.214} &\\scalebox{0.78}{0.327} &{\\scalebox{0.78}{0.193}} &{\\scalebox{0.78}{0.296}} &\\scalebox{0.78}{0.227} &\\scalebox{0.78}{0.338} \\\\ %&\\scalebox{0.78}{0.311} &\\scalebox{0.78}{0.397} \\\\\n    \\midrule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\update{\\scalebox{0.95}{Exchange}}}}\n    &  \\scalebox{0.78}{96} & \\boldres{\\scalebox{0.78}{0.086}} & \\secondres{\\scalebox{0.78}{0.206}} & \\scalebox{0.78}{0.093} & \\scalebox{0.78}{0.217} & \\secondres{\\scalebox{0.78}{0.088}} & \\boldres{\\scalebox{0.78}{0.205}} & \\scalebox{0.78}{0.256} & \\scalebox{0.78}{0.367} & \\scalebox{0.78}{0.094} & \\scalebox{0.78}{0.218} & \\scalebox{0.78}{0.107} & \\scalebox{0.78}{0.234} & \\scalebox{0.78}{0.088} & \\scalebox{0.78}{0.218} & \\scalebox{0.78}{0.267} & \\scalebox{0.78}{0.396} & \\scalebox{0.78}{0.148} & \\scalebox{0.78}{0.278} & \\scalebox{0.78}{0.111} & \\scalebox{0.78}{0.237} & \\scalebox{0.78}{0.197} & \\scalebox{0.78}{0.323} \\\\ %& \\scalebox{0.78}{0.847} & \\scalebox{0.78}{0.752} \\\\\n    &  \\scalebox{0.78}{192} & \\scalebox{0.78}{0.177} & \\secondres{\\scalebox{0.78}{0.299}} & \\scalebox{0.78}{0.184} & \\scalebox{0.78}{0.307} & \\boldres{\\scalebox{0.78}{0.176}} & \\boldres{\\scalebox{0.78}{0.299}} & \\scalebox{0.78}{0.470} & \\scalebox{0.78}{0.509} & \\scalebox{0.78}{0.184} & \\scalebox{0.78}{0.307} & \\scalebox{0.78}{0.226} & \\scalebox{0.78}{0.344} & \\secondres{\\scalebox{0.78}{0.176}} & \\scalebox{0.78}{0.315} & \\scalebox{0.78}{0.351} & \\scalebox{0.78}{0.459} & \\scalebox{0.78}{0.271} & \\scalebox{0.78}{0.315} & \\scalebox{0.78}{0.219} & \\scalebox{0.78}{0.335} & \\scalebox{0.78}{0.300} & \\scalebox{0.78}{0.369} \\\\ %& \\scalebox{0.78}{1.204} & \\scalebox{0.78}{0.895} \\\\\n    &  \\scalebox{0.78}{336} & \\scalebox{0.78}{0.331} & \\secondres{\\scalebox{0.78}{0.417}} & \\scalebox{0.78}{0.351} & \\scalebox{0.78}{0.432}& \\boldres{\\scalebox{0.78}{0.301}} & \\boldres{\\scalebox{0.78}{0.397}} & \\scalebox{0.78}{1.268} & \\scalebox{0.78}{0.883} & \\scalebox{0.78}{0.349} & \\scalebox{0.78}{0.431} & \\scalebox{0.78}{0.367} & \\scalebox{0.78}{0.448} & \\secondres{\\scalebox{0.78}{0.313}} & \\scalebox{0.78}{0.427} & \\scalebox{0.78}{1.324} & \\scalebox{0.78}{0.853} & \\scalebox{0.78}{0.460} & \\scalebox{0.78}{0.427} & \\scalebox{0.78}{0.421} & \\scalebox{0.78}{0.476} & \\scalebox{0.78}{0.509} & \\scalebox{0.78}{0.524} \\\\ %& \\scalebox{0.78}{1.672} & \\scalebox{0.78}{1.036} \\\\\n    &  \\scalebox{0.78}{720} & \\secondres{\\scalebox{0.78}{0.847}} & \\boldres{\\scalebox{0.78}{0.691}} & \\scalebox{0.78}{0.886} & \\scalebox{0.78}{0.714} & \\scalebox{0.78}{0.901} & \\scalebox{0.78}{0.714} & \\scalebox{0.78}{1.767} & \\scalebox{0.78}{1.068} & \\scalebox{0.78}{0.852} & \\scalebox{0.78}{0.698} & \\scalebox{0.78}{0.964} & \\scalebox{0.78}{0.746} & \\boldres{\\scalebox{0.78}{0.839}} & \\scalebox{0.78}{0.695} & \\scalebox{0.78}{1.058} & \\scalebox{0.78}{0.797} & \\scalebox{0.78}{1.195} & \\secondres{\\scalebox{0.78}{0.695}} & \\scalebox{0.78}{1.092} & \\scalebox{0.78}{0.769} & \\scalebox{0.78}{1.447} & \\scalebox{0.78}{0.941} \\\\ %& \\scalebox{0.78}{2.478} & \\scalebox{0.78}{1.310} \\\\\n    \\cmidrule(lr){2-24}\n    &  \\scalebox{0.78}{Avg} & \\secondres{\\scalebox{0.78}{0.360}} & \\boldres{\\scalebox{0.78}{0.403}} & \\scalebox{0.78}{0.378} & \\scalebox{0.78}{0.417} & \\scalebox{0.78}{0.367} & \\secondres{\\scalebox{0.78}{0.404}} & \\scalebox{0.78}{0.940} & \\scalebox{0.78}{0.707} & \\scalebox{0.78}{0.370} & \\scalebox{0.78}{0.413} & \\scalebox{0.78}{0.416} & \\scalebox{0.78}{0.443} & \\boldres{\\scalebox{0.78}{0.354}} & \\scalebox{0.78}{0.414} & \\scalebox{0.78}{0.750} & \\scalebox{0.78}{0.626} & \\scalebox{0.78}{0.519} & \\scalebox{0.78}{0.429} & \\scalebox{0.78}{0.461} & \\scalebox{0.78}{0.454} & \\scalebox{0.78}{0.613} & \\scalebox{0.78}{0.539} \\\\ %& \\scalebox{0.78}{1.550} & \\scalebox{0.78}{0.998} \\\\\n\n\n    \\midrule\n    \n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Traffic}}} \n    & \\scalebox{0.78}{96} & \\boldres{\\scalebox{0.78}{0.395}} & \\boldres{\\scalebox{0.78}{0.268}} & \\scalebox{0.78}{0.649} & \\scalebox{0.78}{0.389} & \\secondres{\\scalebox{0.78}{0.462}} & \\scalebox{0.78}{0.295} & \\scalebox{0.78}{0.522} & \\secondres{\\scalebox{0.78}{0.290}} & \\scalebox{0.78}{0.805} & \\scalebox{0.78}{0.493} &{\\scalebox{0.78}{0.593}} &{\\scalebox{0.78}{0.321}} &\\scalebox{0.78}{0.650} &\\scalebox{0.78}{0.396} & \\scalebox{0.78}{0.788} & \\scalebox{0.78}{0.499} &{\\scalebox{0.78}{0.587}} &\\scalebox{0.78}{0.366} &\\scalebox{0.78}{0.612} &{\\scalebox{0.78}{0.338}} &\\scalebox{0.78}{0.613} &\\scalebox{0.78}{0.388} \\\\ %&\\scalebox{0.78}{0.719} &\\scalebox{0.78}{0.391}\\\\\n    & \\scalebox{0.78}{192} & \\boldres{\\scalebox{0.78}{0.417}} & \\boldres{\\scalebox{0.78}{0.276}} & \\scalebox{0.78}{0.601} & \\scalebox{0.78}{0.366} & \\secondres{\\scalebox{0.78}{0.466}} & \\scalebox{0.78}{0.296} & \\scalebox{0.78}{0.530} & \\secondres{\\scalebox{0.78}{0.293}} & \\scalebox{0.78}{0.756} & \\scalebox{0.78}{0.474} &\\scalebox{0.78}{0.617} &{\\scalebox{0.78}{0.336}} &{\\scalebox{0.78}{0.598}} &\\scalebox{0.78}{0.370} & \\scalebox{0.78}{0.789} & \\scalebox{0.78}{0.505} &\\scalebox{0.78}{0.604} &\\scalebox{0.78}{0.373} &\\scalebox{0.78}{0.613} &{\\scalebox{0.78}{0.340}} &\\scalebox{0.78}{0.616} &\\scalebox{0.78}{0.382}  \\\\ %&\\scalebox{0.78}{0.696} &\\scalebox{0.78}{0.379} \\\\\n    & \\scalebox{0.78}{336} & \\boldres{\\scalebox{0.78}{0.433}} & \\boldres{\\scalebox{0.78}{0.283}} & \\scalebox{0.78}{0.609} & \\scalebox{0.78}{0.369} & \\secondres{\\scalebox{0.78}{0.482}} & \\secondres{\\scalebox{0.78}{0.304}} & \\scalebox{0.78}{0.558} & \\scalebox{0.78}{0.305}  & \\scalebox{0.78}{0.762} & \\scalebox{0.78}{0.477} &\\scalebox{0.78}{0.629} &{\\scalebox{0.78}{0.336}}  &{\\scalebox{0.78}{0.605}} &\\scalebox{0.78}{0.373} & \\scalebox{0.78}{0.797} & \\scalebox{0.78}{0.508}&\\scalebox{0.78}{0.621} &\\scalebox{0.78}{0.383} &\\scalebox{0.78}{0.618} &{\\scalebox{0.78}{0.328}} &\\scalebox{0.78}{0.622} &\\scalebox{0.78}{0.337} \\\\ %&\\scalebox{0.78}{0.777} &\\scalebox{0.78}{0.420} \\\\\n    & \\scalebox{0.78}{720} & \\boldres{\\scalebox{0.78}{0.467}} & \\boldres{\\scalebox{0.78}{0.302}} & \\scalebox{0.78}{0.647} & \\scalebox{0.78}{0.387} & \\secondres{\\scalebox{0.78}{0.514}} & \\secondres{\\scalebox{0.78}{0.322}} & \\scalebox{0.78}{0.589} & \\scalebox{0.78}{0.328}  & \\scalebox{0.78}{0.719} & \\scalebox{0.78}{0.449} &\\scalebox{0.78}{0.640} &{\\scalebox{0.78}{0.350}} &\\scalebox{0.78}{0.645} &\\scalebox{0.78}{0.394} & \\scalebox{0.78}{0.841} & \\scalebox{0.78}{0.523} &{\\scalebox{0.78}{0.626}} &\\scalebox{0.78}{0.382} &\\scalebox{0.78}{0.653} &{\\scalebox{0.78}{0.355}} &\\scalebox{0.78}{0.660} &\\scalebox{0.78}{0.408} \\\\ %&\\scalebox{0.78}{0.864} &\\scalebox{0.78}{0.472} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.428}} & \\boldres{\\scalebox{0.78}{0.282}} & \\scalebox{0.78}{0.626} & \\scalebox{0.78}{0.378} & \\secondres{\\scalebox{0.78}{0.481}} & \\secondres{\\scalebox{0.78}{0.304}}& \\scalebox{0.78}{0.550} & \\secondres{\\scalebox{0.78}{0.304}} & \\scalebox{0.78}{0.760} & \\scalebox{0.78}{0.473} &{\\scalebox{0.78}{0.620}} &{\\scalebox{0.78}{0.336}} &\\scalebox{0.78}{0.625} &\\scalebox{0.78}{0.383} & \\scalebox{0.78}{0.804} & \\scalebox{0.78}{0.509} &{\\scalebox{0.78}{0.610}} &\\scalebox{0.78}{0.376} &\\scalebox{0.78}{0.624} &{\\scalebox{0.78}{0.340}} &\\scalebox{0.78}{0.628} &\\scalebox{0.78}{0.379} \\\\ %&\\scalebox{0.78}{0.764} &\\scalebox{0.78}{0.416} \\\\\n    \\midrule\n    \n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Weather}}} \n    &  \\scalebox{0.78}{96} & \\scalebox{0.78}{0.174} & \\boldres{\\scalebox{0.78}{0.214}} & \\scalebox{0.78}{0.192} & \\scalebox{0.78}{0.232} & \\scalebox{0.78}{0.177} & \\secondres{\\scalebox{0.78}{0.218}} & \\boldres{\\scalebox{0.78}{0.158}} & \\scalebox{0.78}{0.230}  & \\scalebox{0.78}{0.202} & \\scalebox{0.78}{0.261} &\\secondres{\\scalebox{0.78}{0.172}} &{\\scalebox{0.78}{0.220}} & \\scalebox{0.78}{0.196} &\\scalebox{0.78}{0.255} & \\scalebox{0.78}{0.221} & \\scalebox{0.78}{0.306} & \\scalebox{0.78}{0.217} &\\scalebox{0.78}{0.296} & {\\scalebox{0.78}{0.173}} &{\\scalebox{0.78}{0.223}} & \\scalebox{0.78}{0.266} &\\scalebox{0.78}{0.336} \\\\ %& \\scalebox{0.78}{0.300} &\\scalebox{0.78}{0.384}  \\\\\n    & \\scalebox{0.78}{192} & \\scalebox{0.78}{0.221} & \\boldres{\\scalebox{0.78}{0.254}} & \\scalebox{0.78}{0.240} & \\scalebox{0.78}{0.271} & \\scalebox{0.78}{0.225} & \\scalebox{0.78}{0.259} & \\boldres{\\scalebox{0.78}{0.206}} & \\scalebox{0.78}{0.277} & \\scalebox{0.78}{0.242} & \\scalebox{0.78}{0.298} &\\secondres{\\scalebox{0.78}{0.219}} &\\secondres{\\scalebox{0.78}{0.261}}  & \\scalebox{0.78}{0.237} &\\scalebox{0.78}{0.296} & \\scalebox{0.78}{0.261} & \\scalebox{0.78}{0.340} & \\scalebox{0.78}{0.276} &\\scalebox{0.78}{0.336} & \\scalebox{0.78}{0.245} &\\scalebox{0.78}{0.285} & \\scalebox{0.78}{0.307} &\\scalebox{0.78}{0.367} \\\\ %& \\scalebox{0.78}{0.598} &\\scalebox{0.78}{0.544} \\\\\n    & \\scalebox{0.78}{336} & \\secondres{\\scalebox{0.78}{0.278}} & \\boldres{\\scalebox{0.78}{0.296}} & \\scalebox{0.78}{0.292} & \\scalebox{0.78}{0.307} & \\scalebox{0.78}{0.278} & \\secondres{\\scalebox{0.78}{0.297}} & \\boldres{\\scalebox{0.78}{0.272}} & \\scalebox{0.78}{0.335} & \\scalebox{0.78}{0.287} & \\scalebox{0.78}{0.335} &{\\scalebox{0.78}{0.280}} &{\\scalebox{0.78}{0.306}} & \\scalebox{0.78}{0.283} &\\scalebox{0.78}{0.335} & \\scalebox{0.78}{0.309} & \\scalebox{0.78}{0.378} & \\scalebox{0.78}{0.339} &\\scalebox{0.78}{0.380} & \\scalebox{0.78}{0.321} &\\scalebox{0.78}{0.338} & \\scalebox{0.78}{0.359} &\\scalebox{0.78}{0.395}\\\\ % &\\scalebox{0.78}{0.578} &\\scalebox{0.78}{0.523} \\\\\n    & \\scalebox{0.78}{720} & \\scalebox{0.78}{0.358} & \\boldres{\\scalebox{0.78}{0.347}} & \\scalebox{0.78}{0.364} & \\scalebox{0.78}{0.353} & \\scalebox{0.78}{0.354} & \\secondres{\\scalebox{0.78}{0.348}} & \\scalebox{0.78}{0.398} & \\scalebox{0.78}{0.418} & \\secondres{\\scalebox{0.78}{0.351}} & \\scalebox{0.78}{0.386} &\\scalebox{0.78}{0.365} &{\\scalebox{0.78}{0.359}} & \\boldres{\\scalebox{0.78}{0.345}} &{\\scalebox{0.78}{0.381}} & \\scalebox{0.78}{0.377} & \\scalebox{0.78}{0.427} & \\scalebox{0.78}{0.403} &\\scalebox{0.78}{0.428} & \\scalebox{0.78}{0.414} &\\scalebox{0.78}{0.410} & \\scalebox{0.78}{0.419} &\\scalebox{0.78}{0.428} \\\\ %& \\scalebox{0.78}{1.059} &\\scalebox{0.78}{0.741} \\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.258}} & \\boldres{\\scalebox{0.78}{0.278}} & \\scalebox{0.78}{0.272} & \\scalebox{0.78}{0.291} & \\secondres{\\scalebox{0.78}{0.259}} & \\secondres{\\scalebox{0.78}{0.281}} & \\scalebox{0.78}{0.259} & \\scalebox{0.78}{0.315} & \\scalebox{0.78}{0.271} & \\scalebox{0.78}{0.320} &{\\scalebox{0.78}{0.259}} &{\\scalebox{0.78}{0.287}} &\\scalebox{0.78}{0.265} &\\scalebox{0.78}{0.317} & \\scalebox{0.78}{0.292} & \\scalebox{0.78}{0.363} &\\scalebox{0.78}{0.309} &\\scalebox{0.78}{0.360} &\\scalebox{0.78}{0.288} &\\scalebox{0.78}{0.314} &\\scalebox{0.78}{0.338} &\\scalebox{0.78}{0.382} \\\\ %&\\scalebox{0.78}{0.634} &\\scalebox{0.78}{0.548} \\\\\n    \\midrule\n    \n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Solar-Energy}}} \n    &  \\scalebox{0.78}{96} &\\boldres{\\scalebox{0.78}{0.203}} &\\boldres{\\scalebox{0.78}{0.237}} & \\scalebox{0.78}{0.322} & \\scalebox{0.78}{0.339} & \\secondres{\\scalebox{0.78}{0.234}} & \\secondres{\\scalebox{0.78}{0.286}} &\\scalebox{0.78}{0.310} &\\scalebox{0.78}{0.331} &\\scalebox{0.78}{0.312} &\\scalebox{0.78}{0.399} &\\scalebox{0.78}{0.250} &\\scalebox{0.78}{0.292} &\\scalebox{0.78}{0.290} &\\scalebox{0.78}{0.378} &\\scalebox{0.78}{0.237} &\\scalebox{0.78}{0.344} &\\scalebox{0.78}{0.242} &\\scalebox{0.78}{0.342} &\\scalebox{0.78}{0.215} &\\scalebox{0.78}{0.249} &\\scalebox{0.78}{0.884} &\\scalebox{0.78}{0.711}\\\\ % &\\scalebox{0.78}{0.236} &\\secondres{\\scalebox{0.78}{0.259}} \\\\\n    & \\scalebox{0.78}{192} &\\boldres{\\scalebox{0.78}{0.233}} &\\boldres{\\scalebox{0.78}{0.261}} & \\scalebox{0.78}{0.359} & \\scalebox{0.78}{0.356}& \\secondres{\\scalebox{0.78}{0.267}} & \\secondres{\\scalebox{0.78}{0.310}} &\\scalebox{0.78}{0.734} &\\scalebox{0.78}{0.725} &\\scalebox{0.78}{0.339} &\\scalebox{0.78}{0.416} &\\scalebox{0.78}{0.296} &\\scalebox{0.78}{0.318} &\\scalebox{0.78}{0.320} &\\scalebox{0.78}{0.398} &\\scalebox{0.78}{0.280} &\\scalebox{0.78}{0.380} &\\scalebox{0.78}{0.285} &\\scalebox{0.78}{0.380} &\\scalebox{0.78}{0.254} &\\scalebox{0.78}{0.272} &\\scalebox{0.78}{0.834} &\\scalebox{0.78}{0.692} \\\\ %&\\boldres{\\scalebox{0.78}{0.217}} &\\secondres{\\scalebox{0.78}{0.269}} \\\\\n    & \\scalebox{0.78}{336} &\\boldres{\\scalebox{0.78}{0.248}} &\\boldres{\\scalebox{0.78}{0.273}} & \\scalebox{0.78}{0.397} & \\scalebox{0.78}{0.369}& \\secondres{\\scalebox{0.78}{0.290}}  &\\secondres{\\scalebox{0.78}{0.315}} &\\scalebox{0.78}{0.750} &\\scalebox{0.78}{0.735} &\\scalebox{0.78}{0.368} &\\scalebox{0.78}{0.430} &\\scalebox{0.78}{0.319} &\\scalebox{0.78}{0.330} &\\scalebox{0.78}{0.353} &\\scalebox{0.78}{0.415} &\\scalebox{0.78}{0.304} &\\scalebox{0.78}{0.389} &\\scalebox{0.78}{0.282} &\\scalebox{0.78}{0.376} &\\scalebox{0.78}{0.290} &\\scalebox{0.78}{0.296} &\\scalebox{0.78}{0.941} &\\scalebox{0.78}{0.723} \\\\ %&\\secondres{\\scalebox{0.78}{0.249}} &\\secondres{\\scalebox{0.78}{0.283}}\\\\\n    & \\scalebox{0.78}{720} &\\boldres{\\scalebox{0.78}{0.249}} &\\boldres{\\scalebox{0.78}{0.275}} & \\scalebox{0.78}{0.397} & \\scalebox{0.78}{0.356} & \\secondres{\\scalebox{0.78}{0.289}} &\\secondres{\\scalebox{0.78}{0.317}} &\\scalebox{0.78}{0.769} &\\scalebox{0.78}{0.765} &\\scalebox{0.78}{0.370} &\\scalebox{0.78}{0.425} &\\scalebox{0.78}{0.338} &\\scalebox{0.78}{0.337} &\\scalebox{0.78}{0.356} &\\scalebox{0.78}{0.413} &\\scalebox{0.78}{0.308} &\\scalebox{0.78}{0.388} &\\scalebox{0.78}{0.357} &\\scalebox{0.78}{0.427} &\\scalebox{0.78}{0.285} &\\scalebox{0.78}{0.295} &\\scalebox{0.78}{0.882} &\\scalebox{0.78}{0.717} \\\\ %&\\boldres{\\scalebox{0.78}{0.241}} &\\boldres{\\scalebox{0.78}{0.317}}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} &\\boldres{\\scalebox{0.78}{0.233}} &\\boldres{\\scalebox{0.78}{0.262}} & \\scalebox{0.78}{0.369} & \\scalebox{0.78}{0.356} &\\secondres{\\scalebox{0.78}{0.270}} &\\secondres{\\scalebox{0.78}{0.307}} &\\scalebox{0.78}{0.641} &\\scalebox{0.78}{0.639} &\\scalebox{0.78}{0.347} &\\scalebox{0.78}{0.417} &\\scalebox{0.78}{0.301} &\\scalebox{0.78}{0.319} &\\scalebox{0.78}{0.330} &\\scalebox{0.78}{0.401} &\\scalebox{0.78}{0.282} &\\scalebox{0.78}{0.375} &\\scalebox{0.78}{0.291} &\\scalebox{0.78}{0.381} &\\scalebox{0.78}{0.261} &\\scalebox{0.78}{0.381} &\\scalebox{0.78}{0.885} &\\scalebox{0.78}{0.711} \\\\ %&\\secondres{\\scalebox{0.78}{0.235}} &\\secondres{\\scalebox{0.78}{0.280}}\\\\\n    \\midrule\n     \\multicolumn{2}{c|}{\\scalebox{0.78}{{$1^{\\text{st}}$ Count}}} & \\scalebox{0.78}{\\boldres{16}} & \\scalebox{0.78}{\\boldres{22}} & \\scalebox{0.78}{6} & \\scalebox{0.78}{\\secondres{12}} & \\scalebox{0.78}{\\secondres{12}} & \\scalebox{0.78}{11} & \\scalebox{0.78}{3} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{1} & \\scalebox{0.78}{0} & \\scalebox{0.78}{3} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{4} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} \\\\ %& \\scalebox{0.78}{2} & \\scalebox{0.78}{1}\\\\\n    \\bottomrule\n  \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n}\n\\end{table}",
            "tab:full_market_results": "\\begin{table}[htbp]\n  \\caption{\\update{Full results of the Market dataset}. We compare extensive competitive models on the real-world transaction forecasting task. \\emph{Avg} means the average results from all prediction lengths.}\\label{tab:full_market_results}\n  \\vskip 0.05in\n  \\centering\n  \\resizebox{1\\columnwidth}{!}{\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{1pt}\n  \\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n    \\toprule\n    \\multicolumn{2}{c}{\\multirow{2}{*}{Models}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.7}{\\textbf{iTransformer}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{\\update{RLinear}}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{PatchTST}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Crossformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{TiDE}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{TimesNet}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{DLinear}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{SCINet}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{FEDformer}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Stationary}}} &\n    \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Autoformer}}} \\\\ %& \\multicolumn{2}{c}{\\rotatebox{0}{\\scalebox{0.8}{Informer}}} \\\\\n    \\multicolumn{2}{c}{} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{(\\textbf{Ours})}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{li2023revisiting}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{PatchTST}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Crossformer}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{das2023long}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Timesnet}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{DLinear}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{SCINet}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{fedformer}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Stationary}}} \n    & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Autoformer}}} \\\\ % & \\multicolumn{2}{c}{\\scalebox{0.8}{\\citeyearpar{Informer}}} \\\\\n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\\cmidrule(lr){11-12}\\cmidrule(lr){13-14}\\cmidrule(lr){15-16}\\cmidrule(lr){17-18}\\cmidrule(lr){19-20}\\cmidrule(lr){21-22}\\cmidrule(lr){23-24}\n    \\multicolumn{2}{c}{Metric} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE} & \\scalebox{0.78}{MSE} & \\scalebox{0.78}{MAE}\\\\\n    \\toprule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Merchant}}} \n    & \\scalebox{0.78}{12} & \\boldres{\\scalebox{0.78}{0.058}} & \\boldres{\\scalebox{0.78}{0.126}} & \\scalebox{0.78}{0.139} & \\scalebox{0.78}{0.232}  &\\scalebox{0.78}{0.072} & \\scalebox{0.78}{0.155} & \\secondres{\\scalebox{0.78}{{0.068}}} & \\secondres{\\scalebox{0.78}{{0.141}}} & \\scalebox{0.78}{0.173} & \\scalebox{0.78}{0.273} & \\scalebox{0.78}{0.088} & \\scalebox{0.78}{0.177} & \\scalebox{0.78}{0.093} & \\scalebox{0.78}{0.183} & \\scalebox{0.78}{0.202} & \\scalebox{0.78}{0.310} &\\scalebox{0.78}{0.277} &\\scalebox{0.78}{0.384} &\\scalebox{0.78}{0.143} &\\scalebox{0.78}{0.243} & \\scalebox{0.78}{0.365} & \\scalebox{0.78}{0.444} \\\\ % &\\scalebox{0.78}{0.489} &\\scalebox{0.78}{0.459}\\\\\n    & \\scalebox{0.78}{24} & \\boldres{\\scalebox{0.78}{0.066}} & \\boldres{\\scalebox{0.78}{0.138}} & \\scalebox{0.78}{0.155} & \\scalebox{0.78}{0.250} & \\secondres{\\scalebox{0.78}{{0.079}}} & \\scalebox{0.78}{0.164} & \\scalebox{0.78}{0.091} & \\secondres{\\scalebox{0.78}{{0.161}}} & \\scalebox{0.78}{0.170} & \\scalebox{0.78}{0.274} & \\scalebox{0.78}{0.103} & \\scalebox{0.78}{0.195}  & \\scalebox{0.78}{0.105} & \\scalebox{0.78}{0.200} & \\scalebox{0.78}{0.215} & \\scalebox{0.78}{0.323} &\\scalebox{0.78}{0.268} &\\scalebox{0.78}{0.378} &\\scalebox{0.78}{0.167} &\\scalebox{0.78}{0.270} & \\scalebox{0.78}{0.669} & \\scalebox{0.78}{0.636} \\\\ % &\\scalebox{0.78}{0.507} &\\scalebox{0.78}{0.461}\\\\\n    & \\scalebox{0.78}{72} & \\boldres{\\scalebox{0.78}{0.079}} & \\boldres{\\scalebox{0.78}{0.157}} & \\scalebox{0.78}{0.156} & \\scalebox{0.78}{0.252} &\\scalebox{0.78}{0.090} & \\scalebox{0.78}{0.180} & \\scalebox{0.78}{0.123} & \\scalebox{0.78}{0.202} & \\scalebox{0.78}{0.197} & \\scalebox{0.78}{0.298} & \\secondres{\\scalebox{0.78}{{0.089}}} & \\secondres{\\scalebox{0.78}{{0.180}}} & \\scalebox{0.78}{0.116} & \\scalebox{0.78}{0.215} & \\scalebox{0.78}{0.388} & \\scalebox{0.78}{0.431}  & \\scalebox{0.78}{0.281}& \\scalebox{0.78}{0.390} & \\scalebox{0.78}{0.193} & \\scalebox{0.78}{0.300} & \\scalebox{0.78}{0.404} & \\scalebox{0.78}{0.479} \\\\ % &\\scalebox{0.78}{0.523} &\\scalebox{0.78}{0.473}\\\\\n    & \\scalebox{0.78}{144} & \\boldres{\\scalebox{0.78}{0.086}} & \\boldres{\\scalebox{0.78}{0.167}} & \\scalebox{0.78}{0.157} & \\scalebox{0.78}{0.253} &\\scalebox{0.78}{0.093} & \\scalebox{0.78}{0.185} & \\scalebox{0.78}{0.185} & \\scalebox{0.78}{0.218} & \\scalebox{0.78}{0.208} & \\scalebox{0.78}{0.311} & \\secondres{\\scalebox{0.78}{{0.091}}} & \\secondres{\\scalebox{0.78}{{0.183}}} & \\scalebox{0.78}{0.124} & \\scalebox{0.78}{0.225}  & \\scalebox{0.78}{0.459} & \\scalebox{0.78}{0.477} & \\scalebox{0.78}{0.359} & \\scalebox{0.78}{0.453} & \\scalebox{0.78}{0.183} & \\scalebox{0.78}{0.294} & \\scalebox{0.78}{0.536} & \\scalebox{0.78}{0.566} \\\\ % &\\scalebox{0.78}{0.543} &\\scalebox{0.78}{0.483}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.072}} & \\boldres{\\scalebox{0.78}{0.147}} & \\scalebox{0.78}{0.152} & \\scalebox{0.78}{0.247} & \\secondres{\\scalebox{0.78}{{0.084}}} & \\secondres{\\scalebox{0.78}{{0.171}}} &\\scalebox{0.78}{0.117} &\\scalebox{0.78}{0.181} & \\scalebox{0.78}{0.187} & \\scalebox{0.78}{0.289} & \\scalebox{0.78}{0.093} & \\scalebox{0.78}{0.184}  & \\scalebox{0.78}{0.110} & \\scalebox{0.78}{0.206} & \\scalebox{0.78}{0.316} & \\scalebox{0.78}{0.385}  &\\scalebox{0.78}{0.296} &\\scalebox{0.78}{0.401} &\\scalebox{0.78}{0.172} &\\scalebox{0.78}{0.277} & \\scalebox{0.78}{0.494} & \\scalebox{0.78}{0.531} \\\\ % &\\scalebox{0.78}{0.516} &\\scalebox{0.78}{0.469}\\\\\n    \\midrule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Wealth}}} \n    & \\scalebox{0.78}{12} & \\boldres{\\scalebox{0.78}{0.189}} & \\boldres{\\scalebox{0.78}{0.205}} & \\scalebox{0.78}{0.479} & \\scalebox{0.78}{0.411} &\\secondres{\\scalebox{0.78}{{0.255}}} & \\scalebox{0.78}{0.250} & \\scalebox{0.78}{0.270} & \\secondres{\\scalebox{0.78}{{0.208}}} & \\scalebox{0.78}{0.486} & \\scalebox{0.78}{0.427} & \\scalebox{0.78}{0.275} & \\scalebox{0.78}{0.277}  & \\scalebox{0.78}{0.380} & \\scalebox{0.78}{0.355} & \\scalebox{0.78}{0.525} & \\scalebox{0.78}{0.451}  & \\scalebox{0.78}{0.553} & \\scalebox{0.78}{0.508} & \\scalebox{0.78}{0.355} & \\scalebox{0.78}{0.332} & \\scalebox{0.78}{0.653} & \\scalebox{0.78}{0.555} \\\\ % &\\scalebox{0.78}{0.837} &\\scalebox{0.78}{0.470}\\\\\n    & \\scalebox{0.78}{24} & \\boldres{\\scalebox{0.78}{0.254}} & \\secondres{\\scalebox{0.78}{0.244}} & \\scalebox{0.78}{0.543} & \\scalebox{0.78}{0.446} & \\scalebox{0.78}{{0.320}} & \\scalebox{0.78}{{0.291}} & \\scalebox{0.78}{0.329} & \\boldres{\\scalebox{0.78}{{0.233}}} & \\scalebox{0.78}{0.545} & \\scalebox{0.78}{0.463} & \\secondres{\\scalebox{0.78}{0.300}} & \\scalebox{0.78}{0.285}  & \\scalebox{0.78}{0.456} & \\scalebox{0.78}{0.397} & \\scalebox{0.78}{0.583} & \\scalebox{0.78}{0.479}  & \\scalebox{0.78}{0.567} & \\scalebox{0.78}{0.514} & \\scalebox{0.78}{0.430} & \\scalebox{0.78}{0.377} & \\scalebox{0.78}{0.761} & \\scalebox{0.78}{0.611} \\\\ % &\\scalebox{0.78}{0.905} &\\scalebox{0.78}{0.502}\\\\\n    & \\scalebox{0.78}{72} & \\secondres{\\scalebox{0.78}{0.421}} & \\scalebox{0.78}{0.327} & \\scalebox{0.78}{0.634} & \\scalebox{0.78}{0.481} &\\scalebox{0.78}{{0.459}} & \\scalebox{0.78}{0.360} & \\scalebox{0.78}{0.484} & \\boldres{\\scalebox{0.78}{{0.324}}} & \\scalebox{0.78}{0.651} & \\scalebox{0.78}{0.510} & \\boldres{\\scalebox{0.78}{0.384}} & \\secondres{\\scalebox{0.78}{0.326}}  & \\scalebox{0.78}{0.555} & \\scalebox{0.78}{0.438} & \\scalebox{0.78}{0.761} & \\scalebox{0.78}{0.558}  & \\scalebox{0.78}{0.636} & \\scalebox{0.78}{0.548} & \\scalebox{0.78}{0.573} & \\scalebox{0.78}{0.454} & \\scalebox{0.78}{0.857} & \\scalebox{0.78}{0.658} \\\\ % &\\scalebox{0.78}{1.069} &\\scalebox{0.78}{0.573}\\\\\n    & \\scalebox{0.78}{144} & \\secondres{\\scalebox{0.78}{0.517}} & \\boldres{\\scalebox{0.78}{0.379}} & \\scalebox{0.78}{0.683} & \\scalebox{0.78}{0.504} &\\scalebox{0.78}{{0.541}} & \\scalebox{0.78}{0.404} & \\scalebox{0.78}{0.633} & \\scalebox{0.78}{{0.388}} & \\scalebox{0.78}{0.698} & \\scalebox{0.78}{0.526} & \\boldres{\\scalebox{0.78}{0.481}} & \\secondres{\\scalebox{0.78}{0.383}}  & \\scalebox{0.78}{0.611} & \\scalebox{0.78}{0.459} & \\scalebox{0.78}{0.770} & \\scalebox{0.78}{0.568}  & \\scalebox{0.78}{0.744} & \\scalebox{0.78}{0.604} & \\scalebox{0.78}{0.637}& \\scalebox{0.78}{0.498} & \\scalebox{0.78}{0.817} & \\scalebox{0.78}{0.627} \\\\ % &\\scalebox{0.78}{1.094} &\\scalebox{0.78}{0.622}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.345}} & \\secondres{\\scalebox{0.78}{0.289}} & \\scalebox{0.78}{0.585} & \\scalebox{0.78}{0.461} &\\scalebox{0.78}{{0.394}} &\\scalebox{0.78}{0.326} &\\scalebox{0.78}{0.429} & \\boldres{\\scalebox{0.78}{{0.288}}} & \\scalebox{0.78}{0.595} & \\scalebox{0.78}{0.481} &\\secondres{\\scalebox{0.78}{0.360}} &\\scalebox{0.78}{0.318}  & \\scalebox{0.78}{0.501} & \\scalebox{0.78}{0.412} & \\scalebox{0.78}{0.660} & \\scalebox{0.78}{0.514}  & \\scalebox{0.78}{0.625} & \\scalebox{0.78}{0.543} & \\scalebox{0.78}{0.499} & \\scalebox{0.78}{0.415} & \\scalebox{0.78}{0.772} & \\scalebox{0.78}{0.612} \\\\ % &\\scalebox{0.78}{0.976} &\\scalebox{0.78}{0.542}\\\\\n    \\midrule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Finance}}} \n    & \\scalebox{0.78}{12} & \\boldres{\\scalebox{0.78}{0.123}} & \\boldres{\\scalebox{0.78}{0.170}} & \\scalebox{0.78}{0.329} & \\scalebox{0.78}{0.304} & \\secondres{\\scalebox{0.78}{{0.164}}} & \\secondres{\\scalebox{0.78}{{0.206}}} & \\scalebox{0.78}{4.630} & \\scalebox{0.78}{0.520} & \\scalebox{0.78}{0.512} & \\scalebox{0.78}{0.350} &\\scalebox{0.78}{0.465} &\\scalebox{0.78}{0.291}   & \\scalebox{0.78}{0.321} & \\scalebox{0.78}{0.271} & \\scalebox{0.78}{1.865}& \\scalebox{0.78}{0.602} & \\scalebox{0.78}{1.537} & \\scalebox{0.78}{0.538}& \\scalebox{0.78}{0.537} & \\scalebox{0.78}{0.384} & \\scalebox{0.78}{1.651} & \\scalebox{0.78}{0.593} \\\\ % &\\scalebox{0.78}{7.159} &\\scalebox{0.78}{0.911}\\\\\n    & \\scalebox{0.78}{24} & \\boldres{\\scalebox{0.78}{0.158}} & \\boldres{\\scalebox{0.78}{0.197}} & \\scalebox{0.78}{0.386} & \\scalebox{0.78}{0.332} & \\secondres{\\scalebox{0.78}{{0.198}}} & \\secondres{\\scalebox{0.78}{{0.228}}} & \\scalebox{0.78}{4.987} & \\scalebox{0.78}{0.568} & \\scalebox{0.78}{0.635} & \\scalebox{0.78}{0.388}  &\\scalebox{0.78}{0.503} &\\scalebox{0.78}{0.297}  & \\scalebox{0.78}{0.464} & \\scalebox{0.78}{0.318} & \\scalebox{0.78}{2.228}& \\scalebox{0.78}{0.664} & \\scalebox{0.78}{1.553} & \\scalebox{0.78}{0.547} & \\scalebox{0.78}{0.551} & \\scalebox{0.78}{0.386} & \\scalebox{0.78}{1.671} & \\scalebox{0.78}{0.594} \\\\ % &\\scalebox{0.78}{7.328} &\\scalebox{0.78}{0.915}\\\\\n    & \\scalebox{0.78}{72} & \\boldres{\\scalebox{0.78}{0.212}} & \\boldres{\\scalebox{0.78}{0.240}}& \\scalebox{0.78}{0.436} & \\scalebox{0.78}{0.353}& \\secondres{\\scalebox{0.78}{{0.268}}} & \\secondres{\\scalebox{0.78}{{0.273}}} & \\scalebox{0.78}{5.631} & \\scalebox{0.78}{0.675} & \\scalebox{0.78}{1.239} & \\scalebox{0.78}{0.490} &\\scalebox{0.78}{0.534} &\\scalebox{0.78}{0.310}  & \\scalebox{0.78}{0.986} & \\scalebox{0.78}{0.423} & \\scalebox{0.78}{3.084}& \\scalebox{0.78}{0.793}  & \\scalebox{0.78}{1.612} &\\scalebox{0.78}{0.554} & \\scalebox{0.78}{2.004} & \\scalebox{0.78}{0.853} & \\scalebox{0.78}{2.054} & \\scalebox{0.78}{0.758} \\\\ % &\\scalebox{0.78}{7.967} &\\scalebox{0.78}{1.004}\\\\\n    & \\scalebox{0.78}{144} & \\boldres{\\scalebox{0.78}{0.245}} & \\boldres{\\scalebox{0.78}{0.257}} & \\scalebox{0.78}{0.429} & \\scalebox{0.78}{0.355}& \\secondres{\\scalebox{0.78}{{0.293}}} & \\secondres{\\scalebox{0.78}{{0.286}}} & \\scalebox{0.78}{6.083} & \\scalebox{0.78}{0.708} & \\scalebox{0.78}{1.562} & \\scalebox{0.78}{0.538} &\\scalebox{0.78}{0.564} &\\scalebox{0.78}{0.333}   & \\scalebox{0.78}{1.287} & \\scalebox{0.78}{0.473} & \\scalebox{0.78}{4.089}& \\scalebox{0.78}{0.875} & \\scalebox{0.78}{1.784} & \\scalebox{0.78}{0.636} & \\scalebox{0.78}{2.379} & \\scalebox{0.78}{0.947} & \\scalebox{0.78}{2.114} & \\scalebox{0.78}{0.778} \\\\ % &\\scalebox{0.78}{7.832} &\\scalebox{0.78}{0.995}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.184}} & \\boldres{\\scalebox{0.78}{0.216}} & \\scalebox{0.78}{0.395} & \\scalebox{0.78}{0.336} & \\secondres{\\scalebox{0.78}{{0.231}}} & \\secondres{\\scalebox{0.78}{{0.248}}} & \\scalebox{0.78}{5.333} & \\scalebox{0.78}{0.618} & \\scalebox{0.78}{0.987} & \\scalebox{0.78}{0.442} &\\scalebox{0.78}{0.516} &\\scalebox{0.78}{0.308}  & \\scalebox{0.78}{0.765} & \\scalebox{0.78}{0.372} & \\scalebox{0.78}{2.817} & \\scalebox{0.78}{0.734} & \\scalebox{0.78}{1.621} & \\scalebox{0.78}{0.569} & \\scalebox{0.78}{1.368} & \\scalebox{0.78}{0.643} & \\scalebox{0.78}{1.872} & \\scalebox{0.78}{0.681} \\\\ % &\\scalebox{0.78}{7.571} &\\scalebox{0.78}{0.956}\\\\\n    \\midrule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Terminal}}} \n    & \\scalebox{0.78}{12} & \\boldres{\\scalebox{0.78}{0.051}} & \\boldres{\\scalebox{0.78}{0.127}} & \\scalebox{0.78}{0.168} & \\scalebox{0.78}{0.272} &\\scalebox{0.78}{0.068} & \\scalebox{0.78}{0.164} & \\secondres{\\scalebox{0.78}{{0.055}}} & \\secondres{\\scalebox{0.78}{{0.140}}} & \\scalebox{0.78}{0.212} & \\scalebox{0.78}{0.304} &\\scalebox{0.78}{0.074} &\\scalebox{0.78}{0.169}   & \\scalebox{0.78}{0.096} & \\scalebox{0.78}{0.198} & \\scalebox{0.78}{0.199} & \\scalebox{0.78}{0.301}  &\\scalebox{0.78}{0.268} &\\scalebox{0.78}{0.379} & \\scalebox{0.78}{0.140} & \\scalebox{0.78}{0.252} & \\scalebox{0.78}{0.386} & \\scalebox{0.78}{0.461} \\\\ % &\\scalebox{0.78}{0.279} &\\scalebox{0.78}{0.365}\\\\\n    & \\scalebox{0.78}{24} & \\boldres{\\scalebox{0.78}{0.059}} & \\boldres{\\scalebox{0.78}{0.139}} & \\scalebox{0.78}{0.185} & \\scalebox{0.78}{0.290} &\\scalebox{0.78}{0.074} & \\scalebox{0.78}{0.173} & \\secondres{\\scalebox{0.78}{{0.065}}} & \\secondres{\\scalebox{0.78}{{0.155}}} & \\scalebox{0.78}{0.201} & \\scalebox{0.78}{0.301} &\\scalebox{0.78}{0.081} &\\scalebox{0.78}{0.178}  & \\scalebox{0.78}{0.105} & \\scalebox{0.78}{0.209} & \\scalebox{0.78}{0.225} & \\scalebox{0.78}{0.325}  & \\scalebox{0.78}{0.256} & \\scalebox{0.78}{0.370} & \\scalebox{0.78}{0.174} & \\scalebox{0.78}{0.289} & \\scalebox{0.78}{0.708} & \\scalebox{0.78}{0.644} \\\\ % &\\scalebox{0.78}{0.302} &\\scalebox{0.78}{0.378}\\\\\n    & \\scalebox{0.78}{72} & \\boldres{\\scalebox{0.78}{0.071}} & \\boldres{\\scalebox{0.78}{0.160}} & \\scalebox{0.78}{0.183} & \\scalebox{0.78}{0.291} &\\scalebox{0.78}{0.081} & \\scalebox{0.78}{0.187} & \\secondres{\\scalebox{0.78}{{0.077}}} & \\secondres{\\scalebox{0.78}{{0.170}}} & \\scalebox{0.78}{0.222} & \\scalebox{0.78}{0.316} &\\scalebox{0.78}{0.077} &\\scalebox{0.78}{0.178}  & \\scalebox{0.78}{0.109} & \\scalebox{0.78}{0.215} & \\scalebox{0.78}{0.317} & \\scalebox{0.78}{0.338}  & \\scalebox{0.78}{0.285} & \\scalebox{0.78}{0.396} & \\scalebox{0.78}{0.202} & \\scalebox{0.78}{0.321} & \\scalebox{0.78}{0.510} & \\scalebox{0.78}{0.552} \\\\ % &\\scalebox{0.78}{0.313} &\\scalebox{0.78}{0.377}\\\\\n    & \\scalebox{0.78}{144} & \\boldres{\\scalebox{0.78}{0.079}} & \\boldres{\\scalebox{0.78}{0.171}} & \\scalebox{0.78}{0.184} & \\scalebox{0.78}{0.292} & \\scalebox{0.78}{0.085} & \\scalebox{0.78}{0.193} & \\secondres{\\scalebox{0.78}{{0.085}}} & \\secondres{\\scalebox{0.78}{{0.181}}} & \\scalebox{0.78}{0.229} & \\scalebox{0.78}{0.322}  &\\scalebox{0.78}{0.088} &\\scalebox{0.78}{0.192}  & \\scalebox{0.78}{0.113} & \\scalebox{0.78}{0.220} & \\scalebox{0.78}{0.378} & \\scalebox{0.78}{0.425}  & \\scalebox{0.78}{0.372} & \\scalebox{0.78}{0.468} & \\scalebox{0.78}{0.204} & \\scalebox{0.78}{0.322} & \\scalebox{0.78}{0.468} & \\scalebox{0.78}{0.528} \\\\ % &\\scalebox{0.78}{0.329} &\\scalebox{0.78}{0.393}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.065}} & \\boldres{\\scalebox{0.78}{0.150}} & \\scalebox{0.78}{0.180} & \\scalebox{0.78}{0.286} &\\scalebox{0.78}{0.077} &\\scalebox{0.78}{0.179} & \\secondres{\\scalebox{0.78}{{0.071}}} & \\secondres{\\scalebox{0.78}{{0.162}}} & \\scalebox{0.78}{0.216} & \\scalebox{0.78}{0.311} &\\scalebox{0.78}{0.080} &\\scalebox{0.78}{0.179}  & \\scalebox{0.78}{0.106} & \\scalebox{0.78}{0.210} & \\scalebox{0.78}{0.280} & \\scalebox{0.78}{0.360}  & \\scalebox{0.78}{0.295} & \\scalebox{0.78}{0.403} & \\scalebox{0.78}{0.180} & \\scalebox{0.78}{0.296} & \\scalebox{0.78}{0.518} & \\scalebox{0.78}{0.547} \\\\ % &\\scalebox{0.78}{0.306} &\\scalebox{0.78}{0.378}\\\\\n    \\midrule\n\n     \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Payment}}} \n    & \\scalebox{0.78}{12} & \\boldres{\\scalebox{0.78}{0.050}} & \\boldres{\\scalebox{0.78}{0.121}} & \\scalebox{0.78}{0.123} & \\scalebox{0.78}{0.230} & \\secondres{\\scalebox{0.78}{{0.065}}} & \\scalebox{0.78}{0.156} & \\scalebox{0.78}{0.152} & \\secondres{\\scalebox{0.78}{{0.145}}} & \\scalebox{0.78}{0.184} & \\scalebox{0.78}{0.265} &\\scalebox{0.78}{0.094} &\\scalebox{0.78}{0.171}  & \\scalebox{0.78}{0.090} & \\scalebox{0.78}{0.180} & \\scalebox{0.78}{0.164} & \\scalebox{0.78}{0.249}  & \\scalebox{0.78}{0.272} & \\scalebox{0.78}{0.349} & \\scalebox{0.78}{0.129} & \\scalebox{0.78}{0.229} & \\scalebox{0.78}{0.382} & \\scalebox{0.78}{0.437} \\\\ % &\\scalebox{0.78}{0.492} &\\scalebox{0.78}{0.347}\\\\\n    & \\scalebox{0.78}{24} & \\boldres{\\scalebox{0.78}{0.062}} &\\boldres{\\scalebox{0.78}{0.135}} & \\scalebox{0.78}{0.144} & \\scalebox{0.78}{0.249} & \\secondres{\\scalebox{0.78}{{0.077}}} & \\scalebox{0.78}{0.167} & \\scalebox{0.78}{0.178} & \\secondres{\\scalebox{0.78}{{0.165}}} & \\scalebox{0.78}{0.183} & \\scalebox{0.78}{0.266} &\\scalebox{0.78}{0.099} &\\scalebox{0.78}{0.178}  & \\scalebox{0.78}{0.108} & \\scalebox{0.78}{0.196} & \\scalebox{0.78}{0.216} & \\scalebox{0.78}{0.280} & \\scalebox{0.78}{0.265} & \\scalebox{0.78}{0.343} & \\scalebox{0.78}{0.157} & \\scalebox{0.78}{0.266} & \\scalebox{0.78}{0.345} & \\scalebox{0.78}{0.412} \\\\ % &\\scalebox{0.78}{0.514} &\\scalebox{0.78}{0.367}\\\\\n    & \\scalebox{0.78}{72} & \\boldres{\\scalebox{0.78}{0.082}} & \\boldres{\\scalebox{0.78}{0.155}} & \\scalebox{0.78}{0.151} & \\scalebox{0.78}{0.251}  & \\secondres{\\scalebox{0.78}{{0.094}}} & \\secondres{\\scalebox{0.78}{{0.184}}} & \\scalebox{0.78}{0.236} & \\scalebox{0.78}{0.193} & \\scalebox{0.78}{0.226} & \\scalebox{0.78}{0.287} &\\scalebox{0.78}{0.111} &\\scalebox{0.78}{0.189}  & \\scalebox{0.78}{0.129} & \\scalebox{0.78}{0.209} & \\scalebox{0.78}{0.360} & \\scalebox{0.78}{0.370}  & \\scalebox{0.78}{0.284} & \\scalebox{0.78}{0.360} & \\scalebox{0.78}{0.183} & \\scalebox{0.78}{0.291} & \\scalebox{0.78}{0.437} & \\scalebox{0.78}{0.471} \\\\ % &\\scalebox{0.78}{0.523} &\\scalebox{0.78}{0.359}\\\\\n    & \\scalebox{0.78}{144} & \\boldres{\\scalebox{0.78}{0.093}} & \\boldres{\\scalebox{0.78}{0.166}} & \\scalebox{0.78}{0.154} & \\scalebox{0.78}{0.251} & \\secondres{\\scalebox{0.78}{{0.101}}} & \\scalebox{0.78}{0.190} & \\scalebox{0.78}{0.260} & \\scalebox{0.78}{0.214} & \\scalebox{0.78}{0.240} & \\scalebox{0.78}{0.294} &\\scalebox{0.78}{0.115} & \\secondres{\\scalebox{0.78}{{0.189}}}  & \\scalebox{0.78}{0.138} & \\scalebox{0.78}{0.215} & \\scalebox{0.78}{0.410} & \\scalebox{0.78}{0.391}  & \\scalebox{0.78}{0.379} & \\scalebox{0.78}{0.441} & \\scalebox{0.78}{0.194} & \\scalebox{0.78}{0.296} & \\scalebox{0.78}{0.501} & \\scalebox{0.78}{0.518} \\\\ % &\\scalebox{0.78}{0.547} &\\scalebox{0.78}{0.386}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.072}} & \\boldres{\\scalebox{0.78}{0.144}} & \\scalebox{0.78}{0.143} & \\scalebox{0.78}{0.245} & \\secondres{\\scalebox{0.78}{{0.084}}} & \\secondres{\\scalebox{0.78}{{0.174}}} &\\scalebox{0.78}{0.207} &\\scalebox{0.78}{0.179} & \\scalebox{0.78}{0.208} & \\scalebox{0.78}{0.278} &\\scalebox{0.78}{0.105} &\\scalebox{0.78}{0.182}  & \\scalebox{0.78}{0.116} & \\scalebox{0.78}{0.200} & \\scalebox{0.78}{0.288} & \\scalebox{0.78}{0.322} & \\scalebox{0.78}{0.300} & \\scalebox{0.78}{0.373} & \\scalebox{0.78}{0.166} & \\scalebox{0.78}{0.271} & \\scalebox{0.78}{0.417} & \\scalebox{0.78}{0.460} \\\\ % &\\scalebox{0.78}{0.519} &\\scalebox{0.78}{0.365}\\\\\n    \\midrule\n\n    \\multirow{5}{*}{\\rotatebox{90}{\\scalebox{0.95}{Customer}}} \n    & \\scalebox{0.78}{12} & \\boldres{\\scalebox{0.78}{0.065}} & \\boldres{\\scalebox{0.78}{0.129}} & \\scalebox{0.78}{0.191} & \\scalebox{0.78}{0.247} & \\secondres{\\scalebox{0.78}{{0.091}}} & \\scalebox{0.78}{{0.160}} & \\scalebox{0.78}{0.243} & \\secondres{\\scalebox{0.78}{0.156}} & \\scalebox{0.78}{0.267} & \\scalebox{0.78}{0.289}  &\\scalebox{0.78}{0.123} &\\scalebox{0.78}{0.180}  & \\scalebox{0.78}{0.143} & \\scalebox{0.78}{0.195} & \\scalebox{0.78}{0.310} & \\scalebox{0.78}{0.326}  & \\scalebox{0.78}{0.309} & \\scalebox{0.78}{0.366} & \\scalebox{0.78}{0.175} & \\scalebox{0.78}{0.243} & \\scalebox{0.78}{0.640} & \\scalebox{0.78}{0.580} \\\\ % &\\scalebox{0.78}{0.828} &\\scalebox{0.78}{0.427}\\\\\n    & \\scalebox{0.78}{24} & \\boldres{\\scalebox{0.78}{0.078}} & \\boldres{\\scalebox{0.78}{0.141}} & \\scalebox{0.78}{0.214} & \\scalebox{0.78}{0.264} & \\secondres{\\scalebox{0.78}{{0.107}}} & \\secondres{\\scalebox{0.78}{{0.173}}} & \\scalebox{0.78}{0.293} & \\scalebox{0.78}{0.177} & \\scalebox{0.78}{0.267}& \\scalebox{0.78}{0.291} &\\scalebox{0.78}{0.130} &\\scalebox{0.78}{0.183}  & \\scalebox{0.78}{0.170} & \\scalebox{0.78}{0.212} & \\scalebox{0.78}{0.338} & \\scalebox{0.78}{0.344} & \\scalebox{0.78}{0.313} & \\scalebox{0.78}{0.369} & \\scalebox{0.78}{0.188} & \\scalebox{0.78}{0.264} & \\scalebox{0.78}{0.763} & \\scalebox{0.78}{0.642} \\\\ % &\\scalebox{0.78}{0.860} &\\scalebox{0.78}{0.439}\\\\\n    & \\scalebox{0.78}{72} & \\boldres{\\scalebox{0.78}{0.108}} & \\boldres{\\scalebox{0.78}{0.161}} & \\scalebox{0.78}{0.222} & \\scalebox{0.78}{0.266} & \\secondres{\\scalebox{0.78}{{0.131}}} & \\secondres{\\scalebox{0.78}{{0.190}}} & \\scalebox{0.78}{0.331} & \\scalebox{0.78}{0.215} & \\scalebox{0.78}{0.334} & \\scalebox{0.78}{0.317} &\\scalebox{0.78}{0.149} &\\scalebox{0.78}{0.196}  & \\scalebox{0.78}{0.202} & \\scalebox{0.78}{0.228} & \\scalebox{0.78}{0.511} & \\scalebox{0.78}{0.408} & \\scalebox{0.78}{0.330} & \\scalebox{0.78}{0.374} & \\scalebox{0.78}{0.267} & \\scalebox{0.78}{0.324} & \\scalebox{0.78}{0.616} & \\scalebox{0.78}{0.564} \\\\ % &\\scalebox{0.78}{0.904} &\\scalebox{0.78}{0.451}\\\\\n    & \\scalebox{0.78}{144} & \\boldres{\\scalebox{0.78}{0.126}} & \\boldres{\\scalebox{0.78}{0.172}} & \\scalebox{0.78}{0.227} & \\scalebox{0.78}{0.268} & \\secondres{\\scalebox{0.78}{{0.141}}} & \\secondres{\\scalebox{0.78}{{0.195}}} & \\scalebox{0.78}{0.368} & \\scalebox{0.78}{0.226} & \\scalebox{0.78}{0.363} & \\scalebox{0.78}{0.332} &\\scalebox{0.78}{0.166} &\\scalebox{0.78}{0.206}  & \\scalebox{0.78}{0.222} & \\scalebox{0.78}{0.239} & \\scalebox{0.78}{0.687} & \\scalebox{0.78}{0.461} & \\scalebox{0.78}{0.450}& \\scalebox{0.78}{0.456} & \\scalebox{0.78}{0.336} & \\scalebox{0.78}{0.373} & \\scalebox{0.78}{0.658} & \\scalebox{0.78}{0.586} \\\\ % &\\scalebox{0.78}{0.903} &\\scalebox{0.78}{0.458}\\\\\n    \\cmidrule(lr){2-24}\n    & \\scalebox{0.78}{Avg} & \\boldres{\\scalebox{0.78}{0.094}} & \\boldres{\\scalebox{0.78}{0.150}} & \\scalebox{0.78}{0.214} & \\scalebox{0.78}{0.261} & \\secondres{\\scalebox{0.78}{{0.118}}} & \\secondres{\\scalebox{0.78}{{0.180}}} &\\scalebox{0.78}{0.309} &\\scalebox{0.78}{0.194} & \\scalebox{0.78}{0.308} & \\scalebox{0.78}{0.307} &\\scalebox{0.78}{0.142} &\\scalebox{0.78}{0.191}  & \\scalebox{0.78}{0.184} & \\scalebox{0.78}{0.219} & \\scalebox{0.78}{0.461} & \\scalebox{0.78}{0.385} & \\scalebox{0.78}{0.350} & \\scalebox{0.78}{0.391} & \\scalebox{0.78}{0.242} & \\scalebox{0.78}{0.301} & \\scalebox{0.78}{0.669} & \\scalebox{0.78}{0.593} \\\\ % &\\scalebox{0.78}{0.874} &\\scalebox{0.78}{0.444}\\\\\n     \\midrule\n     \\multicolumn{2}{c|}{\\scalebox{0.78}{{$1^{\\text{st}}$ Count}}} & \\scalebox{0.78}{\\boldres{28}} & \\scalebox{0.78}{\\boldres{27}} &\\scalebox{0.78}{0}& \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{\\secondres{3}} & \\scalebox{0.78}{0} & \\scalebox{0.78}{0} & \\scalebox{0.78}{\\secondres{2}} & \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0}& \\scalebox{0.78}{0} \\\\\n    \\bottomrule\n  \\end{tabular}\n    \\end{small}\n  \\end{threeparttable}\n  }\n\\end{table}"
        },
        "figures": {
            "fig:radar": "\\begin{wrapfigure}{r}{0.33\\textwidth}\n\\label{fig:radar}\n  \\begin{center}\n  \\vspace{-17pt}\n    \\includegraphics[width=0.33\\textwidth]{pic/radar.pdf}\n  \\end{center}\n  \\vspace{-13pt}\n  \\caption{\\small{Performance of iTransformer. Average results (MSE) are reported following TimesNet~\\citeyearpar{Timesnet}}.}\n  \\vspace{-14pt}\n\\end{wrapfigure}",
            "fig:hyper": "\\begin{figure}[htbp]\n  \\includegraphics[width=\\columnwidth]{pic/hyper.pdf}\n  \\centering\n  \\vspace{-15pt}\n  \\caption{\\update{Hyperparameter sensitivity with respect to the learning rate, the number of Transformer blocks, and the hidden dimension of variate tokens. The results are recorded with the lookback window length $T=96$ and the forecast window length $S=96$.}}\n  \\label{fig:hyper}\n\\end{figure}",
            "fig:eff": "\\begin{figure}[htbp]\n  \\includegraphics[width=\\columnwidth]{pic/eff.pdf}\n  \\centering\n    \\vspace{-15pt}\n  \\caption{\\update{Model efficiency comparison under input-$96$-predict-$96$ of Weather and Traffic.}}\n  \\label{fig:eff}\n\\end{figure}",
            "fig:multivariate_corr": "\\begin{figure}[htbp]\n  \\includegraphics[width=\\columnwidth]{pic/multivariate_corr.pdf}\n  \\centering\n  \\vspace{-15pt}\n  \\caption{\\update{Multivariate correlations of the lookback series and future series and the learned score maps by inverted self-attention of different layers. Cases all come from the Solar-Energy dataset.}}\n  \\label{fig:multivariate_corr}\n\\end{figure}",
            "fig:multivariate_corr_grouped": "\\begin{figure}[htbp]\n  \\includegraphics[width=\\columnwidth]{pic/groups.pdf}\n  \\centering\n  \\vspace{-15pt}\n  \\caption{\\update{Visualization of the variates from the Market dataset and the learned multivariate correlations. Each variate represents the monitored interface values of an application, and the applications can be further grouped into refined categories. The color bar is shared with Figure~\\ref{fig:multivariate_corr}}.}\n  \\label{fig:multivariate_corr_grouped}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n \\begin{aligned}\n     \\mathbf{h}^{0}_n &= \\operatorname{Embedding}(\\mathbf{X}_{:,n}), \\\\\n    \\mathbf{H}^{l+1} &= \\operatorname{TrmBlock}(\\mathbf{H}^{l}),\\ l=0,\\dots, L-1, \\\\\n     \\hat{\\mathbf{Y}}_{:,n} &= \\operatorname{Projection}(\\mathbf{h}^{L}_n), \\\\\n\\end{aligned}\n\\end{equation}",
            "eq:equ:norm": "\\begin{equation}\\label{equ:norm}\n    \\operatorname{LayerNorm}(\\mathbf{H}) = \\left\\{\\frac{\\mathbf{h}_n - \\operatorname{Mean}(\\mathbf{h}_n)}{ \\sqrt{\\operatorname{Var}(\\mathbf{h}_n)}} \\bigg| \\ n=1,\\dots,N \\right\\}\n\\end{equation}",
            "eq:2": "\\begin{equation*}\n    \\rho_{xy} = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i (x_i - \\bar{x})^2} \\sqrt{\\sum_i (y_i - \\bar{y})^2}},\n\\end{equation*}"
        },
        "git_link": "https://github.com/thuml/iTransformer"
    }
}