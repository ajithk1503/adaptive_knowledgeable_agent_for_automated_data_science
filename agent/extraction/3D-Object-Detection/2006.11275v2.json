{
    "meta_info": {
        "title": "Center-based 3D Object Detection and Tracking",
        "abstract": "Three-dimensional objects are commonly represented as 3D boxes in a\npoint-cloud. This representation mimics the well-studied image-based 2D\nbounding-box detection but comes with additional challenges. Objects in a 3D\nworld do not follow any particular orientation, and box-based detectors have\ndifficulties enumerating all orientations or fitting an axis-aligned bounding\nbox to rotated objects. In this paper, we instead propose to represent, detect,\nand track 3D objects as points. Our framework, CenterPoint, first detects\ncenters of objects using a keypoint detector and regresses to other attributes,\nincluding 3D size, 3D orientation, and velocity. In a second stage, it refines\nthese estimates using additional point features on the object. In CenterPoint,\n3D object tracking simplifies to greedy closest-point matching. The resulting\ndetection and tracking algorithm is simple, efficient, and effective.\nCenterPoint achieved state-of-the-art performance on the nuScenes benchmark for\nboth 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single\nmodel. On the Waymo Open Dataset, CenterPoint outperforms all previous single\nmodel method by a large margin and ranks first among all Lidar-only\nsubmissions. The code and pretrained models are available at\nhttps://github.com/tianweiy/CenterPoint.",
        "author": "Tianwei Yin, Xingyi Zhou, Philipp Krähenbühl",
        "link": "http://arxiv.org/abs/2006.11275v2",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "update nuScenes and Waymo results"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nStrong 3D perception is a core ingredient in many state-of-the-art driving systems~\\cite{bansal2018chauffeurnet,wang2019monocular}.\nCompared to the well-studied 2D detection problem, 3D detection on point-clouds offers a series of interesting challenges:\nFirst, point-clouds are sparse, and most regions of 3D space are without measurements~\\cite{hu2019exploiting}.\nSecond, the resulting output is a three-dimensional box that is often not well aligned with any global coordinate frame.\nThird, 3D objects come in a wide range of sizes, shapes, and aspect ratios, e.g., in the traffic domain, bicycles are near planer, buses and limousines elongated, and pedestrians tall.\nThese marked differences between 2D and 3D detection made a transfer of ideas between the two domain harder~\\cite{shi2019pointrcnn,yang20203dssd,simony2018complex}.\nThe crux of it is that an axis-aligned 2D box~\\cite{girshick2014rich,girshick2015fast} is a poor proxy of a free-form 3D object.\nOne solution might be to classify a different template (anchor) for each object orientation~\\cite{yang2019scrdet, yang2019r3det},\nbut this unnecessarily increases the computational burden and may introduce a large number of potential false-positive detections.\nWe argue that the main underlying challenge in linking up the 2D and 3D domains lies in this representation of objects. \n\n\n\n\nIn this paper, we show how representing objects as points~(\\reffig{teaser}) greatly simplifies 3D recognition.\nOur two-stage 3D detector, CenterPoint, finds centers of objects and their properties using a keypoint detector~\\cite{zhou2019objects}, a second-stage refines all estimates.\nSpecifically, CenterPoint uses a standard Lidar-based backbone network, i.e., VoxelNet~\\cite{voxelnet, yan2018second} or PointPillars~\\cite{pillar}, to build a representation of the input point-cloud.\nIt then flattens this representation into an overhead map-view and uses a standard image-based keypoint detector to find object centers~\\cite{zhou2019objects}.\nFor each detected center, it regresses to all other object properties such as 3D size, orientation, and velocity from a point-feature at the center location.\nFurthermore, we use a light-weighted second stage to refine the object locations.\nThis second stage extracts point-features at the 3D centers of each face of the estimated objects 3D bounding box.\nIt recovers the lost local geometric information due to striding and a limited receptive field, and brings a decent performance boost with minor cost.\n\nThe \\pb representation has several key advantages: First, unlike bounding boxes, points have no intrinsic orientation.\nThis dramatically reduces the object detector's search space while allowing the backbone to learn the rotational invariance of objects and rotational equivariance of their relative rotation.\nSecond, a \\pb representation simplifies downstream tasks such as tracking.\nIf objects are points, tracklets are paths in space and time.\nCenterPoint predicts the relative offset (velocity) of objects between consecutive frames, which are then linked up greedily.\nThirdly, point-based feature extraction enables us to design an effective two-stage refinement module that is much faster than previous approaches~\\cite{pvrcnn, PartA,shi2019pointrcnn}.\n\nWe test our models on two popular large datasets: Waymo Open Dataset~\\cite{sun2019scalability}, and nuScenes Dataset~\\cite{caesar2019nuscenes}. \nWe show that a simple switch from the box representation to center-based representation yields a $3$-$4$ mAP increase in 3D detection under different backbones ~\\cite{pillar, voxelnet, yan2018second, zhu2019classbalanced}. \nTwo-stage refinement further brings an additional $2$ mAP boost with small~($<10\\%$) computation overhead.\nOur best single model achieves $71.8$ and $66.4$ level 2 mAPH for vehicle and pedestrian detection on Waymo, $58.0$ mAP and $65.5$ NDS on nuScenes, outperforming all published methods on both datasets.   \nNotably, in NeurIPS 2020 nuScenes 3D Detection challenge, CenterPoint is adopted in 3 of the top 4 winning entries. \nFor 3D tracking, our model performs at $63.8$ AMOTA outperforming the prior state-of-the-art by $8.8$ AMOTA on nuScenes. \nOn Waymo 3D tracking benchmark, our model achieves $59.4$ and $56.6$ level 2 MOTA for vehicle and pedestrian tracking, respectively, surpassing previous methods by up to $50\\%$. \nOur end-to-end 3D detection and tracking system runs near real-time, with $11$ FPS on Waymo and $16$ FPS on nuScenes. \n\n"
            },
            "section 2": {
                "name": "Related work",
                "content": "\n\\label{related_work}\n\\noindent\n\\textbf{2D object detection} predicts axis-algined bounding box from image inputs.\nThe RCNN family~\\cite{girshick2014rich,girshick2015fast,ren2015faster,he2017mask} finds a category-agnostic bounding box candidates, then classify and refine it.\nYOLO~\\cite{redmon2017yolo9000}, SSD~\\cite{liu2016ssd}, and RetinaNet~\\cite{focal2017} directly find a category-specific box candidate, sidestepping later classification and refinement.\nCenter-based detectors, e.g. CenterNet~\\cite{zhou2019objects} or CenterTrack~\\cite{zhou2020tracking}, directly detect the implicit object center point without the need for candidate boxes.\nMany 3D object detectors~\\cite{shi2019pointrcnn,yang20203dssd,simony2018complex, sassd} evolved from these 2D object detector.\nWe argue center-based representation~\\cite{zhou2019objects,zhou2020tracking} is a better fit in 3D application comparing to axis-aligned boxes.\n\n\\noindent\n\\textbf{3D object detection}  \naims to predict three dimensional rotated bounding boxes~\\cite{yan2018second, pillar, std, qi2018frustum, yang20203dssd, liang2019multi, kitti}.\nThey differ from 2D detectors on the input encoder.\nVote3Deep~\\cite{vote_3deep} leverages feature-centric voting~\\cite{wang2015voting} to efficiently process the sparse 3D point-cloud on equally spaced 3D voxels.\nVoxelNet~\\cite{voxelnet} uses a PointNet~\\cite{qi2017pointnet} inside each voxel to generate a unified feature representation from which a head with 3D sparse convolutions~\\cite{sparse_conv} and 2D convolutions produces detections.\nSECOND~\\cite{yan2018second} simplifies the VoxelNet and speeds up sparse 3D convolutions. \nPIXOR~\\cite{pixor} project all points onto a 2D feature map with 3D occupancy and point intensity information to remove the expensive 3D convolutions.\nPointPillars~\\cite{pillar} replaces all voxel computation with a pillar representation, a single tall elongated voxel per map location, improving backbone efficiency.\nMVF~\\cite{mvf} and Pillar-od~\\cite{wang2020pillar} combine multiple view features to learn a more effective pillar representation. \nOur contribution focuses on the output representation, and is compatible with any 3D encoder and can improve them all.\n\nVoteNet~\\cite{votenet} detects objects through vote clustering using point feature sampling and grouping. \nIn contrast, we directly regress to 3D bounding boxes through features at the center point without voting.  \nWong et al.~\\cite{wong2019identifying} and Chen et al.~\\cite{chen2019hotspot} used similar multiple points representation in the object center region (i.e., point-anchors) and regress to other attributes.\nWe use a single positive cell for each object and use a keypoint estimation loss.\n\n\\noindent\n\\textbf{Two-stage 3D object detection.} \nRecent works considered directly applying RCNN style 2D detectors to the 3D domains~\\cite{pvrcnn, PartA, shi2019pointrcnn, std, fast_pointrcnn}. \nMost of them apply RoIPool~\\cite{ren2015faster} or RoIAlign~\\cite{he2017mask} to aggregate RoI-specific features in 3D space, using PointNet-based point~\\cite{shi2019pointrcnn} or voxel~\\cite{pvrcnn} feature extractor. \nThese approaches extract region features from 3D Lidar measurements~(points and voxels), \nresulting in a prohibitive run-time due to massive points.\nInstead, we extract sparse features of 5 surface center points from the intermediate feature map.\nThis makes our second stage very efficient and keeps effective.\n\n\\noindent \n\\textbf{3D object tracking.} Many 2D tracking algorithms~\\cite{mass, sort, deep_sort, tractor} readily track 3D objects out of the box.\nHowever, dedicated 3D trackers based on 3D Kalman filters~\\cite{weng2019baseline,chiu2020probabilistic} still have an edge as they better exploit the three-dimensional motion in a scene.\nHere, we adopt a much simpler approach following CenterTrack~\\cite{zhou2020tracking}.\nWe use a velocity estimate together with the point-based detection to track centers of objects through multiple frames.\nThis tracker is much faster and more accurate than dedicated 3D trackers~\\cite{weng2019baseline,chiu2020probabilistic}.\n\n"
            },
            "section 3": {
                "name": "Preliminaries",
                "content": "\n\n\\noindent\n\\textbf{2D CenterNet}~\\cite{zhou2019objects} rephrases object detection as keypoint estimation.\nIt takes an input image and predicts a $w \\times h$ heatmap $\\hat{Y} \\in [0,1]^{w \\times h \\times K}$ for each of $K$ classes.\nEach local maximum (i.e., pixels whose value is greater than its 8 neighbors) in the output heatmap corresponds to the center of a detected object.\nTo retrieve a 2D box, CenterNet regresses to a size map $\\hat{S} \\in \\mathbb{R}^{w \\times h \\times 2}$ shared between all categories.\nFor each detection object, the size-map stores its width and height at the center location.\nThe CenterNet architecture uses a standard fully convolutional image backbone and adds a dense prediction head on top.\nDuring training, CenterNet learns to predict heatmaps with rendered Gaussian kernels at each annotated object center $\\vec{q}_i$ for each class $c_i \\in \\{1 \\ldots K\\}$, and regress to object size $S$ at the center of the annotated bounding box.\nTo make up for quantization errors introduced by the striding of the backbone architecture, CenterNet also regresses to a local offset $\\hat{O}$.\n\nAt test time, the detector produces $K$ heatmaps and dense class-agnostic regression maps.\nEach local maxima (peak) in the heatmaps corresponds to an object, with confidence proportional to the heatmap value at the peak.\nFor each detected object, the detector retrieves all regression values from the regression maps at the corresponding peak location.\nDepending on the application domain, Non-Maxima Suppression (NMS) may be warranted.\\\\\n\n\\noindent \n\\textbf{3D Detection}\n\\lblsec{3dframework}\nLet $\\mathcal{P} = \\{(x, y, z, r)_i\\}$ be an orderless point-cloud of 3D location $(x, y, z)$ and reflectance $r$ measurements.\n3D object detection aims to predict a set of 3D object bounding boxes $\\mathcal{B} = \\{b_k\\}$ in the bird eye view from this point-cloud.\nEach bounding box $b = (u, v, d, w, l, h, \\alpha)$ consists of a center location $(u, v, d)$, relative to the objects ground plane, and 3D size $(w, l, h)$, and rotation expressed by yaw $\\alpha$.\nWithout loss of generality, we use an egocentric coordinate system with sensor at $(0, 0, 0)$ and yaw$=0$.\n\nModern 3D object detectors~\\cite{voxelnet,yan2018second,pillar,sassd} uses a 3D encoder that quantizes the point-cloud into regular bins.\nA point-based network~\\cite{qi2017pointnet} then extracts features for all points inside a bin.\nThe 3D encoder then pools these features into its primary feature representation.\nMost of the computation happens in the backbone network, which operates solely on these quantized and pooled feature representations.\nThe output of a backbone network is a map-view feature-map $\\vec{M} \\in \\mathbb{R}^{W \\times L \\times F}$ of width $W$ and length $L$ with $F$ channels in a map-view reference frame.\nBoth width and height directly relate to the resolution of individual voxel bins and the backbone network's stride.\nCommon backbones include VoxelNet~\\cite{voxelnet,yan2018second} and PointPillars~\\cite{pillar}. \n\nWith a map-view feature map $\\vec{M}$, a detection head, most commonly a one-~\\cite{focal2017} or two-stage~\\cite{ren2015faster} bounding-box detector, then produces object detections from some predefined bounding boxes anchored on this overhead feature-map.\nAs 3D bounding boxes come with various sizes and orientation, anchor-based 3D detectors have difficulty fitting an axis-aligned 2D box to a 3D object. \nMoreover, during the training, previous anchor-based 3D detectors rely on 2D Box IoU for target assignment~\\cite{yan2018second, pvrcnn}, which creates unnecessary burdens for choosing positive/negative thresholds for different classes or different dataset. \nIn the next section, we show how to build a principled 3D object detection and tracking model based on point representation.  \nWe introduce a novel center-based detection head but rely on existing 3D backbones (VoxelNet or PointPillars). \n\n\n\n\n"
            },
            "section 4": {
                "name": "CenterPoint",
                "content": "\n\n\\reffig{framework} shows the overall framework of the CenterPoint model. \nLet $\\vec{M} \\in \\mathbb{R}^{W\\times H\\times F}$ be the output of the 3D backbone.\nThe first stage of CenterPoint predicts a class-specific heatmap, object size, a sub-voxel location refinement, rotation, and velocity.\nAll outputs are dense predictions.\n\n\\noindent \n\\textbf{Center heatmap head.}\nThe center-head's goal is to produce a heatmap peak at the center location of any detected object.\nThis head produces a $K$-channel heatmap $\\hat Y$, one channel for each of $K$ classes.\nDuring training, it targets a 2D Gaussian produced by the projection of 3D centers of annotated bounding boxes into the map-view. \nWe use a focal loss~\\cite{cornernet, zhou2019objects}.\nObjects in a top-down map view are sparser than in an image.\nIn map-view, distances are absolute, while an image-view distorts them by perspective.\nConsider a road scene, in map-view the area occupied by vehicles small, but in image-view, a few large objects may occupy most of the screen.\nFurthermore, the compression of the depth-dimension in perspective projection naturally places object centers much closer to each other in image-view.\nFollowing the standard supervision of CenterNet~\\cite{zhou2019objects} results in a very sparse supervisory signal, where most locations are considered background.\nTo counteract this, we increase the positive supervision for the target heatmap $Y$ by enlarging the Gaussian peak rendered at each ground truth object center.\nSpecifically, we set the Gaussian radius to $\\sigma = \\max(f({wl}), \\tau)$, where $\\tau=2$ is the smallest allowable Gaussian radius, and $f$ is a radius function defined in CornerNet~\\cite{cornernet}.\nIn this way, CenterPoint maintains the simplicity of the center-based target assignment; the model gets denser supervision from nearby pixels.\n\n\\noindent \n\\textbf{Regression heads.}\nWe store several object properties at center-features of objects: a sub-voxel location refinement $o \\in \\mathbb{R}^2$, height-above-ground $h_g \\in \\mathbb{R}$, the 3D size $s \\in \\mathbb{R}^3$, and a yaw rotation angle $(\\sin(\\alpha),\\cos(\\alpha)) \\in \\mathbb{R}^2$.\nThe sub-voxel location refinement $o$ reduces the quantization error from voxelization and striding of the backbone network.\nThe height-above-ground $h_g$ helps localize the object in 3D and adds the missing elevation information removed by the map-view projection.\nThe orientation prediction uses the sine and cosine of the yaw angle as a continuous regression target.\nCombined with box size, these regression heads provide the full state information of the 3D bounding box.\nEach output uses its own head.\nAt training time, only ground truth centers are supervised using an L1 regression loss.\nWe regress to logarithmic size to better handle boxes of various shapes.\nAt inference time, we extract all properties by indexing into dense regression head outputs at each object's peak location.\n\n\\noindent \n\\textbf{Velocity head and tracking.}\n\\lblsec{tracking}\nTo track objects through time, we learn to predict a two-dimensional velocity estimation $\\vec{v} \\in \\mathbb{R}^2$ for each detected object as an additional regression output.\nThe velocity estimate is special, as it requires two input map-views the current and previous time-step.\nIt predicts the difference in object position between the current and the past frame.\nLike other regression targets, the velocity estimation is also supervised using L1 loss at the ground truth object's location at the current time-step.\n\nAt inference time, we use this offset to associate current detections to past ones in a greedy fashion.\nSpecifically, we project the object centers in the current frame back to the previous frame by applying the negative velocity estimate and then matching them to the tracked objects by closest distance matching.\nFollowing SORT~\\cite{sort}, we keep unmatched tracks up to $T=3$ frames before deleting them.\nWe update each unmatched track with its last known velocity estimation. \nSee supplement for the detailed tracking algorithm diagram.\n\nCenterPoint combines all heatmap and regression losses in one common objective and jointly optimizes them.\nIt\nsimplifies and improves previous anchor-based 3D detectors~(see experiments).\nHowever, all properties of the object \nare currently inferred from the object's center-feature, \nwhich may not contain sufficient information for accurate object localization.\nFor example, in autonomous driving, the sensor often only sees the side of the object, but not its center.\nNext, we improve CenterPoint by using a second refinement stage with a light-weight point-feature extractor.\n\n",
                "subsection 4.1": {
                    "name": "Two-Stage CenterPoint",
                    "content": "\nWe use CenterPoint unchanged as a first stage.\nThe second stage extracts additional point-features from the output of the backbone.\nWe extract one point-feature from the 3D center of each face of the predicted bounding box.\nNote that the bounding box center, top and bottom face centers all project to the same point in map-view.\nWe thus only consider the four outward-facing box-faces together with the predicted object center.\nFor each point, we extract a feature using bilinear interpolation from the backbone map-view output $\\vec M$.\nNext, we concatenate the extracted point-features and pass them through an MLP.\nThe second stage predicts a class-agnostic confidence score and box refinement on top of one-stage CenterPoint's prediction results. \n\nFor class-agnostic confidence score prediction, we follow ~\\cite{pvrcnn, PartA, li2019gs3d, jiang2018acquisition} and use a score target $I$ guided by the box's 3D IoU with the corresponding ground truth bounding box:\n\\begin{equation}\n   I = \\min(1, \\max(0, 2\\times IoU_{t} - 0.5))\n\\end{equation}\n\\noindent \nwhere $IoU_{t}$ is the IoU between the $t$-th proposal box and the ground-truth. \nThe training is supervised with a binary cross entropy loss:\n\\begin{equation}\n    L_{score} =  -I_t \\log(\\hat{I_t}) - (1-I_t)\\log(1-\\hat{I_t}) \n\\end{equation}\n\\noindent \nwhere $\\hat{I_t}$ is the predicted confidence score. \nDuring the inference, we directly use the class prediction from one-stage CenterPoint and computes the final confidence score as the geometric average of the two scores $\\hat{Q}_{t} = \\sqrt{\\hat{Y}_{t} * \\hat{I}_t}$\nwhere $\\hat{Q}_t$ is the final prediction confidence of object $t$ and $\\hat{Y}_t =  \\max_{0 \\leq k \\leq K} \\hat{Y}_{p,k}$ and $\\hat{I}_t$ are the first stage and second stage confidence of object $t$, respectively. \n\nFor box regression, the model predicts a refinement on top of first stage proposals, and we train the model with L1 loss.\nOur two-stage CenterPoint simplifies and accelerates previous two-stage 3D detectors that use expensive PointNet-based feature extractor and RoIAlign operations~\\cite{shi2019pointrcnn, pvrcnn}.\n\n"
                },
                "subsection 4.2": {
                    "name": "Architecture",
                    "content": "\nAll first-stage outputs share a first $3 \\times 3$ convolutional layer, Batch Normalization~\\cite{ioffe2015batch}, and ReLU.\nEach output then uses its own branch of two $3 \\times 3$ convolutions separated by a batch norm and ReLU.\nOur second-stage uses a shared two-layer MLP, with a batch norm, ReLU, and Dropout~\\cite{hinton2012improving} with a drop rate of $0.3$, followed by two branches of three fully-connected layers, one for confidence score and one for box regression prediction. \n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\nWe evaluate CenterPoint on Waymo Open Dataset and nuScenes dataset.\nWe implement CenterPoint using two 3D encoders: VoxelNet~\\cite{voxelnet,yan2018second,zhu2019classbalanced} and PointPillars~\\cite{pillar}, termed CenterPoint-Voxel and CenterPoint-Pillar respectively.\\\\\n\n\\noindent \n\\textbf{Waymo Open Dataset.}\nWaymo Open Dataset~\\cite{sun2019scalability} contains 798 training sequences and 202 validation sequences for vehicle and pedestrian. \nThe point-clouds are captured with a 64 lanes Lidar, which produces about 180k Lidar points every 0.1s. \nThe official 3D detection evaluation metrics include the standard 3D bounding box mean average precision (mAP) and mAP weighted by heading accuracy (mAPH).\nThe mAP and mAPH are based on an IoU threshold of 0.7 for vehicles and 0.5 for pedestrians. \nFor 3D tracking, \nthe official metrics are\nMultiple Object Tracking Accuracy~(MOTA) and Multiple Object Tracking Precision~(MOTP)~\\cite{bernardin2006multiple}.\nThe official evaluation toolkit also provides a performance breakdown for two difficulty levels: \nLEVEL\\_1 for boxes with more than five Lidar points, and LEVEL\\_2 for boxes with at least one Lidar point. \n\nOur Waymo model uses a detection range of $[-75.2\\text{m}, 75.2\\text{m}]$ for the $X$ and $Y$ axis, and $[-2\\text{m}, 4\\text{m}]$ for the $Z$ axis. \nCenterPoint-Voxel uses a $(0.1\\text{m}, 0.1\\text{m}, 0.15\\text{m})$ voxel size following PV-RCNN~\\cite{pvrcnn} while CenterPoint-Pillar uses a grid size of $(0.32\\text{m}, 0.32\\text{m})$. \n\n\\paragraph{nuScenes Dataset.}\nnuScenes~\\cite{caesar2019nuscenes} contains 1000 driving sequences, with 700, 150, 150 sequences for training, validation, and testing, respectively.\nEach sequence is approximately 20-second long, with a Lidar frequency of 20 FPS.\nThe dataset provides calibrated vehicle pose information for each Lidar frame but only provides box annotations every ten frames (0.5s).\nnuScenes uses a 32 lanes Lidar, which produces approximately 30k points per frame. \nIn total, there are 28k, 6k, 6k, annotated frames for training, validation, and testing, respectively.\nThe annotations include 10 classes with a long-tail distribution. \nThe official evaluation metrics are an average among the classes.\nFor 3D detection, the main metrics are mean Average Precision (mAP)~\\cite{everingham2010pascal} and nuScenes detection score (NDS).\nThe mAP uses a bird-eye-view center distance $<{0.5\\text{m}, 1\\text{m}, 2\\text{m}, 4\\text{m}}$ instead of standard box-overlap.\nNDS is a weighted average of mAP and other attributes metrics, including translation, scale, orientation, velocity, and other box attributes~\\cite{caesar2019nuscenes}.\nAfter our test set submission, the nuScenes team adds a new neural planning metric~(PKL)~\\cite{philion2020learning}.  \nThe PKL metric measures the influence of 3D object detection for down-streamed autonomous driving tasks based on the KL divergence of a planner's route~(using 3D detection) and the ground truth trajectory.\nThus, we also report the PKL metric for all methods that evaluate on the test set.\n\nFor 3D tracking, nuScenes uses AMOTA~\\cite{weng2019baseline}, which penalizes ID switches, false positive, and false negatives and is averaged among various recall thresholds. \n\nFor experiments on nuScenes, we set the detection range to $[-51.2\\text{m}, 51.2\\text{m}]$ for the $X$ and $Y$ axis, and $[-5\\text{m}, 3\\text{m}]$ for $Z$ axis.\nCenterPoint-Voxel use a $(0.1\\text{m}, 0.1\\text{m}, 0.2\\text{m})$ voxel size and CenterPoint-Pillars uses a $(0.2\\text{m}, 0.2\\text{m})$ grid. \\\\ \n\n\\noindent \n\\textbf{Training and Inference.}\nWe use the same network designs and training schedules as prior works~\\cite{pvrcnn, zhu2019classbalanced}.\nSee supplement for detailed hyper-parameters. % also include network details  \nDuring the training of two-stage CenterPoint, we randomly sample $128$ boxes with $1$:$1$ \npositive negative ratio~\\cite{ren2015faster} from the first stage predictions. \nA proposal is positive if it overlaps with a ground truth annotation with at least 0.55 IoU~\\cite{pvrcnn}.\nDuring inference, we run the second stage on the top 500 predictions after Non-Maxima Suppression~(NMS).\nThe inference times are measured on an Intel Core i7 CPU and a Titan RTX GPU.\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{@{}l@{\\ \\ }l@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{}}\n  \\toprule \n   \\multirow{2}{4em}{Difficulty} & \\multirow{2}{4em}{Method} &  \\multicolumn{2}{c}{Vehicle} & \\multicolumn{2}{c}{Pedestrian}  \\\\\n   & & mAP & mAPH &  mAP & mAPH \\\\\n    \\cmidrule(r){1-2}\n    \\cmidrule(r){3-4}\n    \\cmidrule(r){5-6}\n    \\multirow{5}{3em}{Level 1}\n  & StarNet \\cite{ngiam2019starnet} & 61.5 & 61.0 & 67.8 & 59.9  \\\\   \n  & PointPillars \\cite{pillar} & 63.3 & 62.8 & 62.1 & 50.2   \\\\  \n  & PPBA \\cite{ngiam2019starnet} & 67.5 & 67.0 & 69.7 & 61.7   \\\\   \n  & RCD \\cite{bewley2020range} & 72.0 & 71.6 & \\_ & \\_  \\\\ \n  & Ours & \\textbf{80.2} & \\textbf{79.7} & \\textbf{78.3} & \\textbf{72.1} \\\\\n    \\cmidrule(r){1-2}\n    \\cmidrule(r){3-4}\n    \\cmidrule(r){5-6}\n    \\multirow{5}{3em}{Level 2}\n  & StarNet \\cite{ngiam2019starnet} & 54.9 & 54.5 & 61.1 & 54.0   \\\\   \n  & PointPillars \\cite{pillar} & 55.6 & 55.1 & 55.9 & 45.1   \\\\  \n  & PPBA \\cite{ngiam2019starnet} & 59.6 & 59.1 & 63.0 & 55.8  \\\\   \n  & RCD \\cite{bewley2020range} & 65.1 & 64.7 & \\_ & \\_ \\\\  \n  & Ours & \\textbf{72.2} & \\textbf{71.8} & \\textbf{72.2} & \\textbf{66.4}  \\\\ \n\n\\bottomrule\n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\small\n\\caption{State-of-the-art comparisons for 3D detection on Waymo test set. We show the mAP and mAPH for both level 1 and level 2 benchmarks.}\n\\lbltab{waymo_detection}\n\\end{table}\n}\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{lccc}\n  \\toprule \n  Method  & mAP$\\uparrow$ & NDS$\\uparrow$ & PKL$\\downarrow$ \\\\ \n  \\midrule\n  WYSIWYG \\cite{hu2019exploiting}  & 35.0 & 41.9 & 1.14 \\\\ \n  PointPillars \\cite{pillar} & 40.1 & 55.0 & 1.00 \\\\ \n  CVCNet \\cite{chen2020view}  & 55.3 & 64.4 & 0.92 \\\\ \n  PointPainting \\cite{vora2019pointpainting}  & 46.4 & 58.1 & 0.89 \\\\ \n  PMPNet \\cite{yin2020Lidarbased} & 45.4 & 53.1 & 0.81 \\\\ \n  SSN \\cite{zhu2020ssn} & 46.3 & 56.9 & 0.77 \\\\ \n  CBGS \\cite{zhu2019classbalanced} & 52.8 &  63.3 & 0.77 \\\\   \n \\midrule \n  Ours & \\textbf{58.0} & \\textbf{65.5} & \\textbf{0.69} \\\\  \n  \\bottomrule\n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\small\n\\caption{State-of-the-art comparisons for 3D detection on nuScenes test set. We show the nuScenes detection score~(NDS), and mean Average Precision~(mAP). }\n\\lbltab{nuscenes_detection}\n\\vspace{-3mm}\n\\end{table}\n}\n\n",
                "subsection 5.1": {
                    "name": "Main Results",
                    "content": "\n\n\\paragraph{3D Detection} \nWe first present our 3D detection results on the test sets of Waymo and nuScenes.\nBoth results use a single CenterPoint-Voxel model. \n\\reftab{waymo_detection} and \\reftab{nuscenes_detection} summarize our results.\nOn Waymo test set, our model achieves $71.8$ level 2 mAPH for vehicle detection and $66.4$ level 2 mAPH for pedestrian detection, surpassing previous methods by $7.1\\%$ mAPH for vehicles and $10.6\\%$ mAPH for pedestrians.\nOn nuScenes~(\\reftab{nuscenes_detection}), our model outperforms the last-year challenge winner CBGS~\\cite{zhu2019classbalanced} with multi-scale inputs and multi-model ensemble by $5.2\\%$ mAP and $2.2\\%$ NDS.\nOur model is also much faster, as shown later.\nA breakdown along classes is contained in the supplementary material.\nOur model displays a consistent performance improvement over all categories and shows more significant improvements in small categories ($+5.6$ mAP for traffic cone) and extreme-aspect ratio categories ($+6.4$ mAP for bicycle and $+7.0$ mAP for construction vehicle).\nMore importantly, our model significantly outperforms all other submissions under the neural planar metric (PKL), a hidden metric evaluated by the organizers after our leaderboard submission.\nThis highlights the generalization ability of our framework.\n\n\n\n\n\n\\paragraph{3D Tracking} \n\\reftab{track_waymo} shows CenterPoint's tracking performance on the Waymo test set. \nOur velocity-based closest distance matching described in \\refsec{tracking} significantly outperforms the official tracking baseline in the Waymo paper~\\cite{sun2019scalability}, which uses a Kalman-filter based tracker~\\cite{weng2019baseline}.\nWe observe a $19.4$ and $18.9$ MOTA improvement for vehicle and pedestrian tracking, respectively.\nOn nuScenes (\\reftab{track_nusc}), our framework outperforms the last challenge winner Chiu et al. \\cite{chiu2020probabilistic} by $8.8$ AMOTA. \nNotably, our tracking does not require a separate motion model and runs in a negligible time, $1ms$ on top of detection.\n\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{l@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c}\n  \\toprule \n  Encoder & Method & Vehicle & Pedestrain & mAPH \\\\\n  \\midrule \n    \\multirow{2}{5em}{VoxelNet}\n   & Anchor-based & 66.1 & 54.4 & 60.3  \\\\\n   & Center-based &  \\textbf{66.5} & \\textbf{62.7} & \\textbf{64.6}  \\\\\n  \\midrule\n    \\multirow{2}{5em}{PointPillars}\n  & Anchor-based  &  64.1 & 50.8 & 57.5   \\\\ \n  & Center-based &  \\textbf{66.5} & \\textbf{57.4} & \\textbf{62.0}  \\\\\n\\bottomrule\n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\caption{Comparison between anchor-based and center-based methods for 3D detection on Waymo validation. We show the per-calss and average LEVEL\\_2 mAPH.}\n\\lbltab{waymo_first}\n\\end{table}\n}\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{l@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c}\n  \\toprule \n  Encoder & Method & mAP & NDS \\\\\n  \\midrule \n    \\multirow{2}{5em}{VoxelNet}\n   & Anchor-based & 52.6 & 63.0 \\\\\n   & Center-based & \\textbf{56.4} & \\textbf{64.8}    \\\\\n  \\midrule\n    \\multirow{2}{5em}{PointPillars}\n  & Anchor-based  & 46.2  & 59.1  \\\\ \n  & Center-based & \\textbf{50.3} & \\textbf{60.2}  \\\\\n\\bottomrule\n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\caption{Comparison between anchor-based and center-based methods for 3D detection on nuScenes validation. We show mean average precision (mAP) and nuScenes detection score (NDS).}\n\\lbltab{nusc_det}\n\\vspace{-2mm}\n\\end{table}\n}\n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Ablation studies",
                    "content": "\n\\label{ablation}\n\n\\noindent \n\\textbf{Center-based vs~Anchor-based}\nWe first compare our center-based one-stage detector with its anchor-based counterparts~\\cite{yan2018second, zhu2019classbalanced, pillar}. \nOn Waymo, we follow the state-of-the-art PV-RCNN~\\cite{pvrcnn} to set the anchor hyper-parameters: we use two anchors per-locations with $0$\\textdegree and $90$\\textdegree; The positive/ negative IoU thresholds are set as $0.55/ 0.4$ for vehicles and $0.5/ 0.35$ for pedestrians.\nOn nuScenes, we follow the anchor assignment strategy from the last challenge winner CBGS~\\cite{zhu2019classbalanced}. \nAll other parameters are the same as our CenterPoint model. \n\nAs is shown in \\reftab{waymo_first}, on Waymo dataset, simply\nswitching from anchors to our centers \ngives $4.3$ mAPH and $4.5$ mAPH improvements for VoxelNet and PointPillars encoder, respectively.\nOn nuScenes (\\reftab{nusc_det}) CenterPoint improves anchor-based counterparts by $3.8$-$4.1$ mAP and $1.1$-$1.8$ NDS across different backbones.\nTo understand where the improvements are from, \nwe further show the performance breakdown on different subsets based on object sizes and orientation angles on the Waymo validation set.\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{@{}l@{\\ }c@{\\ }c@{\\ }c@{\\ }c@{\\ }c@{\\ }c@{\\ }c@{}}\n  \\toprule \n   &  \\multicolumn{3}{c}{Vehicle} & \\multicolumn{3}{c}{Pedestrian} \\\\ \n   Rel. yaw & 0\\textdegree-15\\textdegree & 15\\textdegree-30\\textdegree & 30\\textdegree-45\\textdegree & 0\\textdegree-15\\textdegree & 15\\textdegree-30\\textdegree & 30\\textdegree-45\\textdegree\\\\ \n  \\# annot. & 81.4\\% & 10.5\\% & 8.1\\% & 71.4\\% & 15.8\\% & 12.8\\% \\\\ \n\\midrule \n    Anchor-based & 67.1 & \\textbf{47.7} & 45.4 & 55.9 & 32.0 & 26.5  \\\\\n    Center-based & \\textbf{67.8} & 46.4 & \\textbf{51.6} & \\textbf{64.0} & \\textbf{42.1} & \\textbf{35.7}\\\\ \n\\bottomrule\n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\caption{Comparison between anchor-based and center based methods for detecting objects of different heading angles. The ranges of the rotation angle and their corresponding portion of objects are listed in line 2 and line 3. We show the LEVEL\\_2 mAPH for both methods on the Waymo validation. }\n\\lbltab{orientation_ablation}\n\\end{table}\n}\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{@{}l@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{}}\n  \\toprule \n  \\multirow{2}{4em}{Method} &  \\multicolumn{3}{c}{Vehicle} & \\multicolumn{3}{c}{Pedestrian} \\\\ \n   & small & medium & large & small & medium & large\\\\ \n\\midrule \n    Anchor-based & 58.5 & \\textbf{72.8} & 64.4 & 29.6 & 60.2 & 60.1  \\\\\n    Center-based & \\textbf{59.0} & 72.4 & \\textbf{65.4} & \\textbf{38.5} & \\textbf{69.5} & \\textbf{69.0} \\\\ \n\\bottomrule\n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\caption{Effects of object size for the performance of anchor-based and center-based methods. We show the per-class LEVEL\\_2 mAPH for objects in different size range:~{small 33\\%, middle 33\\%, and large 33\\%}}\n\\lbltab{size_ablation}\n\\vspace{-3mm}\n\\end{table}\n}\n\n\nWe first divide the ground truth instances into three bins based on their heading angles: 0\\textdegree to 15\\textdegree, 15\\textdegree to 30\\textdegree, and 30\\textdegree to 45\\textdegree.\nThis division tests the detector's performance for detecting heavily rotated boxes, which is critical for the safe deployment of autonomous driving. \nWe also divide the dataset into three splits: small, medium, and large, and each split contains $\\frac{1}{3}$ of the overall ground truth boxes.  \n\n\\reftab{orientation_ablation} and \\reftab{size_ablation} summarize the results. \nOur center-based detectors perform much better than the anchor-based baseline when the box is rotated or deviates from the average box size, demonstrating the model's ability to capture the rotation and size invariance when detecting objects. \nThese results convincingly highlight the advantage of using a point-based \nrepresentation of 3D objects.\\\\\n\n\\noindent \n\\textbf{One-stage vs. Two-stage}\nIn \\reftab{waymo_second}, we show the comparison between single and two-stage CenterPoint models using 2D CNN features on Waymo validation. \nTwo-stage refinement with multiple center features gives a large accuracy boost to both 3D encoders with small overheads (6ms-7ms).\nWe also compare with RoIAlign, which densely samples $6\\times6$ points in the RoI~\\cite{PartA, pvrcnn}, our center-based feature aggregation achieved comparable performance but is faster and simpler. \n\nThe voxel quantization limits two-stage CenterPoint's improvements for pedestrian detection with PointPillars as pedestrians typically only reside in 1 pixel in the model input. \n\nTwo-stage refinement does not bring an improvement over the single-stage CenterPoint model on nuScenes in our experiments. \nWe think the reason is that the nuScenes dataset uses 32 lanes Lidar, which produces about 30k Lidar points per frame, about $\\frac{1}{6}$ of the number of points in the Waymo dataset, which limits the potential improvements of two-stage refinement. \nSimilar results have been observed in previous two-stage methods like PointRCNN~\\cite{shi2019pointrcnn} and PV-RCNN~\\cite{pvrcnn}.\n\n\n\n\n\n{\n\\begin{table}[t]\n\\small\n\\begin{center}\n\\begin{tabular}{@{}l@{\\ \\ }l@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{\\ \\ }c@{}}\n  \\toprule \n   \\multirow{2}{4em}{Difficulty} & \\multirow{2}{4em}{Method} &  \\multicolumn{2}{c}{Vehicle} & \\multicolumn{2}{c}{Pedestrian}  \\\\\n   & & mAP & mAPH &  mAP & mAPH \\\\\n       \\cmidrule(r){1-2}\n    \\cmidrule(r){3-4}\n    \\cmidrule(r){5-6}\n  \\multirow{7}{3em}{Level 1}  \n  & DOPS \\cite{najibi2020dops} & 56.4 & \\_ & \\_ \\\\ \n  & PointPillars \\cite{pillar}  & 56.6 & \\_  &  59.3 & \\_  \\\\  \n  & PPBA \\cite{ngiam2019starnet} & 62.4 & \\_ & 66.0 & \\_  \\\\   \n  & MVF \\cite{mvf}  & 62.9 & \\_ & 65.3& \\_  \\\\ \n  & Huang et al. \\cite{huang2020lstm}  & 63.6 & \\_ & \\_  \\\\\n  & AFDet \\cite{ge2020afdet} & 63.7 & \\_ & \\_ \\\\ \n  & CVCNet \\cite{chen2020view} & 65.2 & \\_ & \\_ \\\\ \n  & Pillar-OD \\cite{wang2020pillar}  & 69.8 & \\_ & 72.5& \\_  \\\\ \n  & PV-RCNN \\cite{pvrcnn}  & 74.4 & 73.8 & 61.4& 53.4  \\\\\n  & CenterPoint-Pillar(ours) & 76.1 & 75.5 & 76.1 & 65.1  \\\\ \n  & CenterPoint-Voxel(ours) & \\textbf{76.7} & \\textbf{76.2} & \\textbf{79.0} & \\textbf{72.9}  \\\\\n  \\midrule\n  \\multirow{2}{3em}{Level 2}  \n  & PV-RCNN \\cite{pvrcnn} & 65.4 & 64.8 & 53.9 & 46.7 \\\\ \n  & CenterPoint-Pillar(ours) & 68.0 & 67.5 & 68.1 & 57.9 \\\\ \n  & CenterPoint-Voxel(ours) &  \\textbf{68.8} & \\textbf{68.3} & \\textbf{71.0} & \\textbf{65.3} \\\\ \n  \\bottomrule\n \n \\end{tabular}\n\\end{center}\n\\vspace{-5mm}\n\\caption{State-of-the-art comparisons for 3D detection on Waymo validation.}\n\\lbltab{waymo_val}\n\\end{table}\n}\n\n\n\n\n\n\n\\noindent \n\\textbf{Effects of different feature components}\nIn our two-stage CenterPoint model, we only use features from the 2D CNN feature map. \nHowever, previous methods propose to also utilize voxel features for second stage refinement~\\cite{pvrcnn, PartA}. \nHere, we compare with two voxel feature extraction baselines:\n\n\\textbf{Voxel-Set Abstraction.}\nPV-RCNN~\\cite{pvrcnn} proposes the Voxel-Set Abstraction (VSA) module, which extends PointNet++~\\cite{qi2017pointnet++}'s set abstraction layer to aggregate voxel features in a fixed radius ball.\n\n\\textbf{Radial basis function (RBF) Interpolation.}\nPointNet++\\cite{qi2017pointnet++} and SA-SSD\\cite{sassd} use a radial basis function to aggregate grid point features from three nearest non-empty 3D feature volumes. \n\nFor both baselines, we combine bird-eye view features with voxel features using their official implementations.  \n\\reftab{waymo_voxel} summarizes the results. \nIt shows bird-eye view features are sufficient for good performance while being more efficient comparing to voxel features used in the literatures~\\cite{pvrcnn,sassd,qi2017pointnet++}.\n\nTo compare with prior work that did not evaluate on Waymo test, we also report results on the Waymo validation split in \\reftab{waymo_val}.\nOur model outperforms all published methods by a large margin, especially for the challenging pedestrian class(+18.6 mAPH) of the level 2 dataset, where boxes contain as little as one Lidar point.   \n\n\\noindent \n\\textbf{3D Tracking.} \n\\reftab{track_ablation} shows the ablation experiments of 3D tracking on nuScenes validation.  \nWe compare with last year's challenge winner Chiu et al.~\\cite{chiu2020probabilistic}, which uses mahalanobis distance-based Kalman filter to associate detection results of CBGS~\\cite{zhu2019classbalanced}.  \nWe decompose the evaluation into the detector and tracker to make the comparison strict. \nGiven the same detected objects, using our simple velocity-based closest point distance matching outperforms the Kalman filter-based Mahalanobis distance matching~\\cite{chiu2020probabilistic} by $3.7$ AMOTA (line 1 vs. line 3 and line 2 vs. line4). \nThere are two sources of improvements: 1) we model the object motion with a learned point velocity, rather than modeling 3D bounding box dynamic with a Kalman filter; 2) we match objects by center point-distance instead of a Mahalanobis distance of box states or 3D bounding box IoU. \nMore importantly, our tracking is a simple nearest neighbor matching without any hidden-state computation.\nThis saves the computational overhead of a 3D Kalman filter~\\cite{chiu2020probabilistic} (73ms vs. 1ms).\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nWe proposed a center-based framework for simultaneous 3D object detection and tracking from the Lidar point-clouds. \nOur method uses a standard 3D point-cloud encoder with a few convolutional layers in the head to produce a bird-eye-view heatmap and other dense regression outputs.\nDetection is a simple local peak extraction with refinement, and tracking is a closest-distance matching.\nCenterPoint is simple, near real-time, and achieves state-of-the-art performance on the Waymo and nuScenes benchmarks.\n\n\n% -------------\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n\\clearpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Tracking algorithm",
                "content": "\n\n\\begin{algorithm}\n    \\caption{{Center-based Tracking}}\n\t\\label{alg:association}\n\t\\SetAlgoLined\n\t\\SetKwInOut{Input}{Input} \\SetKwInOut{Output}{Output} \n    \\Input{$T^{(t - 1)} = \\{(\\vec{p}, \\vec{v}, {c}, \\vec{q}, id, a)_j^{(t-1)}\\}_{j=1}^{M}$:\n    Tracked objects in the previous frame, with center $\\vec p$, ground plane velocity $\\vec v$, category label $c$, other bounding box attributes $\\vec{q}$, tracking id $id$, and inactive age $a$ (active tracks will have $a=0$). \\\\\n    $\\hat{D}^{(t)} = \\{(\\hat{\\vec p}, \\hat{\\vec v}, \\hat{c}, \\hat{\\vec q})_i^{(t)}\\}_{i=1}^{N}$: Detections in the current frame in descending confidence.\n    \\\\ }\n\t\\Output{\n\t        $T^{(t)} = \\{(\\vec{p}, \\vec{v}, {c}, \\vec{q}, id, a)_{j=1}^{K} \\}$: Tracked Objects.\n\t}\n    \\textbf{Hyper parameters:} Matching distance threshold $\\tau$; Max inactive age $A$.\\\\\n    \\textbf{Initialization:} Tracks $T^{(t)}$, and matches $\\mathcal{S}$ are initialized as empty sets. \\label{alg:st} \\\\\n    $T^{(t)} \\leftarrow \\emptyset$, $\\mathcal{S} \\leftarrow \\emptyset$ \\\\\n\t$F \\leftarrow Cost(\\hat{D}^{(t)}, T^{(t-1)})$ \\ \\ // $F_{ij} = ||\\hat{\\vec{p}}_i^{(t)}-\\hat{\\vec{v}}, \\vec{p}_j^{(t-1)}||_2$ \\\\\n\t\\For{$i \\leftarrow 1 \\ to \\ N$}{\n\t    $j \\leftarrow \\argmin_{j \\notin \\mathcal{S}} F_{ij}$ \\\\\n\t    // Class-wise distance threshold $\\vec{\\tau}_{c}$ \\\\\n\t    \\If {$\\vec{F}_{ij} \\leq \\vec{\\tau}_{c}$}{\n            // Associate with tracked object \\\\\n            $a_{i}^{(t)} \\leftarrow 0$ \\\\\n            $T^{(t)} \\leftarrow T^{(t)} \\cup \\{(\\hat{D}_{i}^{(t)}, id_j^{(t-1)}, a_{i}^{(t)})\\}$ \\\\\n            $\\mathcal{S} \\leftarrow \\mathcal{S} \\cup \\{j\\}$ \n            // Mark track $j$ as matched\\\\\n\t    }\n\t    \\Else {\n\t        // Initialize a new track \\\\\n\t        $a_{i}^{(t)} \\leftarrow 0$ \\\\\n\t        $T^{(t)} \\leftarrow T^{(t)} \\cup \\{(\\hat{D}_{i}^{(t)}, newID, a_{i}^{(t)})\\}$ \n\t    }\n\t}\n\t\\For{$j \\leftarrow 1 \\ to \\ M $} {\n\t    \\If {$j \\notin \\mathcal{S}$}{ // Unmatched tracks \\\\\n\t        \\If {$T.a_{j}^{(t-1)} < A$}{\n\t            $T.a_{j}^{(t)} \\leftarrow T.a_{j}^{(t-1)} + 1$ \\\\\n\t            $T.p_{j}^{(t)} \\leftarrow T.p_{j}^{(t-1)} + T.v_{j}^{(t-1)}$ // Update the center location \\\\ \n\t            $T^{(t)} \\leftarrow T^{(t)} \\cup \\{T^{(t-1)}_j\\}$  \\\\ \n\t        }\n\t    }\n\t}\n    \\textbf{Return} $T^{(t)}$\n\\end{algorithm}\n\n"
            },
            "section 8": {
                "name": "Implementation Details",
                "content": "\nOur implementation is based on the open-sourced code of CBGS~\\cite{zhu2019classbalanced}\\footnote{\\url{https://github.com/poodarchu/Det3D}}.\nCBGS provides implementations of PointPillars~\\cite{pillar} and VoxelNet~\\cite{voxelnet} on nuScenes. \nFor Waymo experiments, we use the same architecture for VoxelNet and increases the output stride to 1 for PointPillars~\\cite{pillar} following the dataset's reference implementation\\footnote{\\url{https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car}}. \n\nA common practice~\\cite{zhu2019classbalanced, yang20203dssd, vora2019pointpainting, caesar2019nuscenes} in nuScenes is to transform and merge the Lidar points of non-annotated frames into its following annotated frame.\nThis produces a denser point-cloud and enables a more reasonable velocity estimation.\nWe follow this practice in all nuScenes experiments. \n\nFor data augmentation, we use random flipping along both $X$ and $Y$ axis,\nand global scaling with a random factor from $[0.95, 1.05]$.\nWe use a random global rotation between $[-\\pi/8, \\pi/8]$ for nuScenes~\\cite{zhu2019classbalanced} and $[-\\pi/4, \\pi/4]$ for Waymo~\\cite{pvrcnn}.\nWe also use the ground-truth sampling~\\cite{yan2018second} on nuScenes to deal with the long tail class distribution, which copies and pastes points inside an annotated box from one frame to another frame.  \n\nFor nuScenes dataset, we follow CBGS~\\cite{zhu2019classbalanced} to optimize the model using AdamW~\\cite{adamW} optimizer with one-cycle learning rate policy~\\cite{one_cycle},\nwith max learning rate 1e-3, weight decay 0.01, and momentum $0.85$ to $0.95$.\nWe train the models with batch size 16 for 20 epochs on 4 V100 GPUs.\n\nWe use the same training schedule for Waymo models except a learning rate 3e-3, and we train the model for 30 epochs following PV-RCNN~\\cite{pvrcnn}. \nTo save computation on large scale Waymo dataset, we finetune the model for 6 epochs with second stage refinement modules for various ablation studies. \nAll ablation experiments are conducted in this same setting.\n\nFor the nuScenes test set submission, we use a input grid size of $0.075m \\times 0.075m$ and add two separate deformable convolution layers~\\cite{dai2017deformable} in the detection head to learn different features for classification and regression.  \nThis improves CenterPoint-Voxel's performance from $64.8$ NDS to $65.4$ NDS on nuScenes validation. \nFor the nuScenes tracking benchmark, we submit our best CenterPoint-Voxel model with flip testing, which yields a result of $66.5$ AMOTA on nuScenes validation.  \n\n"
            },
            "section 9": {
                "name": "nuScenes Performance across classes",
                "content": "\nWe show per-class comparisons with state-of-the-art methods in \\reftab{nusc_per_cls}. \n\n\n\n\n\n\n\n"
            },
            "section 10": {
                "name": "nuScenes Detection Challenge",
                "content": "\nAs a general framework, CenterPoint is complementary to contemporary methods and was used by three of the top 4 entries in the NeurIPS 2020 nuScenes detection challenge.  \nIn this section, we describe the details of our winning submission which significantly improved 2019 challenge winner CBGS~\\cite{zhu2019classbalanced} by $14.3$ mAP and $8.1$ NDS. \nWe report some improved results in \\reftab{nusc_challenge}. \nWe use PointPainting~\\cite{vora2019pointpainting} to annotate each lidar point with image-based instance segmentation results generated by a Cascade RCNN model trained on nuImages\\footnote{acquired from \\url{https://github.com/open-mmlab/mmdetection3d/tree/master/configs/nuimages}}. \nThis improves the NDS from $65.4$ to $68.0$. \nWe then perform two test-time augmentations including double flip testing and point-cloud rotation around the yaw axis. \nSpecifically, we use [0\\textdegree, $\\pm$ 6.25\\textdegree, $\\pm$ 12.5\\textdegree, $\\pm$ 25\\textdegree] for yaw rotations.\nTheses test time augmentations improve the NDS from $68.0$ to $70.3$.\nIn the end, we ensemble five models with input grid size between $[0.05m, 0.05m]$ to $[0.15m, 0.15m]$ and filter out predictions with zero number of points, which yields our best results on nuScenes validation, with $68.2$ mAP and $71.7$ NDS. \n\n\n\n"
            }
        },
        "equations": {
            "eq:1": "\\begin{equation}\n   I = \\min(1, \\max(0, 2\\times IoU_{t} - 0.5))\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    L_{score} =  -I_t \\log(\\hat{I_t}) - (1-I_t)\\log(1-\\hat{I_t}) \n\\end{equation}"
        },
        "git_link": "https://github.com/tianweiy/CenterPoint"
    }
}