{
    "meta_info": {
        "title": "MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer",
        "abstract": "Monocular 3D object detection is an important yet challenging task in\nautonomous driving. Some existing methods leverage depth information from an\noff-the-shelf depth estimator to assist 3D detection, but suffer from the\nadditional computational burden and achieve limited performance caused by\ninaccurate depth priors. To alleviate this, we propose MonoDTR, a novel\nend-to-end depth-aware transformer network for monocular 3D object detection.\nIt mainly consists of two components: (1) the Depth-Aware Feature Enhancement\n(DFE) module that implicitly learns depth-aware features with auxiliary\nsupervision without requiring extra computation, and (2) the Depth-Aware\nTransformer (DTR) module that globally integrates context- and depth-aware\nfeatures. Moreover, different from conventional pixel-wise positional\nencodings, we introduce a novel depth positional encoding (DPE) to inject depth\npositional hints into transformers. Our proposed depth-aware modules can be\neasily plugged into existing image-only monocular 3D object detectors to\nimprove the performance. Extensive experiments on the KITTI dataset demonstrate\nthat our approach outperforms previous state-of-the-art monocular-based methods\nand achieves real-time detection. Code is available at\nhttps://github.com/kuanchihhuang/MonoDTR",
        "author": "Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, Winston H. Hsu",
        "link": "http://arxiv.org/abs/2203.10981v2",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "Accepted to CVPR 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\n\nThree-dimensional (3D) object detection is a fundamental problem and enables various applications such as autonomous driving. Previous methods have achieved superior performance based on the accurate depth information from multiple sensors, such as LiDAR signal \\cite{Lang2019pointpillars, shi2019pointrcnn, he2020sassd, shi2020pv} or stereo matching \\cite{li2019stereorcnn, wang2019pseudo,chen2020dsgn, sun2020disprcnn}. In order to lower the sensor costs, some image-only monocular 3D object detection methods \\cite{Mousavian2017deep3dbox,ku2019monopsr, ma2019am3d, chen2020monopair, brazil2020kinematic,wang2021fcos3d} have been proposed and made impressive progress relying on geometry constraints between 2D and 3D. However, the performance is still far from satisfactory without the aid of depth cues.\n\nRecently, several works have tried to produce estimated depth from the pre-trained depth estimation models to assist monocular 3D object detection.\nPseudo-LiDAR-based approaches \\cite{ma2019am3d, wang2019pseudo, weng2019mono3dplidar} convert estimated depth maps into 3D point clouds to imitate LiDAR signals,\nfollowed by the existing LiDAR-based detector for 3D object detection (see Figure \\ref{fig:example}(a)). \nSome fusion-based approaches \\cite{ding2020d4lcn, wang2021ddmp, ouyang2021ddf} apply several fusion strategies to combine features extracted from depths and images to detect objects (see Figure \\ref{fig:example}(b)).\nThese methods, though better localize objects with the help of estimated depth, may suffer from the risk of learning 3D detection on inaccurate depth maps.\nAlso, the additional computational cost of the depth estimator makes it impractical for real-world applications \\cite{Ma2021monodle}.\n\nTo address the above issues, we propose MonoDTR, a novel end-to-end depth-aware transformer network for monocular 3D object detection (see Figure \\ref{fig:example}(c)).\nA depth-aware feature enhancement (DFE) module is introduced to learn depth-aware features with the auxiliary depth supervision, which avoids obtaining inaccurate depth priors from the pre-trained depth estimator.\nFurthermore, the DFE module is lightweight yet effective in assisting 3D object detection without constructing the complicated architecture to extract features from off-the-shelf depth maps. It significantly reduces computational time compared with previous depth-assisted methods \\cite{ma2019am3d, ding2020d4lcn, wang2021ddmp} (see Table \\ref{tab:kitti_test_car}).\n\nIn addition, unlike previous fusion-based methods (\\eg, D$^4$LCN\\cite{ding2020d4lcn} and DDMP-3D\\cite{wang2021ddmp}) that apply carefully designed convolution kernels for context- and depth-aware features, \nwe develop the first transformer-based fusion module to globally integrate the image and depth information.\nThe transformer encoder-decoder structure \\cite{vaswani2017SA} has been proven to capture long-range dependency effectively; thus, we apply it to model the relationship between context- and depth-aware features.\nTo better represent the property of the 3D object, we utilize depth-aware features to replace the commonly used object queries \\cite{Nicolas2020detr, zhu2021deform, kim2021hotr} as input of the transformer decoder, which can provide more meaningful cues for 3D reasoning.\nFurthermore, we introduce a novel depth positional encoding (DPE) to involve depth-aware hints to the transformer, achieving better performance than conventional pixel-wise positional encodings.\n\nWe summarize our contributions as follows:\n\\begin{enumerate}\n    \\item\n    We propose a novel framework, MonoDTR, learning depth-aware features via auxiliary supervision to assist monocular 3D object detection,\n    which avoids introducing high computational cost and inaccurate depth priors from using the off-the-shelf depth estimator.\n\n    \\item We present the first depth-aware transformer module to integrate context- and depth-aware features efficiently. A novel depth positional encoding (DPE) is proposed to inject depth positional hints into the transformer. \n    \n    \\item Experimental results on the KITTI dataset show that our approach outperforms state-of-the-art monocular-based methods and achieves real-time detection. Furthermore, the proposed depth-aware modules can be easily plug-and-play in existing image-only frameworks to improve performance.\n\n\\end{enumerate}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n\\noindent {\\bf Image-only monocular 3D object detection.}\nRecently, several works only adopt a single image for 3D object detection \\cite{Mousavian2017deep3dbox, andrea2019monodis, brazil2019m3drpn, roddick2019oft, liu2020SMOKE, Simonelli2020MoVi3D, liu2021ground, wang2021pgd}. Due to the lack of depth information from images, these methods mainly rely on geometric consistency to predict objects.\n%Deep3Dbox \nDeep3Dbox \\cite{Mousavian2017deep3dbox} solves orientation prediction by proposed novel {\\it MultiBin} loss and enforces constraint between 2D and 3D boxes with geometric prior. \n%M3D-RPN\nM3D-RPN \\cite{brazil2019m3drpn} generates 3D object proposals with 2D bounding box constraints and proposes a depth-aware convolution to predict 3D objects.\n%MonoDIS\nOFTNet\\cite{roddick2018orthographic} introduces an orthographic feature transform to map image-based features into a 3D voxel space.\n%monopair\nBesides, MonoPair \\cite{chen2020monopair} explores spatial pair-wise relationship between objects to improve detection performance.\n%M3DSSD\nM3DSSD \\cite{luo2021m3dssd} presents a two-step feature alignment approach to solve the feature mismatching problem.\nFurthermore, some works \\cite{liu2020SMOKE,li2020rtm3d, Ma2021monodle, Zhang2021MonoFlex} predict keypoints of the 3D bounding box as an intermediate task to recover the location of objects. \nHowever, the above purely monocular methods fail to accurately localize objects due to the lack of depth cues.\n\n\\noindent {\\bf Depth-assisted monocular 3D object detection.}\nTo further improve the performance, many approaches propose using depth information to aid 3D object detection \\cite{you2020pseudo, weng2019mono3dplidar, ma2019am3d, ma2020patchnet, ding2020d4lcn, wang2021ddmp}. Some prior works \\cite{wang2019pseudo, weng2019mono3dplidar, ma2019am3d} transform image into pseudo-LiDAR representation by leveraging off-the-shelf depth estimator and calibration parameters, followed by the existing LiDAR-based 3D detector to predict objects, leading to progressive improvement. PatchNet \\cite{ma2020patchnet} reveals that the success of pseudo-LiDAR comes from the coordinate transformation and organizes it into the image representation, which can benefit from the powerful CNNs networks.\nD$^4$LCN\\cite{ding2020d4lcn} and DDMP-3D\\cite{wang2021ddmp} focus on developing the fusion-based approach between image and estimated depth with carefully designed convolutional networks. \nBesides, CaDDN \\cite{Reading2021CaDDN} learns categorical depth distributions for each pixel to construct birdâ€™s-eye-view (BEV) representations and recovers bounding boxes from the BEV projection. \nHowever, most of the abovementioned methods directly using pre-trained depth estimators suffer from additional computational cost and only achieve limited performance caused by inaccurate depth priors.\n\n\n\\noindent {\\bf Transformer.}\n%transformer \nTransformer \\cite{vaswani2017SA} was firstly introduced in sequential modeling and has considerable improvement in natural language processing (NLP) tasks.\nThe self-attention mechanism is the core component in the transformer with its capability of capturing the long-range dependencies. Recently, transformer architecture has been successfully leveraged in the computer vision field, such as image classification \\cite{dosovitskiy2020vit} and human-object interaction \\cite{kim2021hotr}. \nIn addition, DETR \\cite{Nicolas2020detr} proposes developing object detection with the transformer without relying on many hand-designed components used in traditional pipelines.\n\nThough the transformer can perform well in most visual tasks, its usage in monocular 3D object detection has not been explored.\nIn the image-based 3D detection task, the object size at far and near distance in the image varies significantly due to the perspective projection \\cite{ding2020d4lcn, wang2021ddmp}, which makes it challenging to utilize the learned object query mentioned in DETR\\cite{Nicolas2020detr} to fully represent the object property.\nThus, in this paper, we propose to globally integrate context- and depth-aware features with transformers and inject depth hints into the transformer for better 3D reasoning.\n\n\n\n"
            },
            "section 3": {
                "name": "Proposed Approach",
                "content": "\n\\label{sec:proposed}\n",
                "subsection 3.1": {
                    "name": "Framework Overview",
                    "content": "\nFigure \\ref{fig:arch} presents the framework of our MonoDTR, which mainly consists of four components: the backbone, the depth-aware feature enhancement (DFE) module, the depth-aware transformer (DTR) module, and the 2D-3D detection head. We adopt DLA-102 \\cite{fisher2018dla} as our backbone network following \\cite{luo2021m3dssd}. Given an input RGB image with resolution $H_{\\mathrm{inp}}$ $\\times$ $W_{\\mathrm{inp}}$, the backbone outputs a feature map $\\mathbf{F} \\in \\mathbb{R}^{C \\times H \\times W}$, where $H = \\frac{H_{\\mathrm{inp}}}{8}$, $W =\\frac{W_{\\mathrm{inp}}}{8}$, and $C=256$. \nThe DFE module is presented to implicitly learn depth-aware features (Section \\ref{sec:dam}), while several convolution layers are applied to extract context-aware features in parallel.\nThen, we globally integrate two kinds of features by the DTR module and first attempt to inject depth positional hints into the transformer through the depth positional encoding (DPE) module (Section \\ref{sec:dgtr}).\nConsequently, the anchor-based detection heads and loss functions are adopted for 2D and 3D object detection (Section \\ref{sec:loss}).\n\n"
                },
                "subsection 3.2": {
                    "name": "Depth-Aware Feature Enhancement Module",
                    "content": "\n\\label{sec:dam}\n\n\n\nExisting depth-assisted methods \\cite{ding2020d4lcn, wang2021ddmp, wang2019pseudo}, using off-the-shelf depth estimators, suffer from the risk of introducing inaccurate depth priors and extra computation burden.\nTo alleviate this, we propose a depth-aware feature enhancement (DFE) module for depth reasoning as in Figure \\ref{fig:dam}.\nThe precise depth map is utilized for auxiliary supervision in the training stage, making the DFE module implicitly learn the depth-aware features.\nCompared with previous works that apply an additional backbone \\cite{ding2020d4lcn, wang2021ddmp} or complicated architectures \\cite{Reading2021CaDDN} to encode depths, we generate depth-aware features to assist 3D object detection with a lightweight module, significantly reducing the computation budget.\n\n\\smallskip\\noindent\\textbf{Learning initial depth-aware feature.}\nTo generate depth-aware features, we leverage an auxiliary depth estimation task and consider it as a sequential classification problem \\cite{FuCVPR18-DORN, Reading2021CaDDN}.\nAs illustrated in Figure \\ref{fig:dam}(a), given the input feature $\\mathbf{F} \\in \\mathbb{R}^{C \\times H \\times W}$ from the backbone, we adopt two convolution layers to predict the probability of discretized depth bins $\\mathbf{D} \\in \\mathbb{R}^{D \\times H \\times W}$, where $D$ is the number of depth categories (bins). \nThe probability represents the confidence that the depth value of each pixel belongs to a certain depth bin.\nTo discretize the depth ground truth from continuous space to discretization intervals, we utilize linear-increasing discretization (LID) \\cite{tang2020center3d, Reading2021CaDDN} to formulate the depth bins (more details can be found in supplementary materials).\nTo this end, the intermediate feature map $\\mathbf{X} \\in \\mathbb{R}^{C \\times H \\times W}$ can be regarded as initial depth-aware features.\n\n\\smallskip\\noindent\\textbf{Depth prototype representation learning.}\nTo further enhance the capability of depth representation, we augment the feature of each pixel by introducing the central representation of the corresponding depth category (bin), inspired from the class center in \\cite{zhang2019acfnet}.\nThe feature center of each depth category (regarded as the depth prototype) can be computed by aggregating the depth-aware features of each pixel belonging to a specified category.\nIn practice, we first apply a group convolution \\cite{Krizhevsky2012alexnet} to the predicted depth map $\\mathbf{D}$ to merge the adjacent depth categories (bins), reducing the class number from $D$ to $D'=D/r$ with scale $r$. It helps to share similar depth cues and reduce computation.\nThe representation of depth prototype $\\mathbf{F}_d$ can be generated by gathering the feature of all pixels $\\mathbf{X'}$ weighted by their probability to the depth category $d$: \n\\begin{align}\n    \\mathbf{F}_d = \\sum_{i \\in \\mathcal{I}} {\\tilde{P}_{di}} \\mathbf{X'}_i,  d=\\{1,..., D'\\},\n    \\label{eq:regionrepresentation}\n\\end{align}\nwhere $\\mathbf{X'}_i$ denotes the feature of the $i$th pixel in $\\mathbf{X'}$, $\\mathcal{I} \\in \\mathbb{R}^{H \\times W}$ is the set of pixels in the feature map, and $\\tilde{P}_{di}$ is the normalized probability to $d$th depth prototype. In this manner, $\\mathbf{F}_d$ can express the global context information of each depth category as shown in Figure \\ref{fig:dam}(b). \n\n\\smallskip\\noindent\\textbf{Feature enhancement with depth prototype.}\nNow we can reconstruct new depth-aware features based on the depth prototype representation, \nwhich allows each pixel to understand the presentation of the depth category from the global view.\nThe reconstructed feature $\\mathbf{F}'$ is calculated as:\n\\begin{align}\n    \\mathbf{F}' = \\sum_{d=1}^{D'} {\\tilde{P}_{di}} \\mathbf{F}_d. \\label{eq:recons}\n\\end{align}\nConsequently, we obtain the enhanced depth feature by concatenating the initial depth-aware feature $\\mathbf{X}$ and the reconstructed features $\\mathbf{F}'$, followed by a simple 1 $\\times$ 1 convolution layer, as shown in Figure \\ref{fig:dam}(c).\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Depth-Aware Transformer",
                    "content": "\n\\label{sec:dgtr}\n\nInspired by the tremendous success of the transformer \\cite{vaswani2017SA} on modeling the long-range relationships, we exploit the transformer encoder-decoder architecture to construct the depth-aware transformer (DTR) module to globally integrate the context- and depth-aware features.\n\n\\smallskip\\noindent\\textbf{Transformer encoder.}\nOur transformer encoder aims to improve context-aware features, which is built similar to previous works \\cite{Nicolas2020detr, zou2021_hoitrans}. The main component in the transformer is the self-attention mechanism \\cite{vaswani2017SA}. \nGiven the inputs: query $\\mathbf{Q} \\in \\mathbb{R}^{N \\times C}$, key $\\mathbf{K} \\in \\mathbb{R}^{N \\times C}$, and value $\\mathbf{V} \\in \\mathbb{R}^{N \\times C}$ \nwith sequence length $N$, \na single head self-attention layer can be briefly formulated as:\n\\begin{equation}\n    {\\rm{Attention}}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n    = {\\rm{softmax}}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{C}})\\mathbf{V}.\n\\label{eq-att}\n\\end{equation}\nWe take the flatten context-aware feature $ \\mathbf{X}_c \\in \\mathbb{R}^{N \\times C}$, where $N=H \\times W$, as the input to feed into the transformer encoder. The encoded context-aware feature can be obtained through multi-head self-attention operation and the feed-forward network (FFN). \n\n\\smallskip\\noindent\\textbf{Transformer decoder.}\nThe decoder is also built upon the standard transformer architecture.\nWe propose utilizing the depth-aware features as the input of the decoder instead of learnable embeddings (object query) \\cite{Nicolas2020detr}, which is different from the common usage in previous encoder-decoder vision transformer works \\cite{Nicolas2020detr,zhu2021deform, kim2021hotr, tan2021planeTR}.\nThe main reason is that, in the monocular 3D object detection task, the camera views at near and far distances often cause significant changes in object scale due to the perspective projection \\cite{ding2020d4lcn, wang2021ddmp}.\nIt makes the simple learnable embedding hard to fully represent the object's property and handle complex scale variant situations. In contrast, plentiful distance-aware cues are hidden in the depth-aware features. \nThus, we propose adopting depth-aware features as the input of the transformer decoder.\nTo this end, the decoder can take the power of cross-attention modules in the transformer to efficiently model the relationship between context- and depth-aware features, leading to better performance.\n\n\n\n\\smallskip\\noindent\\textbf{Depth positional encoding (DPE).}\nPositional encoding \\cite{vaswani2017SA} plays an important role for the transformer to introduce the location information. It is often generated with sinusoidal functions or in a learnable manner according to the pixel location of the image in vision tasks.\nObserving that the depth information is much better for the machine to understand the 3D world than the pixel-level relation, we first propose a general depth positional encoding (DPE) module to embed the depth positional hints for each pixel to the transformer.\nSpecifically, as shown in Figure \\ref{fig:transformer}, the depth bin encodings  $\\mathbf{E}_{d} = [\\bm{e}_{1},\\dots,\\bm{e}_{D}] \\in \\mathbb{R}^{D \\times C}$\nare constructed with learnable embeddings for each depth interval introduced in Section \\ref{sec:dam}.\nThe initial depth positional encoding $\\mathbf{P} \\in \\mathbb{R}^{H \\times W \\times C}$ can be looked up from $\\mathbf{E}_{d}$ according to the argmax of each pixel predicted depth category $\\mathbf{D}$.\nTo further represent the positional cues from local neighborhoods, \na convolution layer $\\mathcal{G}$ with the kernel size of 3 $\\times$ 3 is applied and added to $\\mathbf{P}$\nto obtain final encoding, referred to as depth positional encoding (DPE).\n\n\\smallskip\\noindent\\textbf{Computation reduction.}\nThe standard self-attention layer in Equation \\ref{eq-att} leads to $\\mathcal{O}(N^2)$ time and memory, which damages the computational budget.\nTo mitigate this issue, more recent works \\cite{choromanski2020rethinking, katharopoulos2020rnn, wang2020linformer} make efforts on accelerating the attention operation. Among these methods, Linear transformer \\cite{katharopoulos2020rnn} proposes to approximate softmax operation with the linear dot product of features. \nSpecifically, the similarity function in original transformer \\cite{vaswani2017SA} can be formulated as: ${\\rm sim}(q,k)={\\rm exp}(\\frac{q^\\top k}{\\sqrt{C}})$. \nIt is replaced with ${\\rm sim}(q,k)=\\phi(q)\\phi(k)$ in \\cite{katharopoulos2020rnn}\n, where $\\phi(x)=\\text{elu}(x)+1$, and $\\text{elu}(\\cdot)$ is the exponential linear unit\n\\cite{Clevert2016elu} activation function. \nTo this end, $\\phi(K)^\\top$ and $V$ can be combined first to reduce computation to $\\mathcal{O}(N)$. We refer the readers to \\cite{katharopoulos2020rnn} for more details. \nIn our transformer, we consider applying the linear attention described in \\cite{katharopoulos2020rnn} to replace the vanilla self-attention for higher inference speed.\n\n"
                },
                "subsection 3.4": {
                    "name": "2D-3D Detection and Loss ",
                    "content": "\n\\label{sec:loss}\n\\noindent {\\bf Anchor definition.}\nWe adopt the single-stage detector \\cite{Joseph2018yolov3, liu2016ssd} with the pre-defined 2D-3D anchors to regress the bounding box.\nEach pre-defined anchor consists parameters with 2D bounding box $[x_{2d}, y_{2d}, w_{2d}, h_{2d}]$ and 3D bounding box $[x_{p}, y_{p}, z, w_{3d}, h_{3d}, l_{3d}, \\theta]$. $[x_{2d}, y_{2d}]$ and $[x_{p}, y_{p}]$ represent the 2D box center and 3D object center projected to image plane. $[w_{2d}, h_{2d}]$ and ${[w_{3d}, h_{3d}, l_{3d}]}$ represent the physical dimension of 2D and 3D bounding box, respectively. $z$ denotes the depth of 3D object center. $\\theta$ is the observation angle.\nDuring training, we project all ground truth into the 2D space to calculate the intersection over union (IoU) with all 2D anchors. \nThe anchor with IoU greater than 0.5 is chosen to assign with the corresponding 3D box for optimization.\n\n\\smallskip\\noindent{\\bf Output transformation.}\nSimilar to prior works \\cite{ding2020d4lcn, wang2021ddmp, liu2021ground, luo2021m3dssd}, \nwe follow Yolov3 \\cite{Joseph2018yolov3} to predict $[t_x, t_y, t_w, t_h]_{2d}$ and $[t_x, t_y, t_w, t_h, t_l, t_z, t_{\\theta}]_{3d}$ for each anchor, which aims at parameterizing the residual value for 2D and 3D bounding box, and also predict the classification scores $cls$.\nThe output bounding box can be restored based on the anchor and the network prediction as follows:\n\\begin{align}\n& [\\hat x_{2d},\\hat y_{2d}] = [t_x, t_y]_{2d} * [w_{2d}, h_{2d}] + [x_{2d}, y_{2d}] \\notag \\\\\n& [\\hat x_{p}, \\hat x_{p}] = [t_x, t_y]_{3d} * [w_{2d}, h_{2d}] + [x_{p}, y_{p}] \\notag \\\\\n& [\\hat w_{3d}, \\hat h_{3d}, \\hat l_{3d}] = \\exp([t_w, t_h, t_l]_{3d}) * [w_{3d}, h_{3d}, l_{3d}] \\notag \\\\\n& [\\hat w_{2d}, \\hat h_{2d}] = \\exp([t_w, t_h]_{2d}) * [w_{2d}, h_{2d}] \\notag \\\\\n& [\\hat z, \\hat\\theta] = [t_z, t_\\theta]_{3d} + [z, \\theta],\n\\label{eq:transform}\n\\end{align}\nwhere $\\hat{(\\cdot)}$ denotes the recovered parameters of the 3D object. \nNote that we apply the same anchor center for 2D box center $[x_{2d}, y_{2d}]$ and 3D projection center $[x_{p}, y_{p}]$.\n\n\\smallskip\\noindent {\\bf Loss function.}\nThe overall loss $\\mathcal{L}$ contains a classification loss $\\mathcal{L}_{cls}$ for objectness and class, a bounding box regression loss $\\mathcal{L}_{reg}$ to optimize Equation \\ref{eq:transform}, and a depth loss $\\mathcal{L}_{dep}$ with auxiliary depth supervision described in Section \\ref{sec:dam}:\n\\vspace{-4mm}\n\\begin{align}\n\\mathcal{L} = \\mathcal{L}_{cls} + \\mathcal{L}_{reg} + \\mathcal{L}_{dep}.\n\\label{eq:loss}\n\\end{align}\nWe adopt the focal loss \\cite{lin2017focal} to balance the samples for the classification task, and the smoothed-L1 loss \\cite{girshickICCV15fastrcnn} for the regression task.\nFor the depth categorical prediction described in Section \\ref{sec:dam}, we utilize the focal loss \\cite{lin2017focal}:\n\\begin{align}\n    \\mathcal{L}_{dep}=\\frac{1}{|\\mathcal{P}|}\\sum_{p \\in \\mathcal{P}}\\mathrm{FL}(\\mathbf{D}(p), \\mathbf{\\hat{D}}(p)),\n\\end{align}\nwhere $\\mathcal{P}$ is the pixel region on the image with the valid depth labels, and $\\mathbf{\\hat{D}}$ is the depth bins ground truth generated from LiDAR (more details are provided in the supplementary material).\n\\vspace{2mm}"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n",
                "subsection 4.1": {
                    "name": "Setup",
                    "content": "\n\\label{sec:setup}\n\\noindent{\\bf {Dataset.}}\nWe evaluate the proposed approach on the challenging KITTI 3D object detection dataset \\cite{Geiger2012kitti}, which is the most commonly used benchmark for the 3D object detection task. It contains 7481 images for training and 7518 images for testing. We follow \\cite{chen2015_3dop} to divide training samples into the training set (3712) and the validation set (3769). The ablation studies are conducted based on this split.\n\n\\smallskip\\noindent{\\bf Evaluation metric.}\nThe average precision (AP) is used as the metric for evaluation in both 3D object detection and bird's eye view (BEV) detection tasks. We utilize 40 recall positions metric \\apForty~instead of original \\apEleven~to avoid the bias \\cite{andrea2019monodis}. The difficulty of the detection in the benchmark is divided into three levels (\"Easy\", \"Mod.\", \"Hard\") according to size, occlusion, and truncation. \nAll methods are ranked based on \\apthreeD~of moderate setting (Mod.) same as the KITTI benchmark.\nThe thresholds of Intersection over Union (IoU) are 0.7, 0.5, 0.5 for cars, cyclists, and pedestrians categories following the official setting.\n\n\n\\smallskip\\noindent{\\bf Implementation details.}\nWe use Adam optimizer to train our network for 120 epochs with batch size 4. \nThe learning rate starts at 0.0001 and decays with a cosine annealing schedule. \nWe apply 48 anchors on each pixel of the feature map with 3 aspect ratios of $\\{0.5, 1.0, 1.5\\}$, and 12 scales in height following the exponential function $24 \\times 2^{i/4}, i = \\{0,...,15\\}$. For 3D anchor parameters, we calculate the mean and variance statistics of 3D ground truth in the training dataset as prior statistical knowledge of each anchor. Following \\cite{liu2021ground}, we crop the top 100 pixels of each image to reduce inference time, and all images are resized to 288 $\\times$ 1280. \nIn the training stage, we apply random horizontal mirroring as data augmentation.\nIn the inference stage, we drop the predictions with a confidence score lower than 0.75 and adopt Non-Maximum Suppression (NMS) with IoU 0.4 to reduce redundancy.\n\n"
                },
                "subsection 4.2": {
                    "name": "Main Results",
                    "content": "\n\\noindent{\\bf Results of the Car category on the KITTI test set.}\nAs shown in Table \\ref{tab:kitti_test_car}, we compare our MonoDTR with several state-of-the-art monocular 3D object detection methods on the KITTI test set.\nIt can be observed that our approach achieves better performance than other methods in terms of the moderate level of the two tasks, which is the most important metric in the benchmark.\nFurthermore, it is worth noting that our approach outperforms other depth-assisted methods by large margins.\nFor instance,\ncompared to top three depth-assist methods, DFRNet \\cite{Zou2021dfr}, CaDDN \\cite{Reading2021CaDDN} and  DDMP-3D \\cite{wang2021ddmp},\nour MonoDTR obtains \\textbf{2.59/1.76/2.38}, \\textbf{2.82/1.98/1.27} and \\textbf{2.28/2.61/2.93} improvements in \\apthreeD~at IoU threshold 0.7 on three settings, which indicates the effectiveness of the proposed depth-aware modules.\n\n\\smallskip{\\noindent{\\bf Results of the Car category on the KITTI validation set.}}\nWe also conduct experiments on the KITTI validation dataset under different IoU thresholds and tasks as listed in Table \\ref{tab:kitti_val}.\nOur approach obtains superior performance over several image-only methods, benefiting from the auxiliary depth supervision.\nSpecifically, compared with GUPNet \\cite{lu2021gupnet}, our method achieves significant improvements of \\textbf{6.41/4.99/4.61} in \\apthreeD~and \\textbf{7.26/5.41/5.02} in \\apBev~at IoU threshold 0.5 on the easy, moderate, and hard settings.\n\n\\smallskip{\\noindent{\\bf Results of Pedestrians and Cyclists categories on the KITTI test set.}}\nWe further present the performance of pedestrians and cyclists categories in Table \\ref{tab:kitti_ped_cyc}. \nDetecting these two categories is more challenging than cars due to their smaller size and non-rigid body, making it difficult to precisely locate the position.\nOverall, our model significantly outperforms all methods on pedestrian category with a considerable margin. For the cyclist 3D detection, we also achieve competitive results to CaDDN \\cite{Reading2021CaDDN} and obtain better performance than other methods.\n\n\n\\smallskip{\\noindent{\\bf Running time analysis.}}\nWe measure the average running time for processing the whole validation set with batch size 1 on a single Nvidia Tesla v100 GPU.\nAs shown in Table \\ref{tab:kitti_test_car}, our model can achieve real-time performance at 27 FPS, which confirms the efficiency of our approach.\nCompared with state-of-the-art depth-assisted methods, our MonoDTR runs 17$\\times$ and 4.8$\\times$ faster than CaDDN \\cite{Reading2021CaDDN} and DDMP-3D \\cite{wang2021ddmp}, respectively.\nThe main reasons can be summarized as follows:\n(1) CaDDN \\cite{Reading2021CaDDN} builds the bird's eye view representation from predicted depth maps to perform 3D detection, which applies more complicated architecture to generate precise depth predictions, leading to slow speed.\n(2) Fusion-based methods \\cite{ding2020d4lcn, wang2021ddmp} often utilize two separate backbones for extracting features of image and depth, which is time-consuming.\nNote that the depth estimator also takes additional inference time, which is not included in Table \\ref{tab:kitti_test_car}.\nOn the contrary, our model learns depth-aware features through the lightweight DFE module with auxiliary supervision, \nwhich reduces running time significantly.\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Study",
                    "content": "\n\\label{sec:ablation}\n\\smallskip{\\noindent{\\bf Effectiveness of each proposed components.}}\nIn Table \\ref{tab:abl_arch}, we conduct an ablation study to analyze the effectiveness of the proposed components:\n(a) Baseline: only using context-aware features for 3D object detection, \\ie, without all proposed depth-aware modules.\n(b) Replacing depth-aware features with object query \\cite{Nicolas2020detr} in the transformer, \\ie baseline + DETR-like transformer.\n(c) Replacing depth-aware features with features extracted from depth images generated by DORN \\cite{FuCVPR18-DORN}.\n(d) Integrating context- and depth-aware features with the convolutional concatenate operation.\n(e) Full model without depth prototype enhanced feature $\\mathbf{F'}$.\n(f) MonoDTR (full model).\n\nFirstly, we can observe from (b$\\rightarrow$f) that utilizing depth-aware features to replace the object query in the transformer can provide meaningful depth hints and improve the performance.\nBesides, compared to our end-to-end training framework (f), simply utilizing depth priors from the pretrained depth estimator (c) leads to worse results.\nNext, we demonstrate that applying our depth-aware transformer (DTR) module (f) can more effectively integrate context- and depth-aware features than simple convolutional concatenation (d).\nFurthermore, utilizing our proposed depth prototype enhancement module can boost the performance (e$\\rightarrow$f).\nFinally, by applying all the designed modules, our full model (f) achieves significant improvement compared to the baseline (a). \nAlso, an in-depth analysis in Figure \\ref{fig:ap3d_vs_depth} suggests that our method surpasses the baseline under different IoU thresholds and object depths.\nThese results prove the effectiveness of our depth-aware modules.\n\n\n\n\n\n\n\\smallskip{\\noindent{\\bf Comparison with different positional encodings.}}\nWe investigate the effectiveness of the proposed depth positional encoding (DPE) in Table \\ref{tab:abl_pe}. \nCompared with several commonly used positional encodings, including absolute positional encoding (APE) \\cite{dosovitskiy2020vit}, conditional positional encoding (CPE) \\cite{chu2021conditional}, sinusoidal positional encoding \\cite{vaswani2017SA}, and without using positional encoding (No PE), our proposed DPE achieves better performance on KITTI validation set. \nWe believe that encoding the depth-aware cues is more effective for learning the position representation of 3D tasks than pixel-level encodings.\n\n\n\n\\smallskip{\\noindent{\\bf Plugging into the existing image-only methods.}}\nOur proposed approach is flexible to extend to existing image-only 3D object detectors to improve the depth reasoning capability. We respectively plug our depth-aware modules into three popular monocular 3D object detectors: M3D-RPN \\cite{brazil2019m3drpn}, GAC \\cite{liu2021ground}, and MonoDLE \\cite{Ma2021monodle}, based on their official codes\\footnote{\\url{https://github.com/garrickbrazil/M3D-RPN}}\\footnote{\\url{https://github.com/Owen-Liuyuxuan/visualDet3D}}\\footnote{\\url{https://github.com/xinzhuma/monodle}}. \nIn practice, we take the features from the above models (before the detection head) as the initial features, and utilize our proposed modules (DFE, DTR, and DPE modules) to generate final integrated features, followed by their original detection head to detect 3D objects.\nAs shown in Table \\ref{tab:abl_equip}, with the aid of our proposed depth-aware modules, these detectors achieve further improvements on the KITTI validation set, which demonstrates the flexibility and efficiency of our approach.\n\n"
                },
                "subsection 4.4": {
                    "name": "Qualitative Results",
                    "content": "\nWe provide the qualitative examples on the KITTI validation set in Figure \\ref{fig:kitti_vis}.\nCompared with the baseline model without the aid of depth-aware modules, the predictions from MonoDTR are much closer to the ground truth.\nIt shows that the proposed depth-aware modules can help to locate the object precisely.\nMore qualitative results are included in the supplementary material.\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\nIn this paper, we propose a depth-aware transformer network for monocular 3D object detection. The proposed lightweight DFE module implicitly learns depth-aware features in an end-to-end fashion to avoid obtaining inaccurate depth priors and high computational cost from an off-the-shelf depth estimator. We also introduce the depth-aware transformer to globally integrate context- and depth-aware features, while the novel depth positional encoding (DPE) is designed to inject depth hints into the transformer.\nComprehensive experiments on the KITTI dataset validate that our model achieves real-time detection and outperforms previous state-of-the-art monocular-based methods.%Acknowledgements\n\\vspace{-5.5mm}\n\\paragraph{Acknowledgements.}\nThis work was supported in part by the Ministry of Science  and  Technology, Taiwan, under Grant MOST 110-2634-F-002-051, Qualcomm Technologies, Inc., and Mobile Drive Technology Co., Ltd (MobileDrive). We are grateful to the National Center for High-performance Computing.\n\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\\clearpage\n\n\\appendix\n\\noindent{\\bf\\Large Supplementary Material}\n"
            },
            "section 6": {
                "name": "Depth-Aware Transformer",
                "content": "\n\\noindent{\\bf Transformer architecture.}\nThe detailed architecture of depth-aware transformer (DTR) is shown in Figure \\ref{fig:arch_dtr}. The encoder aims to generate the encoded context-aware features, while the decoder produces the fused feature from context- and depth-aware features through the multiple self-attention layers. Besides, we supplement two features with the proposed depth positional encoding (DPE) before passing them to the transformer, enabling better 3D reasoning.\n\n\n\n\n\n\\smallskip{\\noindent{\\textbf{Effectiveness of linear attention.}}}\nTable \\ref{tab:abl_sa} shows the results of different self-attention layers on the KITTI dataset, where we can observe that applying linear attention \\cite{katharopoulos2020rnn} can achieve almost 4 $\\times$ faster than\nvanilla self-attention \\cite{vaswani2017SA} with comparable performance. Thus, we adopt the linear attention \\cite{katharopoulos2020rnn} in our transformers for real-time applications.\n\n\n"
            },
            "section 7": {
                "name": "Auxiliary Depth Supervision",
                "content": "\n\\noindent{\\textbf{Depth ground truth generation.}} \nWe project the LiDAR signals into the image plane to generate the sparse ground truth depth map. Then we apply linear-increasing discretization (LID) \\cite{tang2020center3d} method to convert continuous depth $d$ to discretized depth bins. The LID is defined as follows:\n\\vspace{-2mm}\n\\begin{align} \n\\label{eq:disc}\n     d &=  d_{\\min} + \\frac{d_{\\max}-d_{\\min}}{D(D + 1)} \\cdot i (i + 1),~ i=\\{1,..., D\\},\n\\end{align}\nwhere $i$ is the depth bin index. The number of depth bins $D$ is set as 96, and the range of depth [$d_{\\min}, d_{\\max}$] is set as [1, 80]. Note that the pixels with the depth value outside the range will be marked as invalid and not used for optimization during training.\n\n\\smallskip{\\noindent{\\textbf{Different discretization methods.}}}\nIn Table \\ref{tab:abl_depth}, we investigate the effectiveness of different discretization methods for depth auxiliary supervision. \nIn addition to the LID method, the continuous depth can be discretized using uniform discretization (UD) with fixed bin size: $\\frac{d_{\\max}-d_{\\min}}{D}$, or spacing-increasing discretization (SID) \\cite{FuCVPR18-DORN} with the increasing bin size in the log space.\nIt can be observed that using the LID strategy can achieve better performance, so we apply it as our discretization method.\n\n\n\n\n"
            },
            "section 8": {
                "name": "Results on nuScenes Dataset",
                "content": "\nTable \\ref{tab:nuscene_val} shows the experimental results of deploying our proposed approach on nuScenes \\cite{nuscenes2019} val set. \nUnder the same configurations (\\eg, backbone and training schedule), our model achieves better performance than two 3D object detection baselines (FCOS3D \\cite{wang2021fcos3d}, and PGD \\cite{wang2021pgd}), which demonstrates the effectiveness of our approach.\n\n\n"
            },
            "section 9": {
                "name": "Qualitative Visualization",
                "content": "\n\n\n\n\n\n\\noindent{\\textbf{More visualization results.} \nIn Figure \\ref{fig:kitti_vis_multi}, we provide some qualitative results on the KITTI dataset for multiple-category predictions. \nIn Figure \\ref{fig:kitti_vis_supp}, we show the qualitative comparisons of the baseline (without proposed depth-aware modules) and our MonoDTR (full model). It can be observed that our MonoDTR can generate higher quality bounding boxes benefit from the aid of depth cues.\n}\n\n\\smallskip{\\noindent{\\textbf{Failure case.}} \nWe show a representative failure case in Figure \\ref{fig:kitti_fail}. The lower-quality 3D bounding box is caused by the inaccurately predicted object depth, which is typical in most monocular 3D object detection tasks.}\n\n"
            },
            "section 10": {
                "name": "Broader Impacts",
                "content": "\nOur work aims to develop the monocular 3D object detection approach for autonomous driving. The proposed model may generate inaccurate object depth prediction, leading to incorrect downstream decision-making and potential traffic accidents. \nFurthermore, we provide a new perspective of leveraging learned depth-aware features to assist monocular 3D object detection. Although considerable progress has been made with our proposed lightweight depth-aware feature extraction module, we believe it is worth further exploring how to learn depth-aware features to effectively improve detection performance.\n\n\n"
            }
        },
        "figures": {
            "fig:example": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{fig/depth_compare_v7_new.pdf} % \n    \\caption{\n    \\textbf{\n    Comparison of different depth-assisted monocular 3D object detection frameworks.\n    } \n    (a) Pseudo-LiDAR-based methods \\cite{wang2019pseudo, weng2019mono3dplidar, ma2019am3d} lift images to 3D coordinate via monocular depth estimation, followed by a 3D LiDAR-based detector to recover object locations. (b) Fusion-based methods \\cite{ding2020d4lcn, ouyang2021ddf, wang2021ddmp} extract features from images and estimated depth maps, then fuse them to predict objects.\n    (c) Our MonoDTR learns depth-aware features via additional depth supervision and performs 3D object detection in an end-to-end manner. Note that our depth supervision is only leveraged in the training stage.\n    }\n    \\label{fig:example}\n    \\vspace{-12.5pt}\n\\end{figure}",
            "fig:arch": "\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\textwidth]{fig/monodtr_arch_new.pdf}\n\\caption{\\textbf{The overall framework of our proposed MonoDTR.}\nThe input image is first sent to the backbone to extract the features.\nThe Depth-Aware Feature Enhancement (DFE) module learns depth-aware features via auxiliary supervision (Section \\ref{sec:dam}), and context-aware features are extracted by convolution layers in parallel.\nThe Depth-Aware Transformer (DTR) module then integrates two kinds of features, while the Depth Positional Encoding (DPE) module injects depth positional hints into the transformer (Section \\ref{sec:dgtr}).\nFinally, the detection head is applied to predict the 3D bounding boxes (Section \\ref{sec:loss}). Note that the auxiliary depth supervision is only used in the training phase.\n}\n\\label{fig:arch}\n\\vspace{-9.7pt}\n\\end{figure*}",
            "fig:dam": "\\begin{figure}[thpb]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{fig/dfe_v4.pdf}\n\n    \\vspace{-8pt}\n    \\caption{\\textbf{The architecture of depth-aware feature enhancement (DFE) module.}\n    The DFE module aims to implicitly learn depth-aware features via auxiliary supervision.\n    (a) Generate initial depth-aware feature $\\mathbf{X}$ and predict depth distribution $\\mathbf{D}$. (b) Estimate feature representation of depth prototype $\\mathbf{F_d}$. (c) Produce depth prototype enhanced feature $\\mathbf{F'}$, and fuse with initial depth-aware feature $\\mathbf{X}$. See Section \\ref{sec:dam} for details.\n    }\n    \\label{fig:dam}\n\\end{figure}",
            "fig:transformer": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\columnwidth]{fig/dpe_v5.pdf}  % word 5->6\n    \\caption{\\textbf{The proposed depth positional encoding (DPE) module.} The DPE module generates depth positional encoding based on the depth category predicted by the DFE module. See Section \\ref{sec:dgtr} for details.}\n    \\label{fig:transformer}\n    \\vspace{-3mm}\n\\end{figure}",
            "fig:ap3d_vs_depth": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.882\\columnwidth]{fig/ap3d_iou3d_compared_car_v2.png}\n    \\vspace{-2.3mm}\n    \\caption{\\textbf{Comparison of AP with different object depth ranges and IoU thresholds between baseline and MonoDTR} on the KITTI validation set for Car category. Best viewed in color.}\n    \\label{fig:ap3d_vs_depth}\n\\end{figure}",
            "fig:kitti_vis": "\\begin{figure*}[th]\n\\centering\n\\includegraphics[width=0.98\\textwidth]{fig/visualization_example_v3_only4_v4.jpg}\n\\caption{\\textbf{Qualitative examples on the KITTI validation set.} We provide the predictions  on the image view (left) and bird eye view (right). The \\textcolor{plotpurple}{purple} boxes in the image and BEV plane represent the predictions from MonoDTR. The \\textcolor{plotgreen}{green} and \\textcolor{plotred}{pink} boxes on BEV are the ground truth and the predictions from baseline (without depth-aware modules), respectively. Best viewed in color and zoomed in.\n}\n\\label{fig:kitti_vis}\n\\vspace{-6pt}\n\\end{figure*}",
            "fig:arch_dtr": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.97\\columnwidth]{fig/dtr_arch_v5.pdf} % \n    \\caption{\\textbf{The architecture of depth-aware transformer (DTR).} DPE is the depth positional encoding proposed in the main paper.\n    }\n    \\vspace{-5pt}\n    \\label{fig:arch_dtr}\n\\end{figure}",
            "fig:kitti_vis_multi": "\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.99\\textwidth]{fig/supp_vis_val_three_class.jpg}\n\\caption{\\textbf{Qualitative results on the KITTI validation set for multi-class 3D object detection.} We utilize \\textcolor[RGB]{255,128,0}{orange}, \\textcolor[RGB]{0,128,255}{blue}, and \n\\textcolor[RGB]{102,204,0}{green} colors to indicate car, pedestrian, and cyclist categories, respectively.\n}\n\\label{fig:kitti_vis_multi}\n\\vspace{-6pt}\n\\end{figure*}",
            "fig:kitti_vis_supp": "\\begin{figure*}[th]\n\\centering\n\\includegraphics[width=0.99\\textwidth]{fig/supp_vis_val_v2.jpg}\n\\caption{\\textbf{Qualitative comparison on the KITTI validation set} for the car category. The \\textcolor{plotpurple}{purple} boxes in the image and BEV plane represent the predictions from MonoDTR. The \\textcolor{plotgreen}{green} and \\textcolor{plotred}{pink} boxes on BEV are the ground truth and the predictions from baseline (our full model without proposed depth-aware modules), respectively. Best viewed in color and zoomed in.\n}\n\\label{fig:kitti_vis_supp}\n\\vspace{-6pt}\n\\end{figure*}",
            "fig:kitti_fail": "\\begin{figure*}[th]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{fig/fail_case.jpg}\n\\caption{\\textbf{Failure case.} The \\textcolor{plotpurple}{purple} and \\textcolor{plotgreen}{green} boxes represent the predictions from MonoDTR and ground truth, respectively. The failure case is caused by the inaccurate object center depth estimation.}\n\\label{fig:kitti_fail}\n\\vspace{-6pt}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n    \\mathbf{F}_d = \\sum_{i \\in \\mathcal{I}} {\\tilde{P}_{di}} \\mathbf{X'}_i,  d=\\{1,..., D'\\},\n    \\label{eq:regionrepresentation}\n\\end{align}",
            "eq:2": "\\begin{align}\n    \\mathbf{F}' = \\sum_{d=1}^{D'} {\\tilde{P}_{di}} \\mathbf{F}_d. \\label{eq:recons}\n\\end{align}",
            "eq:3": "\\begin{equation}\n    {\\rm{Attention}}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n    = {\\rm{softmax}}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{C}})\\mathbf{V}.\n\\label{eq-att}\n\\end{equation}",
            "eq:4": "\\begin{align}\n& [\\hat x_{2d},\\hat y_{2d}] = [t_x, t_y]_{2d} * [w_{2d}, h_{2d}] + [x_{2d}, y_{2d}] \\notag \\\\\n& [\\hat x_{p}, \\hat x_{p}] = [t_x, t_y]_{3d} * [w_{2d}, h_{2d}] + [x_{p}, y_{p}] \\notag \\\\\n& [\\hat w_{3d}, \\hat h_{3d}, \\hat l_{3d}] = \\exp([t_w, t_h, t_l]_{3d}) * [w_{3d}, h_{3d}, l_{3d}] \\notag \\\\\n& [\\hat w_{2d}, \\hat h_{2d}] = \\exp([t_w, t_h]_{2d}) * [w_{2d}, h_{2d}] \\notag \\\\\n& [\\hat z, \\hat\\theta] = [t_z, t_\\theta]_{3d} + [z, \\theta],\n\\label{eq:transform}\n\\end{align}",
            "eq:5": "\\begin{align}\n\\mathcal{L} = \\mathcal{L}_{cls} + \\mathcal{L}_{reg} + \\mathcal{L}_{dep}.\n\\label{eq:loss}\n\\end{align}",
            "eq:6": "\\begin{align}\n    \\mathcal{L}_{dep}=\\frac{1}{|\\mathcal{P}|}\\sum_{p \\in \\mathcal{P}}\\mathrm{FL}(\\mathbf{D}(p), \\mathbf{\\hat{D}}(p)),\n\\end{align}",
            "eq:7": "\\begin{align} \n\\label{eq:disc}\n     d &=  d_{\\min} + \\frac{d_{\\max}-d_{\\min}}{D(D + 1)} \\cdot i (i + 1),~ i=\\{1,..., D\\},\n\\end{align}"
        },
        "git_link": "https://github.com/kuanchihhuang/MonoDTR"
    }
}