{
    "meta_info": {
        "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient  Transformer Inference",
        "abstract": "Deploying pre-trained transformer models like BERT on downstream tasks in\nresource-constrained scenarios is challenging due to their high inference cost,\nwhich grows rapidly with input sequence length. In this work, we propose a\nconstraint-aware and ranking-distilled token pruning method ToP, which\nselectively removes unnecessary tokens as input sequence passes through layers,\nallowing the model to improve online inference speed while preserving accuracy.\nToP overcomes the limitation of inaccurate token importance ranking in the\nconventional self-attention mechanism through a ranking-distilled token\ndistillation technique, which distills effective token rankings from the final\nlayer of unpruned models to early layers of pruned models. Then, ToP introduces\na coarse-to-fine pruning approach that automatically selects the optimal subset\nof transformer layers and optimizes token pruning decisions within these layers\nthrough improved $L_0$ regularization. Extensive experiments on GLUE benchmark\nand SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning\nand model compression methods with improved accuracy and speedups. ToP reduces\nthe average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE,\nand provides a real latency speedup of up to 7.4x on an Intel CPU.",
        "author": "Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang Yan, Yunqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, Mao Yang",
        "link": "http://arxiv.org/abs/2306.14393v1",
        "category": [
            "cs.CL"
        ],
        "additionl_info": "KDD 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nPre-trained transformer models~\\cite{bert,albert,roberta,t5} have achieved great success for a wide variety of NLP tasks. \nHowever, the superior performance comes at the cost of increasingly larger model sizes and computation overhead, making it difficult to efficiently deploy them on different downstream tasks in various latency-critical scenarios such as online servers and edge devices.  \n\n%There have been many model compression techniques proposed to accelerate transformer  inference by reducing model size, such as pruning~\\cite{movement,nn_pruning}, quantization~\\cite{shen2020q,kim2021bert,chen2021quantization} and distillation~\\cite{sanh2020distilbert,jiao2020tinybert}. While quantization and distillation typically result in a fixed model  with a reduced size, structured pruning, which directly removes redundant heads or intermediate dimensions,  has been shown to be very effective at compressing a model to meet various deployment requirements~\\cite{swiftpruner,cofi}. However, \n% structured pruning cannot guarantee an ideal accuracy when the target model is already a small transformer or when it has long input sequences. This is because the computation complexity of the attention mechanism is $O(n^2)$ with the input token length of $n$. In order to meet tight deployment constraints, a significant portion of the original  model is required to be pruned, resulting in a potential loss of accuracy. \n\nAccelerating transformer inference is often achieved through model compression methods such as pruning~\\cite{movement,nn_pruning}, quantization~\\cite{shen2020q,kim2021bert,chen2021quantization}, and knowledge distillation~\\cite{sanh2020distilbert,jiao2020tinybert}. These techniques aim to reduce the size of the model, with quantization and distillation resulting in a smaller, fixed model. Structured pruning, which eliminates redundant heads or dimensions, can effectively meet deployment requirements~\\cite{swiftpruner,cofi}. However, structured pruning may not guarantee optimal accuracy, particularly for small transformers or long input sequences, as the attention mechanism has a $O(n^2)$ computation complexity with input token length $n$. This means a significant portion of the model must be pruned to meet tight deployment constraints, potentially compromising accuracy.\n\n \n \n% Accelerating transformer inference is commonly accomplished through model compression techniques, such as pruning~\\cite{movement,nn_pruning}, quantization~\\cite{shen2020q,kim2021bert,chen2021quantization}, and knowledge distillation~\\cite{sanh2020distilbert,jiao2020tinybert}. Quantization and distillation result in a smaller, fixed model, while structured pruning, which eliminates redundant heads or dimensions, effectively satisfies deployment requirements~\\cite{swiftpruner,cofi}. However, structured pruning may not guarantee optimal accuracy for small transformers or long input sequences. This is due to the attention mechanism's $O(n^2)$ computation complexity with input token length $n$, meaning a substantial part of the model must be pruned to meet tight deployment constraints, potentially causing accuracy loss.\n \n% \\begin{figure}[t]\n %\t\\centering\n% \t\\includegraphics[width=1\\columnwidth]{figs/tokenpruning_example.png}\t\n% \t\\caption{Dynamic token pruning during  model inference.}\n% \t\\label{fig:example}\n% \\end{figure}\n \n\nRecently, a promising subfield in NLP has emerged that focuses on reducing latency during model inference by pruning input tokens.  It's based on the intuition that not all tokens in the input sequence are critical for making a final prediction. As  tokens pass through the encoder layers,  some tokens have been captured by other tokens via attention in the early layer and do not require future modeling in a higher layer~\\cite{rogers-etal-2020-primer,powerbert}. Pruning these uninformative tokens within each layer can increase the model's inference speed without sacrificing accuracy. Moreover, the removal of these tokens in each layer will also reduce the computation and memory requirements in its subsequent layers, resulting in linear or even quadratic reductions and providing greater compression benefits.\n\n\n\n%Moreover, the removed tokens will not be considered in subsequent layers, contributing to the computation and memory reduction of all operations linearly or quadratically for subsequent layers, leading to greater compression benefits. \n\n\n%Additionally, token pruning will result in linear or even quadratic reduction of operations, providing greater compression benefits than weight pruning.\nSome prior works~\\cite{powerbert,LAT,ltp,transkimmer,trbert} have examined the potential of layer-wise token pruning of input sequences.  However, these approaches face several limitations. First,  they treat all layers equally, leading to a vast design space, as pruning decisions must be made for each token at every layer through the use of token-level masks.\n Second, existing methods primarily aim to minimize accuracy drop and  use regularization loss terms to encourage maximum token pruning~\\cite{powerbert,ltp,transkimmer}, lacking effective control over the given token sparsity ratio. This can be problematic in real-world scenarios where a specific sparsity ratio or deployment constraint is often required.\n \n %lacking effective control over the achieved token sparsity ratio. This can be problematic in real-world applications where a specific sparsity ratio or deployment constraint is often required.\n \n \n\n %First,previous methods equally consider each token and use token-level masks to make pruning decisions at each layer, resulting in a large design space. Second, existing methods tend to focus on minimizing  accuracy drop and  often use regularization loss terms to encourage the model to prune as many tokens as possible~\\cite{powerbert,ltp,transkimmer}. However, this design lacks effective control over the achieved token sparsity ratio, which can be problematic because a specific sparsity ratio or deployment constraint is often required in real-world applications. \n \n Finally, existing token importance scoring criterion struggles to achieve both high accuracy and real inference efficiency. Attention value-based approaches~\\cite{powerbert,ltp}, which utilize the self-attention mechanism to score token importance,  can be efficient, but may inadvertently remove important tokens that receive little attention in early layers~\\cite{rogers-etal-2020-primer}, leading to a huge drop in accuracy.  On the other hand, prediction module-based approaches~\\cite{transkimmer,trbert}, which insert extra neural networks to predict token importance scores, can be more accurate. But they also come with the cost of introducing considerable additional inference cost (i.e., $\\sim30\\%$ latency overhead on GPU) and may impede overall speedup. Given the tight latency constraints in  many real-world deployments, attention value-based scoring is a more promising method for achieving efficiency. However, the challenge of improving accuracy for determining token importance in early layers remains unresolved.\n \n In our work, we introduce \\textbf{To}\\textbf{P} (\\textbf{To}ken \\textbf{P}runing), a deployment-friendly and constraint-aware token pruning approach that addresses all the above challenges. {\\sysname} trains an optimal token pruning decision based on our improved attention value-based scoring, enabling dynamic removal of unnecessary tokens layer-by-layer during inference, while preserving accuracy and meeting deployment constraints. Our approach incorporates two key techniques.\n \n First, we introduce a new token distillation method called \\textit{ranking-aware token distillation} to enhance the ability of self-attention values to rank token importance in early layers, thereby resolving the issue of unintended removal of top-ranked tokens  during inference which can affect the model's accuracy. Our solution is inspired by the observation that attention values in deeper layers rank token importance more accurately. We thus utilize the importance rankings generated by the final layers of the unpruned model as knowledge and distill it to the early layers.\n Conventional distillation methods~\\cite{cofi,Sun2019PatientKD} commonly use the MSE loss to measure the layer-wise representations' difference  between teacher and student, but this may not be effective for transformer models such as BERT, which capture different levels of information across early and deep layers. Directly minimizing the MSE loss of absolute attention values may lead to suboptimal results. Instead, our method proposes a ranking-aware distillation loss that minimizes the differences in token importance \\textit{rankings} between the final layer of the teacher and the early layers of the student using the LambdaRank loss~\\cite{lambdandcg}. This distillation effectively retains the most important tokens and results in significant accuracy improvements.\n \n\n \nNext, we present a generic learning algorithm that optimizes token pruning decisions based on our improved attention value-based importance scoring. Different from prior works, we utilize two-tier binary masks, consisting of coarse-grained gate masks and fine-grained token ranking masks, to automatically determine the optimal subset of transformer layers for fine-grained token pruning. The gate masks act as layer selectors, while the ranking masks dynamically identify which sorted tokens (\\textit{i.e.,} based on the attention value scores) within the selected layers to be pruned. This design allows for a more flexible pruning space and eases the optimization compared to learning all token masks equally. %To handle dynamic input during inference, we follow PoWER-BERT~\\cite{powerbert} to apply the ranking masks. Specifically, ranking masks prune tokens based on their importance, as determined by attention value scores, preserving the most significant tokens in the input sequence. \nTo find the optimal mask values while achieving the desired pruning ratio, we solve an end-to-end optimization problem using improved $L_0$ regularization~\\cite{lagrangian}, which  jointly learns these masks and updates model parameters on the target downstream tasks, resulting in better model accuracy.  \nWe summarize our contributions as follows:\n\\begin{itemize}\n\t\\item For the first time, we propose ranking-aware token distillation to  effectively improve token importance rankings based on attention values, which  greatly enhances the effectiveness of token pruning methods relying on attention values.\n\t\\item We further propose a constraint-aware token pruning algorithm ({\\sysname}). For a given deployment constraint, {\\sysname} automatically selects the optimal subset of transformer layers and optimizes token pruning decisions within these layers through improved $L_0$ regularization.\n\t\n\t\\item Extensive experiments on GLUE benchmark~\\cite{glue} and SQuAD v2.0~\\cite{squadv2} demonstrate that {\\sysname} consistently outperform  state-of-the-art token pruning and model compression baselines with higher accuracy and speedups. By removing unnecessary tokens, {\\sysname} improves accuracy by up to 4.3\\% on GLUE and reduces FLOPs by an average of 6.7$\\times$.\n\tFurthermore,   {\\sysname} delivers substantial real latency reduction, with up to 7.4$\\times$ acceleration for BERT inference on CPU. \n\\end{itemize}\n\n\n\n\n\n\n\n \n\n%In our work, we address all the above challenges and propose  an inference-friendly,  constraint-aware token pruning approach called {\\sysname} (\\textbf{To}ken \\textbf{P}runing), which dynamically removes layer-wise unimportant tokens based on the content of a given input sequence.   Our approach presents two key techniques. First, we propose a generic constraint-aware token pruning algorithm. We begin by introducing a set of coarse- to fine-grained binary masks that determine \\textit{which tokens in which layers to be pruned} in a dynamic way. Specifically, the coarse-grained gate masks are used to select layers,  and the fine-grained token ranking masks are used to dynamically identify which specific tokens within the selected layers should be pruned.  This design allows for a more flexible pruning space and eases the optimization compared to learning all token masks equally.  To learn these masks while achieving the desired pruning ratio, we formulate it as a learning  optimization problem and utilize  an improved $l_0$ regularization~\\cite{lagrangian}, which  jointly learns these masks and updates model parameters on the target downstream tasks, resulting in better model accuracy.  \n \n% However, the use of attention values for scoring can be limited in determining token importance effectively in early layers, resulting in important tokens being ranked as non-critical and removed by the token ranking mask. To overcome this limitation, we propose a novel token distillation technique called \\textit{ranking-aware token distillation}. This solution is based on our observation that attention values in later layers are able to rank token importance accurately. Therefore, our key idea is to utilize the token importances generated by the late layers of the unpruned model as the knowledge and distill it to the early layers during the token pruning process. Unlike the conventional distillation methods~\\cite{cofi,Sun2019PatientKD} that commonly use an MSE loss to measure the layer-wise difference in knowledge between teacher and student, transformer models such as BERT capture different levels of information across early and late layers. Thus, directly minimizing the MSE loss of absolute attention values between these layers can be problematic. Instead, our goal is to align the token importance rankings between early layers and the teacher. To achieve this, we propose a ranking-aware distillation loss, which uses the LambdaNDCG loss~\\cite{lambdandcg} to minimize the differences in token importance rankings between the last layer of the teacher and the early layers of the student. We show that this distillation can effectively preserve the most important tokens and lead to significant performance gains. \n\n\n"
            },
            "section 2": {
                "name": "Related Works",
                "content": "\n",
                "subsection 2.1": {
                    "name": "Model Compression",
                    "content": "\nTo reduce the inference cost of pre-trained transformer models, a variety of compression techniques have been proposed, including weight pruning~\\cite{movement,gordon2020compressing,nn_pruning}, quantization~\\cite{shen2020q,kim2021bert,chen2021quantization} and distillation~\\cite{sanh2020distilbert,jiao2020tinybert}. Token-level pruning has been shown to complement knowledge distillation and quantization~\\cite{ltp}.  Here, we focus on pruning and distillation and briefly discuss the related work. \n\n%\\vspace{2px}\n\\noindent\\textbf{Weight pruning} is categorized into 1) unstructured and 2) structured pruning. Unstructured methods~\\cite{movement, gordon2020compressing} achieve high sparsity without accuracy drop but offer minimal latency benefits due to irregular sparse patterns.\n%Although unstructured methods~\\cite{movement,gordon2020compressing} reach a relatively high sparsity ratio without significant accuracy drop, theyyield very little actual latency benefits as it is difficult to leverage irregular sparse patterns for acceleration. \nIn contrast, structured pruning removes coherent weight groups, reducing latency without special hardware support.\nCoFi~\\cite{cofi} achieves 10$\\times$ speedup with a small accuracy drop by jointly pruning layers, attention heads, FFN, and hidden units. SwiftPruner~\\cite{swiftpruner} is a latency-aware pruning method that finds  optimal layer-wise pruning policies under a given latency requirement through AutoML. However, structure pruning may result in a loss of accuracy when the deployment requirements are highly constrained and the downstream task has a long input sequence. This is because the model complexity increases quadratically with token length. When the token length is long, the original model must be compressed to a high ratio, which can cause accuracy loss. \n\n%\\vspace{2px}\n\\noindent\\textbf{Knowledge distillation}~\\cite{kd,Sun2019PatientKD,Turc2019WellReadSL} aims to transfer knowledge from a large teacher model to a small student model. \n It is well known that model pruning with a distillation objective  can significantly improve accuracy~\\cite{movement,nn_pruning}.\n Common distillation objectives include cross-entropy loss for output probability distributions~\\cite{kd, sanh2020distilbert} and MSE loss for layer-wise representations~\\cite{cofi, Sun2019PatientKD, jiao2020tinybert}. % Commonly used distillation objective functions include the cross-entropy loss between the student and teacher output probability distributions~\\cite{kd,sanh2020distilbert} and the MSE loss between the student and teacher layer-wise representations~\\cite{cofi,Sun2019PatientKD,jiao2020tinybert}.\n  However, the combination of distillation with token pruning has not been widely explored. Our aim is to transfer the knowledge of token importance rankings from the teacher's final layer to the early layers of the student model during token pruning, which poses a new challenge and requires new distillation objective functions.\n \n\\vspace{-1ex}\n"
                },
                "subsection 2.2": {
                    "name": "Token Pruning",
                    "content": "\n\\vspace{-0.5ex}\nExisting token pruning works can be categorized into two classes based on  token removal or retention criteria. % the criteria used to determine which tokens to remove or retain. \nThe first class uses attention value-based scoring~\\cite{spatten,powerbert,ltp} to identify unimportant tokens.\nFor instance,\n%In the first class, methods like SpAtten~\\cite{spatten}, PoWER-BERT~\\cite{powerbert}, and LTP~\\cite{ltp},  identify unimportant tokens via attention value-based scoring. \nSpAtten~\\cite{spatten} ranks tokens using importance scores and retains the top-k highest-scoring tokens.\n PoWER-BERT~\\cite{powerbert} %incorporates a soft-extraction layer to \n learns a layer-wise token pruning ratio, and prunes all input sequences to the same length. LTP~\\cite{ltp} improves PoWER-BERT by introducing a learnable layer-wise threshold, enabling adaptive pruning length. However, these approaches rely on the effectiveness of token importance scoring. As shown in Fig.~\\ref{fig:attentionscore},\n crucial tokens may receive little attention in early layers, leading them to be misclassified as redundant tokens. Removing these essential tokens can result in a drastic accuracy loss.\n  %As a result, it becomes challenging to prune a significant number of tokens from early layers, limiting the final achieved compression rate. \n \nThe second class of token pruning methods~\\cite{trbert,transkimmer}  inserts a prediction module before each transformer layer to provide a more accurate token importance score prediction. \n% In the second class, they insert a prediction module before each transformer layer, which can provide a more accurate token importance score prediction. \n%TR-BERT~\\cite{trbert} adopts reinforcement learning to independently optimize a policy network that drops tokens. % It uses a separate policy network as the token importance predictor. \n%However,  the  learning is difficult to converge especially on small downstream datasets.\nTranskimmer~\\cite{transkimmer} is a notable example that inserts a 2-layer MLP network at each layer as the prediction module.  However, the extra prediction module can also introduce considerable inference latency overhead, which is unfriendly on resource-limited devices.\n\n {\\sysname} addresses all the above limitations by introducing the ranking-aware token distillation technique. Since it can effectively improve the effectiveness of attention value-based scoring, we can achieve the same or better level of model accuracy as prediction-based approaches, while also being more efficient and inference-friendly. \n \n \n In addition, these works  primarily aim to minimize accuracy loss while reducing as many numbers of tokens as possible. They introduce a regularization term with hyper-parameter $\\lambda$ to balance  accuracy and efficiency. However, this approach lacks effective control on pruning tokens to a desired computation budget. %For instance, it requires many manual trials, and it can be difficult to find the optimal balance when the dataset, model, or pruning strategy changes. \nLAT~\\cite{LAT} address this with an evolutionary search strategy. %It first trains a transformer with LengthDrop, which stochastically determines a sequence length at each layer, then it conducts an evolutionary search to find a layer-wise token ratio under a given computational budget. \nIn contrast, {\\sysname} solves this limitation using a different approach - a  constraint-aware token pruning algorithm, which is an optimization-based solution. \n\n\n\\vspace{-2ex}\n"
                },
                "subsection 2.3": {
                    "name": "Efficient Transformers",
                    "content": "\n\\vspace{-0.5ex}\nSince the computation and memory cost in self-attention is quadratic in the token length, there have been a number of attempts in designing sparse attention. \nSparse Transformer~\\cite{sparsetransformer}, Longformer~\\cite{longformer}, and Big Bird~\\cite{bigbird} employ sparse attention to allow the model to handle long sequences. However, these methods only reduce the CUDA memory but cannot be faster than the full attention. Other efforts~\\cite{xu2021bert,hanruiwang2020hat} leverage neural architecture search to design efficient transformer models with smaller depths and fewer heads. {\\sysname} is orthogonal to these techniques on the input dimension reduction.\n\n\n\n%\\vspace{-1ex}\n"
                }
            },
            "section 3": {
                "name": "Background and motivations",
                "content": "\n",
                "subsection 3.1": {
                    "name": "Background",
                    "content": "\nTransformer models, such as BERT~\\cite{bert}, are stacked up with  multiple encoder layers. A basic transformer layer wraps a multi-head self-attention (MHA) and feed-forward (FFN) layer with residual connection and layer normalization (LN).\nGiven an input sequence of $n$ tokens and hidden size of $d$, \n the hidden state of the $i^{th}$ layer, $\\bm{X}_i=(x_1, x_2,...x_n)\\in\\mathbb{R}^{n\\times d }$, is computed from the previous layer:\n\\begin{equation}\n\t\\label{eq:transformer}\n\t\\begin{aligned}\n\t\t\\displaystyle \\bm{X}_{i-1}^\\prime=\\text{LN}(\\bm{X}_{i-1}+\\text{MHA}(\\bm{X}_{i-1}))\n\t\t\\\\\n\t\t\\bm{X}_i=\\text{LN}(\\bm{X}_{i-1}^\\prime+\\text{FFN}(\\bm{X}_{i-1}^\\prime)),\n\t\\end{aligned} \n\\end{equation}\n\nSpecifically, \nan MHA consists of $N_h$ heads, where each head $h$ is associated with query $\\bm{W}_q^h$, key $\\bm{W}_k^h$ and value $\\bm{W}_v^h$ matrix. Each head $h$ first computes an attention probability matrix $\\bm{A}_h$ and then computes the self-attention mechanism as follows:\n\\begin{equation}\n\t\\label{eq:attention}\n\t\\begin{aligned}\n\t\t\\displaystyle \\bm{A}_h=\\text{softmax}((\\bm{X}\\times \\bm{W}_q^h )\\times(\\bm{X}\\times\\bm{W}_k^h)^T)\\\\\n\t\t\\text{Attention}_h=\\bm{A}_h\\times (\\bm{X}\\times\\bm{W}^h_v)\n\t\\end{aligned} \n\\end{equation}\n\n\\noindent\\textbf{Complexity analysis}. The above self-attention measures the pairwise importance of each token on every other token in the input, and the total complexity of MHA layer is $O(d^2n+n^2d)$, which is quadratic with $n$. For FFN layer, the computation complexity is $O(nd^2)$, which is linear with $n$. When applied to long input sequences (i.e., a large $n$), the computation and memory of MHA layer grow quadratically and become very expensive. \n\nTo address this limitation, our work introduces token pruning, where unimportant tokens are gradually dropped as the inference proceeds. For each transformer layer, which initially has $n$ tokens,  we aim to remove a specific number of unimportant tokens from them.  These removed tokens will not be considered in subsequent layers. This leads to a linear (for FFN) or quadratic (for MHA) reduction in operations, resulting in significantly faster model inference.\n\n\n\n\n\n\n%Our work introduces token pruning as a solution to address limitations. Token pruning gradually drops unimportant tokens as inference proceeds. Our goal is to remove a number of unimportant tokens from each transformer layer, allowing the remaining, important tokens to meet a specified deployment constraint.\n\n%To address this limitation, our work presents token pruning, which gradually drops unimportant tokens as the inference proceeds.  For each transformer layer, we aim to remove a  number of unimportant tokens so that the model with the retentive tokens can meet a given deployment constraint. \n\n%For the $i^{th}$ transformer layer  which initially has $n$ tokens, we aim to remove a specified ratio, $r_i$, of unimportant tokens.\n\n%while model $m$ with retentive tokens can meet the deployment constraint $C$.\n\n%For the $i^{th}$ transformer layer  which initially has $n$ tokens, we aim to remove a specified ratio, $r_i$, of unimportant tokens. These removed tokens will not be considered in subsequent layers, contributing to the reduction of all operations lineary (i.e., FFN) or quadratically (i.e., MHA) for subsequent layers. As a result, the inference can be largely accelerated. \n\nIdentifying unimportant tokens to be discarded is a major challenge in token pruning. Current methods address this\nby either using self-attention values to assign a score to each token, or by adding a prediction module to predict scores. However, both methods have their limitations, which will be discussed in detail.\n\n%\\vspace{-1ex}\n"
                },
                "subsection 3.2": {
                    "name": "Limitations of attention value-based methods",
                    "content": "\n\\label{sec:analysis1}\n\\vspace{-1ex}\n\n\\vspace{3px}\n\\noindent\\textbf{Token importance scoring}. Attention value-based methods~\\cite{powerbert, ltp, spatten} define the importance score of token $x_i$ in layer $l$ as: %In the attention value-based  methods~\\cite{powerbert,ltp,spatten},  the importance score of token $x_i$ in layer $l$ is defined as:\n\\begin{equation}\n\t\\label{eq:importance_score}\n\t\\begin{aligned}\n\t\t\\displaystyle  s^l(x_i)=\\frac{1}{N_h}\\frac{1}{n}\\sum_{h=1}^{N_h}\\sum_{j=1}^n\\bm{A}^l_{h}[x_i, x_j]\n\t\\end{aligned} \n\\end{equation}\nwhere $N_h$ denotes the number of heads.  $\\bm{A}^l_{h}[x_i, x_j]$ indicates the attention received by token $x_j$ from $x_i$ on head $h$. Thus, token $x_i$ is considered important if it receives more attention from all tokens across all heads. The term $\\bm{A}^l_{h}[x_i, x_j]$ can reuse the results from the self-attention mechanism in Equation~\\ref{eq:attention}. Therefore, attention value-based scoring in Equation~\\ref{eq:importance_score} is computationally lightweight.\n\nHowever, the attention values scoring in Equation~\\ref{eq:importance_score} can inaccurately measure token importance in early transformer layers. \nPrevious works~\\cite{clark-etal-2019-bert,rogers-etal-2020-primer} have shown that some important tokens, such as [\\texttt{SEP}], receive little attention in early layers but gain increased attention in deeper layers. As such, these critical tokens obtain low importance scores in Equation~\\ref{eq:importance_score} initially and higher scores in deeper layers.\n%these crucial tokens are assigned low importance scores according to Equation~\\ref{eq:importance_score} in early layers and higher scores in deeper layers. \nHowever, this creates a problem where crucial tokens may be misclassified as unimportant in early layers and removed before they have the chance to reach deeper layers, where they would have received the correct importance scores.\n\n\nTo validate this, we analyze importance score distribution in a trained BERT$_{base}$ model finetuned with the GLUE MRPC dataset. Fig.~\\ref{fig:attentionscore} compares the distribution of importance scores at layers 1, 2, 9, and 12. Tokens are selected based on their ranking positions according to the importance scores. The scores of  top3 tokens in deep layers, such as layers 9 and 12, are significantly higher than those of less important tokens ranked in the top10-15 and top20-25. \n%However, at early layers such as layers 1and 2, the scores for top3 tokens are not well distinguishable from those of other low-ranked tokens, indicating that early layers tend to assign relatively average attention scores to all tokens.\nHowever, at early layers such as layers 1 and 2, the scores for top3 tokens  are not well distinguishable from those of other low-ranked tokens. This indicates that early layers tend to assign relatively similar  scores to both top-ranked and low-ranked tokens, potentially resulting in important tokens being deemed unimportant. As a result, while being computationally efficient, using attention value-based scoring can be problematic at early layers.\n\n\n\n %Additionally, it can be challenging to achieve high token pruning sparsity or significant latency reduction, since it is difficult to accurately  identify as many unimportant tokens as possible in early layers.\n%\\vspace{-1ex}\n"
                },
                "subsection 3.3": {
                    "name": "Limitations of prediction-based methods",
                    "content": "\n\n\n\n\n\n%Existing approaches can be categorized into two classes: \\textit{(i)} the attention-based scoring which naturally utilizes the attention weights calculated by self-attention mechansim~\\cite{powerbert,ltp,LAT}, and \\textit{(ii)} prediction module based scoring which inserts an additional neural network that learns the importance of each token~\\cite{transkimmer,trbert,dynamicvit}. However, both approaches have limitations. Attention value based scoring can unexpectedly rank important tokens as unimportant in early layers, while prediction module based scoring may improve token importance predictions, but at the cost of a significant inference latency introduced by the prediction modules. Next, we will explain the details. \n\n \n\n\n\n%The self-attention mechanism allows each token in the input sequence to attend independently to every other token. As a result, each token $x$ in a single  head will receives a set of attention values, in which each represents the attention imposed by $t$ on the other tokens. PoWER-BERT~\\cite{powerbert} firstly utilizes these attention values to define token importance score. The behind intuition is that a more important token should receive more attention from other tokens. Formally, the token importance score is defined as the aggregate over the heads as follows:\n\n\n\n\n\n In contrast to the attention value-based scoring method, prediction-based methods~\\cite{transkimmer,trbert,dynamicvit} incorporate an additional neural network to predict the importance score for each token. The recently proposed Transkimmer~\\cite{transkimmer}  adds an extra prediction module before each transformer layer, which  composes of 2 linear layers with a layernorm~\\cite{layernorm}, GeLU activation~\\cite{gelu} and GumbelSoftmax~\\cite{jang2017categorical}.  These prediction modules gradually update their parameters and learn to predict which tokens should be pruned, which has been shown to outperform attention value-based approaches. \n \n \n However, prediction-based token pruning faces limitations in achieving real  latency reduction. First, the  prediction module itself introduces additional inference latency and FLOPs. As shown in Table~\\ref{tbl:costcompare}, the additional FLOPs and latency introduced by Transkimmer prediction modules account for 8.43\\% and $\\sim$30\\% of BERT, respectively. This suggests that token pruning needs to  prune much more tokens to counteract the 30\\% latency slowdown. \n Second, Transkimmer's dynamic token pruning relies on the computationally-intensive GumbelSoftmax operation, necessitating specialized runtime and hardware support for efficient implementation.\n %Specifically, the additional FLOPs takeleads to a considerable increase in inference cost for small transformer models on CPU and an additional 30\\% latency on GPU, suggesting that token pruning needs to prune much more tokens to counteract the 30\\% latency slowdown. \n % the use of prediction modules in Transkimmer greatly increases inference latency on both CPU and GPU. Specifically, the additional FLOPs take leads to a considerable increase in inference cost for small transformer models on CPU and an additional 30\\% latency on GPU, suggesting that token pruning needs to prune much more tokens to counteract the 30\\% latency slowdown. \n% which may impede the speedup gains from token pruning. \n\n \n \\label{sec:analysis2}\n \n \n\n%\\vspace{3px}\n%\\noindent\\textbf{Challenges and opportunities}. \n%The above analysis shows that both attention value and prediction module-based scoring methods have limitations, creating a dilemma when determining the best approach. In resource-constrained scenarios, efficiency may be prioritized over accuracy. Therefore, we propose using the attention value method for token importance scoring.  On the other hand, if we can address the limitations of inaccurately determining token importance in early layers, we can achieve better efficiency while maintaining high model accuracy. \n\n\n\n\n%Given the fact that inference latency is a crucial metric in real-world deployment, we adopt the attention value-based scoring approach, which naturally leverages the self-attention mechanism. To address the problem of inaccurate token importance predictions in early layers, we introduce a novel distillation technique that effectively addresses this issue in the next section.\n\n\n\n\n%\\vspace{3px}\n%\\noindent\\textbf{Problem fomulation}. To address this limitation, our work present token pruning, which gradually drops the unimportant tokens as the inference proceeds.  For the $i^{th}$ transformer layer in model $m$ which initially has $n$ tokens, we aim to remove a specified ratio, $r_i$, of unimportant tokens. These removed tokens will not be considered in subsequent layers, contributing to the reduction of all operations lineary (i.e., FFN) or quadratically (i.e., MHA) for subsequent layers. As a result, the inference can be largely accelerated. For a  model $m$ with $l$ encoders, we formulate our problem as:\n%\\begin{equation}\n%\t\\label{eq:problem}\n%\t\\begin{aligned}\n%\t\t\\displaystyle (r_1, r_2, ...r_l)^*=\\mathop{\\arg\\min}\\limits_{r_1, r_2,... r_l}{L}(m (r_1, r_2, ... r_l))\n%\t\t\\\\\n%\t\t\\mathrm{ s.t. }\\;    g(m(r_1, r_2, ...r_l)^*)<=C,\n%\t\\end{aligned} \n%\\end{equation}\n%where  $L(\\cdot)$ is the loss function. $g(\\cdot)$ is a function calculating inference cost while $C$ denotes a given  constraint (e.g. FLOPs or latency). Our goal is to find the optimal layer-wise removed token ratios $ (r_1, r_2, ...r_l)^*$ from the $1^{st}$ to $l^{th}$ encoder layer that has the minimum loss (corresponding to the maximum accuracy) while model $m$ with retentive tokens can meet the deployment constraint $C$. It is notably that our  optimization problem in Equation~\\ref{eq:problem} is different with prior\n%works~\\cite{powerbert,ltp,trbert,transkimmer}, which are constraint-free and hence they lacks of effective control on the achieved inference cost. \n\n\n\nIn our work, our goal is to implement token pruning in practical applications that accelerate real inference latency. While prediction-based approaches can be challenging in achieving actual latency reduction, %Recognizing the difficulty in obtaining actual latency reduction with prediction-based approaches, \nwe instead leverage attention values for token pruning. \n\n%\\vspace{-1ex}\n"
                }
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n%\\vspace{-0.5ex}\n\n\nIn this section, we present {\\sysname}, a novel token pruning approach that incorporates two key techniques for learning the optimal token pruning decisions. First, we introduce the end-to-end token pruning algorithm that leverages $L_0$ regularization. Then, we describe the ranking-aware token distillation approach that enhances the ability of self-attention values to rank token importance. %This results in early layers retaining important tokens, leading to improved model accuracy.\n%In the section, we present {\\sysname} that learns an optimal token pruning decisions with two key techniques. In the following, we first introduce the end-to-end constraint-aware token pruning algorithm with $l_0$ regularization. After that, we introduce ranking-aware token distillation that enhance the capability of self-attention values in ranking token importance.\n\n\n\n%In this section, we present {\\sysname}, a learning-based optimization method that utilizes attention value scoring and  prune layer-wise unimportant tokens in transformers models. To meet various computation and latency requirements during deployment, we employ a range of coarse to fine token pruning masks and an enhanced $l_0$ regularization technique to learn these masks under specific constraints. To enhance the performance of attention value-based scoring, \n% we introduce ranking-aware token distillation, a new technique that allows early layers to retain important tokens for better model accuracy.\n\n\n \n% In this section, we present {\\sysname}, a method for pruning unimportant tokens in transformer models using learning-based optimization. To meet various computation and latency requirements during deployment, we employ a range of coarse to fine token pruning masks and an enhanced $l_0$ regularization technique to learn these masks under specific constraints. Additionally, we introduce ranking-aware token distillation, a new technique that allows early layers to retain important tokens for improved attention value-based scoring.\n  \n  \n  %In this section, we introduce {\\sysname}, a system that uses learning-based optimization to prune unimportant tokens in transformer models on a layer-by-layer basis. To accommodate various computation and latency constraints during deployment, we utilize a range of coarse to fine-grained token pruning masks and employ an enhanced $l_0$ regularization method to learn these masks under the specified constraints. Additionally, to enhance the accuracy of attention value-based scoring, we introduce a new technique called ranking-aware token distillation that enables early layers to retain important tokens.\n  \n  \n \n  \n\\vspace{-1ex}\n",
                "subsection 4.1": {
                    "name": "Constraint-aware token pruning",
                    "content": "\n\\vspace{-0.5ex}\n\\label{sec:tokenpruning}\nFor a given deployment constraint, we propose constraint-aware token pruning to remove a set of unimportant tokens so that the model with the retained tokens can achieve the best accuracy under the constraint. \nThe basic idea is \\textit{(1) we introduce a set of binary decision masks $M\\in\\{0, 1\\}$ to represent the sparsity ratio  and indicate whether to drop ($M=0$) or keep each token ($M=1$); (2) use these masks to construct a constraint-aware loss function; (3) optimize the constraint-aware loss using an  improved $L_0$ regularization~\\cite{l0} method. } Next, we will introduce the details. \n\n%To solve the constraint-aware optimization problem in  Equation~\\ref{eq:problem}, we first introduce a set of binary decision masks $M\\in\\{0, 1\\}$ to represent sparsity ratio of a layer and indicate whether to drop ($M=0$) or keep each token ($M=1$). Next, we construct a differentiable constraint-aware loss function that incorporates these masks. Lastly, we optimize this constraint-aware loss using an improved $L_0$ regularization~\\cite{l0} technique that utilizes an augmented Lagrangian method, which directly controls the final inference cost of the model after token pruning. \n\nUnlike prior works~\\cite{spatten,powerbert,ltp,LAT,trbert,transkimmer} that treat all layers in the same manner, leading to a vast design space, we introduce a novel coarse-to-fine token pruning scheme, as shown in Fig.~\\ref{fig:overview}.\n Specifically, gate masks are used to select  a subset of layers for  token pruning, while token ranking masks dynamically determine which specific tokens within the selected layers should be pruned. \n\n\n \n\n%As illustrated in   Fig.~\\ref{fig:overview}, we introduce a coarse- to fine-grained token pruning scheme, which contains two types of masks.  Specifically, gate masks are used to select layers for fine-grained token pruning.    Token masks determine which specific tokens within the selected layers should be pruned. \n\n \\vspace{3px}\n\\noindent\\textbf{Layer gate mask $M_{gate}$}. We introduce a gate mask $M_{gate}$ for each transformer layer to control whether token pruning is performed in that layer. Concretely, if $M_{gate}^i$ is set to 1, token pruning will be applied to layer $i$. If $M_{gate}^i$ is 0, layer $i$ will be skipped, and retain the \n tokens from its previous layer $i-1$.\n\n\nThe design choice is built on two insights. First,  it has been observed that pre-trained transformer models have varying levels of token redundancy across different layers~\\cite{powerbert,rogers-etal-2020-primer}. Second, pruning tokens across all layers significantly expands the design space and  poses unnecessary difficulty in optimization, particularly  when trying to maintain a low or medium level of compression sparsity. \n\nIn our experiments, we observe an interesting trend where as the target sparsity increases (i.e., constraint becomes tighter), the gate masks progressively activate earlier layers, starting from the deepest one. Eventually, the gate masks activate all transformer layers under high pruning sparsity. Inspired by the observation, we gradually increase the sparsity level from 0 to the target  during  pruning. By doing this, gate masks provide improved decision-making for pruning fewer tokens within early layers. This is because gate masks prioritize learning masks in deeper layers, where token importance is more easily predictable by self-attention values.\n%gate masks help to preferentially learn masks for each token in deeper layers, where token importance is more easily predictable by self-attention values. \n\n\n%When under a relaxed constraint, such as when the target reduction in inference cost is relatively small at 20\\%, only a low sparsity ratio of tokens is required to be pruned. In this case, there is no need to mask tokens in every layer for two main reasons:\\textit{ (i)} deeper layers already have a higher token redundancy ratio, which has been widely observed in previous works~\\cite{powerbert,transkimmer,blockskim}. As the tokens pass through the transformer layers, more  tokens become uninformative as they have already been effectively handled by previous layers, leading to increased redundancy in deeper layers. Furthermore, as discussed in Sec.~\\ref{sec:analysis}, redundant tokens can be more easily identified in deeper layers using attention value-based scoring. We can effectively remove these redundant tokens from deeper layers without negatively impacting the model's accuracy;  (ii) pruning tokens across all layers significantly expands the design space and  poses unnecessary difficulty in optimization, particularly  when trying to maintain a low level of compression sparsity. \n\n\n\n%Therefore, we introduce an extra gate mask $M_{gate}$ before each transformer layer. Token pruning is only applied on layer $i$ when its corresponding gate mask $M_{gate}^i$ is set to 1. If the gate mask value is 0, the layer is skipped, and the retained tokens from the previous layer are used instead.\n\n\n%In our experiments, we observe an interesting trend where as the target sparsity increases (i.e., constraint becomes tighter), the gate masks progressively activate earlier layers, starting from the deepest one. Eventually, the gate masks activate all transformer layers under a high pruning sparsity. Inspired by the observation, we gradually increase the sparsity level from 0 to the target one during token pruning. By doing this, gate masks also provide improved decision-making for pruning fewer tokens within early layers. This is because gate masks help to preferentially learn masks for each token in deeper layers, where token importance is more easily predictable. \n\n\n\n%Despite the tendency for gate masks to activate all transformer layers in a high-sparsity regime, they can still provide improved decision-making for  pruning less important tokens within each layer. This is because gate masks help to preferentially learn masks for each token in deeper layers, where token importance is more easily predictable.  \n\n\n\n\\vspace{3px}\n \\noindent\\textbf{Token ranking position mask $M_{rank}$}. For fine-grained token pruning,  assigning a mask to each token and removing those with a mask value of 0 may seem an intuitive solution. However, it can lead to a problem known as \"static token pruning.\" This occurs because the final mask values are fixed after training, causing  tokens to be pruned at the same positions for all input sequences in the dataset  during inference. This can be problematic as informative tokens can appear in different positions across different input sequences. Removing tokens  at the same positions for all input sequences can also inadvertently remove important tokens and result in a significant loss of accuracy. %Pruning all input sequences to the same length can potentially under-prune shorter sequences or over-prune longer sequences.\n \n %we must handle two types of dynamics for best performance. First,   Second, the effective input sequence lengths vary greatly within a task. As a consequence, fixing a single pruning configuration can under-prune shorter sequences so as to retain sufficient tokens for processing longer sequences or, conversely, over-prune longer sequences to remove sufficient tokens to efficiently process shorter sequences. \n \n\n \n%Intuitively, we can insert a  mask for each token in the input sequence. However, this approach can lead to a problem known as \"static token pruning\". Since the final values of these masks are fixed, directly masking tokens can result in pruning tokens at the same positions across all input sequences in the dataset. This can be problematic because informative tokens can appear in different positions across different inputs. As a result, these valuable tokens may be unexpectedly pruned by the static masks, leading to a significant decrease in accuracy. \n\nTo address this challenge, we follow PoWER-BERT~\\cite{powerbert} to use ranking masks (see Fig.~\\ref{fig:overview}). Instead of applying a mask to each token directly, we mask tokens'  ranking positions  based on their importance score, which is  computed by utilizing attention values, as outlined in Equation~\\ref{eq:importance_score}. The insight is that by scoring tokens based on their importance to the final model prediction, crucial tokens will always rank in the topmost positions after sorting. This means that although informative tokens may appear in different positions based on input content, their ranking positions are static (i.e., topmost) and can be indicated by a static mask. For example, given  an input sequence of $X_{i-1}=(x_1, x_2,...x_n)$ for layer $i$, we sort the tokens by their importance scores, resulting in a ranking of  $(n, 3,...1)$. The corresponding ranking masks are then defined as $(M_{rank}^{i,n}, M_{rank}^{i,3},...M_{rank}^{i,1})$, where the value of $M_{rank}^{i,j}$ indicates whether prune or keep the $j^{th}$ ranked tokens for layer $i$.\n\n\n%To avoid under-pruning of shorter sequences or over-pruning of longer ones, a ranking mask should be used to  automatically determine the appropriate number of tokens to remove for each input sequence, based on its length. To achieve this, we introduce the concept of \"sparsity bins\" which allows for a consistent \\textit{ratio} of tokens to be pruned across both short and long sequences. This approach is based on the assumption  that the percentage of informative tokens in an input sequence is similar regardless of the sequence length in a given task. Specifically, for a given input sequence with $n$ tokens, we first exclude the \\texttt{PAD}  tokens and mask the remaining $n'$ effective, where $n'\\leq n$.  We denote $N$ as the number of sparsity bins. Each bin represents $\\frac{n'}{N}$ consecutive effective rankings and the token at  $i^{th}$ ranking position is associated with the $round(i/n'\\times N)^{th}$ mask. \n\n\n\n\n\n%Here, we provide an example to illustrate the process. Given  an input sequence $H_i=(t_1, t_2,...t_n)$ for layer $i$, we first remove the \\texttt{PAD} tokens to obtain the effective tokens of  $H'_i=(t_1, t_2,...t_{n'})$. We then sort the tokens by their importance scores, resulting in sorted tokens $H'^{sort}_i=(t_{n'}, t_3,...t_1)$ and the ranking of \n%$(n', 3,...1)$. We set $N$ sparsity bins and the corresponding ranking masks are defined as $(M_{rank}^{i,1}, M_{rank}^{i,2},...M_{rank}^{i,N})$. The mask $M_{rank}^{i,j}$ represents the pruning decision for the token at the ranking position of $round(i/N\\times n')$.  \n\n\n\n\n%Inspired by recent top-$k$ token selection strategy proposed in ~\\cite{powerbert,LAT,spatten}, we introduce ranking masks to address the problem of static token pruning. The key insight behind this method is that if tokens can be scored by their significance to the final model prediction, then crucial tokens will always rank in the topmost positions after sorting by significance scores. Despite the fact that the positions of crucial tokens vary across the dataset, their ranking positions after sorting are static (i.e., topmost) and  can be indicated by a mask. \\lz{may add a figure to explain.}\n\n%As shown in Fig.~\\ref{fig:overview}, we define a set of ranking masks $M_{rank}$ for each token. Specifically, we sort tokens by their importance scores (see Section~\\ref{sec:distillation}) to get their rankings. Then, each token is logically associated with its ranking mask. Here we use an example to illustrate the process. Given  an input sequence of $H_i=(t_1, t_2,...t_n)$ for layer $i$, we sort the tokens by their importance scores, resulting in a ranking of  $(n, 3,...1)$. The corresponding ranking masks are then defined as $(M_{rank}^{i,n}, M_{rank}^{i,3},...M_{rank}^{i,1})$, where the value of $M_{rank}^{i,j}$ indicates whether prune or keep the $j^{th}$ important tokens for layer $i$.\n \n \n% \\begin{equation}\n% \t\\label{eq:layer-sparsity}\n %\t\\begin{aligned}\n %\t\t\t\\displaystyle  \n %\t\t\tr_i=M_{gate}^i\\cdot\\sum_{j=1}^{n}(M_{rank}^{i,j}==0)\n %\t\t\\end{aligned} \n %\\end{equation}\n \n\n\n\n%\\lz{rank and sparsity bin to avoid all input sequences prune to a same length}\n\n\n\\vspace{3px}\n\\noindent\\textbf{FLOPs-aware constraint}. With these masks, we can calculate the number of retained tokens and use them to measure the computation and memory cost that is required for  model inference. In our work, we use FLOPs as a metric for evaluating the cost of model inference due to its ease of use. Formally, let $\\bm{M}=\\{M_{gate}^1,...M_{gate}^L, M_{rank}^{1,1},...\\}$,  denoting all the inserted masks. Then the expected model FLOPs after token pruning can be calculated from $\\bm{M}$ as follows:\n\\begin{equation}\n\t\\label{eq:sparsity}\n\t\\begin{aligned}\n\t\t\\displaystyle  c(\\bm{M})=\\Sigma_{i=1}^{L}(4\\cdot \\text{d}^2\\cdot T_i+2\\cdot \\text{d}\\cdot T_i^2+N_{h}\\cdot T_i^2)\\\\+\\Sigma_{i=1}^{L}(2\\cdot \\text{d}\\cdot d'\\cdot T_i)\n\t\\end{aligned} \n\\end{equation}\nwhere the two items calculate the FLOPs of MHA and FFN layers, respectively.   $d$ denotes the hidden size, $N_h$ is the number of heads in MHA layer, $d'$ denotes FFN intermediate size.  $T_i$ represents the number of tokens that are retained for  the $i^{th}$ layer, which can be easily computed by multiplying $\\mathbb{E}(M_{rank}^i>0)$ with the original token length $n$. Note that when the gate mask $M_{gate}^i$ turns off fine-grained token pruning, $T_i$ keeps the same number of tokens with its previous layer (i.e., $T_i$ =$T_{i-1}$). \n\n\n\n\n%In our work, we consider the FLOPs as the metric for measuring the inference cost of a transformer model. During the training, the hard concrete parameter $\\bm{\\alpha}$ determines the values of masks $\\textbf{M}$ and we can calculate the current achieved FLOPs with these masks as follows:\n\n\\vspace{3px}\n\\noindent\\textbf{Learning masks under a desired constraint}.  Now we introduce how to determine the values of gate masks and ranking masks for minimal accuracy loss under a given FLOPs constraint. % so that the resulting model can achieve minimal accuracy loss under a given FLOPs constraint. \nLet $\\bm{\\theta}$ denote the original model and $\\bm{M}$  denote all the pruning masks.  We formalize the task of token pruning as an end-to-end learning problem by adding a regularization term $L_{reg}$:\n\n\\begin{equation}\n\t\\label{eq:loss1}\n\t\\begin{aligned}\n\t\t%\t\\displaystyle  \n\t\tL=L_{downstream}(\\bm{\\theta},\\bm{M})+\\lambda L_{reg} (\\bm{M})\n\t\\end{aligned} \n\\end{equation}\n%where $m$ is the original model, and $\\bm{M}$  denotes all the pruning masks.  \n\nOptimizing the above loss function requires the masks $M_{gate}$ and $M_{rank}$ to be differentiable. However, the original masks are discrete binary values.\nTo overcome this, we use the $L_0$ reparameterization method proposed by ~\\cite{l0}, which is specifically designed for model pruning. Following the standard $L_0$ reparameterization, masks $\\bm{M}$ are regulated using the hard concrete distribution as follows\n%We relax the masks $M_{gate}$ and $M_{rank}$ to continuous variables within the interval [0, 1] using a hard-sigmoid nonlinearity. To enable learning these masks through stochastic gradient-based optimization, we follow the standard $L_0$ reparameterization and regulate masks  $\\bm{M}$ using the hard concrete distribution as follows:\n\\begin{equation}\n\t\\label{eq:l0}\n\t\\begin{aligned}\n\t%\t\\displaystyle  \n\t\\bm{u}\\sim U(0,1)\\\\\n\t\\text{s}=\\text{sigmoid}((\\text{log}\\frac{\\bm{u}}{1-\\bm{u}}+\\text{log}\\bm{\\alpha)}/\\beta)\\\\\n\t\\tilde{s}=s\\times(r-l)+l\\\\\n\t\\bm{M}=\\text{min(1, max}(0,\t\\tilde{s}))\n\t\\end{aligned} \n\\end{equation}\nwhere $U(0,1)$ is a uniform distribution in the interval [0,1]; $l<0$ and $r>0$ are two constants that stretch the sigmoid output into the interval $(l,r)$. $\\beta$ is  a hyperparameter that controls the steepness of the sigmoid function. We adopt the common practice of setting $l$ to -0.1, $r$ to 1.1 and $\\beta$ to $\\frac{2}{3}$.  $\\bm{\\alpha}=\\{\\alpha_j\\}_{j=1}^{|\\bm{M}|}$ are the main learnable parameters. \nThe hard concrete distribution assigns a significant portion of probability mass on the integer values \\{0,1\\}, which serves as a good continuous approximation of the binary (Bernoulli) distribution. %As a result, the $L{reg}$ in Equation ~\\ref{eq:loss1} under the $l_0$ reparameterization method can be conveniently expressed as follows:\n%\\begin{equation}\n%\t\\label{eq:loss2}\n%\t\\begin{aligned}\n\t\t%\t\\displaystyle  \n%\tL_{reg}(\\bm{M})=||\\bm{M}||_0=\\sum_{j=1}^{|\\bm{M}|}\\text{sigmoid}(\\text{log}\\alpha_j - \\beta \\text{log}\\frac{-r}{l})\n%\t\\end{aligned} \n%\\end{equation}\n\n\n\nDuring training, the hard concrete parameters $\\bm{\\alpha}$ and $\\bm{u}$ determine the values of masks $\\bm{M}$. We learn masks $\\bm{M}$ by updating these learnable parameters of the distributions\nfrom which the masks are sampled in the forward pass.\nMoreover, these learnable parameters and masks can be jointly optimized with the original model parameters, resulting in better performance.\n\n\nIn prior token pruning works~\\cite{powerbert,transkimmer,ltp}, $L_1$  is widely used as the regularization loss in Equation~\\ref{eq:loss1}, and $\\lambda$ controls the trade-off between final model accuracy and the achieved sparsity. However, it requires careful hyper-parameter tuning to make sure it converges to a desired sparsity~\\cite{cofi,lagrangian},  which lacks effective control on the complexity of final model inference.\n\nIn our work, we aim to control the achieved model FLOPs after token pruning. We  follow~\\cite{lagrangian} to replace the vanilla $L_0$ objective with a Lagrangian multiplier. \nLet $C$ be the target FLOPs, $c(\\bm{M})$ be the expected FLOPs determined by the masks $\\bm{M}$ in Equation~\\ref{eq:sparsity}.   We  impose an equality constraint $c(\\bm{M})=C$ by introducing a  penalty:\n\\begin{equation}\n\t\\label{eq:loss3}\n\t\\begin{aligned}\n\t\t\\displaystyle L_{reg}(\\bm{M})=\\lambda_1\\cdot(c( \\bm{M})-C)+\\lambda_2\\cdot(c(\\bm{M})-C)^2\n\t\\end{aligned} \n\\end{equation}\nwhere the masks $\\bm{M}$ are determined by hard concrete parameters $\\bm{\\alpha}$ and $\\bm{u}$ in Equation~\\ref{eq:l0}.\n\n\n\n\n\n%\\lz{add information about self-learning $\\lambda_1$ and $\\lambda_2$}\n\n\n\n\n\n\n\n%\\begin{equation}\n%\t\\label{eq:layer-sparsity}\n%\t\\begin{aligned}\n%\t\t\\displaystyle  \n%\t\tr_i=M_{gate}^i\\cdot\\sum_{j=1}^{n}(M_{rank}^{i,j}==0)\n%\t\\end{aligned} \n%\\end{equation}\n\n\n\n\n\\vspace{-1ex}\n"
                },
                "subsection 4.2": {
                    "name": "Distillation of token importance rankings",
                    "content": "\n\\label{sec:distillation}\n%In our proposed constraint-aware token pruning algorithm, the ranking masks are applied to the sorted tokens based on their importance scores. An ideal importance scoring mechanism that provides reliable token importance rankings is essential. This way, the most crucial tokens will always be at the top and will be retained by the ranking masks, which in turn minimizes the impact on the final model prediction.\n\n\nIn our proposed constraint-aware token pruning algorithm, the ranking masks are applied to the sorted tokens based on their importance scores.  Ideal importance scoring mechanism that provides reliable token importance rankings is essential for maintaining the performance of the final model prediction. The most crucial tokens will be at the top and retained by the ranking masks.\n\n\nIn {\\sysname}, we utilize self-attention values to calculate token importance (Equation~\\ref{eq:importance_score}) for ease of hardware deployment. However, as discussed in Sec.~\\ref{sec:analysis1}, accurately ranking the importance of tokens using the self-attention mechanism is challenging in the early  layers of the transformer. Despite the potential for improved token rankings in deeper layers, crucial tokens may be mistakenly ranked as unimportant and removed before they have the opportunity to reach these layers, leading to a substantial decrease in accuracy.\n\nTo overcome this limitation, we propose a method to distill more accurate ranking decisions to the early layers during the training process, enhancing the ability of self-attention values to rank token importance. To this end, we enable the preservation of the most important tokens during inference, thus improving overall accuracy.\n \n\n% However, as discussed in Sec.~\\ref{sec:analysis1}, self-attention mechanism originally is not designed for token importance evaluation and has limitations in  accurately ranking tokens in early layers. This can result in crucial tokens being unexpectedly ranked as unimportant and removed, leading to significant accuracy loss. To address the limitation, we propose to distill more accurate token rankings to these early layers, which can effectively preserve the most important tokens and lead to significant performance gains. \n\n\n\n\n\n\n\n\\vspace{3px}\n\\noindent\\textbf{Ranking-aware token distillation}. Before distillation, we need to obtain accurate token importance rankings as the knowledge. However, obtaining such ground truth or labeled rankings is challenging. Our experiments, as shown in Fig.~\\ref{fig:attentionscore}, suggest that attention values in final layers can effectively identify important tokens. Thus, we use the token importance rankings generated from the final transformer layer of the unpruned model as our knowledge source. %We distill this knowledge to the early layers during the token pruning process as described in Sec.~\\ref{sec:tokenpruning}. \n\nThe next challenge is to develop an effective distillation objective. Previous studies~\\cite{movement, cofi, nn_pruning}  have shown that combining distillation with model weight pruning can improve performance. These studies have used distillation objectives based on cross-entropy loss~\\cite{kd, sanh2020distilbert} or Mean Squared Error (MSE) loss between the student and teacher layer-wise representations~\\cite{cofi, Sun2019PatientKD, jiao2020tinybert}.\n% These studies have employed distillation objectives that use either cross-entropy loss between the student and teacher output probabilities~\\cite{kd, sanh2020distilbert} or Mean Squared Error (MSE) loss between the student and teacher layer-wise representations~\\cite{cofi, Sun2019PatientKD, jiao2020tinybert}. \nHowever, when it comes to token distillation, the conventional MSE loss may not be suitable due to the different levels of information captured by early and deep layers of transformer models like BERT~\\cite{jawahar-etal-2019-bert}. The representations of early and deep layers are meant to be distinct, making it difficult to use conventional distillation objectives.\n\n%Previous work~\\cite{movement,cofi,nn_pruning} has proven that combining distillation with model weight pruning improves performance. They  use distillation objectives that involve either a cross-entropy loss between the student and teacher output probabilities\\cite{kd,sanh2020distilbert} or a Mean Squared Error loss between the student and teacher layer-wise representations~\\cite{cofi,Sun2019PatientKD,jiao2020tinybert}.  In the context of token distillation, it is straightforward to use MSE loss to distill layer representations between the teacher's final layer and the student's early layers. However, transformer models like BERT capture different levels of information across early and final layers~\\cite{jawahar-etal-2019-bert}, where the representations of early and final layers are supposed to be different. Therefore,  \n%we cannot use these conventional distillation objectives. \n\nInstead, we  use token importance rankings from the final layer of the teacher model as the information to be distilled and develop a ranking-aware distillation loss. Our objective is to align the token importance rankings of the early transformer layers with those of the final layer in the teacher model. This way, self-attention layers are encouraged to give more importance to the crucial tokens, increasing the chance of retaining them in early layers. % The process is illustrated in Fig.~\\ref{fig:kd}.\n\nFig.~\\ref{fig:kd} illustrates the overview process of our proposed ranking-aware token distillation. To effectively distill the knowledge, we adopt a ranking loss as the distillation objective. Specifically, we use the LambdaLoss  as proposed in~\\cite{rankloss} to optimize Normalized Discounted Cumulative Gain (NDCG), which is a widely used ranking metric in information retrieval systems that places more emphasis on the importance of top-ranked tokens.\n%aim to optimize the Normalized Discounted Cumulative Gain (NDCG), which is a widely used ranking metric in information retrieval systems that places more emphasis on the importance of top-ranked tokens. We use the LambdaLoss  as proposed in~\\cite{rankloss} to optimize NDCG.\n\n%To optimize the token importance rankings, we adopt the Normalized Discounted Cumulative Gain (NDCG) as the distillation objective. NDCG is a widely used ranking metric in information retrieval systems that places more emphasis on the importance of top-ranked tokens. We use the LambdaNDCG loss as proposed in~\\cite{rankloss} to optimize NDCG.\n\n%The objective of distillation is to align the token importance rankings of the early transformer layers with those of the final layer in the teacher model. This way, by distilling the importance rankings, the model weights get updated such that important tokens have more chance to be retained in early layers. This can be formulated as a ranking optimization problem. Figure~\\ref{fig:kd} illustrates the overview process of our proposed token importance distillation. To effectively distill the knowledge, we adopt a ranking loss as the distillation objective. Specifically, we aim to optimize the Normalized Discounted Cumulative Gain (NDCG), which places more emphasis on the importance of the top-ranked tokens. NDCG is a widely used ranking metric in information retrieval systems. To optimize NDCG, we adopt the LambdaNDCG loss as proposed in~\\cite{rankloss}.\n\n\n\n\n\n\n\n%To effectively use attention value-based scoring, we need to improve its ability to predict the importance of tokens in early layers. Based on the observation that attention values in late layers can effectively rank token importance, our approach is to distill this knowledge to early layers.\n\n%Intuitively, the most important tokens in late transformer layers cannot be removed when they pass through early layers, as they play a crucial role in the final model prediction. Fortunately, these tokens can be easily identified via attention-based scoring in late layers. Our goal is to utilize this information during the token pruning process to ensure that the early layers also identify and preserve these important tokens. To this end, we propose a novel knowledge distillation approach for token pruning. \n\n%Fig.~\\ref{fig:kd} illustrates the overview of our approach.We utilize the original model as a teacher and distill token importance scores predicted in the final transformer layer to the student (i.e., model during token pruning). However, we cannot directly distill the attention values from the teacher, due to the reason that transformer models like BERT capture different levels of information across early and late layers~\\cite{jawahar-etal-2019-bert}. Instead, we use  token rankings from the final layer of the teacher model as the information to be distilled. The objective of distillation is to align the token importance rankings of the early transformer layers with those of the final layer in the teacher model. \n\n%For down-stream tasks with long input token length, minimizing the ranking differences between teacher and early layers in student model can be difficult.  However, our token pruning only cares about the most important tokens (i.e., the topmost)\n\n%~\\cite{rankloss} \\lz{the criterion to select loss function}\n%There are many existing ranking metrics such as NDCG and MAP\n%used in IR problems. A common property of these metrics is that\n%they are rank-dependent and place more emphasis on performance\n%of the top ranked documents.\n\n Specifically, let $\\bm{R}$ denote the rankings obtained by sorting the importance scores of each token in the final layer in the teacher model.  $\\bm{S}^i$ refers to the token importance scores in the $i^{th}$ layer of the student model, and $\\bm{X}$ represents a mini-batch of training data. We define the distillation loss as follows: \n\t\\begin{equation}\n\t\\label{eq:distill}\n\t\\begin{aligned}\n\t\t\\displaystyle  \n\t\tL_{distill}=\\sum_{i=1}^{L'} LambdaLoss(\\bm{R},\\bm{S}^i, \\bm{X})\n\t\\end{aligned} \n\\end{equation}\n\n In our experiments, we empirically set the first 1/3 layers ($L'=\\frac{1}{3}L$) as the early layers for distillation. \n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Training and inference",
                    "content": "\nWe now describe the full training process of our {\\sysname}. Given a deploy constraint, {\\sysname} simultaneously trains  the masks and model parameters, allowing for effective token pruning  and improved model adaptation to token sparsification. Additionally, we incorporate ranking-aware distillation to enhance the ability of the attention values in determining the importance of tokens in early layers.\nThe full training objective is a combination of the above objectives:\n\\begin{equation}\n\t\\label{eq:finalloss}\n\t\\begin{aligned}\n\t\t%\t\\displaystyle  \n\t\tL=L_{downstream}(\\bm{\\theta}, \\bm{M}) + L_{reg}(\\bm{M}) + \\lambda L_{distill}\n\t\\end{aligned} \n\\end{equation}\nwhere $L_{reg}(\\bm{M})$ is defined as in Equation~\\ref{eq:loss3}, and $\\lambda$ is a hyperparameter controlling the contribution of distillation loss. In particular, the two hyperparameters $\\lambda_1$ and $\\lambda_2$ introduced by $L_{reg}(\\bm{M})$ are automatically adjusted using the AdamW optimizer. Therefore, the only additional hyperparameter that needs to be tuned is $\\lambda$.\n\nOnce the training finishes, the token pruning decisions are determined by combining the gate masks and ranking masks. Specifically, only the layers chosen by the gate masks with $M_{rank}=1$ are assigned with ranking masks for token selection. The other layers are not considered for token pruning.\n\nDuring the inference, we use self-attention values to calculate token importance scores in the selected layers. We then sort the tokens based on these scores.\n By discarding the tokens through use of ranking masks ($M=0$), we can effectively eliminate unnecessary tokens and improve the efficiency of the model.\n \n \n \n\n\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n",
                "subsection 5.1": {
                    "name": "Experiment Setup",
                    "content": "\n\\noindent\\textbf{Datasets and Models}. \nWe   evaluate {\\sysname} on a diverse set of datasets, including 8 classification and regression tasks from the GLUE benchmark~\\cite{glue}, the SQuAD-v2.0~\\cite{squadv2} extractive question answering dataset, and the 20News~\\cite{20news} long sequence classification dataset. The diversity of these tasks and the range of token lengths (i.e., 64, 128, 256, 384, 512) showcase the general applicability of our method. A detailed summary of these datasets can be found in Appendix. To demonstrate the generality of our approach on both large and small pre-trained language models, we implement {\\sysname} on RoBERTa$_{base}$~\\cite{roberta}, BERT$_{base}$ and BERT$_6$.\n\n\\vspace{2pt}\n\\noindent\\textbf{Training setup}. Following existing works~\\cite{powerbert,ltp,transkimmer}, we perform  fine-tuning on the downstream datasets before the token pruning. Then we conduct token pruning with the optimization objective in Equation~\\ref{eq:finalloss} under multiple  FLOPs constraints. We use \\texttt{{FLOPs sparsity}} to denote the constraint, which is defined as the ratio of FLOPs that have been pruned or removed from the model.  We also define \\texttt{FLOPs reduction} as the ratio of the full model FLOPs under the original input sequence length to the remaining FLOPs.\n\n\n The token pruning process begins with a warmup stage where we gradually increase sparsity to the target value using a linear scheduler. The initial value of $\\lambda$ is determined by hyper-parameter grid search from [1e-2, 1e-3, 1e-4] on development set with 20\\% data randomly picked from training set.   At the start, layer gate masks are initialized to 0, and ranking masks are initialized to 1, meaning all tokens are retained. After reaching the final sparsity level, we fix it and continue training the model and pruning masks until convergence (i.e., fine-tuning stage). See Appendix for detailed settings of other hyper-parameters.  We conduct all  experiments\n with random seed 57.\n \n\n\n%We follow CoFi~\\cite{cofi} for setting the total pruning epochs. Specifically, we use 100 pruning epochs for small GLUE datasets (such as CoLA, RTE, MRPC, and STS-B), and 60 epochs for large  datasets (such as QQP, SST-2, MNLI, QNLI, and 20News). For the warmup stage, we use half of the total  epochs for small datasets, and 25\\% for large datasets. The hyper-parameter $\\lambda$ in Equation~\\ref{eq:finalloss} is used to balance the significance of token importance distillation. We employ a linear scheduler that reduces the value of $\\lambda$ from its initial value throughout the warmup stage. This setting allows the model parameters to adapt more effectively to the teacher's token importance rankings in the early pruning stages, while shifting focus to fine-tuning the model parameters and pruning masks for improved accuracy in the later stages. The initial value of $\\lambda$ is determined by hyper-parameter grid search from [1e-2, 1e-3, 1e-4] on development set with 20\\% data random picked from training set. We perform all the experimentswith random seed 57.  We follow ~\\cite{cofi} to set all the other hyper-parameters like batch size and learning rate (see Appendix for detailed hyper-parameters). \n\n\\vspace{2px}\n\\noindent\\textbf{Baselines}. We compare {\\sysname} with the state-of-the-art token pruning and model compression methods. Token pruning baselines include attention value-based methods such as PoWER-BERT~\\cite{powerbert}, LAT~\\cite{LAT}, LTP~\\cite{ltp}, and strong prediction-based method  Transkimmer~\\cite{transkimmer}. Model compression baselines include DistilBERT~\\cite{sanh2020distilbert}, and CoFi~\\cite{cofi}, \nwhich is  a state-of-the-art structured pruning method.\n\n \n\n%\\begin{table}[t]\n%\t\\begin{center}\n\t\t%\t\\fontsize{7.5}{7.5} \\selectfont\n%\t\t\\small\n%\t\t\\begin{tabular}\t%{@{\\hskip2pt}c@{\\hskip2pt}|c@{\\hskip2pt}|@{\\hskip2pt}c@{\\hskip2pt}|@{\\hskip2pt}c@{\\hskip2pt}}\n\t%\t\t{@{\\hskip1pt}c@{\\hskip4pt}c@{\\hskip4pt}c@{\\hskip7pt}c@{\\hskip7pt}c@{\\hskip7pt}c@{\\hskip7pt}c@{\\hskip7pt}c@{\\hskip1pt}}\n%\t\t\t\\hline\n%\t\t\t\\multirow{2}{*}{Model} & \\multirow{2}{*}{Method}  &  \\multicolumn{2}{c}{SQuADv2.0} &\\multicolumn{2}{c}{20News}& \\multicolumn{2}{c}{IMDB}\\\\\n%\t\t\t& &F1&FLOPs&Acc. & FLOPs & Acc. & FLOPs\\\\\n%\t\t\t\\midrule\n\t\t%\tBERT$_{base}$ & - & 77.1 & 1.00$\\times$ & 86.7&1.00$\\times$ & 94.0 & 1.00$\\times$\\\\\n\t\t\t%\tTR-BERT&Prediction & No & 75.7 & 2.08$\\times$ & 87.4 & 4.22$\\times$ & 93.6 & 2.26$\\times$\\\\\n\t\t\t%\tTR-BERT&Prediction &  75.7 & 4.63$\\times$ & 87.4 & 6.50$\\times$ & 93.6 & 4.43$\\times$\\\\\n\t\t\t\n\t\t\t%\tTranskimmer &Prediction& No & 75.7 & 2.10$\\times$ & 86.1&5.27$\\times$ & 93.7&2.70$\\times$\\\\\n%\t\t\tTranskimmer &Prediction&75.7 & 4.67$\\times$ & 86.1&$ 8.11\\times$ & \\textbf{93.7}& \\textbf{5.30$\\times$}\\\\\n%\t\t\tPoWER-BERT& Atten-value& - & - & 86.5 & 2.91$\\times$ & 92.1 & 3.05$\\times$\\\\\n\t%\t\tLTP& Atten-value& 75.6 & 3.10$\\times$ & 85.2 & 4.66$\\times$ & 92.8 & 3.93$\\times$ \\\\\n\t\t\t%\t{\\sysname}&Atten-value & No & 74.1 & 1.87$\\times$ & 87.0& 5.31$\\times$ & 92.7 & 2.34$\\times$ \\\\\n%\t\t\t\\textbf{{\\sysname}}&Atten-value & \\textbf{75.9} & 4.12$\\times$ & \\textbf{87.0}&\\textbf{8.26$\\times$} & 93.2 & 4.47$\\times$ \\\\\n%\t\t\t\\hline\n%\t\t\\end{tabular}\n%\t\t\\caption{ Results by different token pruning methods on downstream tasks with long sequence length.}\n%\t\t\\label{tbl:results2}\n%\t\\end{center}\n%\\end{table}\n\n\n\n\n\n%\\vspace{-2ex}\n"
                },
                "subsection 5.2": {
                    "name": "Main results",
                    "content": "\n\\textbf{Overall performance}. We evaluate  {\\sysname} on both BERT$_{base}$ and RoBERTa$_{base}$, and compare with the state-of-the-art token pruning baselines.  LTP lacks an implementation for BERT$_{base}$, we  follow their official code and implement for BERT$_{base}$. For a fair comparison, we follow the provided instructions and conduct a grid search for the optimal hyper-parameters. To prove {\\sysname}'s effectiveness on varying input sequence lengths, we conduct experiments on both the GLUE benchmark and two long sequence tasks. \n\n\n\nWe start by comparing {\\sysname} to the original BERT and RoBERTa. As shown in Table~\\ref{tbl:results1}  and Table~\\ref{tbl:results2}, {\\sysname} achieves comparable accuracy on GLUE benchmark, SQuAD and 20News while significantly reducing FLOPs. For instance,  {\\sysname} even outperforms the original BERT with +2.7\\%, +4.3\\%, +0.3\\%, and +0.5\\% improvement on CoLA, RTE, MRPC, and SST-2 respectively, and achieves an average FLOP reduction of 6.7$\\times$.   On other datasets, the accuracy drop is minimal at $<0.8\\%$. \nThese results demonstrate the effectiveness of {\\sysname} in reducing computational cost while maintaining accuracy.\n\n\nCompared to other token pruning methods based on attention values,  {\\sysname} significantly surpasses strong baselines such as PoWER-BERT, LAT, and LTP across all datasets, despite being based on the same approach. This is mainly due to our ranking-aware token distillation mechanism, which greatly enhances the ability of self-attention values to rank token importance.% Specifically, {\\sysname} achieves up to 8.2\\% higher accuracy compared to LTP while reducing FLOPs by a much larger amount. \nAlso, as can be seen from Table~\\ref{tbl:results1} and Table~\\ref{tbl:results2}, current attention value-based methods cannot surpass the prediction method-based Transkimmer. However,  {\\sysname} outperforms Transkimmer on many tasks. Specifically, under the same-level FLOPs reduction, {\\sysname} on BERT achieves +2.9\\%, +0.1\\%, +0.3\\%, +0.9\\%, +0.2\\%, +0.9\\% higher accuracy on CoLA, QQP, MRPC, SST-2, MNLI, SQuAD  and 20News, respectively. %On a limited number of datasets,  Transkimmer  outperforms {\\sysname}, but  the difference is less than 1\\%. \n\nIt's worth noting that {\\sysname} also outperforms Transkimmer in terms of real inference latency improvement. As discussed in Sec.~\\ref{sec:analysis2}, it's challenging to deploy Transkimmer for real latency reduction. In contrast, {\\sysname} delivers real latency improvement of up to 7.4$\\times$, which will be further discussed in later sections.\n\n%Notably, {\\sysname} also outperforms Transkimmer in terms of real inference latency speedup. As detailed in Sec.~\\ref{sec:analysis2}, Transkimmer experiences significant extra latency of up to 30\\% on GPU due to its prediction modules and the resource-intensive GumbelSoftmax operation without specific hardware support. In comparison, {\\sysname} delivers real latency speedup of up to 7.4$\\times$, which will be discussed in subsequent sections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%\\vspace{3pt}\n\\noindent\\textbf{Under various deployment constraints.} % In the previous section, we show that {\\sysname} outperforms the state-of-the-art token pruning methods under specific token sparsity ratios. In this section, \nWe now evaluate the effectiveness of {\\sysname} under various FLOPs sparsity and compare it to two state-of-the-art methods: LTP (representing attention value-based methods) and Transkimmer (representing prediction-based methods). For a fair comparison, we use their official code implementations and perform a grid search to find the optimal hyper-parameters that achieve the desired token sparsity.\n\nFig.~\\ref{fig:sparsity} summarizes the results.  {\\sysname} surpasses both LTP and Transkimmer at all FLOPs sparsity levels.\nAt equivalent levels of accuracy on the 20News long sequence, {\\sysname} reduces 17\\% and 20\\% more FLOPs compared to Transkimmer and LTP respectively. % {\\sysname} also demonstrates accuracy improvements over the original BERT$_{base}$.\n Without any loss in BERT$_{base}$ accuracy, {\\sysname} reduces FLOPs by 86\\%, 78\\%, 76\\%, and 88\\% on MRPC, SST-2, MNLI, and 20News, respectively.\n\n\n%\\vspace{3pt}\n\\noindent\\textbf{Comparison with model compression}. In addition to token pruning baselines, we compare {\\sysname} with state-of-the-art model compression techniques, including structured pruning and knowledge distillation (i.e., the DistilBERT$_6$). %, that are commonly used for faster inference. \nWe specifically compare with CoFi~\\cite{cofi}, a  top-performing structured pruning method. We evaluate CoFi and {\\sysname} on two model sizes: BERT$_{base}$, a large transformer model, and BERT$_6$, a small model. \n\n\nTable~\\ref{tbl:compression1} shows the results by different methods. {\\sysname} and CoFi outperform DistilBERT$_6$ in terms of higher compression ratios. {\\sysname} consistently surpasses the  original BERT$_{base}$ consistently with higher accuracy and a 6.7$\\times$ average FLOPs reduction, whereas CoFi sees a significant drop in accuracy for CoLA and 20News at high compression ratios. %Our analysis of CoFi's pruned structures reveals that it tends to remove entire FFN layers at high pruning ratios, leading to  performance degradation.\n On the smaller BERT$_6$ model, {\\sysname}  exhibits a much smaller accuracy loss compared to CoFi, showcasing its effectiveness and robustness across models of various sizes.\n\n\n\n\n\n\n\n%\\vspace{-2ex}\n"
                },
                "subsection 5.3": {
                    "name": "Ablation study",
                    "content": "\n\n\n\\noindent\\textbf{The effectiveness of coarse-to-fine grained pruning strategy}. Compared to prior works, {\\sysname} introduces a new coarse-to-fine pruning approach that uses gate masks to choose the best subset of transformer layers for fine-grained token pruning. To evaluate the impact of this design on performance, we compare the results with and without gate masks by removing the masks and pruning all layers at the fine-grained level. We run all experiments under the same FLOPs sparsity (\\cref{tbl:results1,tbl:results2}) and hyper-parameter settings. \n\n\n\nTable~\\ref{tbl:ablationstudy} shows the accuracy results on  GLUE benchmark. Removing gate masks causes a noticeable accuracy drop  on all datasets except SST-2. Specifically, the accuracy drops by a substantial  6.2\\%, 3.5\\%, 2.9\\%, and 2.9\\% on RTE, MRPC, QNLI, and STS-B, respectively. These results highlight the effectiveness of our coarse-to-fine grained  approach in making much better token selection decisions.\n\n\\vspace{3px}\n\\noindent\\textbf{Different token  distillation approaches}. We also ablate on the ranking-aware token distillation component to evaluate its contribution to the performance of {\\sysname}. We first remove the entire distillation process from the token pruning process. As shown in Table~\\ref{tbl:ablationstudy}, the removal of our proposed ranking-aware token distillation results in accuracy drops across all datasets. Moreover, we observe  that the effect of ranking-aware token distillation varies based on the length of the input sequences. On datasets with relatively short sample lengths (see Table~\\ref{tbl:dataset_summary} in Appendix), such as CoLA, QQP, SST-2, and MNLI, token distillation slightly improves accuracy by 0.6\\%, 0.2\\%, 0.3\\%, and 0.2\\%, respectively. However, on datasets with longer sample lengths, such as RTE, MRPC, QNLI, and STS-B, token distillation has a crucial role in improving accuracy by 7.2\\%, 4.4\\%, 1.1\\%, and 3.7\\%, respectively. The underlying reason is that it's much easier for conventional attention-based scoring methods to determine the significance of tokens in shorter sequences compared to longer sequences. However, when the task involves longer sequences, accurately identifying the most critical tokens at early transformer layers becomes challenging. In such cases, our proposed ranking-aware token distillation effectively tackles this problem and leads to a significant improvement in accuracy.\n\nMoreover, we  compare the use of conventional distillation objectives to the $LambdaLoss$ in Equation~\\ref{eq:distill}. \n%instead of the LambdaLoss in Equation~\\ref{eq:distill} on model accuracy. \n%Moreover, we examine the effect of using conventional distillation objectives instead of the $LambdaLoss$  in Equation~\\ref{eq:distill} on the model's accuracy. \nOur alternatives include the MSE loss~\\cite{cofi} which seeks to reduce the discrepancy between  teacher's and student's token importance scores, and the general cross-entropy (CE) loss~\\cite{kd} that aims to minimize the KL divergence between teacher's and student's token importance score distributions. Table~\\ref{tbl:ablationstudy}  indicates that relying on conventional distillation objectives fails to improve the accuracy effectively.\n\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Retained Tokens Visualization and Analysis",
                    "content": "\nTo further understand the behavior of {\\sysname}, we analyze the number of retained tokens in each layer. We conduct experiments on four datasets using two groups of FLOPs sparsity ratios: (i) low sparsity {45\\%, 20\\%, 20\\%, 50\\%} and (ii) high sparsity {65\\%, 65\\%, 60\\%, 80\\%} for QQP, SST-2, MNLI, and 20News respectively.  The low sparsity numbers are used for experiments in Fig.~\\ref{fig:sparsity} and the high sparsity numbers are evaluated in Table~\\ref{tbl:results1} and Table~\\ref{tbl:results2}. The model accuracy loss  is negligible as demonstrated in previous sections. Notably, for better visualization, we exclude \\texttt{PAD} tokens when analyzing {\\sysname}'s  pruning decision on the original effective input tokens. % Hence, the FLOPs sparsity here is computed as the number of pruned FLOPs divided by the full model FLOPs under the length of effective tokens. \n\nFig.~\\ref{fig:insight} illustrates the number of remaining tokens used as input for each layer in BERT$_{base}$ under different levels of sparsity ratios. Interestingly, we discover common token pruning patterns in the models: (1) deep transformer layers have high token redundancy. Under high sparsity,   over 95\\% of tokens are pruned when they arrive at layer 8, indicating that earlier layers have effectively extracted their information, making them non-informative for deep layers. This observation is consistent with the conclusions of  studies on BERT behaviours~\\cite{clark-etal-2019-bert,rogers-etal-2020-primer}, indicating that {\\sysname} is capable of automatically identifying the optimal patterns for token pruning. (2) {\\sysname} prioritizes token pruning in deeper layers over an even distribution across all layers. We observe that {\\sysname} selects different layers for token pruning under varying levels of sparsity ratios. In {\\sysname}, token pruning is initially performed in deeper layers when the sparsity is low, and as the sparsity increases, it gradually extends to earlier layers. For example, on the SST-2 task, when the sparsity is set to 20\\%, token pruning is not applied to layers 1 to 7. However, when the sparsity increases to 65\\%, token pruning is activated in layers 3 to 7, while only layers 1 and 2 are excluded from pruning. \n\n\n\n%\\vspace{-2ex}\n"
                },
                "subsection 5.5": {
                    "name": "Inference latency on Hardware",
                    "content": "\n%\\vspace{-0.5ex}\n\nFinally, we assess the practical efficiency of {\\sysname} in resource-limited environments by evaluating BERT on MRPC, RTE, SQuAD, and 20News datasets using an 8-core Intel(R) Xeon(R) CPU@2GHz. The learned ranking masks are applied layer-by-layer to discard tokens, and latency is measured using the high-performance Onnxruntime inference engine~\\cite{onnxruntime}. The batch size is set to 1 for simplicity.\n\nAs shown in Table~\\ref{tbl:latency}, BERT inference without token pruning incurs a high latency on the CPU, particularly for long input sequences. However, {\\sysname} significantly reduces this latency by discarding unnecessary tokens. Specifically, {\\sysname} delivers inference acceleration of 2.9$\\times$, 5.8$\\times$, 4.1$\\times$, 7.4$\\times$ on MRPC, RTE, SQuAD, 20News respectively, with minimal impact on accuracy.\nThe latency reduction increases with the input length, indicating the big potential of {\\sysname} in handling long sequence tasks.\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this paper, we propose {\\sysname}, a novel token pruning approach that leverages $L_0$ regularization to  determine the optimal token removal under a target inference constraint. {\\sysname} adopts  hierarchical pruning, where it selects the ideal subset of transformer layers for fine-grained token pruning. \n Coupled with ranking-aware token importance distillation, {\\sysname} significantly enhances the ability of self-attention values to rank token importance, leading to superior accuracy even at high compression ratios. Extensive evaluations on the GLUE benchmark and SQuAD have shown that {\\sysname} outperforms existing token pruning and model compression baselines. With the removal of unnecessary tokens, {\\sysname} achieves up to 12.6$\\times$ FLOP reduction for BERT with less than 1\\% drop in accuracy.\n \n  \n\n\n\n% Entries for the entire Anthology, followed by custom entries\n%\\bibliography{anthology,custom}\n\n\n%\\balance\n{\n\t\\bibliographystyle{ACM-Reference-Format}\n\t\\balance\n\t\\bibliography{ref}\n}\n\\appendix\n\\newpage\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 7.1": {
                    "name": "Datasets",
                    "content": "\nTable~\\ref{tbl:dataset_summary} summarizes the datasets used in our experiments. Various types of tasks (classification, regression, and QA) with various dataset sizes and input sequence lengths are included to demonstrate the wide applicability of our method.\n\n\n"
                },
                "subsection 7.2": {
                    "name": "Hyperparameters",
                    "content": "\n\nWe follow CoFi~\\cite{cofi} for setting the total pruning epochs. We use 100 pruning epochs for small GLUE datasets (such as CoLA, RTE, MRPC, and STS-B), and 60 epochs for large  datasets (such as QQP, SST-2, MNLI, QNLI, and 20News). For the warmup stage, we use half of the total  epochs for small datasets, and 25\\% for large datasets. \n\nThe hyper-parameter $\\lambda$ in Equation~\\ref{eq:finalloss} is used to balance the significance of token importance distillation. We use a linear scheduler that reduces the value of $\\lambda$ from its initial value throughout the warmup stage. This setting allows the model parameters to adapt more effectively to the teacher's token importance rankings in the early pruning stages, while shifting focus to fine-tuning the model parameters and pruning masks for improved accuracy in the later stages.\n\nWe report the hyperparameters used in our experiments in Table ~\\ref{tbl:hyperparameter}. $\\lambda$ for rank-distillation loss is chosen from \\{1e-2, 1e-3, 1e-4\\}. LTP, Transkimmer, and CoFiPruning are trained based on their implementation details in the paper and open-source code.\n\n\n\n\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Case Study",
                    "content": "\n\nOne example from SQuADv2.0 dataset is presented in Table~\\ref{tbl:case_study} to show the reason why we use the attention score from the last layer to distill the attention score of the first few layers. For finetuned $BERT_{base}$ model, the attention score for the answer token ranks 114th in layer 2 among all the attention scores of the total 170 tokens, while in layer 12 it ranks 24th. The answer token is a very important token, and should not be pruned during inference. However, it ranks low in layer 2, so it will be probably pruned for a high sparsity, resulting in a wrong answer. After applying rank distillation, its rank goes higher, making it less likely to be pruned.\n\n\n\n\n\n\\begin{comment}\n\\begin{table*} [ht]\n\t\\begin{center}\n\t\t\\small\n\t\t\\begin{tabular} {l|cccc|cccc|c|c}\n\t\t\t\\hline\n\t\t\tHyperparameter & CoLA & RTE & MRPC & STS-B & SST-2 & MNLI & QNLI & QQP & SQuADv2.0 & 20News \\\\\n            \\hline\n            learning rate & 1e-5 & 8e-5 & 8e-5 & 4e-5 & 1e-5 & 4e-5 & 4e-5 & 2e-5 & 4e-5 & 6e-5 \\\\\n            % sparsity bin number & 20 & 100 & 50 & 30 & 25 & 50 & 50 & 50 & 150 & 512 & 256 \\\\\n            sparsity warmup epoch & \\multicolumn{4}{c|}{50} & \\multicolumn{4}{c|}{10} & 5 & 10 \\\\\n            total epoch & \\multicolumn{4}{c|}{100} & \\multicolumn{4}{c|}{60} & 10 & 60 \\\\\n            \n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\t\\caption{ Hyperparemeters in the experiments.  }\n\t\t\\label{tbl:hyperparameter}\n\t\\end{center}\n\\end{table*}\n\\end{comment}\n\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Inference on GPU",
                    "content": "\n We implement a simple inference version using PyTorch to measure latency on V100 GPU. Our ToP approach demonstrates latency speedup on GPUs, achieving 1.2x speedup on RTE (20.04 ms) and 1.24x speedup on MRPC (11.00 ms), although the acceleration ratio is not as high as on CPUs.\n\nWe believe that there is still potential for improving GPU inference acceleration with high-performance inference engines and system optimizations. We found that token pruning requires some memory operations, such as removing tokens with mask value 0 from hidden states. Although this operation requires no computation, it is time-consuming on GPU. In our future work, we plan to utilize high-performance inference engines and leverage system optimizations to achieve greater GPU inference acceleration.\n\n\n"
                }
            }
        },
        "figures": {
            "fig:attentionscore": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=1\\columnwidth]{attentionscores.png}\t\n%\t\\vspace{-2ex}\n\t\\caption{Comparison of token importance score distributions on the GLUE MRPC dataset using attention values from the BERT$_{base}$ model. Note that the y-axis is log-scaled to better visualization of tokens with low importance scores.}\n\t\\label{fig:attentionscore}\n\\end{figure}",
            "fig:overview": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=1\\columnwidth]{overview.png}\t\n\t\\caption{Our approach learns  layer gate masks and token ranking masks to prune tokens  under a desired constraint. When a layer gate turns off (i.e., mask=0), we skip the current layer. When a layer gate turns on (i.e., mask=1), unimportant tokens are removed after the self-attention mechanism.}\n\t\\label{fig:overview}\n\\end{figure}",
            "fig:kd": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.95\\columnwidth]{kd.png}\n\t\\vspace{-2ex}\t\n\t\\caption{Our ranking-aware token distillation uses  importance rankings generated from the unpruned model's final layer and distill it to early layers during the training.}\n\t\\label{fig:kd}\n\\end{figure}",
            "fig:sparsity": "\\begin{figure*}[ht]\n\t\\centering\n\t\\includegraphics[width=1\\textwidth]{sparsity.png}\t\n\t\\vspace{-5ex}\n\t\\caption{Comparison of token pruning methods under various FLOPs sparsity ratios.}\n\t\\label{fig:sparsity}\n\\end{figure*}",
            "fig:insight": "\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=1\\columnwidth]{insight.png}\t\n\t\\vspace{-3ex}\n\t\\caption{The retained tokens as input for each layer in BERT. }\n\t\\label{fig:insight}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\t\\label{eq:transformer}\n\t\\begin{aligned}\n\t\t\\displaystyle \\bm{X}_{i-1}^\\prime=\\text{LN}(\\bm{X}_{i-1}+\\text{MHA}(\\bm{X}_{i-1}))\n\t\t\\\\\n\t\t\\bm{X}_i=\\text{LN}(\\bm{X}_{i-1}^\\prime+\\text{FFN}(\\bm{X}_{i-1}^\\prime)),\n\t\\end{aligned} \n\\end{equation}",
            "eq:2": "\\begin{equation}\n\t\\label{eq:attention}\n\t\\begin{aligned}\n\t\t\\displaystyle \\bm{A}_h=\\text{softmax}((\\bm{X}\\times \\bm{W}_q^h )\\times(\\bm{X}\\times\\bm{W}_k^h)^T)\\\\\n\t\t\\text{Attention}_h=\\bm{A}_h\\times (\\bm{X}\\times\\bm{W}^h_v)\n\t\\end{aligned} \n\\end{equation}",
            "eq:3": "\\begin{equation}\n\t\\label{eq:importance_score}\n\t\\begin{aligned}\n\t\t\\displaystyle  s^l(x_i)=\\frac{1}{N_h}\\frac{1}{n}\\sum_{h=1}^{N_h}\\sum_{j=1}^n\\bm{A}^l_{h}[x_i, x_j]\n\t\\end{aligned} \n\\end{equation}",
            "eq:4": "\\begin{equation}\n\t\\label{eq:sparsity}\n\t\\begin{aligned}\n\t\t\\displaystyle  c(\\bm{M})=\\Sigma_{i=1}^{L}(4\\cdot \\text{d}^2\\cdot T_i+2\\cdot \\text{d}\\cdot T_i^2+N_{h}\\cdot T_i^2)\\\\+\\Sigma_{i=1}^{L}(2\\cdot \\text{d}\\cdot d'\\cdot T_i)\n\t\\end{aligned} \n\\end{equation}",
            "eq:5": "\\begin{equation}\n\t\\label{eq:loss1}\n\t\\begin{aligned}\n\t\t%\t\\displaystyle  \n\t\tL=L_{downstream}(\\bm{\\theta},\\bm{M})+\\lambda L_{reg} (\\bm{M})\n\t\\end{aligned} \n\\end{equation}",
            "eq:6": "\\begin{equation}\n\t\\label{eq:l0}\n\t\\begin{aligned}\n\t%\t\\displaystyle  \n\t\\bm{u}\\sim U(0,1)\\\\\n\t\\text{s}=\\text{sigmoid}((\\text{log}\\frac{\\bm{u}}{1-\\bm{u}}+\\text{log}\\bm{\\alpha)}/\\beta)\\\\\n\t\\tilde{s}=s\\times(r-l)+l\\\\\n\t\\bm{M}=\\text{min(1, max}(0,\t\\tilde{s}))\n\t\\end{aligned} \n\\end{equation}",
            "eq:7": "\\begin{equation}\n\t\\label{eq:loss3}\n\t\\begin{aligned}\n\t\t\\displaystyle L_{reg}(\\bm{M})=\\lambda_1\\cdot(c( \\bm{M})-C)+\\lambda_2\\cdot(c(\\bm{M})-C)^2\n\t\\end{aligned} \n\\end{equation}",
            "eq:8": "\\begin{equation}\n\t\\label{eq:distill}\n\t\\begin{aligned}\n\t\t\\displaystyle  \n\t\tL_{distill}=\\sum_{i=1}^{L'} LambdaLoss(\\bm{R},\\bm{S}^i, \\bm{X})\n\t\\end{aligned} \n\\end{equation}",
            "eq:9": "\\begin{equation}\n\t\\label{eq:finalloss}\n\t\\begin{aligned}\n\t\t%\t\\displaystyle  \n\t\tL=L_{downstream}(\\bm{\\theta}, \\bm{M}) + L_{reg}(\\bm{M}) + \\lambda L_{distill}\n\t\\end{aligned} \n\\end{equation}"
        }
    }
}