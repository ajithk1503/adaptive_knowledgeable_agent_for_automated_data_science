{
    "meta_info": {
        "title": "Pushing Joint Image Denoising and Classification to the Edge",
        "abstract": "In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.",
        "author": "Thomas C Markhorst, Jan C van Gemert, Osman S Kayhan",
        "link": "http://arxiv.org/abs/2409.08943v1",
        "category": [
            "cs.CV",
            "eess.IV"
        ],
        "additionl_info": "Accepted paper at the ECCV 2024 workshop on Advances in Image  Manipulation (AIM)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nThe intersection of edge devices, such as security cameras, and deep learning has sparked an interest in optimizing neural networks for inference time, further referred to as latency. Common tasks to optimize for such efficiency are object classification and object detection, which unlock automatic recognition. However, in noisy settings, the recognition accuracy might not be perfect and it is important to allow the ability to validate the automatic recognition by human inspection. Thus, in addition to automatic recognition, the perceptual quality of the processed image is equally significant. In particular, this is relevant for images containing noise, which can arise from various sources such as low-light conditions, sensor noise, or other recording conditions. We focus on using an efficient model that can be used on the edge with the aim of enhancing human perception for validating the recognition output of noisy images.\n\nDomains relying on human image perception but challenged by noisy images, like medical imaging \\cite{Medical_application}, surveillance systems \\cite{Suveillance_application}, and industrial inspections \\cite{Industrial_application}, can benefit from recently proposed denoising Convolutional Neural Networks (CNNs) \\cite{DnCNN, RDUNet}. As CNNs denoise better than traditional methods \\cite{BM3D, WNNM}. Fast CNN denoisers \\cite{FFDNet, SGN} are required to accommodate the real-time requirement of the affected domains. However, denoisers are not able to remove all noise, which is not always enough for human image perception.\n% Real-time denoising techniques\n\n% GOAL: part 2 of sketching what we need\nWe further improve human understanding of the image by combining denoising with machine perception, like image classification. From the Human-Computer Cooperation strategies in \\cite{HumanClassifierCoop}, we use the Classifier as Aid to Human. Where the image classifier can be used as a direct advisor or an independent agent to the security guard, the latter being analogous to a second opinion in medical diagnosis. In different use cases, fusing skills of humans and computers has been shown to improve performance beyond using only one type \\cite{HumanClassifierCoop, HCI_ZoneOuts, HCI_underwater}. Therefore, we investigate models that can leverage the benefits of both denoising and classification to enhance human understanding in real time.\n\n\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=0.7\\columnwidth]{images/thanks_brain.png}\n%     \\caption{Placeholder for Figure 1, which summarizes the study. Till further notice this image represents the study pretty well.}\n%     \\label{fig:fig1}\n% \\end{figure}\n\n% GOAL: sketch problem with current work + what needs to be different\nA model combining both denoising and classification is studied in \\cite{joint_denoising_classification}, focusing on denoising performance. In addition, we optimize for efficiency, which is required for edge devices, and classification. Our efficiency definition is based on two elements: (i) latency reduction while (ii) retaining denoising performance and classification accuracy. These elements could be optimized using independent classification and denoising models. However, we propose an architecture combining the tasks more efficiently.\n\n% GOAL: sketch what we did to achieve the goal\nFirst, we employ established model design approaches to enhance independent denoising and classification models, such as model scaling \\cite{EfficientNet, UNet_reduction} and %the introduction of\nefficient operators \\cite{MobileNetV2}. Although the models are optimized, they still operate separately, resulting in unnecessary overhead. Hence we propose and compare two methods that join both tasks, yielding a novel and efficient architecture. \n\nAdjusting this architecture for each device and desired latency can be laborious and requires expert knowledge. These issues have recently garnered interest, leading to the emergence of new automated architecture search techniques, which have achieved competitive results in image classification \\cite{EfficientNetV2, FB_net}. Moreover, recent Neural Architecture Search (NAS) approaches incorporate latency in their loss function, enabling the design of architectures tailored to specific latency requirements \\cite{MobileNetV3, TF_NAS, FB_net}. Combining NAS with the proposed architecture provides a seamless and efficient approach to designing denoising and classification models for diverse use cases. %This integration simplifies the model design process and empowers researchers to effortlessly create efficient models that suit their specific requirements.\n\nWe find that our proposed efficiency-focused architecture consistently outperforms our more straightforward one. This is observed for both the manually and NAS designed models. In addition, our NAS models significantly outperform the manually designed ones in denoising and classification performance.\n% We find that integrated is better than sequential\n% We find that NAS models outperform our manually constructed ones significantly, although the structure and latency are similar.\n\nWe have the following contributions. (i) We introduce a novel architecture to efficiently combine denoising and classification. The novelty lies in sharing an encoder between the denoiser and the classifier. (ii) We propose modifications to an existing NAS method for classification \\cite{TF_NAS} to stabilize its search, improving the performance of the found architectures. (iii) We extend an existing NAS method to search for a model that combines denoising and classification, optimized for a target latency, classification accuracy, and denoising performance.\n\nSince no prior work proposes a joint efficient model for denoising and classification, we study the tasks both separately and joint in Sec. \\ref{sec:toy-setup}. The findings are used as expert knowledge to construct the NAS method in Sec. \\ref{sec:NAS}.\\footnote{Project site: \\url{https://thomas-markhorst.github.io}}\n\n\n"
            },
            "section 2": {
                "name": "Related work",
                "content": "\n\\label{sec:related_work}\n\\textbf{Denoising.}\nImage denoising aims to reconstruct a clean image \\(x\\) from its observed noisy variant \\(y\\). This relation can be formulated as \\(y = x + n\\), where we assume \\(n\\) to be additive white Gaussian noise (AWGN). Neural network-based denoisers offer faster inference and good performance compared to traditional denoising methods like BM3D \\cite{BM3D} and WNNM \\cite{WNNM}. The interest in deep learning for denoising started with DnCNN \\cite{DnCNN}, a simple Convolutional Neural Network (CNN). Encoder-decoder architectures became popular due to their efficient hierarchical feature extraction. Specifically, UNet \\cite{UNet} whose skip-connections between the encoder and decoder enhance the denoising process as shown in follow-up methods \\cite{RDUNet, MWCNN, DHDN}. The interest in the UNet structure continues with transformer architectures \\cite{Uformer, SUNet}. In this paper, our denoisers are based on UNet, ensuring our findings can translate to most related work.\n\n\\textbf{Efficient classification.} Optimization for efficiency is generally achieved by either compressing pre-trained networks \\cite{compression_survey} or designing small networks directly \\cite{MobileNetV2, EfficientNetV2}. We focus on efficient design, for which handcrafted models and neural architecture search (NAS) play essential roles. Studies proposing handcrafted models often introduce efficient operators \\cite{MobileNetV2, MobileNet, ShuffleNet} or scaling methods \\cite{EfficientNet}. These efficient operators are used in NAS methods \\cite{EfficientNetV2, FB_net} aiming for the automated design of efficient neural networks. Such an operator is the inverted residual with a linear bottleneck (MBConv), as introduced in MobileNetV2 \\cite{MobileNetV2}. \nIn our models, we study scaling methods and MBConv's efficiency characteristic.\n\n\\textbf{Neural Architecture Search.} The use of reinforcement learning (RL) for neural architecture search introduced efficient architectures with competitive classification performance \\cite{EfficientNetV2, MobileNetV3, MNASNet, ENAS}. However, their discrete search space is computationally expensive.\n% because their search space involves numerous discrete decisions, which requires searching through an exponential amount of architectures.\nDifferentiable NAS (DNAS) methods \\cite{DARTS, FB_net, ProxylessNAS} significantly reduce this cost by relaxing the search space to be continuous using learnable vectors \\(\\alpha\\) for selecting candidate operations, which allows for gradient-based optimization. The popularity of DNAS started with DARTS \\cite{DARTS}, which searches a cell structure. \n%This cell is repeated throughout all network layers. \nDue to the complex design and repetitiveness throughout the network of the cell structure, follow-up works \\cite{FB_net, TF_NAS} search operators for every layer instead of constructing repeating cells. \n\nPitfalls of DNAS are the collapse of search into some fixed operations and a performance drop when converting from the continuous search network to the discretized inference network \\cite{FairDARTS, SNAS, BDARTS}. TF-NAS \\cite{TF_NAS} addresses these issues with an adaptation in the search algorithm, which lets the search model mimic the discrete behavior of the inference model. \nIn addition, TF-NAS searches an architecture with a target latency by adding a latency loss to the search optimization. Because of these properties, we use TF-NAS as a baseline for our NAS study.\n\nExisting NAS methods for denoising are either not reproducible \\cite{SuperkernelNAS_denoising}, have a cell-based search space \\cite{HiNAS_denoising}, or do not have an encoder-decoder \\cite{DPNAS_denoising} architecture. Instead, we use a layer-based search space and encoder-decoder structure.\n\n\\textbf{Joint classification and denoising.} In \\cite{joint_relation}, the positive influence of denoising methods on classification performance is discussed. Moreover, \\cite{joint_denoising_classification} proposed a joint model where a VGG classifier \\cite{VGG} is attached to a denoiser similar to UNet. This method \\cite{joint_denoising_classification} reports a qualitative improvement of the denoised images when adding the classification loss to the denoiser's optimization, whereas \\cite{joint_relation} reports a quantitative improvement. Although these models denoise and classify well, they are not optimized for efficiency. In this paper, we design a joint image denoising and classification method for edge devices.  \n\n\n"
            },
            "section 3": {
                "name": "Exploiting Expert Knowledge",
                "content": "\n\\label{sec:toy-setup}\nWe start in a controlled setting with separate baseline models for classification and denoising. Additionally, methods to increase their respective efficiency are studied, resulting in a reduced version of the baseline denoiser and classifier. Both the construction and efficiency improvement of the models are described in Suppl. \\ref{app:classification}, where a UNet (Fig. \\ref{fig:integrated}) and simple 2-block CNN (Fig. \\ref{fig:integrated}{\\color{black}.i} and \\ref{fig:integrated}{\\color{black}.ii}) are used as baseline denoiser and classifier respectively. This section describes how the different sizes of the classifiers and denoisers are used to study joining methods and their efficiency. \n\n\\textbf{Dataset \\& settings.} For the experiments in this section, we generate a controlled synthetic data set to study the behavior of the classifier and denoiser when applying model scaling, replacing the convolutional operations, and combining both models. The dataset consists of 30k images, each with a random constant background in a gray tint [0.1 - 0.3] with two randomly placed non-overlapping MNIST \\cite{MNIST} digits. We use two digits to increase the complexity of the denoising task. For experiments including classification, the two digits are extracted from the image using ground truth locations. These extracted digits are separately used as input for the classifier. In the experiments where noise is required, for either denoising or noisy classification, synthetic Gaussian noise is added. This noise is zero mean, and the intensity of the noise is controlled using the standard deviation (\\(\\sigma\\)) of the distribution. Fig. \\ref{fig:sub1} shows a sample, and Fig. \\ref{fig:sub3} its noisy variant. To test the model behavior on an extensive noise range, every model is trained and tested on eleven \\(\\sigma\\) values evenly spaced on the interval \\([0,1]\\) (Tabs. \\ref{tab:joint}, \\ref{tab:classification-appendix} and \\ref{tab:denoising}). The models are trained using Adam optimizer with 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.\n\n% \\begin{figure}\n%      \\centering\n%      \\begin{subfigure}[b]{0.2\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=\\columnwidth]{images/toy-setup-data/clean1.png}\n%          \\caption{sample 1}\n%          \\label{fig:clean1}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.2\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=\\columnwidth]{images/toy-setup-data/clean2.png}\n%          \\caption{sample 2}\n%          \\label{fig:clean2}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.2\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=\\columnwidth]{images/toy-setup-data/noisy1_06std.png}\n%          \\caption{1: std=0.6}\n%          \\label{fig:noisy1}\n%      \\end{subfigure}\n%      \\hfill\n%      \\begin{subfigure}[b]{0.2\\columnwidth}\n%          \\centering\n%          \\includegraphics[width=\\columnwidth]{images/toy-setup-data/noisy2_02std.png}\n%          \\caption{2: std=0.2}\n%          \\label{fig:noisy2}\n%      \\end{subfigure}\n%         \\caption{Two example images from the data set with different background tints (a, b) and the corresponding noisy versions (c, d).}\n%         \\label{fig:toy-setup-data}\n% \\end{figure}\n\n\n\nSince the experiments with the controlled data set are not targeted at a specific device, the metric defining efficiency should not depend on a device. Such a metric is computational power, most commonly defined as Floating Point Operations (FLOPs), which we use as the primary metric. Despite being device dependent, we assess latency as a secondary metric. The latency is measured with a batch size of 32, 100 warm-up inference passes and averaged over 1000 inference passes. Classification performance is quantified using accuracy, while for denoising performance the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics \\cite{SSIM_PSNR} are used. \n%For all three metrics, a higher score is better.\nHigher is better for all our metrics.\n\n\n\n\n\n",
                "subsection 3.1": {
                    "name": "Joint model: DC-Net",
                    "content": "\n\\label{subsec:dc-net}\n\\textbf{Experimental setup.} We construct a baseline and reduced joint model, Denoising-Classifying Network (DC-Net). Both the baseline and reduced DC-Net use the same classifier (MB2.5-M Suppl. \\ref{app:classification}). Whereas UNet-S and UNet are used for the reduced and baseline DC-Net respectively.\n\nFor joining the denoiser and classifier, we propose two models: (i) a Sequential model where the classifier is attached after the denoiser (Fig. \\ref{fig:integrated}{\\color{black}.i}), and (ii) an Integrated model, the classifier is attached to the UNet encoder (Fig. \\ref{fig:integrated}{\\color{black}.ii}). For the Integrated model, classification and denoising branches share the encoder.\n\nThe benefits of the Integrated model could come in threefold. First, using a shared encoder removes the need for a second large classifier, as in the Sequential method. Second, the decoder and classifier branches could run in parallel compared to running sequentially, which can result in lower latency. Thirdly, the decoder is only optimized for denoising, since the optimization of the classifier does not influence it anymore. It can result in better image quality.\n\nThe models are trained using a weighted combination of the Cross-Entropy and Charbonnier loss \\cite{AIMWinnerCharbonnier, CharbonnierBaron} (Eq. \\ref{eq:DC-Net}). We report the metrics averaged over all 11 noise levels, \\(\\sigma\\) in [0, 1]. \n\n\\begin{equation}\n    \\mathcal{L} = 0.1 \\cdot \\mathcal{L}_\\text{CE} + 0.9 \\cdot \\mathcal{L}_\\text{Char}\n    \\label{eq:DC-Net}\n\\end{equation}\n\n% \\textbf{1. Baseline integrated.} The proposed integrated and concatenated setups placed the classifier before and after the decoder, respectively. Does altering the integrated model by attaching the classifier one level higher in the decoder improve performance? We tested this with the baseline models. \n% - RQ: interested in where to put connect the classifier, at the bottom or something in between integrated and concatenated\n% - show and compare results of the two different setups\n% - mention we tried moving it up further which did not have a positive effect\n% - conclude moving up a bit is better for these experiments\n% % discuss moving up classifier one layer\n\n\\textbf{Exp 1. Integrated vs. Sequential.} Which joining method performs better for the baseline, and does the same conclusion hold when reducing its size? We compare the \\textit{Sequential} and \\textit{Integrated} models. In Tab. \\ref{tab:joint}, we see that for both the baseline and reduced DC-Net models, the Integrated version performs significantly better at denoising, while the Sequential version performs better at classification. The difference in denoising performance is visualized in Fig. \\ref{fig:joint}. We see that both the reduced (\\ref{fig:joint}\\textcolor{black}{c}) and baseline (\\ref{fig:joint}\\textcolor{black}{e}) Integrated models reconstruct the digit clearly. Whereas both sizes of the Sequential model (\\ref{fig:joint}\\textcolor{black}{d} and \\textcolor{black}{f}) fail to reconstruct the digit. \n\n\n\n\n\n\\textbf{Conclusion.}\nThe integrated model has a slightly lower classification accuracy compared to the Sequential model, yet it has superior performance in terms of image quality. When aiming for improved human perception, it is still required for the human to see the content of the image. Therefore, the Integrated model is more suitable for joint denoising and classification and is called DC-Net."
                }
            },
            "section 4": {
                "name": "Neural Architecture Search",
                "content": "\n\\label{sec:NAS}\nWe follow similar experimentation strategies as in the previous section. TF-NAS\\cite{TF_NAS} is used to construct a classifier, which we use as a basis for our denoiser and joint model. All our proposed models in this Section contain searchable blocks, the models and which parts are searchable are defined in Figure \\ref{fig:DC-NAS}.\n\n\\textbf{Dataset \\& settings.}\nThe following experiments are conducted on Imagenet \\cite{Imagenet}, randomly cropped\n    to 224x224 pixels. To reduce search and training time, 100 classes (Imagenet 100) from the original 1000 classes were chosen, as in \\cite{TF_NAS}. In the experiments requiring noise, Gaussian noise is sampled uniformly with a continuous range of \\(\\sigma\\) in \\([0,1]\\) (Tabs. \\ref{tab:NAS-denoising}, \\ref{tab:main-results}, \\ref{tab:ablation1}, \\ref{tab:ablation2} and \\ref{tab:ablation3}).\n\nThe models are searched using SGD with momentum, 2E-2 LR with 90 epochs. Afterward, the found architecture is trained from scratch with 2E-1 LR for 250 epochs. All other settings are similar to \\cite{TF_NAS}. The loss function depends on the task of the experiment, Cross-Entropy with label smoothing for classification ($\\mathcal{L}_\\text{CE}$), combined Charbonnier and SSIM losses for denoising ($\\mathcal{L}_\\text{Den}$), and a weighted combination for the joint model ($\\mathcal{L}_\\text{Both}$), see Eq. \\ref{eq:DC-NAS_Den}, \\ref{eq:DC-NAS_Both}.\n\n\\begin{align}\n    \\mathcal{L}_\\text{Den} = 0.8 \\cdot \\mathcal{L}_\\text{Char} + 0.2 \\cdot \\mathcal{L}_\\text{SSIM} \\label{eq:DC-NAS_Den}\\\\\n    \\mathcal{L}_\\text{Both} = 0.1 \\cdot \\mathcal{L}_\\text{CE} + 0.9 \\cdot \\mathcal{L}_\\text{Den}\\label{eq:DC-NAS_Both}\n\\end{align}\n\nSince our NAS method uses a latency look-up table constructed for our device, these experiments target a specific device, GeForce RTX 3090 GPU. Therefore latency is suitable for defining efficiency in the NAS experiments.\n\n\n\n",
                "subsection 4.1": {
                    "name": "Classification: C-NAS",
                    "content": "\n\\label{subsec:NAS-classification}\n\\textbf{Experimental Setup.}\nSince TF-NAS \\cite{TF_NAS} learns \\(\\beta\\)'s to control the number of convolutional operators per stage, \\(\\beta\\)'s can reduce the model size. However, in the models proposed by \\cite{TF_NAS}, only 2 out of 24 stages are reduced by \\(\\beta\\). So the \\(\\beta\\)'s have little effect on the found architectures, yet they make the search space more complex. Therefore, we propose a version of TF-NAS where the \\(\\beta\\)'s are removed so that all convolutional blocks are used. \n\nThe candidate operations in the search space of TF-NAS are MBConvs with 8 different configurations, see Suppl. \\ref{app:search-space}. The configurations differ in kernel size, expansion rate, and in- or excluding a squeeze- and excitation layer (SE) \\cite{SE-layer}. \n\nThe classification experiments are performed using data without noise, as the aim is to examine the NAS method, which is designed for clean images. We investigate key components of TF-NAS and try to improve its stability and classification performance.\n\n\\textbf{Exp 1. Learned vs. Removed \\(\\beta\\).} We conduct an experiment to study the effect of removing \\(\\beta\\) on the search quality. The SE-layer is excluded from the candidate blocks, halving the search space to ensure the number of candidate operations does not cause search instability. We set a low target latency of 6 ms, as learning \\(\\beta\\) should have a positive effect on small networks. For both the learned and removed settings, we run two searches, search 1 and 2.\n\nFig. \\ref{fig:NAS-beta} shows that when \\(\\beta\\) is learned, the \\(\\alpha\\)'s selecting a candidate operation oscillate and therefore do not decide on an architecture. Whereas with Removed \\(\\beta\\), the search is stable. This stability is reflected in the performance, as the average accuracy of the Removed \\(\\beta\\) models is 86.3\\%, compared to 84.2\\% for Learned \\(\\beta\\). The separate results for each model are shown in Suppl. \\ref{app:nas-beta}.\n\n\\textbf{Exp 2. Number of operators in search space.}\nDoes reducing the number of operators during search positively influence the performance of the found models? We test this by comparing the performance of architectures searched with three different search space sizes, \\{4, 6, or 8\\} operations, defined in Suppl. \\ref{app:search-space}. For each of these search spaces, three different latency targets are used: \\{6, 8, and 12\\} ms. \n\nIn Fig. \\ref{fig:NAS-number-blocks}, we see that for lower target latencies, 6 and 8 ms, using fewer operations in the search space does not alter performance significantly. When targeting 12 ms latency, reducing the number of operations in the search space does show a significant improvement.\nAdditionally, we find that when using the larger search spaces, the operators from the small search space are still preferred for lower latencies.\n\n\\textbf{Exp 3. Compare with original TF-NAS.} How do architectures found using our proposed changes to TF-NAS perform compared to models with similar latency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2, and ResNet-18. MobileNetV2 our model have similar latency, architecture, and operator types. ResNet only differs in that it uses the Conv operator. We include these standard baseline architectures to indicate where C-NAS, see Fig. \\ref{fig:DC-NAS}{\\color{black}.i}, stands on Imagenet100.\n\nFig. \\ref{fig:NAS-classification} shows that the model found using our method has lower latency yet higher accuracy than TF-NAS C as proposed in \\cite{TF_NAS}. The model is searched with target latency 8.0. We observe that our search method is able to find a model that matches its target latency. Although ResNet-18 and MobileNetV2 run faster than our model, our classification accuracy is superior, especially when compared to ResNet-18, which only uses Convs.\n\n\\textbf{Conclusion.} By Removing \\(\\beta\\) and reducing the number of operators used in the search, the search stability increases, and we find architectures that have better accuracy. An architecture found using our changes classifies better than a TF-NAS architecture with similar latency.\n\nThe comparison between our model and ResNet-18 shows that our search space is able to compete with widely accepted Conv-based classifiers. Moreover, our model performs on par with MobileNetV2, a manually designed classifier using MBConvs.\n\n"
                },
                "subsection 4.2": {
                    "name": "Denoising: D-NAS",
                    "content": "\n\\textbf{Experimental setup.}\nTo construct a denoiser, D-NAS (Fig. \\ref{fig:DC-NAS}{\\color{black}.ii}), we use the first six stages of a found C-NAS classifier, which has four levels of resolution. Afterwards, we attach a UNet style decoder by using both a transposed convolution and two normal convolutions for each decoder level. Like UNet, we also add skip connections between the encoder and decoder layers. The decoder is not searched.\n\n\\textbf{Exp 1. D-NAS vs UNet denoiser.}\nDoes our denoiser D-NAS perform similarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2) \\{\\textit{d} = 4, \\textit{b} = 8, \\textit{c} = 2, \\textit{m} = 1.5\\}, with a latency of 9.2 ms and the larger UNet-M,  \\{\\textit{d} = 4, \\textit{b} = 16, \\textit{c} = 2, \\textit{m} = 2\\} with a latency of 16.9 ms.  We compare them with our D-NAS M, with similar latency.\n\n\n\nTab. \\ref{tab:NAS-denoising} shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2\\% SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant, UNet-M, denoises better than our proposed model, by 0.3 dB and 1\\% SSIM.\n\n\\textbf{Conclusion.} D-NAS performs similarly to our baseline UNets. Therefore D-NAS is a suitable denoising architecture and it can form the backbone of our Integrated model.\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Joint Model: DC-NAS",
                    "content": "\n\\label{subsec:nas-joint}\n\\textbf{Experimental setup.}\nTo construct the joint model, we use the Integrated setup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We connect the decoder after the first six stages of C-NAS  (Fig. \\ref{fig:DC-NAS}, but still use the remaining C-NAS stages(Fig. \\ref{fig:DC-NAS}{\\color{black}.iii})) as a classification branch. The design choices for DC-NAS are discussed in the ablations study (Sec. \\ref{subsec:ablation}).\n\nUsing our search method, we search for DC-NAS models of three different sizes \\{S, M, L\\}. Apart from our manually designed Integrated model, we compare our searched models with separate state-of-the-art classifiers and denoisers, as there are no existing models that jointly optimize denoising, classification, and efficiency. For each DC-NAS model, we also separately train the classifier (C-NAS) to evaluate the influence of joint denoising. The classifier and denoiser baselines are chosen to have similar latency as the C-NAS or D-NAS model on which the corresponding DC-NAS is based.\n\n\\textbf{Results.} We discuss the results in Tab. \\ref{tab:main-results} in three separate sections for the different target latencies. Our smallest Integrated model, DC-NAS S, outperforms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that the latter shows that the classifier, C-NAS S, performs better when integrated into DC-NAS S. Moreover, our Integrated model denoises better than its baseline UNet-S (Suppl. \\ref{app:classification}). DC-NAS S also significantly outperforms our manually designed DC-Net S (Reduced), which is the only Integrated baseline. We display the denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. \\ref{fig:visual-comparison}. We observe better denoising on smooth areas, sharper edges, and more realistic color reconstruction for DC-NAS S.\n\nThe results of DC-NAS M follow a similar pattern, where our DC-NAS outperforms its baseline denoiser and classifier, using C-NAS M in the Integrated model boosts accuracy by 0.5\\%. When comparing DC-NAS M to DC-NAS S, the classification performance improves by 1.7\\%, yet the denoising performance plateaus. LPIENet denoises the worst. Comparing DC-NAS M's denoising performance to 3.2 ms slower UNet-M we observe slightly worse denoising performance. However, our Integrated model denoises and classifies with lower latency.\n\nFor DC-NAS L, we observe that both the classification and denoising baseline slightly outperform our Integrated model. EfficientNetV2-b1 has 0.3\\% higher classification accuracy than DC-NAS L, and UNet-M improves with 0.7 dB PSNR and 2\\% SSIM. However, our Integrated model performs both denoising and classification at a similar latency as UNet-M, which only denoises. When comparing DC-NAS L with DC-NAS M, we again note an improvement in classification performance. However, the PSNR score drops by 0.2 dB while the more important SSIM score remains at 0.70.\n\n% Size L might observe, that when the denoiser is big enough (UNet-M) the denoising performance is lower when trained integrated. however, we might have lower denoising performance but we can classify without additional cost.\n\n\\textbf{Conclusion.} Our results demonstrate that the proposed DC-NAS models perform similar or better than their denoising and classification baselines for their target latency. In addition, the searched model performs better than our manually designed joint denoiser and classifier.\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\n\\label{subsec:ablation}\n\\textbf{Exp 1. Encoder search.} C-NAS forms the encoder of DC-NAS and contains the searchable operations within DC-NAS. We test multiple search approaches: (i) using clean images, and (ii) using noisy images. \nFor both approaches, we search the encoder using only classification loss $\\mathcal{L}_\\text{Cls}$. In addition, we also search the DC-NAS encoder on noisy images using the combined denoising and classification loss $\\mathcal{L}_\\text{Both}$. Therefore it searches for the optimal encoder for both tasks within the Integrated model DC-NAS. Regardless of the search method, the found models are trained using noisy images and the combined loss.\n\nTab. \\ref{tab:ablation1}, shows that when using $\\mathcal{L}_\\text{Cls}$ with noisy images during search improves classification accuracy by 0.3\\%. Surprisingly, the denoising performance is the same. Using both the denoising and classification objectives during the search reduces the classification accuracy. Caused by the denoising loss complicating the search, without improving denoising performance. Therefore, we search our DC-NAS models by only using $\\mathcal{L}_\\text{Cls}$ loss.\n\n\\textbf{Exp 2. Compare Integrated vs. Sequential.} We compare DC-NAS and DC-NAS$_\\text{seq}$ models with similar latency. Where DC-NAS$_\\text{seq}$ is our Sequential model, which is constructed by attaching C-NAS to the output of D-NAS, see Fig. \\ref{fig:DC-NAS}{\\color{black}.iv}. Since the searched classifier is used twice in DC-NAS$_\\text{seq}$, the Sequential model has a higher latency than the Integrated variant. To counter this, a smaller C-NAS model is used in both the encoder and classifier of DC-NAS$_\\text{seq}$. The classifier, C-NAS, used to construct DC-NAS$_\\text{seq}$ L has a latency of 6.7 ms. Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that these models were searched by using clean instead of noisy images, as this holds for both models it is still a fair comparison.\n\nWe see that both models have similar latency and the same classification accuracy, however, DC-NAS L improves denoising performance with 0.5 dB PSNR and 1\\% SSIM (Tab. \\ref{tab:ablation2}). This improvement is caused by DC-NAS L's Integrated design as this allows for a bigger encoder without increasing latency.\n\n\n\n\n\n% }\n\n\\newcommand\\denoisedwidth{0.185}\n\n\n\\textbf{Exp 3. Decoder tuning.} The DC-NAS models found in Tab. \\ref{tab:main-results} and \\ref{tab:ablation2}, have similar denoising performance. These models differ only in the type of MBConvs that are selected during search in the encoder. We test the hypothesis if the denoising performance is influenced by adjusting the operators in the decoder while retaining the latency. DC-NAS M is used as a basis in this experiment. We construct three alternatives. First, the convolutional operators in the decoder are replaced with MBConvs (MB-k3-e3) \\cite{MobileNetV2}, which significantly increases the latency of the model. To account for this, we scale down the decoder by (i) using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3 instead of 4 decoder layers. \n\nIn Tab. \\ref{tab:ablation3}, we see that using the MBConvs compared to Convs improves the denoising performance. However, at the cost of a 14 ms latency increase, only caused by the MBConv decoder. When reducing the complexity of the MBConv decoder with \\textit{1 operator} and \\textit{3 layers}, the denoising performance reduces to the original level again, but the latency is still higher than for DC-NAS M which has only standard convolutional layers in the decoder block.\n\n\n\n\\textbf{Conclusion.} We have seen that the Integrated combining method outperforms its Sequential counterpart in denoising. To construct the integrated model (DC-NAS), we find that searching for a classifier on noisy data, without taking the denoising objective into account results in the best classification performance. Surprisingly, the search method does not influence the denoising performance. Furthermore, manually altering the decoder does not benefit denoising efficiency either. However, the NAS denoising experiments demonstrate that our denoising setup is competitive. Since tuning the decoder operators does not improve performance, our method is focused on searching for only the encoder of the integrated model. The models found by this approach, outperform our manually designed models with similar latency. "
                }
            },
            "section 5": {
                "name": "Limitations \\& Conclusion",
                "content": "\nOne limitation of our NAS method is its inability to alter the decoder. It is designed this way as manually altering the decoder does not improve efficiency. However, when targeting a significantly different latency, a change in denoising architecture could be required. Therefore, designing model scaling rules for the searched models is of interest, similar to the EfficientNets \\cite{EfficientNet, EfficientNetV2}.\n\nAnother limitation is the fixation of \\(\\beta\\) in our NAS method. Although this improves the stability of search and network performance, learning \\(\\beta\\) while retaining a stable search would be preferred. This would introduce more possibilities in the search space for optimizing efficiency.\n\nIn addition, the latency of Integrated models can be reduced further by running the denoising and classification branches in parallel.\n\nTo conclude, we show that using efficient operators and scaling methods proposed in previous work \\cite{EfficientNet, UNet_reduction, MobileNetV2} are relevant for denoising and noisy classification. In addition, we present the Integrated model DC-Net to join the two tasks efficiently and show that the Integrated design is more suitable across various latencies than the Sequential variant. To simplify the design process of the joint model when targeting a latency, we present a NAS method. We alter an existing NAS method to improve the stability and performance of the search. This method searches a classifier. Using the searched classifier as a basis, we build the Integrated DC-NAS model. We demonstrate that the proposed model outperforms the manually constructed model. We believe that our study can be a precursor of efficient joint low-level and high-level computer vision tasks.\n\n\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n\\bibliographystyle{splncs04}\n\\bibliography{main}\n\n\\clearpage\n\\setcounter{page}{1}\n\\appendix\n\n"
            },
            "section 6": {
                "name": "Efficient Classification \\& Denoising: Additional results",
                "content": "\n\\label{app:classification}\nThe joint models in Section \\ref{sec:toy-setup} are constructed using a separate denoiser and classifier. We describe the baseline models and several methods to construct the reduced versions.\n\n\\textbf{Overview of the models used in the main paper.} UNet-S: \\{\\textit{d} = 4, \\textit{b} = 8, \\textit{c} = 2, \\textit{m} = 1.5\\}, which is also called Reduced UNet. UNet-M: \\{\\textit{d} = 4, \\textit{b} = 16, \\textit{c} = 2, \\textit{m} = 2\\}. UNet: \\{\\textit{d} = 5, \\textit{b} = 64, \\textit{c} = 2, \\textit{m} = 2\\}, which is also called Baseline UNet. MB2.5-M: the classifier described in Section \\ref{sec:toy-experiments-classification} with an MBConv (expansion rate = 2.5) as second convolutional layer.\n\n",
                "subsection 6.1": {
                    "name": "Efficient Classification",
                    "content": "\n\\label{sec:toy-experiments-classification}\n\\textbf{Experimental setup.} Our baseline classifier (Conv-L) consists of two convolutional, one global max pooling, and a linear layer. Each convolutional layer also has a group normalization \\cite{group_norm}, max pooling, and ReLU activation function.\n\nTo construct the reduced version, we use two methods similar to previous works \\cite{MobileNetV2, EfficientNet}. In the first method, we replace the second convolutional layer with an MBConv layer. Three expansion rates are used \\(\\{1, 2.5, 4\\}\\): (i) rate 1 is the lowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline, and (iii) rate 2.5 is in the middle of those two. The second reduction method is to lower the number of filters in the baseline, also called the model width. Using these techniques, models with three different FLOP sizes are constructed, \\{S, M, L\\}. We use the following naming scheme, Conv-\\(x\\) and MB\\(e\\)-\\(x\\), where \\(x\\) represents the FLOP size and \\(e\\) is the expansion rate of the MBConv.\n\nThe models are trained using Cross Entropy loss. We report the accuracy averaged over all 11 noise levels.\n\n\\textbf{Exp. 1: Conv vs MBConv comparison.} According to \\cite{MobileNetV2}, the MBConv layer should be more efficient than a normal convolutional layer. Therefore, when comparing the two operators in our network, we expect the version with an MBConv layer to need fewer FLOPs for the same accuracy. In Table \\ref{tab:classification-appendix}, the MB models with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better than the Conv-L model with fewer FLOPs. However, with an expansion rate of 1 (MB1-S), the accuracy drops 7\\% compared to Conv-L. Therefore, \\cite{MobileNetV2}'s theory also holds for the noisy classifier, but only for the higher expansion rates.\n\n\n\n\\textbf{Exp. 2: MBConv width \\& expansion rate scaling.} Since MBConv layers can be used to improve efficiency, we question how to further reduce the MB model's FLOP size. We compare two options: (i) reducing the expansion rate and (ii) scaling the width of the network. We take MB4-L as the starting model, as this is our best and largest model.\n\nFrom the MB models with size S in Table \n\\ref{tab:classification-appendix}, MB1-S performs the worst. It only has a reduced expansion rate from 4 to 1. MB4-S, which is obtained by scaling the width of MB-L, increases classification performance by only 0.4\\%. However, when slightly reducing MB4-L's width and expansion rate, we derive MB2.5-S, which reaches 58.4\\% accuracy, significantly outperforming both other S-sized MB models. So the combination of the two methods is most effective.\n\n% To examine the behavior for a less significant reduction in size, we compare the MB models of size M. MB2.5-M, which only has a reduced expansion rate, outperforms MB4-M with reduced width. For a smaller decrease in size, such as L to M, scaling the expansion rate slightly is more efficient than scaling the width.\n\n\\textbf{Exp. 3: Conv width scaling.} In this experiment, we compare the width scaling of the Conv-L model. Table \\ref{tab:classification-appendix} shows that all S-sized MB models outperform Conv-S, MB2.5-S even by 3.0\\%. MB2.5-M also outperforms Conv-M, by 2.7\\%. Therefore, scaling is more efficient for the MB models than the Conv models when optimizing for FLOPs.  \n\n% \\begin{itemize}\n%     \\item Design simple baseline for MNIST classification + explain arch\n%     \\item Goal: reduce FLOPs of classifier while remaining good score\n%     \\item Replace with MB's, expansion rate influence on efficiency\n%     \\item Try different Conv scaling methods --> picked best one\n%     \\item Results\n%     \\item Short discussion (add lat is acting different than FLOPs --> interesting experiment for real setup)\n% \\end{itemize}\n\n% \\textbf{4. Scaling Conv-L latency to MB2.5-M/MB4-L.} Add this?\n\n% \\textbf{5. Scaling models up.} Add this?\n\n\\textbf{Conclusion.}\nThe MBConv layer can replace the convolutional layers. We find that compared to the Conv models, the MB models also scale down more efficiently by first reducing the expansion rate, possibly followed by a width reduction. Scaling effectively reduces the number of FLOPs.\n\nMB2.5-M has the second-best accuracy with low FLOPs and a latency close to the baseline, Conv-L. Therefore, MB2.5-M is used as the reduced classifier. We also use MB2.5-M as the new baseline classifier as it outperforms the old baseline, Conv-L, in FLOPs and accuracy.\n\nIt is important to note that the reduction in FLOPs instantiated by using MBConvs, does not translate to a latency reduction in these experiments. This issue is discussed previously in \\cite{EfficientNetV2}. Since the target of these experiments is FLOPs and the latency increase is manageable, we place minimal emphasis on the latency.\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Efficient Denoising",
                    "content": "\n\\label{subsubsec:eff-den}\n\n\\textbf{Experimental setup.} For denoising, the baseline and reduced version are constructed by performing a hyperparameter study on UNet similar to \\cite{UNet_reduction}. Figure \\ref{fig:integrated} shows the UNet architecture along with its hyperparameters to tune. We explore the parameters one at a time, starting with the number of base features maps \\textit{b}, then the UNet depth \\textit{d}, the feature map multiplier \\textit{m}, and the number of convolutional blocks per layer \\textit{c}. In the original UNet: \\{\\textit{b} = 64, \\textit{d} = 5, \\textit{m} = 2, \\textit{c} = 2\\}. Altering these hyperparameters can greatly reduce the model size. Similar to the classification experiments, we also study the ability of the MBConv operator to increase efficiency in the denoiser. The models are trained using Charbonnier loss \\cite{charbonnier}. % In the results, we only present noise level \\(\\sigma=0.8\\), as the results for the other noise levels show the same trend.\n\n\n\n\\textbf{Exp. 1: The base feature map width \\textit{b}.} In this experiment, we aim to find the relevant range of \\textit{b}. Since \\textit{b} is multiplied at every level, the number of feature maps throughout all layers depends on it, which makes it a powerful hyper-parameter. We use \\textit{d} = 4 and the other hyper-parameters as in the original UNet, then we test \\textit{b} \\(\\in\\) \\{4, 8, 16, 32, 64\\}. % Parameter \\textit{d} is reduced as we are interested in the behavior of \\textit{b} on smaller UNets.\nThe trend in Figure \\ref{fig:denoising}{\\color{black}.a} shows that the performance and FLOPs increase with \\textit{b}. We observe that the trend is significantly disrupted by \\textit{b} = 4.\n%, indicating that the last size reduction is not worth the performance drop. \nConversely, the performance difference between \\textit{b} = 32 and \\textit{b} = 64 is small, but the network size quadrupled. Therefore in further experiments, we focus on \\textit{b} \\(\\in\\) \\{8, 16, 32\\}.\n\n\\textbf{Exp. 2: The UNet depth \\textit{d}.} Given the robustness of \\textit{b}, we are interested in how reducing \\textit{d} compares in terms of efficiency. Figure \\ref{fig:denoising}{\\color{black}.b} displays the performance of the architectures with the selected \\textit{b} \\(\\in\\) \\{8, 16, 32\\} testing \\textit{d} \\(\\in\\) \\{2, 3, 4, 5\\}. We observe that reducing \\textit{d} causes a drop in denoising performance, whereas \\textit{b} retains performance better, also in Figure \\ref{fig:denoising}{\\color{black}.a}. Therefore \\textit{b} scales down more efficiently. The models with \\textit{d} = 3 or 4 denoise most efficient. Especially for the smaller models, \\textit{d} = 4 performs well.\n\n\\textbf{Exp. 3: The number of conv blocks per layer \\textit{c}.} Does reducing \\textit{c} further increase efficiency? To test this, we take the best-performing settings, \\textit{d} = 4 and \\textit{b} \\(\\in\\) \\{8, 16, 32, 64\\}, and compare \\textit{c} = 1 and \\textit{c} = 2. Figure \\ref{fig:denoising}{\\color{black}.c} shows that the model with \\textit{c} = 2 outperforms \\textit{c} = 1. Therefore reducing \\textit{c} does not benefit the model's efficiency.\n\n\\textbf{Exp. 4: The feature map multiplier \\textit{m}.} We test if our smallest model could be further reduced in size by lowering \\textit{m}. We take \\textit{d} = 4 and \\textit{b} = 8, and compare \\textit{m} \\(\\in\\) \\{1, 1.5, 2\\}. Figure \\ref{fig:denoising}{\\color{black}.d} shows that the reduction to \\textit{m} = 1.5 retains performance. For \\textit{m} = 1, the performance drops. Reducing \\textit{m} to 1.5 could therefore be used to scale down the model when further reducing \\textit{b} significantly decreases performance. \n\n\\textbf{Conclusion.} To construct the reduced and baseline denoiser, we use the smallest and largest values from the found hyperparameter ranges. Resulting in baseline (UNet): \\{\\textit{b} = 32, \\textit{d} = 4, \\textit{m} = 2, \\textit{c} = 2\\} and reduced (UNet-S): \\{\\textit{b} = 8, \\textit{d} = 4, \\textit{m} = 1.5, \\textit{c} = 2\\}. Table \\ref{tab:denoising} compares the two models for a selection of the noise levels. Although the reduced model has significantly fewer FLOPs and lower latency, the denoising performance is relatively similar to the baseline denoiser.\n\nThe UNet hyper-parameter experiments are replicated using MBConvs, which lead to similar findings. Moreover, the Conv UNet slightly outperforms the MB model. Therefore, the Conv model is used.\n% Similar to the classification experiments, the convolutional layers in the UNets are replaced by MBConv layers. The hyper-parameter experiments are replicated using the MB UNet, which led to similar findings. Moreover, the efficiency of the MB UNet varies between being just below and on par with its Conv counterpart. Therefore in the UNet, we use the convolutional layers. However, using MBConvs in the denoiser could use more testing in the next experimental setup.\n\n\n\n"
                }
            },
            "section 7": {
                "name": "Search space",
                "content": "\n\\label{app:search-space}\nIn Section \\ref{subsec:NAS-classification}, different variations of the TF-NAS search space are used \\cite{TF_NAS}. Table \\ref{tab:searched-operators} displays the candidate operations and for which search space size they are used. The search space with 4 operators is constructed using the MBConvs without SE-layer, as this is most common in recent NAS methods \\cite{EfficientNetV2, FB_net}. For the 6-operator search space, we add the possibility of using an SE layer on the operators where the kernel size is three and the expansion rate is three or six. We use the two smallest operators as they can be used for smaller target latencies too. The search space with 8 operators simply uses all combinations.\n\n\n\n% \\def\\hmath$#1${\\texorpdfstring{{\\rmfamily\\textit{#1}}}{#1}}\n\n"
            },
            "section 8": {
                "name": "beta",
                "content": "\n\\label{app:nas-beta}\nIn Experiment 1 of Section \\ref{subsec:NAS-classification}, we test the influence of removing \\(\\beta\\) from the search approach. The models with Removed \\(\\beta\\) significantly outperform the models with Learned \\(\\beta\\) in accuracy. Besides, the found models are more similar for Removed than Fixed, Removed \\(\\beta\\) differs only 0.04ms and 0.2\\% accuracy, while Learned \\(\\beta\\) differs 0.57ms and 1.4\\% accuracy. This indicates that the search for Removed is more stable.\n\n\n\n"
            }
        },
        "tables": {
            "tab:joint": "\\begin{table}[b]\n\\centering\n\\caption{Comparison of the reduced and baseline joint models. Both the Integrated and Sequential methods trained on the synthetic noise dataset. The integrated model performs significantly better in denoising and slightly worse in classification. The integrated model also scales down better.}\n\\resizebox{0.80\\columnwidth}{!}{%\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\nDC-Net & Type & FLOPs (M) \\(\\downarrow\\) & Lat. (ms) \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & Acc. (\\%) \\(\\uparrow\\)\\\\ \\midrule\n\\multirow{2}{*}{Baseline} & \\textbf{Integrated} & 1301.8 & \\textbf{7.14} & \\textbf{32.8} & \\textbf{0.97} & 88.1 \\\\\n & Sequential & 1302.1 & 7.55 & 27.1 & 0.95 & \\textbf{89.6} \\\\ \n\\cmidrule(r){1-2}\n\\multirow{2}{*}{Reduced} & \\textbf{Integrated} & 51.2 & \\textbf{2.41} & \\textbf{29.9} & \\textbf{0.97} & 86.2 \\\\\n & Sequential & 51.5 & 2.83 & 25.2 & 0.92 & \\textbf{87.6} \\\\ \n\\bottomrule\n\\end{tabular}%\n}\n\\label{tab:joint}\n\\end{table}",
            "tab:NAS-denoising": "\\begin{table}[]\n\\caption{Comparison of D-NAS and UNet variants for denoising. D-NAS outperforms slightly faster UNet-S, but UNet-M denoises best at the cost of 45\\% higher latency.} %than D-NAS.}\n\\centering\n\\resizebox{0.55\\columnwidth}{!}{%\n\\begin{tabular}{@{}lccccccc@{}}\n\\toprule\n\\multirow{2}{*}{Model} & \\multicolumn{3}{c}{UNet params:} & \\multirow{2}{*}{Lat. (ms) \\(\\downarrow\\)} & \\multirow{2}{*}{PSNR \\(\\uparrow\\)} & \\multirow{2}{*}{SSIM \\(\\uparrow\\)} \\\\\n & d & b & m &  &  &  &  \\\\ \\midrule\nUNet-S & 4 & 8 & 1.5 & 9.2 & 25.0 & 0.69 \\\\\nUNet-M & 4 & 16 & 2 & 16.9 & 25.9 & 0.72 \\\\ \nD-NAS M & - & - & - & 11.6 & 25.6 & 0.71 \\\\\n% UNet & 5 & 64 & 2 & 116.5 & \\textbf{26.6} & \\textbf{0.74} \\\\\n% D-NAS S upscaled & - & - & - & \\textbf{109.8} & 26.5 & 0.73 \\\\  \n\\bottomrule\n\\end{tabular}%\n}\n\\label{tab:NAS-denoising}\n\\end{table}",
            "tab:main-results": "\\begin{table}\n\\caption{Comparison of DC-NAS models searched for three different latencies, with their corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Integrated models perform similar or better than their corresponding baselines, with the advantage of having a joint denoising and classification network. *See Suppl. \\ref{subsubsec:eff-den}.}\n\\centering\n\\resizebox{0.8\\textwidth}{!}{%\n\\begin{tabular}{@{}llcccc@{}}\n\\toprule\n\\multicolumn{1}{c}{\\multirow{2}{*}{Model}} &\n  \\multicolumn{1}{c}{\\multirow{2}{*}{Type}} &\n  \\multirow{2}{*}{Lat. (ms) \\(\\downarrow\\)} &\n  Classification &\n  \\multicolumn{2}{c}{Denoising} \\\\ \\cmidrule(l){4-6} \n\\multicolumn{1}{c}{} &\n  \\multicolumn{1}{c}{} &\n   &\n  Acc. (\\%) \\(\\uparrow\\) &\n  PSNR \\(\\uparrow\\) &\n  SSIM \\(\\uparrow\\) \\\\ \\midrule\nMobileNetV3 \\cite{MobileNetV3}                     & Classifier & 4.9  & 70.4 & -             & -    \\\\\nC-NAS S (\\textit{ours})                                      & Classifier & 5.9  & 73.5 & -             & -    \\\\\nUNet-S* \\cite{UNet} & Denoiser   & 9.2  & -    & 25.0          & 0.69 \\\\\nDC-Net S (\\textit{ours})                                     & Integrated & 10.0 & 61.9 & 24.5          & 0.68 \\\\ \nDC-NAS S (\\textit{ours})                                     & Integrated & 10.3 & \\textbf{74.3} & \\textbf{25.4}          & \\textbf{0.70} \\\\ \\midrule\nEfficientNetV2-b0 \\cite{EfficientNetV2}            & Classifier & 9.0  & 75.4 & -             & -    \\\\\nC-NAS M (\\textit{ours})                                      & Classifier & 7.9  & 75.5 & -             & -    \\\\\nLPIENet 0.25x \\cite{LPIENet}                        & Denoiser   & 12.7 & -    & 24.1          & 0.65 \\\\\nDC-NAS M (\\textit{ours})                                     & Integrated & 13.7 & \\textbf{76.0} & \\textbf{25.4}          & \\textbf{0.70} \\\\ \\midrule\nEfficientNetV2-b1 \\cite{EfficientNetV2}            & Classifier & 11.8 & \\textbf{76.7} & -             & -    \\\\\nC-NAS L (\\textit{ours})                                      & Classifier & 12.0 &   76.0   & -             & -    \\\\\nUNet-M* \\cite{UNet}  & Denoiser   & 16.9 & -    & \\textbf{25.9 }         & \\textbf{0.72} \\\\\nLPIENet 0.5x \\cite{LPIENet}                         & Denoiser   & 19.8 & -    &       24.7        &   0.68   \\\\\nDC-NAS L (\\textit{ours}) & Integrated & 17.9 & 76.4 & 25.2 & 0.70 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:main-results}\n\\end{table}",
            "tab:ablation1": "\\begin{table}\n\\caption{Different search strategies for DC-NAS, using (i) clean or noisy images and (ii) $\\mathcal{L}_\\text{Cls}$ or $\\mathcal{L}_\\text{Cls} + \\mathcal{L}_\\text{Den}$. Searching on \\textbf{Noisy} images with only $\\mathcal{L}_\\text{Cls}$ performs best.}\n\\centering\n\\resizebox{0.55\\columnwidth}{!}{%\n\\begin{tabular}{@{}llllll@{}}\n\\toprule\n\\multicolumn{2}{c}{Search} &\n  \\multicolumn{1}{c}{\\multirow{2}{*}{Lat. (ms)}} &\n  \\multicolumn{1}{c}{\\multirow{2}{*}{Acc. (\\%) \\(\\uparrow\\)}} &\n  \\multicolumn{1}{c}{\\multirow{2}{*}{SSIM \\(\\uparrow\\)}} &\n  \\multicolumn{1}{c}{\\multirow{2}{*}{PSNR \\(\\uparrow\\)}} \\\\ \\cmidrule(r){1-2}\nImages & Loss                      & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\midrule\nClean  & $\\mathcal{L}_\\text{Cls}$  & 13.9                 & 75.7                 & 25.4                 & 0.70                 \\\\\nNoisy  & $\\mathcal{L}_\\text{Cls}$  & \\textbf{13.7}        & \\textbf{76.0}        & 25.4                 & 0.70                 \\\\\nNoisy  & $\\mathcal{L}_\\text{Both}$ & 13.8                 & 75.5                 & 25.4                 & 0.70                 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:ablation1}\n\\end{table}",
            "tab:ablation2": "\\begin{table}\n\\caption{Comparing Sequential and Integrated DC-NAS, classification performance is similar, yet the Integrated model is faster and denoises better.}\n\\centering\n\\resizebox{0.55\\columnwidth}{!}{%\n\\begin{tabular}{@{}lcccc@{}}\n\\toprule\nModel                 & Lat. (ms) & Acc. (\\%) \\(\\uparrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) \\\\ \\midrule\nDC-NAS$_\\text{seq}$ L & 18.3      & 76.0                   & 25.0              & 0.69              \\\\\nDC-NAS L              & \\textbf{17.9}      & 76.0                   & \\textbf{25.5}              & \\textbf{0.70}              \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:ablation2}\n\\end{table}",
            "tab:ablation3": "\\begin{table}[]\n\\caption{The influence of altering the Conv operators in the DC-NAS M decoder to MBConv and scaling down the MBConv alternative by reducing the number of operators or decoder layers. Using the standard convolutional layers is more efficient.}\n\\centering\n\\resizebox{0.6\\columnwidth}{!}{%\n\\begin{tabular}{@{}lccccc@{}}\n\\toprule\n\\multicolumn{2}{c}{Decoder} &\n  \\multirow{2}{*}{Lat. (ms)} &\n  \\multirow{2}{*}{Acc. (\\%) \\(\\uparrow\\)} &\n  \\multirow{2}{*}{PSNR \\(\\uparrow\\)} &\n  \\multirow{2}{*}{SSIM \\(\\uparrow\\)} \\\\ \\cmidrule(r){1-2}\nOperator & Scaling    &               &               &      &      \\\\ \\midrule\nConv     & -          & \\textbf{13.7}          & 76            & 25.4 & 0.70 \\\\\nMBConv   & -          & 27.7 & 75.5 & 25.8 & 0.71 \\\\\nMBConv   & 1 operator & 16.4          & 75.4          & 25.3 & 0.70 \\\\\nMBConv   & 3 layers   & 22.1          & 75.1          & 25.4 & 0.70 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:ablation3}\n\\end{table}",
            "tab:classification-appendix": "\\begin{table}\n\\caption{Classification baseline and reduced models, designed for three different FLOP targets: \\{S, M, L\\}, to compare scaling methods: expansion rate and model width. Each section of rows is used by the experiments from Sec. \\ref{sec:toy-experiments-classification} defined in the \\textit{Exp.} column. MB models scale down more efficiently than normal Conv models.}\n\\centering\n\\resizebox{0.7\\columnwidth}{!}{%\n\\begin{tabular}{@{}clccccc@{}}\n\\toprule\nExp. & Model & Size & Exp. rate & FLOPs (K) \\(\\downarrow\\) & Lat. (ms) \\(\\downarrow\\) & Acc (\\%) \\(\\uparrow\\)\\\\ \\midrule\n\\multirow{4}{*}{1-3} & Conv-L & L & - & 447 & 0.336 & 63.2 \\\\\n & MB1-S & S & 1 & 177 & 0.300 & 56.2 \\\\\n & MB2.5-M & M & 2.5 & 350 & 0.384 & 64.1 \\\\\n & MB4-L & L & 4 & 424 & 0.468 & 64.9 \\\\ \\midrule\n\\multirow{2}{*}{2-3} & MB2.5-S & S & 2.5 & 178 & 0.390 & 58.4 \\\\\n & MB4-S & S & 4 & 188 & 0.403 & 56.6 \\\\\n % & MB4-M & M & 4 & 358 & 0.436 & 63.3 \\\\ \n \\midrule\n\\multirow{2}{*}{3} & Conv-S & S & - & 163 & 0.281 & 55.4 \\\\\n & Conv-M & M & - & 345 & 0.317 & 61.4 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:classification-appendix}\n\\end{table}",
            "tab:denoising": "\\begin{table}[]\n\\caption{Compares Baseline and Reduced UNet denoisers. The reduced model has significantly lower FLOPs and latency yet similar denoising performance.}\n\\centering\n\\resizebox{0.7\\columnwidth}{!}{%\n\\begin{tabular}{@{}lccccccc@{}}\n\\toprule\n\\multirow{2}{*}{Model} & \\multirow{2}{*}{FLOPs (M) \\(\\downarrow\\)} & \\multirow{2}{*}{Lat. (ms) \\(\\downarrow\\)} & \\multirow{2}{*}{Metric} & \\multicolumn{4}{c}{Noise level   (\\(\\sigma\\))} \\\\\n &  &  &  & 0.2 & 0.4 & 0.8 & 1 \\\\ \\midrule\n\\multirow{2}{*}{UNet} & \\multirow{2}{*}{1301.8} & \\multirow{2}{*}{7.10} & PSNR \\(\\uparrow\\) & 33.9 & 29.5 & 23.8 & 22.3 \\\\\n &  &  & SSIM \\(\\uparrow\\) & 0.99 & 0.98 & 0.95 & 0.92 \\\\ \n\\cmidrule(r){0-3}\n\\multirow{2}{*}{UNet-S} & \\multirow{2}{*}{51.2} & \\multirow{2}{*}{2.38} & PSNR \\(\\uparrow\\) & 33.2 & 28.7 & 23.3 & 22.0 \\\\\n &  &  & SSIM \\(\\uparrow\\) & 0.99 & 0.98 & 0.94 & 0.92 \\\\ \n\\bottomrule\n\\end{tabular}%\n}\n\\label{tab:denoising}\n\\end{table}",
            "tab:searched-operators": "\\begin{table}\n\\caption{Overview of the candidate blocks for the different search space sizes \\{4, 6, 8\\}. MBConv operators are used with different kernel sizes \\textit{k}, expansion rate \\textit{e}, and in- or excluding the squeeze- and excitation-layer.}\n\\centering\n\\resizebox{0.65\\columnwidth}{!}{%\n\\begin{tabular}{@{}lcccccc@{}}\n\\toprule\nName & Kernel & Expansion rate & SE-layer & 4 & 6 & 8 \\\\ \\midrule\nMB-k3-e3 & 3 & 3 & - & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\\nMB-k3-e6 & 3 & 6 & - & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\\nMB-k5-e3 & 5 & 3 & - & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\\nMB-k5-e6 & 5 & 6 & - & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\\nMB-k3-e3-se & 3 & 3 & \\(\\checkmark\\) & - & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\\nMB-k3-e6-se & 3 & 6 & \\(\\checkmark\\) & - & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\\nMB-k5-e3-se & 5 & 3 & \\(\\checkmark\\) & - & - & \\(\\checkmark\\) \\\\\nMB-k5-e6-se & 5 & 6 & \\(\\checkmark\\) & - & - & \\(\\checkmark\\) \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:searched-operators}\n\\end{table}",
            "tab:NAS-beta": "\\begin{table}[h!]\n\\caption{Compares four searched models with target latency 6 ms. Trained on clean images. Two models are searched without \\(\\beta\\) and the other two using learned \\(\\beta\\). Removed outperforms learned \\(\\beta\\).}\n\\label{tab:NAS-beta}\n\\centering\n\\resizebox{0.5\\linewidth}{!}{\n\\begin{tabular}{@{}cccc@{}}\n\\toprule\nType & Search id & LAT (ms) \\(\\downarrow\\)  & Acc (\\%) \\(\\uparrow\\) \\\\ \\midrule\n\\multirow{2}{*}{Removed \\(\\beta\\)} & 1 & 5.85 & \\textbf{86.2} \\\\\n & 2 & 5.81 & \\textbf{86.4} \\\\ \\cmidrule(r){1-1}\n\\multirow{2}{*}{Learned \\(\\beta\\)} & 1 & 5.04 & 84.9 \\\\\n & 2 & 4.47 & 83.5 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\end{table}"
        },
        "figures": {
            "fig:main": "\\begin{figure}[t]\n  \\centering\n  \\begin{minipage}{0.16\\columnwidth}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/figure-12/noisy_human.png}\n    \\subcaption{}\n    \\label{fig:subfigA}\n  \\end{minipage}%\n  \\begin{minipage}{0.16\\columnwidth}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/figure-12/clean_animal.png}\n    \\subcaption{}\n    \\label{fig:subfigB}\n  \\end{minipage}%\n  \\begin{minipage}{0.16\\columnwidth}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/figure-12/clean_human.png}\n    \\subcaption{}\n    \\label{fig:subfigC}\n  \\end{minipage}%\n  \\begin{minipage}{0.16\\columnwidth}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/figure-12/denoised_human.png}\n    \\subcaption{Cls: Human}\n    \\label{fig:subfigD}\n  \\end{minipage}\n  \\caption{We take a noisy image (a), which can be interpreted as an animal (b) or human (c). We denoise and classify the image (a), aiming to improve human perception resulting in (d). Note, in a real application (b) and (c) would not be available, which increases the difficulty of interpreting the noisy image. {\\scriptsize Artist: DALL-E-2 \\cite{DallE2} }}\n  \\label{fig:main}\n\\end{figure}",
            "fig:integrated": "\\begin{figure}[t]\n     \\centering\n     \\centering    \\includegraphics[width=0.7\\columnwidth]{images/integrated-toy-setup/U-Net-joint-improved-no-shadow.pdf}\n    \\caption{A UNet, with hyperparameters base feature map width (\\textit{b}), depth (\\textit{d}), channel multiplier (\\textit{m}) and convolutions per layer (\\textit{c}). For the joint model either attach the classifier to form (i) the Sequential model or (ii) the Integrated model.}\n    \\label{fig:integrated}\n\\end{figure}",
            "fig:joint": "\\begin{figure}[t]\n  \\centering\n  \\begin{subfigure}{0.13\\columnwidth}\n    \\includegraphics[width=\\linewidth]{images/toy-setup-joint/gt.png}\n    \\caption{GT}\n    \\label{fig:sub1}\n  \\end{subfigure}\n  \\begin{subfigure}{0.13\\columnwidth}\n    \\includegraphics[width=\\linewidth]{images/toy-setup-joint/noise-0.8.png}\n    \\caption{\\(\\sigma\\)=0.8}\n    \\label{fig:sub3}\n  \\end{subfigure}\n  \\begin{subfigure}{0.13\\columnwidth}\n    \\includegraphics[width=\\linewidth]{images/toy-setup-joint/6.12-B_std0.8.png}\n    \\caption{\\textbf{Int. S}}\n    \\label{fig:sub8}\n  \\end{subfigure}\n  \\begin{subfigure}{0.13\\columnwidth}\n    \\includegraphics[width=\\linewidth]{images/toy-setup-joint/5.10-B_std0.8.png}\n    \\caption{Seq. S}\n    \\label{fig:sub9}\n  \\end{subfigure}\n  \\begin{subfigure}{0.13\\columnwidth}\n    \\includegraphics[width=\\linewidth]{images/toy-setup-joint/6.12-C_std0.8.png}\n    \\caption{\\textbf{Int. L}}\n    \\label{fig:sub10}\n  \\end{subfigure}\n  \\begin{subfigure}{0.13\\columnwidth}\n    \\includegraphics[width=\\linewidth]{images/toy-setup-joint/5.10-C_std0.8.png}\n    \\caption{Seq. L}\n    \\label{fig:sub11}\n  \\end{subfigure}\n% \\end{flushright}\n  \n  \\caption{Ground-truth sample (a), which is the target for the denoiser when given noisy image (b). S stands for the reduced model and L for the baseline. (c-f) are the cropped denoised outputs for input (b) and the red squares indicate the zoomed-in regions. For higher noise levels, the denoising performance of the Sequential model is worse than the Integrated model.\n  % (c-f) are the denoised outputs for input (b), while (h-k) \n  }\n  \\label{fig:joint}\n\\end{figure}",
            "fig:DC-NAS": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.55\\columnwidth]{images/DC-NAS.pdf}\n    \\caption{C-NAS and D-NAS architecture. Connecting block (iii) for DC-NAS and block (iv) for DC-NAS$_\\text{seq}$. During search models with various latencies can be obtained. Only the orange stages are searchable in the encoder and classifier. }\n    \\label{fig:DC-NAS}\n\\end{figure}",
            "fig:visual-comparison": "\\begin{figure*}[h!]\n    \\centering\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample5_noise0.1.png}\n        \\caption{$\\sigma$=0.1}\n    \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample5_noise0.1_modelunet_d4_wf3.png}\n        \\caption{DC-Net}\n    \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample5_noise0.1_modelunet_d4_wf3denoiser.png}\n        \\caption{UNet-S}\n    \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample5_noise0.1_modelown_nas.png}\n        \\caption{DC-NAS}\n    \\end{subfigure}\\hfill\n    % \\begin{subfigure}{\\denoisedwidth\\textwidth}\n    %     \\includegraphics[width=\\textwidth]{images/final-visuals/sample5_noise0.1_modelunet_d4_wf4denoiser.png}\n    %     \\caption{UNet-M}\n    % \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/gt5.png}\n        \\caption{Ground Truth}\n    \\end{subfigure}\n\n    \\vspace{0.2cm}\n    \\centering\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample7_noise0.2.png}\n        \\caption{$\\sigma$=0.2}\n    \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample7_noise0.2_modelunet_d4_wf3.png}\n        \\caption{DC-Net}\n    \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample7_noise0.2_modelunet_d4_wf3denoiser.png}\n        \\caption{UNet-S}\n    \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/sample7_noise0.2_modelown_nas.png}\n        \\caption{DC-NAS}\n    \\end{subfigure}\\hfill\n    % \\begin{subfigure}{\\denoisedwidth\\textwidth}\n    %     \\includegraphics[width=\\textwidth]{images/final-visuals/sample7_noise0.2_modelunet_d4_wf4denoiser.png}\n    %     \\caption{UNet-M}\n    % \\end{subfigure}\\hfill\n    \\begin{subfigure}{\\denoisedwidth\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/final-visuals/gt7.png}\n        \\caption{Ground Truth}\n    \\end{subfigure}\n\n    \\caption{Denoising performance of DC-NAS S and its baselines. Left to right: noisy image, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c), we see better performance in smooth areas and more correct colors in (d). With (i) and (g), we observe a better color reconstruction for (i). Moreover, (i) has less artifacts than (h). Hence, DC-NAS S denoises better than the other denoisers of similar latency.}\n    \\label{fig:visual-comparison}\n    \n\\end{figure*}",
            "fig:denoising": "\\begin{figure}[t]\n     \\centering\n     \\includegraphics[width=0.8\\columnwidth]{images/toy-setup-denoising/u-net-scaling.pdf}\n    \\caption{UNet hyperparameter (Figure \\ref{fig:integrated}) scaling experiments. Shows how altering a specific hyper-parameter influences denoising performance and FLOPs. We only show PSNR results of \\(\\sigma\\)=0.8, as the other results show the same trend. We find that \\textit{b} and \\textit{m} scale down efficiently, \\textit{d} and \\textit{c} do not.}\n     \\label{fig:denoising}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\mathcal{L} = 0.1 \\cdot \\mathcal{L}_\\text{CE} + 0.9 \\cdot \\mathcal{L}_\\text{Char}\n    \\label{eq:DC-Net}\n\\end{equation}",
            "eq:2": "\\begin{align}\n    \\mathcal{L}_\\text{Den} = 0.8 \\cdot \\mathcal{L}_\\text{Char} + 0.2 \\cdot \\mathcal{L}_\\text{SSIM} \\label{eq:DC-NAS_Den}\\\\\n    \\mathcal{L}_\\text{Both} = 0.1 \\cdot \\mathcal{L}_\\text{CE} + 0.9 \\cdot \\mathcal{L}_\\text{Den}\\label{eq:DC-NAS_Both}\n\\end{align}"
        }
    }
}