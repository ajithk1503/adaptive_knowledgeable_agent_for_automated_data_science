{
    "meta_info": {
        "title": "On Structural Explanation of Bias in Graph Neural Networks",
        "abstract": "Graph Neural Networks (GNNs) have shown satisfying performance in various\ngraph analytical problems. Hence, they have become the \\emph{de facto} solution\nin a variety of decision-making scenarios. However, GNNs could yield biased\nresults against certain demographic subgroups. Some recent works have\nempirically shown that the biased structure of the input network is a\nsignificant source of bias for GNNs. Nevertheless, no studies have\nsystematically scrutinized which part of the input network structure leads to\nbiased predictions for any given node. The low transparency on how the\nstructure of the input network influences the bias in GNN outcome largely\nlimits the safe adoption of GNNs in various decision-critical scenarios. In\nthis paper, we study a novel research problem of structural explanation of bias\nin GNNs. Specifically, we propose a novel post-hoc explanation framework to\nidentify two edge sets that can maximally account for the exhibited bias and\nmaximally contribute to the fairness level of the GNN prediction for any given\nnode, respectively. Such explanations not only provide a comprehensive\nunderstanding of bias/fairness of GNN predictions but also have practical\nsignificance in building an effective yet fair GNN model. Extensive experiments\non real-world datasets validate the effectiveness of the proposed framework\ntowards delivering effective structural explanations for the bias of GNNs.\nOpen-source code can be found at https://github.com/yushundong/REFEREE.",
        "author": "Yushun Dong, Song Wang, Yu Wang, Tyler Derr, Jundong Li",
        "link": "http://arxiv.org/abs/2206.12104v1",
        "category": [
            "cs.LG",
            "cs.CY"
        ],
        "additionl_info": "Published as a conference paper at SIGKDD 2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{intro}\n\n\n\n% 第一段： GNN好用但是不fair\n\n\nGraph Neural Networks (GNNs) have shown satisfying performance in various real-world applications, e.g., online recommendation~\\cite{wu2019session}, chemical reaction prediction~\\cite{do2019graph}, and complex physics simulation~\\cite{sanchez2020learning}, to name a few.\n%\nThe success of GNNs is generally attributed to their message-passing mechanism~\\cite{wu2020comprehensive,zhou2020graph,wang2021tree}. Such a mechanism enables GNNs to capture the correlation between any node and its neighbors in a localized subgraph (i.e., the computation graph of the node~\\cite{ying2019gnnexplainer}), which helps to extract information from both node attributes and network structure for node embedding learning~\\cite{hamilton2017inductive}.\n%\nDespite the remarkable success, most of the existing GNNs do not have fairness consideration~\\cite{dong2021individual,dong2021edits,dai2021say,kang2022rawlsgcn,dong2022fairness,DBLP:conf/wsdm/MaGWYZL22}. \n%\nConsequently, GNN predictions could exhibit discrimination (i.e., bias) towards specific demographic subgroups that are described by sensitive features, e.g., age, gender, and race.\n%\nSuch discrimination has become one of the most critical societal concerns when GNNs are deployed in high-stake decision-making scenarios~\\cite{kang2021fair}.\n\n\n\nThere is a rich body of literature on alleviating the bias of GNNs. Generally, these works aim to decouple the learned node embeddings from sensitive features~\\cite{dai2021say,dong2021edits,DBLP:conf/iclr/LiWZHL21,spinelli2021biased,fairview}.\n%\nHowever, they cannot provide explanations on how bias arises in GNNs.\n%\nIn fact, it is worth noting that in various high-stake decision-making scenarios, we not only need to alleviate bias in GNNs, but also need to understand how bias is introduced to the prediction of each individual data instance (e.g., a node in a graph).\n%\nSuch instance-level understanding is critical for the safe deployment of GNNs in decision-critical applications~\\cite{ying2019gnnexplainer}.\n%\nFor example, GNNs have demonstrated superior performance in many financial applications, such as loan approval prediction for bank clients~\\cite{wang2019semi,xu2021towards}. In this scenario, different clients form a graph based on their transaction interactions, and the records of clients form their features. Here, the goal is to predict whether a client will be approved for a loan, and such a problem can be formulated as a node classification task that can be solved by GNNs. However, GNNs could lead to undesired discrimination against clients from certain demographic subgroups (e.g., rejecting a loan request mainly because the applicant belongs to an underprivileged group). In this example, understanding how bias is introduced to the prediction of each individual client enables bank managers to scrutinize each specific loan decision and take proactive actions to improve the algorithm and reduce potential discrimination.\n% \\yu{One potential follow up is since we use an example here to better justify the necessity of structural bias explanation, maybe providing a detailed benefit as how the bank managers could leverage the explanation to scrutinize and take proactive actions.}  \n%\n% For example, it is crucial for financial institutions to predict the credit risk of clients~\\cite{wang2021temporal}. Towards this goal, the clients can be modeled as a graph, and the similarity of their credit accounts determines whether two clients are connected or not~\\cite{agarwal2021towards}. Based on the graph of clients, GNNs have been adopted as an effective tool for credit risk prediction~\\cite{agarwal2021towards,wang2021temporal}. However, GNNs could also yield discriminatory credit risk predictions based on the sensitive attribute of clients, e.g., race and gender. Understanding how bias is introduced to the instance-level GNN predictions helps to debug GNNs in perspective of fairness and reduce unexpected discrimination in the future.\n%\n% in a web traffic network for anomaly detection, if the administrator has a clear understanding on whether the connections between servers and any client lead to biased decisions towards this specific client, then bias can be exempted in time and final detection results can be safely trusted as fair ones.  fraudulent entities (i.e., manipulated reviews and fake buyers)\n%\n% Nevertheless, to the best of our knowledge, no existing work explains how biased network structure leads to biased GNN predictions.\n\n\n% because: (1) structure is a significant bias source; (2) instance-level understanding of how bias is introduced is critical;\n\n% so: we aim to provide instance-level structural explanation of the bias in GNNs\n\n\n\n\n% Despite the significance of understanding how bias is introduced to the GNN prediction for each instance, it remains a daunting task. A preliminary reason is that bias exhibited in the instance-level GNN predictions can be introduced from multiple sources. \n\nIn fact, biased GNN predictions can be attributed to a variety of factors. Among them, biased network structure has shown to be a critical source~\\cite{DBLP:conf/iclr/LiWZHL21,dong2021edits,spinelli2021biased}. Additionally, bias in the network structure could be amplified by the core operation of GNNs -- the \\emph{message-passing} mechanism~\\cite{dong2021edits}.\n%\nTherefore, understanding which part of the network structure leads to biased GNN predictions for each node is vitally important.\n% Towards this goal\nTowards this goal, we aim to provide an instance-level (i.e., node-level) structural explanation of bias in GNN predictions.\n%\n% \\yu{maybe provide some reasons on why focusing specifically on the instance-level structural explanation of bias could help us achieve this goal instead of group-level. But since we already mention the instance-level understanding is critical from previous paragraph, so probably it is not critical.}\n%\nMore specifically, for any node in an input network for GNNs, we aim to understand and explain how different edges in its computation graph contribute to the level of bias for its prediction\\footnote{Here, we only consider the edges in its corresponding computation graph. This is because the computation graph of a node fully encodes all information that GNN models leverage to generate its prediction~\\cite{ying2019gnnexplainer}.\n% \\yu{probably also justify why we don't care about contributions of other nodes to the bias level could be another way to explain why we care about edges.}\n}.\n%\nNevertheless, it remains a daunting task. Essentially, we mainly face the following three challenges:\n%\n(1) \\textbf{Fairness Notion Gap}: \\emph{how to measure the level of bias for the GNN prediction at the instance level}? For each node, understanding how the edges in its computation graph make its prediction biased requires a principled bias metric at the instance level. However, most of the existing bias metrics are defined over the whole population or the sub-population~\\cite{dwork2012fairness,DBLP:conf/nips/HardtPNS16}, thus they cannot be directly grafted to our studied problem. In this regard, it is crucial to design a bias metric that can quantify the level of bias for the GNN prediction at the instance level.\n%\n(2) \\textbf{Usability Gap}: \\emph{is a single bias explainer sufficient}? \n% An explainer model can be built to explain where the exhibited bias comes from for a given node prediction.\n%\nIt should be noted that our ultimate goal goes beyond explaining bias as we also aim to achieve fairer GNNs, which provide better model usability and enable ethical decision-making. Consequently, it is also critical to explain which edges in a node's computational graph contribute more to the fairness level of its prediction. However, edges that introduce the least bias cannot be simply regarded as the edges that maximally contribute to the fairness level of the prediction. This is because edges that introduce the least bias could also be those prediction-irrelevant edges --- such edges could barely contribute any information to the GNN prediction. Therefore, only explaining how each edge in a computational graph contributes to the exhibited node-level bias is not sufficient. \n%Explaining how each edge contributes to the fairness level of the GNN prediction is our second challenge. \n%\n(3) \\textbf{Faithfulness Gap}: \\emph{how to obtain bias (fairness) explanations that are faithful to the GNN prediction}? \nTo ensure the obtained explanations reflect the true reasoning results based on the given GNN model, most existing works on the instance-level GNN explanation obtain explanations that encode as much critical information as possible for a given GNN prediction~\\cite{ying2019gnnexplainer,vu2020pgm,luo2020parameterized}. In this way, the obtained explanations are considered to be faithful to the given GNN model, as they generally reflect the critical information the GNN utilized to make the given prediction. \n% Similarly, when explaining how each edge in a computational graph contributes to the bias or the fairness level of the GNN prediction, we should guarantee that the explanations are also faithful to the given GNN prediction. \n%\nSimilarly, when explaining how the bias or the fairness level of the GNN prediction is achieved, we are also supposed to identify the critical information the GNN utilized to achieve such a level of bias or fairness for the given prediction.\n%\n%As such, how to maintain faithfulness for the structural explanation of bias or fairness is our third challenge.\n% \\yu{not sure why we want our identified edges to be also critical to the downstream predictions since maybe the classification and the sensitive information is not entangled so much. If edges are very responsible for classification but not encode so much sensitive information, then include these edges in the explainability simply because they are critical for classification might not be consistent with our goal here.}\n%\n% (2) \\emph{Extra Faithfulness Requirement: how to build an explainer that is faithful to both the GNN prediction and the exhibited bias at the instance level}?\n%\n% Most existing works on instance-level GNN explanation aim to obtain explanations that encode as much critical information as possible for a given GNN prediction~\\cite{ying2019gnnexplainer,vu2020pgm,luo2020parameterized}. In this way, the obtained explanations are considered to be faithful to the given GNN prediction. However, in our studied problem, in order to explain the exhibited bias at the instance level, it is also necessary to make the explanation to be faithful to how the bias is introduced. In other words, the obtained explanations should also encode as much information as possible to account for the bias exhibited in the instance-level prediction. Building an explainer to achieve both types of faithfulness is our second challenge.\n% %\n% (3) \\emph{Usability Gap: is a single bias explainer sufficient}? \n% It should be noted that our ultimate goal goes beyond explaining bias as we also aim to improve the usability of GNNs, i.e., making the GNN prediction both fair and accurate. Intuitively, with the bias explainer, a more fair prediction can be achieved by excluding the edges (in the computational graph) that contribute most to bias level of the prediction. However, simply excluding these edges could jeopardize the utility (e.g., prediction accuracy) of the GNN model as these edges could also embed critical information for the GNN prediction. In this regard, building a single bias explainer is not sufficient. Identifying the edges that can lead to both fair and accurate instance-level GNN prediction is our third challenge. \n\n\n\n%the ultimate goal of explaining the exhibited bias at the instance-level GNN prediction is to improve the usability of GNNs in decision-critical applications, i.e., building a GNN model that is both fair and effective in terms of predictions. \n% However, simply excluding the edges that contribute most to bias of an instance-level GNN prediction (based on the bias explainer) could jeopardize the utility of the GNN model, as these edges could also embed critical information for the GNN prediction. In this regard, only building a bias explainer is not sufficient. Therefore, for each node, how to identify the edges in its computational graph that can \n\n\n% . How to identify those edges that can lead to both fair and accurate instance-level GNN prediction is our third challenge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n% Given a computation graph, the bias contribution of an edge to the prediction of the central node\\footnote{The central node here refers to the node at the center of the given computation graph.} could be different for different GNNs. This is because during message-passing, how much the prediction for the central node is influenced by the information delivered by this edge could be different for different GNNs. Therefore, it is necessary to design an approach that is tailored for the given GNN model to identify edge contribution to the bias level of the prediction for any given node.\n\n\n\n\n\n\n\n\n% Considering the significance of understanding how biased input network structure influences the prediction of each instance, we aim to provide instance-level (i.e., node-level) structural explanation of the bias in GNNs.\n% %\n% In particular, we focus on analyzing how edges contribute to the bias\\footnote{Bias also refers to unfairness.} (fairness) level of the prediction for an instance in its computation graph.\n% %\n% The reason is that for any node, the message-passing is only performed through the edges in the computation graph.\n% %\n% Specifically, our first goal is to understand how biased edges lead to biased GNN prediction. At the same time, it is also critical to obtain more fair predictions in real-world applications. Therefore, our second goal is to identify those edges that make the GNN prediction more fair.\n% %\n% Nevertheless, developing a framework to identify the edge contribution to the bias (fairness) for the outcome of a given node is non-trivial. Essentially, we mainly face the following three challenges:\n% %\n% (1) \\emph{Fairness Notion Gap}. For each node, understanding if the edges in its computation graph will make its prediction more biased (more fair) requires a principled fairness notion at the instance level. However, most of the existing fairness notions are defined over the whole population or the sub-population, and thus they cannot be directly adopted. In this regard, it is important to design a fairness notion that can quantify the bias (fairness) at the instance-level for GNN prediction.\n% %\n% (2) \\emph{Edge Contribution Analysis}. \n% It is also challenging to identify the actual contribution of edges to the bias (fairness) level of the GNN prediction. For example, some highly biased edges could be non-critical for the given GNN. Such edges cannot be regarded as being with high bias contribution to the GNN prediction. On the contrary, some less biased edges could also be important for the given GNN. They potentially introduce more significant bias for the GNN prediction.\n% %\n% In this regard, how to identify the actual contribution of edges for the GNN prediction is the second challenge.\n% %\n% (3) \\emph{Joint Edge Identification}. \n% %\n% For any edge, low bias contribution does not necessarily mean high fairness contribution to GNN predictions. The reason is that if an edge is with low bias contribution to the GNN prediction, this could also mean that this edge is relatively non-critical for the prediction. As a consequence, only identifying the edge contribution to bias cannot directly provide information on the edge contribution to the fairness level of the prediction.\n% %\n% In this regard, to identify edges that contribute the most to the bias and fairness level of the prediction, edge identification should be performed jointly.\n% %\n% Correspondingly, we consider how to jointly identify the two types of edge contribution as the third challenge.\n\n\n\nAs an attempt to tackle the challenges above, in this paper, we propose a principled framework named REFEREE (st\\underline{R}uctural \\underline{E}xplanation o\\underline{F} bias\\underline{E}s in g\\underline{R}aph n\\underline{E}ural n\\underline{E}tworks) for post-hoc explanation of bias in GNNs. \n%\nSpecifically, towards the goal of obtaining instance-level structural explanations of bias,\n%\nwe formulate a novel research problem of \\textit{Structural Explanation of Node-Level Bias in GNNs}.\n%\nTo tackle the first challenge, we propose a novel fairness notion together with the corresponding metric to measure the level of bias for a specific node in terms of GNN prediction.\n%\nTo tackle the second challenge, we design two explainers in the proposed framework REFEREE, namely bias explainer and fairness explainer. In any given computation graph, they are able to identify edges that maximally account for the exhibited bias in the prediction and edges that maximally contribute to the fairness level of the prediction, respectively.\n%\nTo tackle the third challenge, we design a constraint to enforce the faithfulness for the identified explanations, which can be incorporated into a unified objective function for the proposed framework. In this way, apart from the goal of explaining the exhibited bias and identifying edges that help with fairness, such a unified objective function also enforces the identified explanations to be faithful to the given GNN prediction.\n%\n%\nTo better differentiate these two types of edges, the two explainers are designed to work in a contrastive manner.\n%\n% Specifically, the bias explainer aims to identify a structural explanation that is faithful to the given (instance-level) GNN prediction and the exhibited bias, while the goal of the fairness explainer is to identify a structural explanation that leads to both fair and accurate (instance-level) prediction.\n%\n% the proposed framework includes a bias explainer and a fairness explainer. The bias explainer aims to identify a structural explanation that is faithful to the given (instance-level) GNN prediction and the exhibited bias, while the goal of the fairness explainer is to identify a structural explanation that leads to both fair and accurate (instance-level) prediction.\n% %\n% The unified objective function takes both goals into consideration.\n%\n% Besides, for any node, the edges (in its computation graph) accounting for the exhibited bias in the GNN prediction are most likely to be different from the edges (in its computation graph) accounting for fairness.\n%\n% Therefore, the two explainers are designed to be trained jointly in a contrastive manner, which helps to effectively minimize the overlap between the explanations from the two explainers.\n%\nFinally, we evaluate the effectiveness of REFEREE on multiple real-world network datasets.\n%\nThe main contributions of this paper are as follows.\n%\n(1) \\textbf{\\emph{Problem Formulation.}} We formulate and study a novel problem of structural explanation of biases in GNNs given any instance-level GNN prediction.\n%\n(2) \\textbf{\\emph{Metric and Algorithmic Design.}} We propose a novel metric to measure how biased the GNN outcome prediction of a node is. We then propose a novel explanation framework named REFEREE to provide explanations on both fairness and bias, and maintain faithfulness to the given prediction.\n%\n(3) \\textbf{\\emph{Experimental Evaluations.}} We perform experimental evaluations on various real-world networks. Extensive experiments demonstrate the effectiveness of REFEREE and its superiority over other alternatives.\n\n% \\begin{itemize}[topsep=0pt]\n%     \\item \\textbf{\\emph{Problem Formulation.}} We formulate and study a novel problem of structural explanation of biases in GNNs given any instance-level GNN prediction.\n%     % A novel metric is proposed to measure the node-level bias for the prediction given by the GNN model.\n%     \\item \\textbf{\\emph{Metric and Algorithmic Design.}} We propose a novel metric to measure how biased the GNN outcome prediction of a node is. We then propose a novel explanation framework named REFEREE to provide explanation on both fairness and bias, and maintain faithfulness to the given prediction.\n%     %\n%     % identify edges that account for the exhibited bias and that contribute to the fairness for any instance-level GNN prediction.\n%     \\item \\textbf{\\emph{Experimental Evaluations.}} We perform comprehensive experimental evaluations on various real-world networks. Extensive experiments demonstrate the effectiveness of REFEREE and its superiority over other alternatives.\n% \\end{itemize}\n\n\n\n% our main goal in this paper is to develop a principled framework to achieve structural explanation of biases in GNNs.\n% %\n% To dive into the node level to handle the first challenge, we propose a novel fairness notion to justify how bias the GNN prediction is for a specific node. %It should be noted that the proposed metric is consistent with traditional group fairness notions. \n% %\n% We then formulate a novel problem of \\textit{structural explanation of biases in Graph Neural Networks}.\n% %\n% To tackle the second challenge, we formulated a unified objective function for the identification of edges that contribute most to the bias and fairness in GNNs for any specific node.\n% %\n% For the last challenge, we propose a framework named REFEREE (st\\underline{R}uctural \\underline{E}xplanation o\\underline{F} bias\\underline{E}s in g\\underline{R}aph n\\underline{E}ural n\\underline{E}tworks) to identify the edge contribution on bias and fairness simultaneously in two dataflow branches.\n% %\n% It should be noted that REFEREE is agnostic for the GNN explanation model for post-hoc GNN explanation, which makes REFEREE a flexible plug-and-play framework.\n% %\n% Finally, we evaluate the effectiveness of our proposed framework on real-world network datasets.\n% %\n% The main contributions of this paper is summarized as follows.\n% \\begin{itemize}[topsep=0pt]\n%     \\item \\textbf{\\emph{Problem Formulation.}} We study a novel problem of structural explanation of biases in GNNs. A novel metric is proposed to determine how much a specific node contribute to the overall bias for the prediction of GNNs.\n%     \\item \\textbf{\\emph{Algorithmic Design.}} We propose a novel explanation framework named REFEREE that is agnostic to the GNN explanation backbone to identify edges that promote and reduce bias for a given node in the GNN input network.\n%     \\item \\textbf{\\emph{Experimental Evaluations.}} We perform comprehensive experimental evaluations on real-world networks. The effectiveness of REFEREE and its superiority over other alternatives is validated through experiments.\n% \\end{itemize}\n\n\n% As an attempt to solve the challenges above, our main goal in this paper is to develop a principled framework to generate structural explanation of biases in GNNs.\n% %\n% To dive into the node level to handle the first challenge, we propose a novel bias metric to justify how bias the GNN outcome is for a specific node . %It should be noted that the proposed metric is consistent with traditional group fairness notions. \n% %\n% We then formulate a novel problem of \\textit{structural explanation of biases in Graph Neural Networks}.\n% %\n% To tackle the second challenge towards better generalization ability, we propose a framework named REFEREE (st\\underline{R}uctural \\underline{E}xplanation o\\underline{F} bias\\underline{E}s in g\\underline{R}aph n\\underline{E}ural n\\underline{E}tworks) that is agnostic for the GNN explanation model for post-hoc GNN explanation. \n% %\n% For the last challenge, we design a training scheme based on contrastive learning to encourage the exploration of more fair and biased edges under the constraint of fidelity, which empirically helps to achieve better balance between fidelity and fairness for the learned explanations.\n% %\n% Finally, we evaluate the effectiveness of our proposed framework on real-world network datasets.\n% %\n% The main contributions of this paper is summarized as follows.\n% \\begin{itemize}[topsep=0pt]\n%     \\item \\textbf{\\emph{Problem Formulation.}} We study a novel problem of structural explanation of biases in GNNs. A novel metric is proposed to determine how much a specific node contribute to the overall bias for the prediction of GNNs.\n%     \\item \\textbf{\\emph{Algorithmic Design.}} We propose a novel explanation framework named REFEREE that is agnostic to the GNN explanation backbone to identify edges that promote and reduce bias for a given node in the GNN input network.\n%     \\item \\textbf{\\emph{Experimental Evaluations.}} We perform comprehensive experimental evaluations on real-world networks. The effectiveness of REFEREE and its superiority over other alternatives is validated through experiments.\n% \\end{itemize}\n\\vspace{-1.0em}\n"
            },
            "section 2": {
                "name": "Problem Definition",
                "content": "\n\n\\vspace{-0.3em}\n\n% \\begin{table}[!t]\n% % \\vspace{-0.2cm}\n% \\caption{Notations and descriptions in this paper.} \n% \\small\n% \\vspace{-0.2cm}\n% \\label{tb:symbols}\n% \\begin{tabular}{cc}\n% \\hline\n% \\textbf{Notations}       & \\textbf{Definitions or Descriptions} \\\\\n% \\hline\n% $\\mathcal{G}$   &  input graph\\\\\n% $\\mathcal{V}$, $\\mathcal{E}$, $\\mathcal{X}$   &  node, edge set, and attribute set \\\\\n% $\\mathcal{G}_i$   &  computation graph centered on the $i$-th node\\\\\n% $\\mathcal{\\tilde{G}}_i$   &  subgraph centered on the $i$-th node as explanation\\\\\n% % \\hline\n% % $\\mathbf{A}$            &    adjacency matrix of graph $\\mathcal{G}$      \\\\\n% % $\\mathbf{X}$            &    node feature matrix of graph $\\mathcal{G}$      \\\\\n% % $\\mathbf{Y}$           &  ground truth of downstream learning task \\\\\n% % $\\mathbf{\\hat{Y}}$           &  GNN outcome matrix of downstream learning task \\\\\n% % $\\mathbf{M}$            &    mask matrix given the computation graph     \\\\\n% % \\hline\n% $\\mathcal{\\hat{Y}}_i$           &  probabilistic GNN outcome set for all nodes \\\\\n% $\\mathbf{\\hat{y}}_i$           &  probabilistic GNN outcome of the $i$-th node \\\\\n% $\\hat{Y}_i$           &  predicted label of the $i$-th node \\\\\n% $N$    & number of nodes in the input graph  \\\\\n% $C$    & number of classes for node classification   \\\\\n% % $L$    & layer number of the studied GNN model  \\\\\n% \\hline\n% \\end{tabular}\n% \\vspace{-0.4cm}\n% \\end{table}\n\n\nIn this section, we first present the notations used in this paper and some preliminaries. We then formulate a novel problem of \\textit{Structural Explanation of Bias in GNNs}.\n\n\\noindent \\textbf{Notations.} We use bold uppercase letters (e.g., $\\mathbf{A}$), bold lowercase letters (e.g., $\\mathbf{x}$), and normal uppercase letters (e.g., $N$) to represent matrices, vectors, and scalars, respectively. \n%\nUppercase letters in math calligraphy font (e.g., $\\mathcal{V}$) represent sets.\n%\nThe $k$-th entry of a vector, e.g., $\\mathbf{x}$, is represented as $\\mathbf{x}[k]$. \n%\nFor any number, $|\\cdot|$ is the absolute value operator; for any set, $|\\cdot|$ outputs its cardinality.\n%\n\n\n\\noindent \\textbf{Preliminaries.} \nWe denote an attributed network as $\\mathcal{G} = \\{\\mathcal{V}, \\mathcal{E}, \\mathcal{X}\\}$, where $\\mathcal{V} = \\{v_1, ..., v_N\\}$ represents the set of $N$ nodes; $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ is the set of all edges; $\\mathcal{X} = \\{\\mathbf{x}_1, ..., \\mathbf{x}_N\\}$ is the set of node attribute vectors. \n%\nA trained GNN model $f_{\\bm{\\Theta}}$ maps each node to the outcome space, where $\\bm{\\Theta}$ denotes the parameters of the GNN model. Without loss of generality, we consider node classification as the downstream task. The GNN outcome for $N$ nodes can be given as $\\mathcal{\\hat{Y}} = \\{\\hat{\\mathbf{y}}_1, ..., \\hat{\\mathbf{y}}_i, ..., \\hat{\\mathbf{y}}_N\\}$, where $ \\hat{\\mathbf{y}}_i \\in \\mathbb{R}^{C}$. Here $C$ is the number of classes for node classification, and each dimension in $\\hat{\\mathbf{y}}_i$ represents the probability of the node belonging to the corresponding class.\n%\nBased on $\\mathcal{\\hat{Y}}$, the predicted label set by GNN for these $N$ nodes is denoted by $\\{\\hat{Y}_1, ..., \\hat{Y}_i, ..., \\hat{Y}_N\\}$. Here $\\hat{Y}_i$ is determined by the highest predicted probability across all $C$ classes given by $\\hat{\\mathbf{y}}_i$.\n%\nFor GNN explanation, we consider the most widely studied instance-level explanation problem in this paper, i.e., we aim to explain the given prediction of a node based on its computation graph~\\cite{ying2019gnnexplainer,vu2020pgm,luo2020parameterized}.\n% \\yu{does the GNN explanation here refers to explain the prediction of nodes based on its computational graph?}\n%\nAt the instance level, the explanations can be provided from different perspectives.\n%\nHere we focus on the structural explanation, i.e., the explanation is given as an edge set $\\mathcal{\\tilde{E}}_i$ by any GNN explanation model $h_{\\bm{\\Phi}}$.\n%\nSpecifically, given a specific node $v_i$, its computation graph $\\mathcal{G}_i = \\{\\mathcal{V}_i, \\mathcal{E}_i, \\mathcal{X}_i\\}$ (i.e., the $L$-hop subgraph centered on node $v_i$~\\cite{ying2019gnnexplainer}, where $L$ is the total layer number of the studied GNN), and the corresponding outcome $\\hat{\\textbf{y}}_i$, the GNN structural explanation model $h_{\\bm{\\Phi}}$ with parameter $\\bm{\\Phi}$ identifies an explanation as an edge set $\\mathcal{\\tilde{E}}_i$ corresponding to the outcome $\\hat{\\textbf{y}}_i$.\n% \\yu{Here does the found edge set by the explainer maximally influence the corresponding outcome? Maybe large reduction on the distance of predicting distributions may not necessarily requires large variation on the prediction itself.}\n%\n$\\mathcal{\\tilde{E}}_i$ is identified through learning a weighted mask matrix $\\mathbf{M} \\in \\mathbb{R}^{|\\mathcal{V}_i| \\times |\\mathcal{V}_i|}$ that indicates the importance score of each edge in $\\mathcal{E}_i$. \n%\nEdges in $\\mathcal{\\tilde{E}}_i$ are selected from $\\mathcal{E}_i$ based on such importance score.\n%\n% For the ease of reference, we utilize a weighted mask matrix $\\mathbf{M} \\in \\mathbb{R}^{|\\mathcal{V}_i| \\times |\\mathcal{V}_i|}$ to indicate the preserved edges in $\\mathcal{\\tilde{E}}_i$ compared with $\\mathcal{E}_i$.\n%\nWe denote the computation graph with the identified edge set $\\mathcal{\\tilde{E}}_i$ as a new subgraph $\\mathcal{\\tilde{G}}_i = \\{\\mathcal{V}_i, \\mathcal{\\tilde{E}}_i, \\mathcal{X}_i\\}$.\n%\nBased on the new subgraph $\\mathcal{\\tilde{G}}_i$ with the identified edge set $\\mathcal{\\tilde{E}}_i$, the given GNN yields a different probabilistic outcome $\\tilde{\\textbf{y}}_i = f_{\\bm{\\Theta}} (\\mathcal{\\tilde{G}}_i)$ compared with the vanilla outcome $\\hat{\\textbf{y}}_i$.\n%\n% The corresponding outcome of the GNN model based on $\\mathcal{\\tilde{G}}_i$ for node $v_i$ is denoted as $\\tilde{\\textbf{y}}_i = f_{\\bm{\\Theta}} (\\mathcal{\\tilde{G}}_i)$, where $ \\tilde{\\mathbf{y}}_i \\in \\mathbb{R}^{C}$. \\yu{One question is in obtaining $\\widetilde{y}_i$ to further optimize Eq.(1) or (3), do we forward the selected edges or forward the weighted edges? I guess here we first forward the model with the weighted edges and then after optimization, we select edges to form the explanation set by thresholding according to the importance score. But the description here looks more like we first select edges and based on the selected edges, we get their predictions.}\n% We present the notations and the corresponding descriptions in Table \\ref{tb:symbols}.\n\n\n\n% To tackle the first challenge \\emph{Fairness Notion Gap}, here we define \\textit{Node-level Bias in GNNs} as follows.\n\n% \\begin{myDef}\n% \\label{fair_def}\n% \\textbf{Node-level Bias in GNNs.} Given a GNN outcome set $\\mathcal{\\hat{Y}}$. \n% %\n% Divide $\\mathcal{\\hat{Y}}$ into $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as the outcome sets of the two demographic subgroups based on the sensitive feature.\n% %\n% Define the distance between the distribution of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as $D_{(0,1)}$.\n% %\n% Obtain $\\mathcal{\\hat{Y}}'$ and the corresponding $\\mathcal{\\hat{Y}}_0'$ and $\\mathcal{\\hat{Y}}_1'$ by replacing the $\\mathbf{\\hat{y}}_i$ in $\\mathcal{\\hat{Y}}$ with a variable $\\mathbf{\\hat{y}}'$ for node $v_i$.\n% %\n% Denote the minimized distance between the distribution of $\\mathcal{\\hat{Y}}_0'$ and $\\mathcal{\\hat{Y}}_1'$ w.r.t. $\\mathbf{\\hat{y}}'$ as $D_{min(0,1)}$.\n% %\n% We define $B_{i} = D_{(0,1)} - D_{min(0,1)}$ as the node-level bias in GNNs for node $v_i$.\n% \\end{myDef}\n\n% \\begin{myDef}\n% \\label{fair_def}\n% \\textbf{Node-level Predictive Bias in GNNs.} Given a trained GNN $f_{\\bm{\\Theta}}$, an input network $\\mathcal{G}$, and the corresponding outcome set $\\mathcal{\\hat{Y}}$. Assume $\\mathcal{\\hat{Y}}_0 \\subseteq \\mathcal{\\hat{Y}}$ and $\\mathcal{\\hat{Y}}_1 \\subseteq \\mathcal{\\hat{Y}}$ ($\\mathcal{\\hat{Y}}_0 \\cup \\mathcal{\\hat{Y}}_1 = \\mathcal{\\hat{Y}}$) are the outcome sets of the two demographic subgroups based on the sensitive feature.\n% %\n% Denote the Wasserstein distance between the distribution of $\\mathcal{\\hat{Y}}_0$ (denoted as $P_{\\mathcal{\\hat{Y}}_0}$) and $\\mathcal{\\hat{Y}}_1$ (denoted as $P_{\\mathcal{\\hat{Y}}_1}$) as W($P_{\\mathcal{\\hat{Y}}_0}, P_{\\mathcal{\\hat{Y}}_1}$).\n% %\n% For node $v_i$, we define $B_{i} = W(P_{\\mathcal{\\hat{Y}}_0}, P_{\\mathcal{\\hat{Y}}_1}) - \\min_{\\mathbf{\\hat{y}}_i} W(P_{\\mathcal{\\hat{Y}}_0}, P_{\\mathcal{\\hat{Y}}_1})$ as the node-level predictive bias in GNNs.\n% %\n% % If $\\mathbf{\\hat{y}}_i$ minimizes the Wasserstein Distance between $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$, we say that node $v_i$ has no node-level contribution on bias.\n% \\end{myDef}\n\n\n% \\begin{myDef}\n% \\textbf{Node-level Group Fairness.} Given an $L$-layered GNN model $f_{\\bm{\\Theta}}$ and the corresponding computation graph $\\mathcal{G}_i$ centered on node $v_i$. Generate $K$ perturbations of $\\mathcal{G}_i$ by making small changes to the attribute of $v_i$ and/or rewire edges between $v_i$ and other nodes in $\\mathcal{G}_i$ with small probability as $\\mathcal{P} = \\{ \\mathcal{\\bar{G}}_i^{(1)}, ..., \\mathcal{\\bar{G}}_i^{(K)}\\}$. Obtain the outcome set corresponding to node $v_i$ as $\\mathcal{\\bar{Y}}_i = \\{ \\bar{y}_i^{(1)}, ..., \\bar{y}_i^{(K)}\\}$, where $\\bar{y}_i^{(k)} = f_{\\bm{\\Theta}}(\\mathcal{\\bar{G}}_i^{(k)})$, $1 \\leq k \\leq K$.\n% %\n% For $\\mathcal{\\bar{Y}}_i$, if $\\text{P}(\\bar{y}_i=1|s_i=0) = \\text{P}(\\bar{y}_i=1|s_i=1)$ ($s_i$ is the sensitive feature of node $v_i$), we say that the outcome of node $v_i$ satisfies \\textit{node-level group fairness}.\n% \\end{myDef}\n\n\n% agnostic   !   !  \n% Definition 1 introduces how to measure the bias exhibited in the node-level prediction given a trained GNN model.\n% %\n% Clearly, the minimum value of $B_{i}$ is 0, i.e., if the value of $\\mathbf{\\hat{y}}_i$ minimizes the distance between the distribution of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$, we say that node $v_i$ has no node-level bias in the GNN outcome.\n% %\n% In this paper, we adopt the Wasserstein distance as the metric for distribution distance measurement considering its superior sensitivity over other distance metrics~\\cite{DBLP:journals/corr/ArjovskyCB17}. \n% %\n% % Correspondingly, we utilize function $\\text{W}(\\cdot, \\cdot)$ to replace $\\text{D}(\\cdot, \\cdot)$ mentioned above as the distribution distance function.\n% %\n% We will validate the consistency between Definition 1 and traditional fairness notions (e.g., \\textit{Statistical Parity} and \\textit{Equal Opportunity}) in Section~\\ref{debiasing_exp}.\n%\nBased on the above notations and preliminaries, we formulate the problem of \\textit{Structural Explanation of Bias in GNNs} as follows.\n\n\\begin{myDef1}\n\\label{p1}\n\\textbf{Structural Explanation of Node-Level Bias in GNNs.} Given a trained GNN $f_{\\bm{\\Theta}}$, a node $v_i$ to be explained, and its computation graph $\\mathcal{G}_i = \\{\\mathcal{V}_i,  \\mathcal{E}_i, \\mathcal{X}_i\\}$, our goal is to: (1) identify edges that are faithful to the prediction of $v_i$ (based on $f_{\\bm{\\Theta}}$) and maximally account for the bias exhibited in the GNN outcome of $v_i$; (2) identify edges that are faithful to the prediction of $v_i$ (based on $f_{\\bm{\\Theta}}$) and maximally contribute to the fairness level of the GNN outcome of $v_i$.\n\\end{myDef1}\n\nIntuitively, Problem~\\ref{p1} aims to identify two edge sets as two structural explanations: the bias explanation that accounts for the exhibited bias, and the fairness explanation that contributes to the fairness level of the given prediction. \n%\n% With both structural explanations being faithful to the given GNN prediction, one structural explanation aims to answer where the bias comes from for GNN predictions, and the other one aims to answer how could we achieve more fair GNN predictions. \n%\nFrom the perspective of usability, the first explanation aims to identify edges that introduce the most bias to the instance-level GNN prediction, while the second explanation aims to identify edges that maximally contribute to the fairness level of the GNN prediction for any given node.\n\n\n% \\begin{figure*}[]\n%     \\centering\n%     \\vspace{-5mm}\n%     \\includegraphics[width=0.76\\textwidth]{structure-2.pdf}\n%     \\vspace{-4mm}\n%     \\caption{Framework structure of REFEREE: the edges in the edge set given by Bias Explainer maximally account for the node-level bias, while the edges in the edge set given by Fairness Explainer maximally alleviates the node-level bias.}\n%     \\vspace{-3.5mm}\n%     \\label{framework}\n% \\end{figure*} \n\n\n \n\n\n"
            },
            "section 3": {
                "name": "The Proposed Framework",
                "content": "\n\n\\vspace{-0.3em}\n\nIn this section, we first present a principled metric to quantify the node-level bias for any given GNN prediction. Then we provide an overview of REFEREE, which is the proposed bias explanation framework for GNNs. Finally, we design a unified objective function for the proposed bias explanation framework REFEREE.\n\n\n\n\n\n% \\begin{figure*}[!t]\n%     \\centering\n%     \\vspace{-5mm}\n%     \\includegraphics[width=0.9\\textwidth]{structure-2.pdf}\n%     \\vspace{-4mm}\n%     \\caption{Framework structure of REFEREE. The goal of Bias Explainer and Fairness Explainer is to identify two edge sets. The edges in the edge set given by Bias Explainer maximally account for the exhibited node-level bias, while the existence of edges in the edge set given by Fairness Explainer maximally alleviates the node-level bias.}\n%     \\vspace{-3mm}\n%     \\label{framework}\n% \\end{figure*} \n\n\n",
                "subsection 3.1": {
                    "name": "Node-Level Bias Modeling",
                    "content": "\n\n\\vspace{-0.3em}\n\n%fairness notion gap ； extra faithfulness requirement ； usability gap\n\nTo tackle the challenge of \\emph{Fairness Notion Gap}, here we aim to formulate a novel metric to quantify the bias for the node-level GNN prediction.\n%\n% \nHere we propose to formulate such a bias metric in the probabilistic outcome space of GNN predictions. The reason is that the information about the exhibited bias in the node-level prediction could be lost when the probabilistic outcomes are transformed into discrete predicted labels. In this regard, a bias metric based on the probabilistic outcome can better reflect the exhibited bias in the node-level predictions.\n%\nThis is also in align with some existing bias measures~\\cite{dai2021say,dong2021edits,fan2021fair}. However, although these existing bias metrics are defined in the probabilistic outcome space, they can only measure the level of bias for the predictions over the whole population, and thus cannot be directly grafted to our problem.\n%\n% in align with some existing bias measures\n%\n% We start by reviewing the limitations of those traditional metrics that quantify the bias level for the whole population.\n% %\n% Traditional metrics such as \\textit{Statistical Parity}~\\cite{DBLP:conf/innovations/DworkHPRZ12} and \\textit{Equal Odds}~\\cite{hardt2016equality} are defined over discrete predicted labels. Nevertheless, in node classification, the outcome of GNN corresponding to each node is a probability distribution across all possible classes. A common approach to quantify the bias level of the GNN prediction is to first turn the probabilistic outcome of GNNs into discrete predicted labels (e.g., assign the class with the highest predicted probability as the predicted label), then compute the value of traditional fairness metrics based on the predicted labels. \n% %\n% However, the exhibited bias in the probabilistic outcome of GNNs cannot be comprehensively quantified after the discretization process. For example, for a trained GNN, most individuals with a disadvantaged gender could be predicted with lower probability to be a member of a privileged class (e.g., approved for a loan) compared with individuals with the other gender. But when the probabilistic outcome is transformed into predicted labels, the tendency of being less likely to be a member of the privileged class for those individuals with the disadvantaged gender is neglected. As a result, if the predicted labels are balanced across demographic subgroups divided by sensitive feature, the GNN could still be considered to be fair based on traditional metrics. In this example, traditional metrics only examine the bias based on the predicted labels, but fail to comprehensively reflect the bias exhibited in the probabilistic outcome of GNNs. \n%\n% In this regard, we argue that the traditional metrics above is not able to comprehensively reflect the exhibited bias in GNN outcome. \n%\n% , which are defined over the whole population and cannot be directly grafted to measure the bias level for an instance-level GNN outcome.\n% intuition: (1) distribution difference can measure the overall bias; (2) how much the outcome corresponding to a node account for the overall bias can generally represent how bias the outcome of this node is;\n%\n% To comprehensively reflect the bias exhibited in the probabilistic outcome of GNNs, we propose to define the node-level bias based on the GNN outcome distribution difference between different demographic groups, which is also in align with some existing bias measures that are defined over the whole population~\\cite{dai2021say,dong2021edits,fan2021fair}.\n%\n% the GNN outcome distribution difference between different demographic groups can be adopted to depict the bias level of the predictions for the whole population~\\cite{dai2021say,dong2021edits}. We follow a similar approach to define instance-level bias. \n%\n% Here we follow a consistent approach to define instance-level bias. \n%\n\nWe introduce the rationale of our proposed bias metric for node-level GNN predictions as follows.\n%\nIntuitively, by measuring how much a node's outcome contributes to the overall bias on the whole population, we can have a better understanding of the bias level of this node's outcome. \n%\nMore specifically, assume the nodes can be divided into two sensitive subgroups based on the values of their sensitive features\\footnote{Without loss of generality, we focus on binary sensitive attribute here.}.\n%\nThe GNN outcome of nodes in the two sensitive subgroups forms two distributions, where the distance between the two distributions generally reflects the overall bias~\\cite{fan2021fair,dong2021edits}.\n%\nFor any specific node, if we change the probabilistic outcome of this node, the distribution distance between the outcome sets of the two sensitive subgroups will also change accordingly.\n%\n% As such, if the distribution distance can be significantly reduced by changing the probabilistic outcome of any node, the contribution of this node to the outcome distribution distance between the two sensitive subgroups can be regarded as high. \n% %\n% If some nodes bear such property, it means these nodes are with a high level of bias contribution to the overall bias, i.e., the node-level biases of these nodes are high, and vice versa.\n% %\n% Based on such intuition, we define \\textit{Node-Level Bias in GNNs} as follows.\n%\nIdeally, if the outcome of a node has no contribution to the outcome distribution distance between the two sensitive subgroups, then the distribution distance cannot be further reduced no matter how the outcome of this node is changed.\n%\nIn other words, a node that does not contribute to the overall bias should have an outcome with which the outcome distribution distance between the two sensitive subgroups is minimized.\n%\nMeanwhile, we can also employ the potential distribution distance reduction to measure the contribution of a node's outcome to the overall bias.\n%\n% the maximum distribution distance reduction to measure the contribution of a node's outcome to the overall bias.\n%\nBased on such intuition, we then define \\textit{Node-Level Bias in GNNs} as follows.\n\n\n% \\begin{myDef}\n% \\label{fair_def}\n% \\textbf{Node-level Bias in GNNs.} Given a GNN outcome set $\\mathcal{\\hat{Y}}$. \n% %\n% Divide $\\mathcal{\\hat{Y}}$ into $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as the outcome sets of the two demographic subgroups based on the sensitive feature.\n% %\n% Define the distance between the distribution of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as $D_{(0,1)}$.\n% %\n% Obtain $\\mathcal{\\hat{Y}}'$ and the corresponding $\\mathcal{\\hat{Y}}_0'$ and $\\mathcal{\\hat{Y}}_1'$ by replacing the $\\mathbf{\\hat{y}}_i$ in $\\mathcal{\\hat{Y}}$ with a variable $\\mathbf{\\hat{y}}'$ for node $v_i$.\n% %\n% Denote the minimized distance between the distribution of $\\mathcal{\\hat{Y}}_0'$ and $\\mathcal{\\hat{Y}}_1'$ w.r.t. $\\mathbf{\\hat{y}}'$ as $D_{min(0,1)}$.\n% %\n% We define $B_{i} = D_{(0,1)} - D_{min(0,1)}$ as the node-level bias in GNNs for node $v_i$.\n% \\end{myDef}\n\n% \\begin{myDef}\n% \\label{fair_def}\n% \\textbf{Node-level Bias in GNNs.} Given a GNN outcome set $\\mathcal{\\hat{Y}}$. \n% %\n% Divide $\\mathcal{\\hat{Y}}$ into $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as the outcome sets of the two demographic subgroups based on the sensitive feature.\n% %\n% For node $v_i$, denote $D_{min}$ as the minimum distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ by only changing the value of $\\mathbf{\\hat{y}}_i \\in \\mathcal{\\hat{Y}}$.\n% %\n% Define how much the distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ is larger than $D_{min}$ as the node-level bias $B_i$ in GNNs for node $v_i$.\n% \\end{myDef}\n\n% \\begin{myDef}\n% \\label{fair_def}\n% \\textbf{Node-level Bias in GNNs.}\n% Given a GNN outcome set $\\mathcal{\\hat{Y}}$. \n% %\n% Divide $\\mathcal{\\hat{Y}}$ into $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as the outcome sets of the two demographic subgroups based on the sensitive feature.\n% %\n% For node $v_i$, denote $D_{min}$ as the minimum distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ by perturbing the value of $\\mathbf{\\hat{y}}_i \\in \\mathcal{\\hat{Y}}$.\n% %\n% Define how much the distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ is larger than $D_{min}$ as the node-level bias $B_i$ in GNNs for node $v_i$.\n% \\end{myDef}\n\n\n\n\n\n\n\\begin{myDef}\n\\label{fair_def}\n\\textbf{Node-Level Bias in GNNs.}\nDenote a probabilistic GNN outcome set as $\\mathcal{\\hat{Y}}$. \n%\nDivide $\\mathcal{\\hat{Y}}$ into $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ as the outcome sets of the two demographic subgroups based on the sensitive feature.\n%\nDenote $D$ as the distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$.\n%\nFor node $v_i$, denote $D_{\\text{min}}(i)$ as the minimum distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ by changing the value of $\\mathbf{\\hat{y}}_i \\in \\mathcal{\\hat{Y}}$ while maintaining $\\sum_{k=1}^{C} \\mathbf{\\hat{y}}_i[k] = 1$. We define $B_i = D - D_{\\text{min}}(i)$ as the node-level bias of node $v_i$ for the GNN prediction.\n%\n% define how much the distance between the distributions of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$ can be reduced compared with $D$ by changing the value of $\\mathbf{\\hat{y}}_i \\in \\mathcal{\\hat{Y}}$ as the node-level bias $B_i$ in GNNs for node $v_i$.\n\\end{myDef}\n% \\yu{The intuition is that we use the contribution of node predictions into pulling closer the distance between two class distributions as the metric to quantify the node-level bias. But why can't $D_{\\text{min}}$ be smaller than D if the prediction value of node i could only be changed towards make the distance increase? Probably defining it be the maximum reduction on the distance between two predicting distributions might be better. Another confusion here is do we want to point out that the definition here only applies to the case where the discrimination is defined to be the difference of predictions between two different sensitive subgroups. Since if the discrimination is defined to be the dependence of the prediction on the sensitive group as many traditional fairness notions, then even though the prediction distribution of two different sensitive group is very separate, it does not mean the discrimination/bias is very high.}\n\n\n\n\n\n\nDefinition 1 introduces how to measure the bias exhibited in the node-level prediction given a trained GNN.\n%\nClearly, the minimum value of $B_{i}$ is 0, i.e., if no change on the value of $\\mathbf{\\hat{y}}_i$ can be found to further reduce the distance between the distribution of $\\mathcal{\\hat{Y}}_0$ and $\\mathcal{\\hat{Y}}_1$, we say that node $v_i$ does not exhibit any node-level bias in the GNN outcome.\n%\nIn this paper, we adopt the Wasserstein distance as the metric for distribution distance measurement, considering its superior sensitivity over other distance metrics~\\cite{DBLP:journals/corr/ArjovskyCB17}. \n%\n% Furthermore, some existing works have demonstrated that if the Wasserstein distance between two groups is small, traditional fairness notions such as statistical parity can also be achieved.\n%\nWe will validate the consistency between Definition 1 and traditional fairness notions (e.g., \\textit{Statistical Parity} and \\textit{Equal Opportunity}) in Section~\\ref{debiasing_exp}.\n\n\n\n\n\n"
                },
                "subsection 3.2": {
                    "name": "Overview of Proposed Framework",
                    "content": "\n\\vspace{-0.3em}\n\nHere we present an overview of the proposed bias explanation framework for node-level GNN predictions.\n%\nIn particular, to tackle the challenge of \\textit{Usability Gap},  REFEREE is designed with two different explainers, i.e., a bias explainer $h_{\\bm{\\Phi}}$ and a fairness explainer $h_{\\bm{\\Phi}'}$. The two explainers aim to identify two different edge sets in the given computation graph as two structural explanations, i.e., the bias explanation and the fairness explanation. The two explanations are learned in a contrastive manner, in which way edges that account for different explanations can be better distinguished.\n%\nThe basic goal of the bias explainer is to identify the edges that maximally account for the exhibited node-level bias, while the goal of the fairness explainer is to identify the edges whose existence can maximally alleviate the node-level bias for the instance-level GNN prediction.\n%\nDifferent GNN explanation models that are able to identify edge sets as the node-level explanations can be the backbone of the two explainers.\n%\nBesides, to reflect the true reasoning result in the given GNN model, both identified explanations should be faithful to the given GNN prediction. \n% There are two different goals to be achieved for both explanations. On the one hand, both explanations should be faithful to the given GNN prediction; on the other hand, the bias explanation should maximally account for the exhibited node-level bias, while the fairness explanation should maximally contribute to the fairness level of the instance-level GNN prediction.\n% introduce challenge 2\nThis leads to the challenge of \\textit{Faithfulness Gap}: how to achieve the bias(fairness)-related goal of each explanation and maintain faithfulness to the given GNN prediction at the same time?\n%\nTo tackle this challenge, we design a constraint to enforce the faithfulness of the identified explanations, and incorporate such constraint into a unified objective function for the proposed framework.\n%\nOptimizing such a unified objective function helps to achieve two goals: (1) the bias(fairness)-related explanation goals of both explainers; and (2) the goal of faithfulness through end-to-end training.\n%\n% It is worth mentioning that the two explainers work in a contrastive manner for more effective joint edge identification.\n%\nThe overall structure of REFEREE is presented in Fig.~\\ref{framework}.\n%\n% We introduce the two explainers as below.\n% the Bias Explainer and Fairness Explainer.\nGiven a trained GNN $f_{\\bm{\\Theta}}$, a node $v_i$, and its computation graph $\\mathcal{G}_i$, the goal of \\emph{Bias Explainer} is to identify an edge set $\\mathcal{\\tilde{E}}_i$ that maximally accounts for the exhibited node-level bias of $v_i$ as the bias explanation, while the goal of \\emph{Fairness Explainer} is to identify an edge set $\\mathcal{\\tilde{E}}_i'$ as the fairness explanation, where the edges in $\\mathcal{\\tilde{E}}_i'$ maximally alleviate the node-level bias of $v_i$.\n\n\n% \\begin{itemize}[topsep=0pt]\n%     \\item \\textbf{\\emph{Bias Explainer.}} Given a trained GNN $f_{\\bm{\\Theta}}$, a node $v_i$, and its computation graph $\\mathcal{G}_i$. The goal here is to identify an edge set $\\mathcal{\\tilde{E}}_i$ that maximally accounts for the exhibited node-level bias of $v_i$ as the bias explanation. \n%     % Any post-hoc GNN explanation model $h_{\\bm{\\Phi}}$ can be its backbone.\n    \n%     \\item \\textbf{\\emph{Fairness Explainer.}} \n%     % This module takes the other post-hoc GNN explanation model $h_{\\bm{\\Phi}'}$ as its backbone. \n%     For the same GNN $f_{\\bm{\\Theta}}$, node $v_i$, and computation graph $\\mathcal{G}_i$, the goal is to identify an edge set $\\mathcal{\\tilde{E}}_i'$ as the fairness explanation, where the edge existence in $\\mathcal{\\tilde{E}}_i'$ can maximally alleviate the exhibited node-level bias of $v_i$.\n%     % identifies the edge set that influence both the node outcome and the corresponidng level of fairness based on the given node and computation graph in a contrastive manner.\n%     % \\item \\textbf{\\emph{Readout Module.}} This module takes the learned edge masks from the two previous modules as input, and outputs the corresponding identification of the edges based on how much each edge introduces bias to the outcome of the given node.\n%     % of the attribute matrix and reachability matrix\n% \\end{itemize}\n\n% \\noindent \\textbf{Bias Explainer.}\n% %\n% This module learns an explanation for a given node based on its computation graph to identify the most influential subset of edges towards the corresponding GNN outcome of the given node. \n% %     % In this paper, we implement $g_{\\bm{\\theta}}$ as a linear mapping function.\n\n\n\n\n% \\noindent \\textbf{Fairness Explainer.}\n% %\n% This module identifies the edge set that influence both the node outcome and the corresponidng level of fairness based on the given node and computation graph in a contrastive manner.\n\n\n\n\n% The basic idea of SE-GNN is\n\n\n% Meanwhile, \n\n\n% There are mainly two challenges:\n \n% To address these challenges, SE-GNN explicitly models the node similarity\n \n% An illustration of the proposed framework is shown in Figure 2. \n \n% It is mainly composed of an interpretable similarity module and a self-supervisor to enhance the explanations.\n \n \n% With the novel similarity modeling process,\n \n% Then,\n \n% And, a novel loss function is designed to ensure\n\n\n% Furthermore, self-supervision for explanations is applied to further benefit the accurate explanation generation.\n\n\n\n\n\n\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Objective Function",
                    "content": "\n\\label{objectives}\n\n\\vspace{-0.3em}\n\n% 讲清楚notations\nIn this subsection, we introduce the unified objective function formulation of our proposed framework REFEREE. Generally, the unified objective function includes three components, namely explaining bias (fairness), enforcing fidelity, and refining explanation.\n% namely the sparsity regularization, fidelity enforcement, and fairness constraint. \n\n\n\n\n\n\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Explaining Bias (Fairness).",
                        "content": "\n% \\noindent \\textbf{Explaining Bias (Fairness).}\n%\n% With the discussed sparsity regularization and fidelity enforcement, the two explainers can identify the most critical edges for the prediction made by the trained GNN.\n%\nHere we first introduce the bias (fairness)-related constraints to enable the two explainers to identify the edges that maximally account for the node-level bias and the edges whose existence maximally alleviate the node-level bias for a given GNN prediction, respectively.\n%\n% We argue that this is a non-trivial problem:\n% %\n% on the one hand, it is impractical for us to leverage the superREFEREE signal due to the lack of ground truth identifying a \"fair\" or \"biased\" edge subset; on the other hand, existing explanation models are too coarse to identify how each edge in the computation graph of any node contributes to the bias of the outcome made by the GNN $f_{\\bm{\\Theta}}$.\n% %\n% To handle these challenges, here we propose a scheme based on constrastive learning between the two backbone explanation models.\n% %\n% Generally, \n% %\n% the basic rationale is to encourage the two explainers to explore and preserve edges that either promote or reduce the nodal contribution to the bias of GNNs.\n%\n% the rationale here is to first utilize an explanation model (i.e., explanation model 1) to identify the most critical edges w.r.t. the given GNN outcome $\\mathbf{\\hat{y}}_i$ for node $v_i$. Then for the other explanation model (i.e., explanation model 2), we aim to find both fair and critical edges corresponding to the outcome.\n% %\n% Compared with the edge subset given by explanation model 1, some edges would be excluded from the subset given by explanation model 2. \n% %\n% If the GNN outcome utility (e.g., classification accuracy) is barely affected however fairness is promoted, then such excluded edges should be considered as both biased and critical edges for the given outcome $\\mathbf{\\hat{y}}_i$. The reason is that the use of such edges could have been avoided towards a more fair and accurate GNN outcome.\n%\n% It is worth mentioning that such exploration should be under the fidelity enforcement and sparsity regularization as Eq. (\\ref{loss_1}) and Eq. (\\ref{loss_2}) or Eq. (\\ref{loss_budget}), as it would be meaningless to find the edges that only promote fairness regardless of the utility of the explanation. \n%\n% Here we only focus on the fairness constraint formulation, and a comprehensive objective function formulation will be provided later. \n%\nWe start from the constraint for the \\emph{Bias Explainer}.\n%\nGiven any computation graph, the basic goal of Bias Explainer is to identify the edges that maximally account for the node-level bias as an obtained edge set for the explanation.\n%\nIntuitively, if only edges that maximally account for the exhibited node-level bias are identified and preserved in such edge set, the probabilistic outcome based on such edge set will exhibit more node-level bias. \n%\nThis is because some edges whose existence help to alleviate the node-level bias in the vanilla computation graph are not involved in the obtained edge set anymore.\n%\nBased on such intuition, we develop the first component of our unified objective function towards the goal of explaining bias (fairness) as follows.\n\nWe denote the identified edge set given by Bias Explainer as $\\mathcal{\\tilde{E}}_i$. Here $\\mathcal{\\tilde{E}}_i \\in \\mathcal{\\tilde{G}}_i$, and $\\mathcal{\\tilde{G}}_i$ is the computation graph with the obtained edge set $\\mathcal{\\tilde{E}}_i$. \n%\nWe represent the probabilistic outcome of the GNN model based on the computation graph $\\mathcal{\\tilde{G}}_i$ for node $v_i$ as $\\tilde{\\textbf{y}}_i = f_{\\bm{\\Theta}} (\\mathcal{\\tilde{G}}_i)$,\n%\nwhere $f_{\\bm{\\Theta}}$ is a given trained GNN model with fixed parameter $\\bm{\\Theta}$.\n%\nWe utilize $\\mathcal{\\tilde{Y}}$ to denote the GNN outcome set $\\mathcal{\\hat{Y}}$ with the original element $\\mathbf{\\hat{y}}_i$ being replaced by $\\mathbf{\\tilde{y}}_i$, i.e., $\\mathcal{\\tilde{Y}} = \\mathcal{\\hat{Y}} \\backslash \\{\\mathbf{\\hat{y}}_i\\} \\cup \\{\\mathbf{\\tilde{y}}_i\\}$. \n%\nAccording to the sensitive feature, we split $\\mathcal{\\tilde{Y}}$ into two outcome sets as $\\mathcal{\\tilde{Y}}_0$ and $\\mathcal{\\tilde{Y}}_1$ ($ \\mathcal{\\tilde{Y}}_0 \\cup \\mathcal{\\tilde{Y}}_1 =  \\mathcal{\\tilde{Y}}$).\n%\nWe denote the distribution of $\\mathcal{\\tilde{Y}}_0$ and $\\mathcal{\\tilde{Y}}_1$ as $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$, respectively. \n%\nGenerally, if the vanilla probabilistic outcome $\\mathbf{\\hat{y}}_i$ is replaced with $\\tilde{\\textbf{y}}_i$, the outcome distribution distance between the two sensitive subgroups will also be changed accordingly. Considering that $\\tilde{\\textbf{y}}_i$ is derived based on the input computation graph $\\mathcal{\\tilde{G}}_i$, the identified edges in $\\mathcal{\\tilde{E}}_i \\in \\mathcal{\\tilde{G}}_i$ then determine how the distribution distance changes.\n%\nAs discussed above, the probabilistic outcome based on $\\mathcal{\\tilde{E}}_i$ will exhibit more node-level bias.\n%\nFrom Definition~\\ref{fair_def}, we know that the identified edges in $\\mathcal{\\tilde{E}}_i$ are supposed to lead to a larger distribution distance between $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$.\n%\n% More specifically, we utilize $\\mathcal{\\tilde{Y}}$ to denote the GNN outcome set $\\mathcal{\\hat{Y}}$ with the original element $\\mathbf{\\hat{y}}_i$ being replaced by $\\mathbf{\\tilde{y}}_i$, i.e., $\\mathcal{\\tilde{Y}} = \\mathcal{\\hat{Y}} \\backslash \\{\\mathbf{\\hat{y}}_i\\} \\cup \\{\\mathbf{\\tilde{y}}_i\\}$. We split $\\mathcal{\\tilde{Y}}$ into two outcome sets as $\\mathcal{\\tilde{Y}}_0$ and $\\mathcal{\\tilde{Y}}_1$ ($ \\mathcal{\\tilde{Y}}_0 \\cup \\mathcal{\\tilde{Y}}_1 =  \\mathcal{\\tilde{Y}}$) based on the sensitive feature.\n% %\n% We denote the distribution of $\\mathcal{\\tilde{Y}}_0$ and $\\mathcal{\\tilde{Y}}_1$ as $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$, respectively. \n% %\n% Based on the discussion above, the identified edges in $\\mathcal{\\tilde{E}}_i$ determines the distance between $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$.\n% %\n% Intuitively, if only edges that maximally account for the exhibited node-level bias is preserved in $\\mathcal{\\tilde{E}}_i$, the probabilistic outcome $\\mathbf{\\tilde{y}}_i$ based on $\\mathcal{\\tilde{E}}_i$ will be more biased in terms of node-level bias. This is because some edges whose existence helps to alleviate the node-level bias in the vanilla outcome $\\mathbf{\\hat{y}}_i$ are not involved in $\\mathcal{\\tilde{E}}_i$ any more.\n%\n% Therefore, the identified edges in $\\mathcal{\\tilde{E}}_i$ are supposed to lead to larger distribution distance between $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$.\n%\n% leads to larger distance between the probabilistic outcome distribution of the two groups based on sensitive feature. This is because the edges in $\\mathcal{\\tilde{G}}_i$ maximally account for the exhibited node-level bias for the obtained $\\tilde{\\textbf{y}}_i$.\n% % \n% Correspondingly, we utilize $\\mathcal{\\tilde{Y}}$ to denote the GNN outcome set $\\mathcal{\\hat{Y}}$ with the original element $\\mathbf{\\hat{y}}_i$ being replaced by $\\mathbf{\\tilde{y}}_i$, i.e., $\\mathcal{\\tilde{Y}} = \\mathcal{\\hat{Y}} \\backslash \\{\\mathbf{\\hat{y}}_i\\} \\cup \\{\\mathbf{\\tilde{y}}_i\\}$. We split $\\mathcal{\\tilde{Y}}$ into two outcome sets as $\\mathcal{\\tilde{Y}}_0$ and $\\mathcal{\\tilde{Y}}_1$ ($ \\mathcal{\\tilde{Y}}_0 \\cup \\mathcal{\\tilde{Y}}_1 =  \\mathcal{\\tilde{Y}}$) based on the sensitive feature.\n% %\n% We denote the distribution of $\\mathcal{\\tilde{Y}}_0$ and $\\mathcal{\\tilde{Y}}_1$ as $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$, respectively. \n% %\n% Intuitively, the distribution distance between $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$ is supposed to be maximized w.r.t. the obtained $\\tilde{\\textbf{y}}_i$, which is a function of $\\mathcal{\\tilde{E}}_i \\in \\mathcal{\\tilde{G}}_i$.\n%\nCorrespondingly, we formulate the goal of bias explanation based on Wasserstein-1 distance as\n% \\yu{I guess $P$ here simply represents the distribution here, maybe simply mention its meaning here could be more clear. Another confusion here is do we fix the parameters of the encoder that is trained on the original graph in learning the parametrized edges of the local computational graph? I think here the encoder to obtain $y_0/y_1$ might need a little bit more explanation.}\n%\n\\begin{align}\n\\label{ws_distance}  \n    \\max_{\\mathcal{\\tilde{E}}_i} \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1)),\n\\end{align}\nwhere $ \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1))$ is formally presented as\n\\begin{align}\n\\begin{small}\n\\label{distance_expression}  \n    W_1(P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1)) = \n    \\inf \\mathbb{E}_{ (\\tilde{\\mathbf{y}}_{(0)}, \\tilde{\\mathbf{y}}_{(1)}) \\sim \\kappa} [\\| \\tilde{\\mathbf{y}}_{(0)} - \\tilde{\\mathbf{y}}_{(1)} \\|_1].\n\\end{small}\n\\end{align}\n\\noindent Here $\\kappa \\in \\Pi (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1))$;\n%\n% $\\tilde{\\mathbf{y}}_{(0)}$ and $\\tilde{\\mathbf{y}}_{(1)}$ are random variables that are drawn following joint distribution $\\kappa (\\tilde{\\mathbf{y}}_{(0)}, \\tilde{\\mathbf{y}}_{(1)})$.\n%\n$\\Pi(P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1))$ is the set including all possible joint distributions of $\\kappa (\\tilde{\\mathbf{y}}_{(0)}, \\tilde{\\mathbf{y}}_{(1)})$ whose marginals are $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$, respectively.\n%\nGenerally, Eq.~(\\ref{ws_distance}) encourages the Bias Explainer to identify edges that maximally account for the Wasserstein-1 distance between $P(\\mathcal{\\tilde{Y}}_0)$ and $P(\\mathcal{\\tilde{Y}}_1)$. \n% In other words, the identified edges reflect how the exhibited bias is introduced for the given node $v_i$.\n% intractable\nNevertheless, the infimum in Eq. (\\ref{distance_expression}) is intractable. To perform effective optimization with gradient-based optimizing techniques (e.g., stochastic gradient descent), we adopted a widely used approximation strategy~\\cite{cuturi2014fast} for the Wasserstein distance, which has been empirically proved to be effective~\\cite{guo2020learning}.\n% 另一个explainer\n\nWe follow a similar approach to set up the other goal for Fairness Explainer to encourage the identification of edges whose existence can maximally alleviate the node-level bias for the given GNN prediction.\n%\nWe assume the edge set given by Fairness Explainer as $\\mathcal{\\tilde{E}}_i'$, where $\\mathcal{\\tilde{E}}_i' \\in \\mathcal{\\tilde{G}}_i'$. Here $\\mathcal{\\tilde{G}}_i'$ is the computation graph with the identified $\\mathcal{\\tilde{E}}_i'$.\n%\nWe denote the outcome of the GNN model based on $\\mathcal{\\tilde{G}}_i'$ for node $v_i$ as $\\tilde{\\textbf{y}}_i' = f_{\\bm{\\Theta}} (\\mathcal{\\tilde{G}}_i')$.\n%\nWe use $\\mathcal{\\tilde{Y}}_0'$ and $\\mathcal{\\tilde{Y}}_1'$ to denote the subsets of $\\mathcal{\\tilde{Y}}' = \\mathcal{\\hat{Y}} \\backslash \\{\\mathbf{\\hat{y}}_i\\} \\cup \\{\\mathbf{\\tilde{y}}_i'\\}$ according to the sensitive feature.\n%\nCorrespondingly, $P(\\mathcal{\\tilde{Y}}_0')$ and $P(\\mathcal{\\tilde{Y}}_1')$ are the distributions of $\\mathcal{\\tilde{Y}}_0'$ and $\\mathcal{\\tilde{Y}}_1'$, respectively. \n%\nWe formulate the goal of Fairness Explainer as\n% \\yu{It is clear that optimizing Eq.(1) and (3) equals to identifying edges such that using these edges to make predictions could maximize/minized the distance between distribution of two different classes. But how it relates to the pre-defined node-level bias requires a little bit explanation.}\n\\begin{align}\n\\label{ws_distance2}  \n\\min_{\\mathcal{\\tilde{E}}_i'} \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0'), P(\\mathcal{\\tilde{Y}}_1')),\n\\end{align}\nwhere $\\mathcal{\\tilde{E}}_i'$ is the edge set given by the Fairness Explainer for explanation.\n%\nTo summarize, we formulate the objective function term towards explaining bias (fairness) as \n\\begin{align}\n\\label{l3}  \n    \\mathscr{L}_1(\\bm{\\Phi}, \\bm{\\Phi}') = \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0'), P(\\mathcal{\\tilde{Y}}_1')) - \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1)).\n\\end{align}\n%\nThe basic rationale is that when $\\mathscr{L}_1$ is minimized, $\\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1))$ is maximized to encourage Bias Explainer to identify edges that account for the exhibited node-level bias; $\\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0'), P(\\mathcal{\\tilde{Y}}_1'))$ is minimized to encourage Fairness Explainer to identify edges whose existence can maximally alleviate the node-level bias.\n\n% contrastive: motivation\nNevertheless, considering that the probabilistic outcome corresponding to only one explained node is changed during the optimization of Eq. (\\ref{ws_distance}) (or Eq.~(\\ref{ws_distance2})),\n%\n% only one element in $\\tilde{\\mathcal{Y}}$ (or $\\tilde{\\mathcal{Y}}'$) is modified towards the optimization of Eq. (\\ref{ws_distance}) (or Eq.~(\\ref{ws_distance2}))\n% \\yu{where do we mention that only one element could be changed when we first mention Eq.~(1) or (3) even though we only optimize the edge set of each node i given by Equation but probably mention it in the manscript could be better}\nthe numerical change of Wasserstein-1 distance could be small. Correspondingly, when using gradient-based techniques to optimize the two explainers in REFEREE, the gradients of $\\mathscr{L}_1$ w.r.t. the learnable parameters in the two explainers could be similar. This could lead to a phenomenon that the two explainers tend to converge at similar solutions, which means that $\\mathcal{\\tilde{E}}_i \\in \\mathcal{\\tilde{G}}_i$ and $\\mathcal{\\tilde{E}}_i' \\in \\mathcal{\\tilde{G}}_i'$ could be close.\n% \\yu{why the little numerical change of W-1 distance could lead to the phenomenon that two explainers tend to have similar solutions. Does that mean the gradient is so small so that one of the two parts may not be optimized?}\n%\n% This problem cannot be solved by simply adding more weights for the optimization of $\\mathscr{L}_1$, as the balance between fairness constraints and explanation fidelity could be affected.\n%\nTo better differentiate the edges that are supposed to be separated into two different explanations, here we propose to add a contrastive loss between the two explainers. The intuition here is to encourage the Bias Explainer and the Fairness Explainer to identify different edges from each other.\n%\nSpecifically, the distribution difference between the edges in $\\mathcal{\\tilde{E}}_i$ and $\\mathcal{\\tilde{E}}_i'$ are maximized as an encouragement for identifying different edges. \n%\nIt should be noted that the edge sets given by both the two explainers (i.e., $\\mathcal{\\tilde{E}}_i$ and $\\mathcal{\\tilde{E}}_i'$) are based on the edge set $\\mathcal{E}_i$ in the given computation graph. Correspondingly, we denote the distribution of $\\mathcal{\\tilde{E}}_i$ and $\\mathcal{\\tilde{E}}_i'$ conditional on the given $\\mathcal{E}_i$ as $P_{\\bm{\\Phi}}(\\mathcal{\\tilde{E}}_i| \\mathcal{E}_i)$ and $P_{\\bm{\\Phi}'}(\\mathcal{\\tilde{E}}_i' | \\mathcal{E}_i)$, respectively.\n%\nWe give the optimization problem as\n\\begin{align}\n\\label{KLD}  \n    \\max_{\\bm{\\Phi}, \\bm{\\Phi}'} \\text{Dist\\_Diff} (    P_{\\bm{\\Phi}'}(\\mathcal{\\tilde{E}}_i' | \\mathcal{E}_i) \\|  P_{\\bm{\\Phi}}(\\mathcal{\\tilde{E}}_i| \\mathcal{E}_i) ),\n\\end{align}\n%\n% where $P_{\\bm{\\Phi}'}(\\mathcal{\\tilde{E}}_i' | \\mathcal{E}_i)$ and $P_{\\bm{\\Phi}}(\\mathcal{\\tilde{E}}_i| \\mathcal{E}_i)$ demote the conditional probability distribution of edges in $\\mathcal{\\tilde{E}}_i'$ and $\\mathcal{\\tilde{E}}_i$ given the vanilla edge set $\\mathcal{E}_i$ in the computation graph $\\mathcal{G}_i$, respectively.\n%\nVarious metrics can be adopted as the distribution difference operator $\\text{Dist\\_Diff} (.)$, e.g., Jensen–Shannon divergence and Wasserstein distance, etc.\n%\nWe give the second objective function term as\n\\begin{align}\n\\label{l4}  \n    \\mathscr{L}_2(\\bm{\\Phi}, \\bm{\\Phi}') = - \\text{Dist\\_Diff} (    P_{\\bm{\\Phi}'}(\\mathcal{\\tilde{E}}_i' | \\mathcal{E}_i) \\|  P_{\\bm{\\Phi}}(\\mathcal{\\tilde{E}}_i| \\mathcal{E}_i) ).\n\\end{align}\n%\nMinimizing $\\mathscr{L}_2$ helps to encourage the two explainers to yield different edge sets from each other as the identified explanations.\n\n\n\n\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Enforcing Fidelity.",
                        "content": "\n% \\noindent  \\textbf{Enforcing Fidelity.}\n% Our ultimate goal is to identify the actual edge contribution to the bias (fairness) level of the vanilla GNN prediction, i.e., the original predicted label in \\textit{node classification} task.\nThe explanations given by the two explainers should be able to reflect the true reasoning result given the node-level GNN prediction.\n%\nHence, for both explainers (i.e., the Bias Explainer and Fairness Explainer), the output explanation should be faithful to the given GNN prediction. In other words, given a node $v_i$, the structural explanations given by both the two explainers should lead to the same predicted label based on the given GNN $f_{\\bm{\\Theta}}$.\n%\n% the prediction $\\mathbf{\\tilde{y}}_i$ corresponding to the masked computation graph $\\mathcal{\\tilde{G}}_i$ should be barely influenced compared with the original prediction $\\mathbf{y}_i$ based on $\\mathcal{G}_i$.\n%\n% The output explanation should be faithful to the GNN model~\\cite{yuan2020explainability}. In other words, given a node $v_i$, the prediction $\\mathbf{\\tilde{y}}_i$ corresponding to the masked computation graph $\\mathcal{\\tilde{G}}_i$ should be barely influenced compared with the original prediction $\\mathbf{y}_i$ based on $\\mathcal{G}_i$. \n%\n% In this regard, our goal here is to enforce such faithfulness for the explanations, i.e., enforcing the prediction of GNN based on the identified edges to preserve as much information of the original prediction as possible. \n%\n% An intuition of achieving faithfulness is that the given GNN should yield the same predicted label based on the subgraph with only the identified edges, as these edges encode the critical information for the given instance-level prediction.\n%\nBased on such intuition, here we leverage the mutual information between the original predicted label and the subgraph with the identified edge set for explanation to formulate fidelity enforcement. This also aligns with some existing works on GNN explanation~\\cite{ying2019gnnexplainer}.\n%\nWe first introduce the fidelity enforcement formulation for the Bias Explainer.\n%\nSpecifically, for node $v_i$, the mutual information between the original GNN prediction $\\hat{Y}_i \\in \\{1, ..., C\\}$ and the underlying subgraph $\\mathcal{\\tilde{G}}_i$ is maximized to ensure that $\\mathcal{\\tilde{E}}_i \\in \\mathcal{\\tilde{G}}_i$ encodes the critical information of the given GNN prediction, which is formulated as\n\\begin{align}\n\\label{mi_term}\n    \\max_{\\mathcal{\\tilde{G}}_i} \\text{MI}(\\hat{Y}_i, \\mathcal{\\tilde{G}}_i) = \\text{H}(\\hat{Y}_i) - \\text{H}(\\hat{Y}_i| \\mathcal{\\tilde{G}}_i).\n\\end{align}\nHere MI($\\cdot, \\cdot$) denotes the mutual information computation operator, and H($\\cdot$) represents the entropy function. \n% intuition\n% Intuitively, xx. \n%\nIt is worth mentioning that in Eq.~(\\ref{mi_term}), the value of the entropy term $\\text{H}(\\hat{Y}_i) = \\text{H}(f_{\\bm{\\Theta}} (\\mathcal{G}_i))$ is fixed, as the explanation model is post-hoc (i.e., the parameters in the given GNN model are fixed).\n%\nTherefore, the optimization problem in Eq. (\\ref{mi_term}) can be reduced to only minimizing the second entropy term, %, i.e., $\\min_{\\mathcal{\\tilde{G}}_i} \\text{H}(\\mathbf{y}_i| \\mathcal{\\tilde{G}}_i)$.\n% \\begin{align}\n% \\label{mi_simplification}\n%     \\min_{\\mathcal{\\tilde{G}}_i} \\text{H}(\\mathbf{y}_i| \\mathcal{\\tilde{G}}_i),\n% \\end{align}\n% where $\\text{H}(\\mathbf{y}_i| \\mathcal{\\tilde{G}}_i)$ can be expressed as \n% \\begin{align}\n% \\label{entropy_term}  %\n%     \\text{H}(\\mathbf{y}_i| \\mathcal{\\tilde{G}}_i) = - E_{\\mathbf{y}_i | \\mathcal{\\tilde{G}}_i} [\\log P_{\\bm{\\Theta}}(\\mathbf{y}_i | \\mathcal{\\tilde{G}}_i)].\n% \\end{align}\n% At the same time, since we only focus on preserving the information regarding the predicted labels given by the GNN for node $v_i$, Eq. (\\ref{mi_term}) can be simplified as $\\min_{\\mathcal{\\tilde{G}}_i} \\text{H}(Y_i| \\mathcal{\\tilde{G}}_i)$. Here $Y_i \\in \\{1, ..., C\\}$ denotes the predicted label for node $v_i$.\n%\nwhere $\\text{H}(\\hat{Y}_i| \\mathcal{\\tilde{G}}_i)$ can be presented as\n\\begin{align}\n\\label{entropy_term}  \n    \\text{H}(\\hat{Y}_i| \\mathcal{\\tilde{G}}_i) = - \\mathbb{E}_{\\hat{Y}_i | \\mathcal{\\tilde{G}}_i} [\\log P_{\\bm{\\Theta}}(\\hat{Y}_i | \\mathcal{\\tilde{G}}_i)].\n\\end{align}\nConsidering fidelity is necessary for both explainers, we give the fidelity constraint for Fairness Explainer similarly. The objective function term to enforce fidelity for both explainers is given as\n\\begin{align}\n\\label{loss_1}  \n\\small\n    \\mathscr{L}_3 (\\bm{\\Phi}, \\bm{\\Phi}') = - \\mathbb{E}_{\\hat{Y}_i | \\mathcal{\\tilde{G}}_i} [\\log P_{\\bm{\\Theta}}(\\hat{Y}_i | \\mathcal{\\tilde{G}}_i)] - \\mathbb{E}_{\\hat{Y}_i | \\mathcal{\\tilde{G}}_i'} [\\log P_{\\bm{\\Theta}}(\\hat{Y}_i | \\mathcal{\\tilde{G}}_i')].\n\\end{align}\n%\nMinimizing $\\mathscr{L}_3$ enforces the identified edges in $\\mathcal{\\tilde{G}}_i$ and $\\mathcal{\\tilde{G}}_i'$ to encode as much critical information to $\\hat{Y}_i$ as possible.\n% \\yu{Here I am not quite convinced that the identified edges should be also responsible for the original predictions. In reality, the edges that encode sensitive information might not encode class-related information or vice versa.}\n\n\n\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "Refining Explanation.",
                        "content": "\n% \\noindent  \\textbf{Refining Explanation.}\nAs mentioned in Section~\\ref{intro}, our proposed explanation framework should be able to identify two edge sets, where the edges in one set can maximally account for the exhibited node-level bias, and the existence of the edges in the other set can maximally alleviate the node-level bias in GNNs.\n%\n% In other words, those non-critical edges should be removed from the outcome explanations for both the two explainers.\n%\nTherefore, the identified explanations for both explainers should be maximally refined.\n%\nIntuitively, to refine the learned explanations, those goal-irrelevant edges for the GNN outcome of node $v_i$ should be maximally identified and removed from the structural explanations of both explainers, i.e., the learned edge sets from both explainers should be sparse.\n%\nHere we propose to regularize the sparsity of the identified edge set to remove those goal-irrelevant edges maximally.\n%\nWe take the sparsity regularization of the Bias Explainer as an example.\n%\nNote that the explanation of the identified edge set is identified via a weighted mask matrix $\\mathbf{M} \\in \\mathbb{R}^{|\\mathcal{V}_i| \\times |\\mathcal{V}_i|}$ which indicates the edge importance score with entry values.\n%\nWe propose to utilize the $\\ell_1$-norm of the mask matrix $\\mathbf{M}$ for the Bias Explainer as the regularization, i.e., $\\| \\mathbf{M}\\|_1$. Considering both explainers, the corresponding objective function term $\\mathscr{L}_4$ is formulated as \n\\begin{align}\n\\label{loss_2}  \n    \\mathscr{L}_4(\\bm{\\Phi}, \\bm{\\Phi}') = \\| \\mathbf{M}\\|_1 + \\| \\mathbf{M}'\\|_1\n\\end{align}\nfor the two explainers. \n%\nHere $\\bm{\\Phi}$ is the parameter of Bias Explainer $h_{\\bm{\\Phi}}$, and $\\bm{\\Phi}'$ denotes the parameter of Fairness Explainer $h_{\\bm{\\Phi}'}$.\n%\n$\\mathbf{M}$ and $\\mathbf{M}'$ are used to indicate the edge weights given by the explanations from the Bias Explainer and Fairness Explainer, respectively.\n%\nBesides, there are also cases where people are only interested in a certain number of top-ranked critical edges. In other words, there could be a pre-assigned budget $T$ for the explanation edge set $\\mathcal{\\tilde{E}}_i$, i.e., $|\\mathcal{\\tilde{E}}_i| \\leq T$. \n% \\textcolor{purple}{highlighting this comment}\n%\nIn this case, we formulate the $\\mathscr{L}_4$ as\n\\begin{align}\n\\label{loss_budget}  \n    \\mathscr{L}_4(\\bm{\\Phi}, \\bm{\\Phi}', T, T') = \\text{ReLU}(\\| \\mathbf{M}\\|_1 -T) + \\text{ReLU}(\\| \\mathbf{M}'\\|_1 -T')\n\\end{align}\ngiven pre-assigned budget $T$ and $T'$ for $\\mathcal{\\tilde{E}}_i$ and $\\mathcal{\\tilde{E}}_i'$, respectively.\n%\nIntuitively, minimizing $\\mathscr{L}_4$ helps to remove those goal-irrelevant edges maximally to refine the identified explanation.\n\n\n\n"
                    },
                    "subsubsection 3.3.4": {
                        "name": "Unified Objective Function Formulation.",
                        "content": "\n% \\noindent \\textbf{Unified Objective Function Formulation.}\nBased on our discussions on enforcing fidelity, explaining bias (fairness), and refining explanation, we formally formulate the unified objective function for the proposed GNN explanation framework REFEREE as\n% \\begin{align}\n% \\label{l_total}  \n%     \\mathscr{L} = \\alpha \\mathscr{L}_1 +  \\beta \\mathscr{L}_2 + \\gamma  \\mathscr{L}_3 +  \\mathscr{L}_4.\n% \\end{align}\n\\begin{align}\n\\label{l_total}  \n    \\mathscr{L} =  \\mathscr{L}_1 +  \\alpha \\mathscr{L}_2 + \\beta  \\mathscr{L}_3 +  \\gamma \\mathscr{L}_4.\n\\end{align}\nHere $\\alpha$, $\\beta$, and $\\gamma$ are hyper-parameters controlling the effect of the three constraining terms. For any specific node to be explained, minimizing the objective function in Eq. (\\ref{l_total}) aims to: (1) encourage the Bias Explainer to identify an edge set that maximally accounts for the node-level bias in the given GNN; and (2) encourage the Fairness Explainer to identify an edge set that maximally contributes to the fairness for the given GNN prediction.\n%\n% We introduce the identification of both biased and fair edges based on the prediction given by explanation model 1 and 2 (i.e., $\\mathcal{\\tilde{G}}_i$ and $\\mathcal{\\tilde{G}}_i'$) in the next subsection.\n\n\n% \\subsection{Framework Training}\n\n\n\n\n\n\n% \\section{Theoretical Analysis}  % 为什么：Wasserstein distance被minimize之后就能公平\n\n% \\begin{myDef3}\n% \\textit{Assume elements in  follows $\\mathcal{N}(\\mu_1, \\sigma_1)$. If $P(Y|s=1) = P(Y|s=0)$ can be satisfied, then $\\text{W}_2 (P_{\\mathcal{\\hat{Y}'}_0}, P_{\\mathcal{\\hat{Y}'}_1})$ is upper-bounded, which can be given by\n% \\begin{align}\n% \\label{upper-bound}  \n%     \\text{W}_2 (P_{\\mathcal{\\hat{Y}'}_0}, P_{\\mathcal{\\hat{Y}'}_1}) \\leq \\sqrt{    \\frac{1}{2}  + (\\sqrt{\\sigma_1} + \\sqrt{\\sigma_2})^{2}     } .\n% \\end{align}\n% }\n% \\end{myDef3}\n\n% \\begin{proof}\n% \\end{proof}\n\n\n\\vspace{-0.2em}\n"
                    }
                }
            },
            "section 4": {
                "name": "Experimental Evaluations",
                "content": "\n\\vspace{-0.3em}\n\nIn this section, we first introduce the downstream learning task and the real-world datasets adopted for evaluation. The experimental settings and the implementation details are then introduced. Next, we present the empirical evaluation results of our proposed framework from the perspective of \\textit{Effectiveness of Explaining Bias (Fairness)}, \\textit{Explanation Fidelity}, and \\textit{Debiasing GNN with Explanations}.\n%\nIn particular, we aim to answer the following research questions:\n%\n\\textbf{RQ1:}  How well can REFEREE identify edges to explain bias (fairness) in GNNs given the prediction of a specific node?\n%\n\\textbf{RQ2:}  How well can the explanations given by the two explainers in REFEREE be faithful to the given GNN?\n%\n\\textbf{RQ3:}  How will the obtained explanations from REFEREE help with GNN debiasing for the whole population?\n\n% \\begin{itemize}[topsep=0pt]\n%     \\item \\textbf{RQ1:}  How well can REFEREE identify edges to explain bias (fairness) in GNNs given the prediction of a specific node?\n%     \\item \\textbf{RQ2:}  How well can the explanations given by the two explainers in REFEREE be faithful to the given GNN?\n%     \\item \\textbf{RQ3:}  How will the obtained explanations from REFEREE help with GNN debiasing for the whole population?\n% \\end{itemize}\n\n\n% \\subsection{Downstream Task and Datasets}\n\n% \\vspace{-0.3em}\n\n% \\noindent \\textbf{Downstream Task.} \n% In this paper, we focus on the widely studied \\textit{node classification} problem as the downstream task to evaluate our proposed bias explanation framework REFEREE for any given GNN.\n\n\n% \\noindent \\textbf{Real-World Datasets.} We adopt three real-world attributed networks in the experiments -- German Credit, Recidivism, and Credit Defaulter~\\cite{agarwal2021towards}. A detailed introduction is in the Appendix.\n%\n% Specifically, for German Credit dataset, nodes and edges represent the bank clients and the connections between client accounts, respectively. Here the gender of the bank clients is regarded as the sensitive feature, and the task is to classify if the credit risk of each client is high or not.\n% %\n% For Recidivism dataset, nodes represent defendants that were released on bail from the year 1990 to the year 2009, and edges represent the connection between defendants based on their past criminal records. Race of the defendants is considered as the sensitive feature, and the task is to classify if a certain defendant deserves bail. A positive bail decision indicates that the corresponding defendant is unlikely to commit a crime if released.\n% %\n% For Credit Defaulter dataset, each node represents a credit card user, and the edge between two nodes is the connection between two credit card users. Age of the credit card users is the sensitive feature, and the task is to predict the future default of credit card payment for each user.\n% %\n% We present the statistics of these three datasets in Appendix (Table~\\ref{datasets}).\n\n\\vspace{-1.6ex}\n\n\n\n\n\n\n\n\n\n",
                "subsection 4.1": {
                    "name": "Experimental Settings",
                    "content": "\n\\label{settings}\n\n\\vspace{-0.3em}\n% \\noindent \\textbf{GNN Models.} \n% We adopt three commonly utilized GNN models for the evaluation of our proposed explanation framework, namely Graph Convolutional Network (GCN)~\\cite{DBLP:conf/iclr/KipfW17}, Graph Attention Network (GAT)~\\cite{velivckovic2017graph}, and Graph Isomorphism Network (GIN)~\\cite{DBLP:conf/iclr/XuHLJ19}.\n\n% The proposed framework REFEREE is implemented in PyTorch~\\cite{paszke2017automatic} and optimized via Adam optimizer~\\cite{kingma2014adam}.\n% %\n% More details are introduced in the Appendix. \n\n",
                    "subsubsection 4.1.1": {
                        "name": "Downstream Task \\& Real-world Dataset.",
                        "content": "\n%\nIn this paper, we focus on the widely studied \\textit{node classification} as the downstream task.\n%\nWe adopt three real-world attributed networks for experiments -- German Credit, Recidivism, and Credit Defaulter~\\cite{agarwal2021towards}, where all node labels are binary. A detailed description is in the Appendix.\n\n\n\n\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Explainer Backbones.",
                        "content": "\n% \\noindent \\textbf{Explainer Backbones.} \nDifferent GNN explanation approaches that are able to identify edge sets as the node-level explanations can be adopted as the backbone of the two explainers in REFEREE. To evaluate how well the proposed framework can be generalized to different explanation backbones, we adopt GNN Explainer~\\cite{ying2019gnnexplainer} and PGExplainer~\\cite{luo2020parameterized} as two backbones of explainers for evaluation.\n\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Baselines.",
                        "content": "\n% \\noindent \\textbf{Baselines.} \nTo the best of our knowledge, no other work is able to give structural explanations for the exhibited node-level bias of GNNs. Therefore, we modify some existing GNN explanation approaches to adapt them to explain exhibited node-level bias in terms of the computation graph structure.\n%\nThe adopted existing GNN explanation approaches for adaptation include the attention-based GNN explanation~\\cite{velivckovic2017graph}, the gradient-based GNN explanation~\\cite{velivckovic2017graph}, and two state-of-the-art GNN explanation approaches (GNN Explainer~\\cite{ying2019gnnexplainer} and PGExplainer~\\cite{luo2020parameterized}).\n%\nWe elaborate more details on how we achieve the adaptation for these approaches as follows.\n%\n% adopt several alternative approaches including attention based explanation, gradient based explanation, existing GNN explanation models, and variants of existing GNN explanation models as our baselines for different evaluation goals.\n\nFirst, we introduce how we adapt these approaches as the baselines to evaluate \\textit{Effectiveness of Explaining Bias (Fairness)}. For attention-based explanation, we directly add a bias(fairness)-related objective onto the vanilla loss function of a Graph Attention Network (GAT) model~\\cite{velivckovic2017graph} to maximize (as Eq.~(\\ref{ws_distance})) or minimize (as Eq.~(\\ref{ws_distance2})) the Wasserstein-1 distance between the outcome distributions of the two sensitive subgroups. This enables the GAT model to identify the two types of critical edges for bias and fairness explanation, i.e., edges that maximally account for the exhibited node-level bias and edges whose existence can maximally alleviate the node-level bias. \n% \\yu{The L1 loss in our model optimizes both Eq.(1) and Eq.(3), I am not sure here whether we would want to consider optimizing both of them in GAT model.}\nThe learned attention weights are regarded as the indicator of the final explanations. For gradient-based explanation, we utilize the same objective function as the objective function adopted by attention-based explanation. The two types of critical edges for bias and fairness explanation are identified through gradient ascend w.r.t. the adjacency matrix of the given computation graph.\n% \\yu{I am confused here how we use gradient ascend to identify the importance of edges. Does it mean we identify edges by gradient ascend they cause? Or by optimizing the loss using gradient ascend?}\n%\nFor GNN Explainer and PGExplainer, we modified their objective function in a similar way as the attention-based explanation. Specifically, a bias(fairness)-related objective is added onto the vanilla loss function for both explanation models. For any given computation graph, the two types of critical edges for bias and fairness explanation are identified through maximizing (as Eq.~(\\ref{ws_distance})) or minimizing (as Eq.~(\\ref{ws_distance2})) the Wasserstein-1 distance between the outcome distributions of the two sensitive subgroups.\n\nSecond, for the evaluation of \\textit{Explanation Fidelity}, we aim to compare whether the GNN explanation backbones in REFEREE can still maintain their faithfulness to the given GNN prediction. Here the most widely-used GNN Explainer is adopted as the baseline model. Correspondingly, GNN Explainer is also adopted as the backbone of the two explainers in REFEREE for a fair comparison.\n\nThird, for the evaluation of \\textit{Debiasing GNNs with Explanation}, we adopt the same baselines as those adopted in the evaluation of \\textit{Effectiveness of Explaining Bias (Fairness)}.\n\n\n\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "Evaluation Metrics.",
                        "content": "\n% \\noindent \\textbf{Evaluation Metrics.} \nWe first introduce the metrics for the evaluation of \\textit{Effectiveness of Explaining Bias (Fairness)}. \n%\nSpecifically, we evaluate how much the node-level bias $B_i$ is promoted or reduced between the two sensitive subgroups when only the identified edge set is utilized for the GNN prediction of the given node. Intuitively, this enables us to evaluate how well each explainer can identify those edges that maximally account for the exhibited bias and edges whose existence can maximally alleviate the node-level bias for the prediction, respectively.\n%\nFor the evaluation of \\textit{Explanation Fidelity}, a widely acknowledged metric is \\textit{Fidelity}$-$ score~\\cite{yuan2020explainability}. Traditionally, \\textit{Fidelity}$-$ score measures the ratio of the consistent pairs between the vanilla correct predictions and the correct predictions based on the identified edge set. Nevertheless, to reflect the true reasoning process in GNNs, we argue that the faithfulness of those incorrect predictions is also critical, as bias may also exhibit and need to be explained for those incorrect predictions from the perspective of the usability of the GNNs. As a consequence, we extend the \\textit{Fidelity}$-$ score to measure the ratio of the consistent pairs between all vanilla predictions and the predictions based on the identified edge set.\n%\nFormally, the extended fidelity metric for $M$ explained nodes can be measured with $\\text { Fidelity }=\\frac{1}{M} \\sum_{i=1}^{M}\\left(\\mathds{1}\\left(\\hat{Y}_{i}=\\tilde{Y}_{i}\\right)\\right)$.\n% \\begin{align}\n% \\text { Fidelity }=\\frac{1}{M} \\sum_{i=1}^{M}\\left(\\mathds{1}\\left(\\hat{Y}_{i}=\\tilde{Y}_{i}\\right)\\right).\n% \\end{align}\n%\nHere $\\hat{Y}_{i}$ represents the vanilla GNN prediction for node $v_i$.\n%\n$\\tilde{Y}_{i}$ denotes the prediction of the given GNN $f_{\\bm{\\Theta}}$ for node $v_i$, where only the identified edges for explanation are preserved in the corresponding computation graph.\n% Here only the identified edges for explanation are preserved in the corresponding computation graph to derive $\\tilde{Y}_{i}$ based on $f_{\\bm{\\Theta}}$.\n%\n$\\mathds{1}(\\cdot)$ is the indicator function, which returns 1 if $\\hat{Y}_{i}=\\tilde{Y}_{i}$ and 0 otherwise.\n% \\yu{Here is the only difference between two predictions are the edge set we sent to the model? Say the GNN is trained using the original graph and then we send the obtained subgraph explanation to the model and get another prediction}\n%\nFinally, for \\textit{Debiasing GNNs with Explanation}, we utilize two traditional fairness metrics $\\Delta_{SP}$ and $\\Delta_{EO}$ to quantitatively evaluate how much the predictions of a GNN are debiased in terms of the whole population. Here $\\Delta_{SP}$ and $\\Delta_{EO}$ measure the positive prediction rate difference between two sensitive subgroups over all nodes and nodes with only positive class labels, respectively. Additionally, we use node classification accuracy to evaluate the GNN utility.\n\n\n\n% \\noindent \\textbf{Implementation Details.}\n% The proposed framework REFEREE is implemented in PyTorch~\\cite{paszke2017automatic} and optimized via Adam optimizer~\\cite{kingma2014adam}.\n% %\n% More details are introduced in the Appendix.\n\n\n\n% Specifically, the loss weight for the Wasserstein distance loss is 10000. The weight of the JS-divergence is $10^{20}$. For evaluation of the outcome, the top 500 edges with the largest weights are selected. The budget is set as 50. For the debiasing experiments, the top 200 edges with the largest weights of each node explanation are incorporated into the edge set to be removed.The learning rate is set as 0.1 with 200 epochs. The hidden size and the output size are both 20 for all three-layer GNNs. The Adam optimizer is utilized.\n% REFEREE的settings也加在这里\n% Jensen–Shannon divergence (JSD) is leveraged to measure the difference of the identified edge distributions between $\\mathcal{\\tilde{G}}_i'$ and $\\mathcal{\\tilde{G}}_i$, as its boundedness empirically makes the optimization process more stable.\n\n\n\n\n\n\n\\vspace{-1.8ex}\n% instead of using B!!\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Effectiveness of Explaining Bias (Fairness)",
                    "content": "\n\\vspace{-0.3em}\n\n\n\n\nTo answer \\textbf{RQ1}, we compare our proposed framework REFEREE with other baselines to evaluate the effectiveness of explaining bias (fairness). Here we adopt the widely used model GAT as the trained GNN for experiments, and similar results can be observed based on other GNNs. Specifically, we first randomly sample 50 nodes to be explained (i.e., $M=50$). \n%\nThen for each node, we obtain the two predictions of the given GNN $f_{\\bm{\\Theta}}$ based on the computation graph corresponding to each of the two identified edge sets given by the two explainers.\n%\nFor obtained predictions, the normalized average value of how much the node-level bias $B_i$ is promoted (given by Bias Explainer) or reduced (given by Fairness Explainer) compared with the vanilla $B_i$ based on the complete computation graph is presented in Table~\\ref{edge_identification}.\n%\nFor both promotion and reduction of $B_i$, a larger value indicates better results, as more biased or fairer node-level outcome can be obtained based on the identified structural explanation.\n%\nWe make the following observations from Table~\\ref{edge_identification}:\n%\n\\begin{itemize}[topsep=0pt]\n    \\item Stable promotion and reduction of node-level bias is observed in all GNN explanation approaches. This indicates that the Wasserstein distance-based objective functions formulated in Eq.~(\\ref{ws_distance}) and Eq.~(\\ref{ws_distance2}) effectively help to identify edges that account for the exhibited node-level bias and edges whose existence can alleviate the exhibited node-level bias.\n    \\item Existing GNN explanation models (e.g., GNN Explainer and PGExplainer) do not show any superior performance over other straightforward GNN explanation approaches such as Att and Grad. This observation implies that for these representative GNN explanation approaches, simply adding a constraint to explain bias (fairness) at the instance level only achieves limited effectiveness.\n    %\n    % This observation implies that straightforward approaches to achieve the bias (fairness)-related goal for explanation such as only adding a constraint to identify edges that account for the exhibited bias and whose existence can alleviate the exhibited node-level bias can only achieve limited effectiveness on edge identification even for those state-of-the-art GNN explanation approaches.\n    %\n    % We conjecture that this is partly because they all tend to converge to a similar local optimal point from the perspective of biased or fair edge identification given any explained node. % GNN explanation model compared to att : no obvious adv\n    \\item Among all GNN explanation approaches, REFEREE yields the structural explanations that lead to the highest promotion and reduction of node-level bias in all datasets. Based on such observations, we argue that REFEREE achieves the best performance over other alternatives on identifying edges that account for the exhibited bias and whose existence can alleviate the exhibited node-level bias for the prediction. % REFEREE: better\n    % \\yu{Maybe provide some reasons as why the proposed REFEREE achieves more reduction or promotion here is better since for other baselines, we also add Eq.(1) or (3) as the optimization objective, does it mean the other two losses $\\mathbb{L}_3, \\mathbb{L}_4$ terms are very critical here since we don't add them to other baselines?}\n\\end{itemize}\n\n\n\n\n\n\n\n\\vspace{-2ex}\n%\\vspace{-0.7em}\n"
                },
                "subsection 4.3": {
                    "name": "Explanation Fidelity",
                    "content": "\n\\vspace{-0.3em}\n\n\n\nWe then answer \\textbf{RQ2} in this subsection. Generally, it is necessary to ensure that the structural explanation results given by both explainers in REFEREE are faithful to the given trained GNN, i.e., the identified edge sets should encode critical information for the given GNN predictions. More specifically, in our experiments, the predicted labels given by the GNN model based on the computation graph with the identified edge sets should be the same as those based on the vanilla computation graph.\n%\n% structural explanation results given by any explanation model cannot reflect the critical edges for the given GNN, then such explanation result should be regarded as being with poor fidelity. \n%\nTo evaluate how well the proposed framework can maintain faithfulness when it is generalized to different GNNs, here we choose three widely used GNNs, namely GCN~\\cite{DBLP:conf/iclr/KipfW17}, GAT~\\cite{velivckovic2017graph}, and GIN~\\cite{DBLP:conf/iclr/XuHLJ19} for explanation.\n%\nFidelity is adopted as the metric for evaluation. Intuitively, fidelity measures to what proportion the predicted labels based on the identified explanation are maintained to be the same as the vanilla ones.\n%\n% This is because for any node, if some critical edges are ignored by the explanation result, the predicted label given by the GNN based on the obtained explanation tends to be different from the vanilla prediction.\n%\nHere we adopt the GNN Explainer as the backbone of the two explainers in REFEREE.\n%\nFor both the baseline model and our proposed framework, we train and make predictions five times separately for 50 randomly selected nodes. We present the performance comparison between the two explainers in our framework and vanilla GNN Explainer on the average performance of fidelity in Table~\\ref{utility_table}.\n% \\yu{Regarding sampling nodes to report the result, do we train separately 50 times our explainer since each time we optimize one computational graph. Also, are the random sampled nodes the same across all different baselines? I think pointing this out would be more fair to compare different baselines.}\n%\nWe can make the observation that both Bias Explainer and Fairness Explainer achieve comparable performance on fidelity with the vanilla GNN Explainer across different datasets and GNNs. Consequently, we argue that the explanation given by REFEREE maintains faithfulness to the GNN predictions.\n%  both Bias Explainer and Fairness Explainer\n% \\begin{itemize}[topsep=0pt]\n%     \\item For both Bias Explainer and Fairness Explainer, they achieve comparable performance on fidelity with the vanilla GNN explanation model across three real-world datasets in different GNNs. Consequently, we argue that the explanation given by REFEREE maintains faithfulness to the prediction given by the trained GNNs.\n%     \\item We observe that better performance on explanation fidelity is mostly achieved by either Bias Explainer or Fairness Explainer in three real-world datasets compared with vanilla GNN Explainer model. We conjecture that this is partly due to that minimizing or maximizing the Wasserstein distance based loss term (Eq.~(\\ref{ws_distance}) or Eq.~(\\ref{ws_distance2})) implies prior knowledge on the prediction from the trained GNN. This potentially helps the prediction based on the explanation maintain the vanilla GNN prediction. \\yu{The last sentence here is a little bit unclear here.}\n% \\end{itemize}\n\n\n\n% \\begin{figure*}[!t]\n% \t\\centering \n% \t\\vspace{-0.6cm}\n%     \\subfloat[Delta statistical parity tendency]{\n%         \\includegraphics[width=0.3\\textwidth]{sp-1.pdf}\n%     }\n%         \\subfloat[Delta equal opportunity tendency]{\n%         \\includegraphics[width=0.3\\textwidth]{eo-1.pdf}\n%     }\n%     \t\\subfloat[Accuracy performance tendency]{\n%         \\includegraphics[width=0.3\\textwidth]{acc-1.pdf}\n%     } \n%     \t\t\\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.}\n% \t\\vspace{-5mm}\n% \t\\label{tendency}\n% \\end{figure*}\n\n\n\n\n\n\n\n% \\begin{table*}[]\n% \\caption{Explanation utility evaluation based on the five datasets. GNN Explainer is adopted as the explanation backbone model. Vanilla denotes the explanation results given by the vanilla GNN Explainer. B. Explainer and F. Explainer represent the Bias Explainer and Fairness Explainer, respectively.}\n% \\label{utility_table}\n% \\centering\n% % \\small\n% \\begin{tabular}{ccccccccccccc}\n% \\hline\n%                         &       & \\multicolumn{3}{c}{\\textbf{GCN}}               &  & \\multicolumn{3}{c}{\\textbf{GAT}}               &  & \\multicolumn{3}{c}{\\textbf{GIN}}               \\\\\n% \\cline{3-5}   \\cline{7-9}  \\cline{11-13} \n%                         &       & \\textbf{Vanilla} & \\textbf{B. EPN} & \\textbf{F. EPN} &  & \\textbf{Vanilla} & \\textbf{B. EPN} & \\textbf{F. EPN} &  & \\textbf{Vanilla} & \\textbf{B. EPN} & \\textbf{F. EPN} \\\\\n%                         \\hline\n% \\multirow{2}{*}{\\textbf{German}} & \\textbf{Prom.}  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         & \\textbf{Fid. (R)}          & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         \\hline\n% \\multirow{2}{*}{\\textbf{Credit}} & \\textbf{Prom.}  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         & \\textbf{Fid. (R)}          & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         \\hline\n% \\multirow{2}{*}{\\textbf{Bail}}   & \\textbf{Prom.}  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         & \\textbf{Fid. (R)}          & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         \\hline\n% \\multirow{2}{*}{\\textbf{SYN1}}   & \\textbf{Fid.}  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         & \\textbf{Spar.}          & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         \\hline\n% \\multirow{2}{*}{\\textbf{SYN2}}   & \\textbf{Fid.}  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         & \\textbf{Spar.}          & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x \\\\\n%                         \\hline\n% \\end{tabular}\n% \\end{table*}\n\n\n\n\n% \\begin{table*}[]\n% \\caption{Explanation utility evaluation based on the five datasets. GNN Explainer is adopted as the explanation backbone model. Vanilla denotes the explanation results given by the vanilla GNN Explainer. B. Explainer and F. Explainer represent the Bias Explainer and Fairness Explainer, respectively.}\n% \\label{utility_table}\n% \\centering\n% \\small\n% \\begin{tabular}{cccccccccccccccc}\n% \\hline\n% &      & \\multicolumn{2}{c}{\\textbf{German Credit}} &  & \\multicolumn{2}{c}{\\textbf{Recidivism}} &  & \\multicolumn{2}{c}{\\textbf{Credit Defaulter}} &  & \\multicolumn{2}{c}{\\textbf{SYN-Community}} &  & \\multicolumn{2}{c}{\\textbf{SYN-Mixture}} \\\\\n% \\cline{3-4}   \\cline{6-7}  \\cline{9-10}  \\cline{12-13}  \\cline{15-16}  \n% \\begin{tabular}[c]{@{}c@{}}\\textbf{BB GNNs}\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}\\textbf{Module}\\end{tabular} & \\textbf{Fid.}     & \\textbf{Spar.}    &  & \\textbf{Fid.}     & \\textbf{Spar.}   &  & \\textbf{Fid.}     & \\textbf{Spar.}    &  & \\textbf{Fid.}     & \\textbf{Spar.}        &  & \\textbf{Fid.}     & \\textbf{Spar.}       \\\\\n% \\hline\n% \\multirow{3}{*}{\\textbf{GCN}}                          & \\textbf{Vanilla}              &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x & &              &             &  &                 &                 &  &                &                \\\\\n%                                                       & \\textbf{B. Explainer}         &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n%                                                       & \\textbf{F. Explainer}         &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n% \\hline\n% \\multirow{3}{*}{\\textbf{GAT}}                          & \\textbf{Vanilla}              &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n%                                                       & \\textbf{B. Explainer}         &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n%                                                       & \\textbf{F. Explainer}         &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n% \\hline\n% \\multirow{3}{*}{\\textbf{GIN}}                          & \\textbf{Vanilla}              &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n%                                                       & \\textbf{B. Explainer}         &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &                \\\\\n%                                                       & \\textbf{F. Explainer}         &   xx.x $\\pm$ x.x    & xx.x $\\pm$ x.x &  & xx.x $\\pm$ x.x & xx.x $\\pm$ x.x &  &              &             &  &                 &                 &  &                &               \\\\\n% \\hline\n% \\end{tabular}\n% \\end{table*}\n\n\\vspace{-2ex}\n%\\vspace{-0.3em}\n"
                },
                "subsection 4.4": {
                    "name": "Debiasing GNNs with Explanations",
                    "content": "\n\\label{debiasing_exp}\n\n\\vspace{-0.3em}\n\nIn this subsection, our goal is to answer \\textbf{RQ3} to study how the instance-level explanations given by REFEREE help with GNN debiasing in terms of the whole population.\n%\nA straightforward approach here is to first identify the edges that tend to introduce bias in the outcome of GNNs for some randomly sampled nodes, then remove such bias-introducing edges and input the network data into the GNN model to obtain less biased predictions.\n%\nNevertheless, the edges involved in the bias structural explanation (given by Bias Explainer) cannot be directly removed as a whole, as some edges could be critical for the GNN prediction of the explained node.\n%\nBesides, it is neither reasonable to only preserve the edges whose existence can maximally alleviate the node-level bias, as some removed non-critical edges for the explained node could be vital for the prediction of other nodes.\n% Nevertheless, on the one hand, the edges involved in the bias structural explanation cannot be directly removed, as some edges could be critical for GNN predictions; on the other hand, it is neither reasonable to only preserve the edges that contribute to fairness, as some non-critical biased edges for the explained node could be vital for the prediction of other nodes.\n%\nHere we adopt an alternative strategy to study how the explanations help with GNN debiasing in terms of the whole population.\n%\nSpecifically, for those baseline explanation models, we randomly sample a subset of nodes for explanation. For each node, baselines are trained to learn structural explanations towards more biased and fairer predictions independently. Then edges that appear in the bias explanation but not in the fairness explanation are removed from the original input network.\n%\nThe intuition here is that if an edge only appears in the edge set that maximally accounts for the exhibited bias but not in the edge set whose existence can maximally alleviate the node-level bias of the prediction, such edge can be regarded as being more critical to the exhibited bias instead of being more critical to an accurate and fair prediction. Therefore, removing edges bearing such property has the potential to reduce the exhibited bias while maintaining the utility (i.e., yielding accurate and fair predictions) of the GNN.\n% \\yu{A small question here is would it be possible that the edge appears in both of these two explanations for the same center node? If so, how to explain this?}\n%\nCorrespondingly, for our proposed framework REFEREE, we also randomly sample nodes and remove edges that appear in the explanation given by Bias Explainer but not in the explanation from Fairness Explainer, i.e., removing edges in set $\\mathcal{\\tilde{E}}_i \\backslash \\mathcal{\\tilde{E}}_i'$.\n%\nIn this way, edges are removed from the input network data towards the goal of debiasing the GNN and maintaining its usability at the same time.\n%\nIt is worth mentioning that such an edge removal strategy does not necessarily lead to graph structure modifications that are globally optimal for debiasing.\n%\nHowever, if fairer GNNs can be achieved via removing edges that exhibit node-level bias defined in Definition~\\ref{fair_def}, the consistency between Definition~\\ref{fair_def} and traditional fairness notions can be validated, i.e., reducing the node-level bias also helps to promote the overall fairness level of the GNN predictions in terms of traditional fairness metrics.\n\n\nWe adopt GAT as the explained GNN model here, and similar observations can also be found based on other GNNs. We vary the random sampling ratio of the number of explained nodes over the number of all nodes among \\{0\\%, 5\\%, 10\\%, 15\\%, 20\\%\\}. The changes of node classification accuracy, $\\Delta_{SP}$, and $\\Delta_{EO}$ w.r.t. the sampled node ratio on German dataset is presented in Fig.~\\ref{tendency}.\n%\n% Specifically, we vary the random sampling ratio of the number of explained nodes over the number of all nodes among \\{0\\%, 5\\%, 10\\%, 15\\%, 20\\%\\}. GAT is adopted as the explained GNN model here, and similar observations can also be found based on other GNNs. The tendency of node classification accuracy, $\\Delta_{SP}$, and $\\Delta_{EO}$ w.r.t. the sampled node ratio on German dataset is presented in Fig.~\\ref{tendency}.\n%\nWe make the following observations:\n%\n(1) With more nodes being sampled and more edges that only appear in the bias explanations being removed, both $\\Delta_{SP}$ and $\\Delta_{EO}$ reduce significantly. This verifies that removing the edges that account for the node-level bias generally alleviates the exhibited bias in terms of the whole population. Besides, the reduction of both $\\Delta_{SP}$ and $\\Delta_{EO}$ also validates the consistency between traditional fairness notions and node-level bias given in Definition~\\ref{fair_def}. (2) Removing the edges that only appear in the bias explanations generally reduces the GAT prediction accuracy. We argue that it is because some edges that lead to more biased results could also be critical for accurate predictions. However, the accuracy reduction is within an acceptable range. (3) Compared with other baseline approaches, REFEREE leads to limited accuracy reduction but achieves a more significant reduction on $\\Delta_{SP}$ and $\\Delta_{EO}$. Such observation indicates that a fairer GNN is achieved (in terms of traditional fairness notions) based on the explanations identified by REFEREE compared with the explanations given by other alternatives. Considering our baselines also bear constraints for fidelity and explaining bias(fairness), it is safe to attribute such superiority to the designed contrastive mechanism of REFEREE. \n%\nConsequently, we argue that REFEREE outperforms baselines in helping achieve fairer GNNs in terms of traditional fairness notions.\n\n\n% \\begin{itemize}[topsep=0pt]\n%     \\item With more nodes being sampled and more edges that only appear in the bias explanations being removed, both $\\Delta_{SP}$ and $\\Delta_{EO}$ reduce significantly. This verifies that removing the edges that account for the node-level bias generally alleviates the exhibited bias in terms of the whole population. Besides, the reduction of both $\\Delta_{SP}$ and $\\Delta_{EO}$ also validates the consistency between traditional fairness notions and node-level bias given in Definition~\\ref{fair_def}.\n%     \\item Removing the edges that only appear in the bias explanations generally reduces the GAT prediction accuracy. We argue that it is because some edges that lead to more biased results could also be critical for accurate predictions. However, the accuracy reduction is within an acceptable range.\n%     \\item Compared with other baseline approaches, REFEREE leads to limited accuracy reduction but achieves a more significant reduction on $\\Delta_{SP}$ and $\\Delta_{EO}$. Such observation indicates that a fairer GNN is achieved (in terms of traditional fairness notions) based on the explanations identified by REFEREE compared with the explanations given by other alternatives. Hence we argue that REFEREE outperforms the alternatives on giving explanations to help achieve fairer GNNs (in terms of traditional fairness notions) for the whole population. \n% \\end{itemize}\n\n% First, removing the edges that only appear in the bias explanations generally reduces the GAT prediction accuracy. We argue that it is because some edges that lead to more biased results could also be critical for accurate predictions. However, the accuracy reduction is within an acceptable range.\n\n% Second, with more nodes being sampled and more edges that only appear in the bias explanations being removed, both $\\Delta_{SP}$ and $\\Delta_{EO}$ reduce significantly. This verifies that removing the edges that account for the node-level bias generally alleviates the bias level of the GNN predictions over the whole population. Besides, the reduction of both $\\Delta_{SP}$ and $\\Delta_{EO}$ also validates the consistency between traditional fairness notions and node-level bias given in Definition~\\ref{fair_def}.\n\n% Third, Compared with other baseline approaches, REFEREE leads to limited accuracy reduction but achieves more significant reduction on $\\Delta_{SP}$ and $\\Delta_{EO}$. Such observation indicates that a fairer GNN is achieved (in terms of traditional fairness notions) based on the explanations identified by REFEREE compared with the explanations given by other alternatives. Hence we argue that REFEREE outperforms the alternatives on giving explanations to help achieve fairer GNNs (in terms of traditional fairness notions) for the whole population. \n\n% %%%% Accuracy data, change file name correspondingly\n% \\begin{filecontents*}{mydata1.csv}\n%     a,    GE-REFEREE, GE,  att, grad, PGE-REFEREE, PGE,  \n%     0, 77, 77, 77, 77, 77, 77, \n%     5 , 75.81,76.20,76.19,76.05,75.93,75.48\n%     10 , 76.38,74.99,74.93,75.24,74.89,75.77\n%     15 , 75.80,74.78,74.41,75.71,74.83,74.17\n%     20 , 74.35,73.24,73.98,73.84,73.55,73.38\n% \\end{filecontents*}\n\n\n% %%%% Delta SP data, change file name correspondingly\n% \\begin{filecontents*}{mydata2.csv}\n%     a,    GE-REFEREE, GE,  att, grad, PGE-REFEREE, PGE,  \n%     0, 17.89, 17.89, 17.89, 17.89, 17.89, 17.89,\n%     5, 15.48, 16.89, 17.86, 16.97, 16.37, 17.23,\n%     10, 11.03, 16.09, 15.63, 14.86, 13.39, 13.28,\n%     15,6.85, 12.85, 14.86, 13.59, 10.04, 12.09, \n%     20,4.14, 9.09, 10.05, 10.70, 7.57, 9.23, \n% \\end{filecontents*}\n\n\n\n% %%%% Delta EO data, change file name correspondingly\n% \\begin{filecontents*}{mydata.csv}\n%     a,    GE-REFEREE, GE,  att, grad, PGE-REFEREE, PGE\n%     0, 10.68, 10.68, 10.68,10.68, 10.68, 10.68\n%     5, 06.83, 07.04, 07.74, 08.95, 07.19, 07.79\n%     10, 05.41, 06.19, 07.46, 08.00, 06.78, 06.75\n%     15,03.40, 05.66, 06.13, 07.12, 04.56, 06.28\n%     20,02.09, 05.38, 04.48, 04.55, 02.93, 04.73\n% \\end{filecontents*}\n\n\n\n\n% \\begin{figure*}[!t]\n% \\vspace{-4mm}\n% \\centering  \n% \\subfloat[Accuracy performance tendency]  \n% {  \n% \\begin{tikzpicture}[scale=0.7]\n%     \\pgfplotsset{every axis legend/.append style={\n%     at={(0.5,1.03)},\n%     anchor=south},every axis y label/.append style={at={(0.07,0.5)}}}\n%     \\begin{axis}[title=(a) Mean STD for ,xlabel=Sampled Node Ratio (\\%),\n%         ylabel=Accuracy (\\%),xtick =data,legend columns=3,legend style={font=\\large},font=\\large,width=8cm, height=5cm]\n%         \\addplot table [x=a, y=att, col sep=comma] {mydata1.csv};  % +[smooth]\n%         \\addplot table [x=a, y=GE, col sep=comma] {mydata1.csv};\n%         \\addplot table [x=a, y=GE-REFEREE, col sep=comma] {mydata1.csv};\n%         \\addplot table [x=a, y=grad, col sep=comma] {mydata1.csv};\n%         \\addplot table [x=a, y=PGE, col sep=comma] {mydata1.csv};\n%         \\addplot table [x=a, y=PGE-REFEREE, col sep=comma] {mydata1.csv};\n%     \\legend{Att,GE,GE-REFEREE,Grand,PGE,PGE-REFEREE}\n%     \\end{axis}\n%     % \\node at (3,-1.3){Fall name of the first subpicture};\n%     \\end{tikzpicture}\n% }  \n% \\hspace{3mm}\n% \\subfloat[Delta statistical parity tendency]\n% {  \n%     \\begin{tikzpicture}[scale=0.7]\n%     \\pgfplotsset{every axis legend/.append style={\n%     at={(0.5,1.03)},\n%     anchor=south},\n%     every axis y label/.append style={at={(0.07,0.5)}}}\n%     \\begin{axis}[xlabel=Sampled Node Ratio (\\%),\n%         ylabel=$\\Delta_{SP}$ (\\%),xtick =data,legend columns=3,legend style={font=\\large},font=\\large,width=8cm, height=5cm]\n%         \\addplot table [x=a, y=att, col sep=comma] {mydata2.csv};\n%         \\addplot table [x=a, y=GE, col sep=comma] {mydata2.csv};\n%         \\addplot table [x=a, y=GE-REFEREE, col sep=comma] {mydata2.csv};\n%         \\addplot table [x=a, y=grad, col sep=comma] {mydata2.csv};\n%         \\addplot table [x=a, y=PGE, col sep=comma] {mydata2.csv};\n%         \\addplot table [x=a, y=PGE-REFEREE, col sep=comma] {mydata2.csv};\n%     \\legend{Att,GE,GE-REFEREE,Grand,PGE,PGE-REFEREE}\n%     \\end{axis}\n%     % \\node at (3,-1.3){Fall name of the second subpicture};\n%     \\end{tikzpicture}\n\n% }\n% \\hspace{3mm}\n% \\subfloat[Delta equal opportunity tendency]\n% {  \n% \\begin{tikzpicture}[scale=0.7]\n%     \\pgfplotsset{every axis legend/.append style={\n%     at={(0.5,1.03)},\n%     anchor=south},every axis y label/.append style={at={(0.07,0.5)}}}\n%     \\begin{axis}[title=(a) Mean STD for ,xlabel=Sampled Node Ratio (\\%),\n%         ylabel=$\\Delta_{EO}$ (\\%),xtick =data,legend columns=3,legend style={font=\\normalfont},font=\\large,width=8cm, height=5cm]\n%         \\addplot table [x=a, y=att, col sep=comma] {mydata.csv};\n%         \\addplot table [x=a, y=GE, col sep=comma] {mydata.csv};\n%         \\addplot table [x=a, y=GE-REFEREE, col sep=comma] {mydata.csv};\n%         \\addplot table [x=a, y=grad, col sep=comma] {mydata.csv};\n%         \\addplot table [x=a, y=PGE, col sep=comma] {mydata.csv};\n%         \\addplot table [x=a, y=PGE-REFEREE, col sep=comma] {mydata.csv};\n%     \\legend{Att,GE,GE-REFEREE,Grand,PGE,PGE-REFEREE}\n%     \\end{axis}\n%     % \\node at (3,-1.3){Fall name of the first subpicture};\n%     \\end{tikzpicture}\n% }  \n% \\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.}\n% \\vspace{-2mm}\n% \\label{tendency}\n% \\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n% \\begin{figure}\n%     \\centering\n% \\begin{tikzpicture}\n%     \\pgfplotsset{every axis legend/.append style={\n%     at={(0.5,1.03)},\n%     anchor=south},every axis y label/.append style={at={(0.07,0.5)}}}\n%     \\begin{axis}[title=(a) Mean STD for ,xlabel=Num of Xlable,\n%         ylabel=Name of Ylable,xtick =data,legend columns=4,legend style={font=\\tiny},font=\\footnotesize,width=8cm]\n%     \\addplot table [x=a, y=Type-1,, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-2, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-3, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-4, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-5, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-6, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-7, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Type-8, col sep=comma] {mydata.csv};\n%     \\legend{Type-1, Type-2, Type-3, Type-4, Item-1, Item-2, Item-3, Item-4}\n%     \\end{axis}\n%     % \\node at (3,-1.3){Fall name of the first subpicture};\n%     \\end{tikzpicture}\n%     \\caption{1111111}\n%     \\label{11}\n% \\end{figure}\n\n\n    \n%     \\begin{tikzpicture}\n%     \\pgfplotsset{every axis legend/.append style={\n%     at={(0.5,1.03)},\n%     anchor=south},\n%     every axis y label/.append style={at={(0.07,0.5)}}}\n%     \\begin{axis}[xlabel=Num of Xlable,\n%         ylabel=Name of Ylable,xtick =data,legend columns=4,legend style={font=\\tiny},font=\\footnotesize,width=8cm]\n%     \\addplot table [x=a, y=Item-1,, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-2, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-3, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-4, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-5, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-6, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-7, col sep=comma] {mydata.csv};\n%     \\addplot table [x=a, y=Item-8, col sep=comma] {mydata.csv};\n%     \\legend{Type-1, Type-2, Type-3, Type-4, Item-1, Item-2, Item-3, Item-4}\n%     \\end{axis}\n%     \\node at (3,-1.3){Fall name of the second subpicture};\n%     \\end{tikzpicture}\n\n\n\n\n\n\n% \\subsection{Ablation Study}\n\n\n\n\n\n\n\n\\vspace{-0.9em}\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\\vspace{-0.3em}\n\n\n\n\n\n\\noindent \\textbf{Explanation of GNNs.}\n%\nGenerally, existing GNN explanation approaches can be divided into data-level approaches and model-level ones~\\cite{yuan2020explainability}. \n%\nFor data-level approaches, the explanation models identify critical components in the input network data of GNNs, e.g., node features or edges.\n%\nFor example, squared gradient values are regarded as the importance scores of different input features in the node classification task~\\cite{baldassarre2019explainability};\n%\ninterpretable surrogate models are leveraged to approximate the prediction of a certain GNN model, where the explanations from the surrogate model can be regarded as the explanation for the corresponding GNN prediction~\\cite{huang2020graphlime,vu2020pgm}.\n%\nAnother popular approach to identify important components of the input network data is to make perturbations on the input network, then observe the corresponding change in the output. The basic rationale is that if small perturbations lead to dramatic changes in the GNN prediction, then what has been perturbed is regarded as critical for the GNN prediction~\\cite{ying2019gnnexplainer,luo2020parameterized,yuan2021explainability,schlichtkrull2020interpreting,wang2020causal}.\n%\n% Different from the explanation models focusing on the data level, the goal of this approach is to understand the general insights of GNNs~\\cite{yuan2020explainability}.\nDespite its significance, this is a less studied topic. To provide model-level explanations for GNNs, graph generation can be leveraged to maximize the prediction of a GNN regarding a specific prediction (e.g., the probability of a class in graph classification)~\\cite{yuan2020xgnn}. If the prediction probability of GNN regarding a specific prediction result can be maximized, then the generated input graph can be regarded as the explanation for this GNN that includes critical graph patterns.\n%\nDifferent from the existing GNN explanation approaches, our proposed framework REFEREE not only explores critical edges for GNN predictions, but also identifies their contribution to the bias in GNNs.\n%\nHence, REFEREE is able to provide explanations for bias in GNNs, which helps understand how bias arises. This is with significance for GNN deployment in decision-critical scenarios and potentially facilitates the development of fairer GNNs.\n\n\n\n\\noindent \\textbf{Fairness of GNNs.}\n%\nWith the increasing societal concerns on the fairness of GNNs~\\cite{wang2019semi}, explorations have been made to alleviate the bias exhibited in GNNs.\n%\nGenerally, existing works focus either on \\textit{group fairness}~\\cite{dwork2012fairness} or \\textit{individual fairness}~\\cite{zemel2013learning}. Group fairness requires that GNNs should not yield biased predictions against any specific demographic subgroups~\\cite{mehrabi2021survey}.\n%\nAmong existing works, promoting group fairness through adversarial learning is one of the most popular GNN debiasing approaches~\\cite{dai2021say}.\n%\nIts goal is to train a discriminator to identify the sensitive information from the learned node embeddings. When the discriminator can barely distinguish the sensitive feature given any learned embedding, the sensitive feature can be regarded as being decoupled from the learned embeddings.\n%\nAdditionally, GNN debiasing can also be performed based on the input network data. For example, the network structure can be modified such that nodes in different demographic subgroups bear similar distributions on their neighbor node attribute values~\\cite{dong2021edits}.\n%\n% A similar problem of debiasing network structure for GNNs is also studied in the link prediction task, where the distinction of edge existence expectation between intra-group node pairs and inter-group ones is adopted as a regularization term for optimization~\\cite{DBLP:conf/iclr/LiWZHL21}.\n%\nMoreover, edge dropout~\\cite{spinelli2021biased} is also proved to be effective in debiasing GNNs.\n%\nOn the other hand, individual fairness requires that similar individuals should be treated similarly~\\cite{zemel2013learning,kang2020inform}.\n%\nHowever, promoting individual fairness for GNNs remains under-explored.\n%\nTo the best of our knowledge, the only approach to fulfill such a goal is developed from a ranking perspective~\\cite{dong2021individual}.\n\n% , where the two similarity rankings between any individual and others in the input and output space are encouraged to be similar.\n\n\n% . Specifically, for each node, two similarity rankings are derived based on the similarity matrix given by domain experts and the one given by the GNN prediction. Then individual fairness is enforced through minimizing the summation of the difference for all ranking lists.\n\n\n\n% \\begin{figure}[!t]\n% \t\\centering \n% \t\\vspace{-0.3cm}\n%     % \\subfloat[Delta statistical parity tendency]{\n%     %     \\includegraphics[width=0.45\\textwidth]{delta-b-1.pdf}\n%     % }\n%     \\subfloat[Delta statistical parity tendency]{\n%         \\includegraphics[width=0.45\\textwidth]{trail-1.pdf}\n%     }\n\n%     \t\t\\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.}\n% \t\\vspace{-2mm}\n% \t\\label{tendency}\n% \\end{figure}\n\n\n\n\n\\vspace{-0.5em}\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\vspace{-0.3em}\n\nIn this paper, we focus on a novel problem of structural explanation of node-level bias in GNNs. Specifically, we first propose to model node-level bias quantitatively, and then develop a principled post-hoc explanation framework named REFEREE with two different explainers: the bias explainer and the fairness explainer. \n%\nConditional on being faithful to the given GNN prediction, the two explainers aim to identify structural explanations that maximally account for the exhibited bias and that maximally contribute to the fairness level of the GNN prediction.\n%\nExperiments on real-world network datasets demonstrate the effectiveness of REFEREE in identifying edges that maximally account for the exhibited node-level bias and edges whose existence can maximally alleviate the node-level bias for any given GNN prediction.\n%\nFurthermore, REFEREE also shows superior performance over baselines on helping debias GNNs.\n\n% the explanations given by REFEREE also show superior performance over that given by other existing GNN explanation models.\n\n\n\n% we also explore how the explanation results can be leveraged to reduce the level of bias for the GNN outcome over the whole population, where the explanations given by REFEREE also show superior performance over that given by other existing GNN explanation models.\n\n\n\n\n\\vspace{-0.5em}\n"
            },
            "section 7": {
                "name": "ACKNOWLEDGMENTS",
                "content": "\n\\vspace{-0.3em}\nThis material is supported by the National Science Foundation (NSF) under grants No. 2006844 and the Cisco Faculty Research Award.\n\n\\vspace{-0.5em}\n\n\n% Biased input network structure has been proved to be a significant source of bias in GNNs.\n% %\n% However, to the best of our knowledge, no existing work provides understanding on how the bias exhibited in the structure of the GNN input network data lead to biased GNN predictions.\n% %\n% In fact, it is important to understand where the bias comes from for the safe deployment of GNNs in high-stake scenarios. Therefore, in this paper, we focus on a novel problem of structural explanation of node-level bias in GNNs. To tackle the problem, we first propose to quantitatively model node-level bias and then develop a principled post-hoc explanation framework named REFEREE with two different explainers: the bias explainer and the fairness explainer. \n% %\n% Conditional on being faithful to the given GNN prediction, the two explainers aim to identify structural explanations that maximally account for the exhibited bias and that maximally contribute to the fairness level of the GNN prediction.\n% %\n% % A principled framework named REFEREE is proposed to identify the edges that contribute to the bias and fairness of the GNN outcome for any explained node. \n% % can provide explanations for different GNNs\n% % REFEREE is able to provide bias and fairness explanations for different GNNs.\n% %\n% Extensive experiments on multiple real-world network datasets demonstrate the effectiveness of REFEREE on identifying edges that maximally account for the exhibited node-level bias and edges whose existence can maximally alleviate the node-level bias for any prediction.\n% %\n% % in the aspect of providing accurate and faithful instance-level structural explanation for the bias exhibited in GNNs.\n% %\n% Furthermore, we also explore how the explanation results can be leveraged to reduce the bias level of the GNN outcome for the whole population, where the explanation results given by REFEREE also shows superior performance over that given by other existing GNN explanation models.\n\n\n\n\n\n\n% \\clearpage\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference}\n% \\bibliography{sample-base}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\clearpage\n% \\linespread{1.25}\n% \\linespread{1.2}\n\\appendix\n"
            },
            "section 8": {
                "name": "Appendix",
                "content": "\n\n% \\subsection{Implementation Details}\n% REFEREE is implemented in PyTorch and optimized through Adam optimizer.\n\n\n\n\n\n\n\n\n\n",
                "subsection 8.1": {
                    "name": "Reproducibility",
                    "content": "\n\n\n\nIn this section, we focus on the reproducibility of our experiments as a supplement of Section~\\ref{settings}. More specifically, we first present a detailed review of the three real-world datasets adopted in our experiments. Then we introduce the experimental settings, followed by the implementation details of our proposed framework REFEREE, GNNs, and baseline models. We finally present some key packages with the corresponding versions for our implementations.\n\n\n\n\n",
                    "subsubsection 8.1.1": {
                        "name": "Real-World Datasets.",
                        "content": "\n% \\noindent \\textbf{Real-World Datasets.} \nThree real-world attributed network datasets are adopted in the experiments of this paper, namely German Credit, Recidivism, and Credit Defaulter~\\cite{agarwal2021towards}. We present the statistics of these three datasets in Table~\\ref{datasets}. Some detailed information corresponding to the three datasets is introduced as follows.\n%\n\\begin{itemize} %[topsep=0pt]\n    \\item \\textbf{German Credit.} In German Credit dataset, nodes and edges represent the bank clients and the connections between client accounts, respectively. Here the gender of the bank clients is regarded as the sensitive feature, and the task is to classify if the credit risk of each client is high or not.\n    \n    \\item \\textbf{Recidivism.} In Recidivism, nodes represent defendants that were released on bail from the year 1990 to the year 2009, and edges represent the connection between defendants based on their past criminal records. Race of the defendants is considered as the sensitive feature, and the task is to classify if a certain defendant deserves bail. A positive bail decision indicates that the corresponding defendant is unlikely to commit a crime if released.\n    \n    \\item \\textbf{Credit Defaulter.} In Credit Defaulter, each node represents a credit card user, and an edge between two nodes is the connection between two credit card users. The age of the credit card users is the sensitive feature, and the task is to predict the future default of credit card payments for users.\n\\end{itemize}\n\n\n\n\n\n\n% Specifically, for German Credit dataset, nodes and edges represent the bank clients and the connections between client accounts, respectively. Here the gender of the bank clients is regarded as the sensitive feature, and the task is to classify if the credit risk of each client is high or not.\n% %\n% For Recidivism dataset, nodes represent defendants that were released on bail from the year 1990 to the year 2009, and edges represent the connection between defendants based on their past criminal records. Race of the defendants is considered as the sensitive feature, and the task is to classify if a certain defendant deserves bail. A positive bail decision indicates that the corresponding defendant is unlikely to commit a crime if released.\n% %\n% For Credit Defaulter dataset, each node represents a credit card user, and the edge between two nodes is the connection between two credit card users. Age of the credit card users is the sensitive feature, and the task is to predict the future default of credit card payment for each user.\n% %\n% We present the statistics of these three datasets in Table~\\ref{datasets}.\n\n\n"
                    },
                    "subsubsection 8.1.2": {
                        "name": "Experimental Settings.",
                        "content": "\n% \\noindent \\textbf{Experimental Settings.}\n%\nFor the three real-world datasets used in this paper, we adopt the split rate for the training set and validation set as 0.8 and 0.1, respectively. The input node features are normalized before they are fed into the GNNs and the corresponding explanation models. For the downstream task \\textit{node classification}, only the training set is available for all models during the training process. \n%\nThe trained GNN models with the best performance on the validation set are preserved for test and explanation.\n\n\n\n"
                    },
                    "subsubsection 8.1.3": {
                        "name": "Implementation of REFEREE",
                        "content": "\n% \\noindent \\textbf{Implementation of REFEREE.}\n%\nREFEREE is implemented in PyTorch~\\cite{paszke2017automatic} and optimized through Adam optimizer~\\cite{kingma2014adam}. In our experiments, the learning rate and training epoch number are set as 1e-5 and 200 for the explanation of all GNNs on all datasets.\n%\nJensen–Shannon divergence is leveraged to measure the difference of the identified edge distributions between $\\mathcal{\\tilde{G}}_i'$ and $\\mathcal{\\tilde{G}}_i$, as its boundedness empirically leads to a more stable optimization process.\n%\nThe edge budget $T$ (and $T'$) is set to be 500 and 200 for \\textit{Effectiveness of Explaining Bias (Fairness)} and \\textit{Debiasing GNNs with Explanations}, respectively.\n%\nIn our experiments, hyper-parameter $\\alpha$, $\\beta$, and $\\gamma$ are set as 1, 1e-4, and 1e-4, respectively. Numerical results for performance evaluation are based on the average of multiple runs.\n%\nOpen-source code can be found at \\href{https://github.com/yushundong/REFEREE}{https://github.com/yushundong/REFEREE}.\n\n"
                    },
                    "subsubsection 8.1.4": {
                        "name": "Implementation of Graph Neural Networks.",
                        "content": "\n% \\noindent \\textbf{Implementation of Graph Neural Networks.}\n%\nFor all explained GNNs (i.e., GCN, GAT, and GIN) in our experiments, their released implementations are utilized for a fair comparison.\n%\nThe layer number for GCN and GIN is set as 3. For GAT, we only adopt two layers due to the memory limit. The attention head number of GAT is set as one.\n%\nThe hidden size is set as 20 for all explained GNNs.\n\n\n"
                    },
                    "subsubsection 8.1.5": {
                        "name": "Implementation of Baselines.",
                        "content": "\n% \\noindent \\textbf{Implementation of Baselines.}\nFor all adopted GNN explanation baselines (i.e., Att, Grad, GNN Explainer, and PGExplainer), we also adopt their released implementations for a fair comparison. \n\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{Att and Grad.} The implementations of Att and Grad are adopted based on the implementations of~\\cite{ying2019gnnexplainer}.\n    \\item \\textbf{GNN Explainer.} For GNN Explainer, we adopt the implementations of~\\cite{ying2019gnnexplainer}. For the training of GNN Explainer, the learning rate is set as 0.001, and the weight decay rate is set as 0.005.\n    \\item \\textbf{PGExplainer.} For PGExplainer, we adopt the implementations of~\\cite{luo2020parameterized}. For the training of PGExplainer, the learning rate is set as 0.003.\n\\end{itemize}\n\n% %\n% (1) \\textbf{\\emph{Att and Grad.}}\n% The implementations of Att and Grad are adopted based on the baseline implementation of Reference~\\cite{ying2019gnnexplainer}.\n% %\n% The weight for the Wasserstein distance based loss is adjusted based on the performance and set as xxx for xxx.\n% %\n% (1) \\textbf{\\emph{GNN Explainer.}}\n% For GNN Explainer, xxx is set as xxx.\n% %\n% (1) \\textbf{\\emph{PGExplainer.}}\n% For PGExplainer, xxx is set as xxx.\n\n\n\n"
                    },
                    "subsubsection 8.1.6": {
                        "name": "Packages Required for Implementations.",
                        "content": "\n% \\noindent \\textbf{Packages Required for Implementations.}\n%\nWe list the key packages and corresponding versions in our implementations as below. \n\\begin{itemize}  %[topsep=0pt]\n    \\item Python == 3.7.10\n    \\item torch == 1.8.1\n    \\item torch-cluster == 1.5.9\n    \\item torch-geometric == 1.4.1\n    \\item torch-scatter == 2.0.6\n    \\item torch-sparse == 0.6.9\n    \\item cuda == 11.0\n    \\item numpy == 1.20.0\n    \\item tensorboard == 1.13.1\n    \\item networkx == 2.5.1\n    \\item scikit-learn == 0.24.1\n    \\item pandas==1.2.3\n    \\item scipy==1.4.1\n\\end{itemize}\n\n\n\n\n\n\n\n% \\begin{figure}[htbp]\n% \t\\centering \n% % \t\\vspace{-0.6cm}\n%     \t\\subfloat[$\\Delta_{SP}$ w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.233\\textwidth]{sp-param-1.pdf}\n%     } \n%         \\subfloat[Accuracy w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.233\\textwidth]{acc-param-1.pdf}\n%     }\n\n%     \t\t\\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE: a parameter study for $\\alpha$ and $\\beta$.}\n% \t\\vspace{-2mm}\n% \t\\label{param_study}\n% \\end{figure}\n\n\n\n\n% \\begin{figure*}[htbp]\n% \t\\centering \n% % \t\\vspace{-0.6cm}\n%         \\subfloat[$\\Delta B_i$ (Reduced) w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.32\\textwidth]{delta-b-1.pdf}\n%     }\n%     \t\\subfloat[$\\Delta_{SP}$ w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.32\\textwidth]{sp-param-1.pdf}\n%     }  \n%         \\subfloat[Accuracy w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.32\\textwidth]{acc-param-1.pdf}\n%     }\n\n%     \t\t\\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE: a parameter study for $\\alpha$ and $\\beta$.}\n% \t\\vspace{-2mm}\n% \t\\label{param_study}\n% \\end{figure*}\n\n% \\linespread{1.1}\n% \\begin{figure}[htbp]\n% \t\\centering \n% % \t\\vspace{-0.6cm}\n%         \\subfloat[The tendency of relative $\\Delta B_i$ (Reduced) w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.48\\textwidth]{delta-b-param-3.pdf}\n%         \\label{param-1}\n%     } \\\\\n%     \t\\subfloat[The tendency of $\\Delta_{SP}$ w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.48\\textwidth]{sp-param-3.pdf}\n%                 \\label{param-2}\n%     }  \\\\\n%         \\subfloat[The tendency of accuracy w.r.t. $\\alpha$ and $\\beta$]{\n%         \\includegraphics[width=0.48\\textwidth]{acc-param-3.pdf}\n%                 \\label{param-3}\n%     }\\\\\n\n%     \t\t\\vspace{-3mm}\n% \\caption{A parameter study of the proposed framework REFEREE based on hyper-parameter $\\alpha$ and $\\beta$. In (a), higher $\\Delta B_i$ (Reduced) indicates better performance on explaining fairness for the Fairness Explainer in REFEREE. In (b), lower $\\Delta_{SP}$ indicates a higher level of fairness is achieved based on the obtained explanations in terms of debiasing GNNs for the whole population. In (c), higher accuracy represents better GNN utility performance.}\n% \t\\vspace{-5mm}\n% \t\\label{param_study}\n% \\end{figure}\n\n\n\n\n\n\n\n% \\linespread{1.1}\n\n% \\linespread{1.25}\n\n\n\n\n\n"
                    }
                },
                "subsection 8.2": {
                    "name": "Summary of Notations",
                    "content": "\n\nTo facilitate understanding, we present a summary of commonly utilized notations and the corresponding descriptions in Table~\\ref{tb:symbols}.\n\n\n"
                },
                "subsection 8.3": {
                    "name": "Parameter Sensitivity",
                    "content": "\n\\label{sensitivity}\n\nWe present the parameter sensitivity of our proposed framework REFEREE in this section. More specifically, we explore how the hyper-parameters $\\alpha$ and $\\beta$ influence the performance of REFEREE on (1) explaining the bias (fairness) in GNNs and (2) debiasing the GNN across the whole population. \n%\nHere $\\alpha$ and $\\beta$ control the effect of the distribution difference constraint between the two explanations from the two explainers and the constraint to achieve better fidelity, respectively.\n%\nIn our experiments, we choose the widely used GAT model as the GNN to be explained, and we present the parameter study based on the performance of debiasing GNNs with explanations on German dataset. Similar observations can also be drawn on other GNN models and datasets.\n\n\n% \\linespread{1.1}\n\n% \\linespread{1.2}\n\n\n\nNow we introduce the experimental settings for the parameter sensitivity study. Specifically, we fix the value for parameter $\\gamma$ as 1e-4 (the same as the setting in our implementation). \n%\nFirst, for the parameter study of $\\alpha$, we set $\\beta=$1e-4 (the same as the setting in our implementation), and we vary $\\alpha$ from \\{1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2\\}. \n%\nSecond, for the parameter study of $\\beta$, we set $\\alpha=1$ (the same as the setting in our implementation), and we also vary $\\beta$ from \\{1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2\\}.\n%\nThe performance changes of the proposed framework on explaining fairness (with $\\Delta B_i$ (Reduced) being the node-level bias metric) and debiasing the GNN predictions over the whole population (with $\\Delta_{SP}$ being the fairness metric and accuracy being the utility metric) are presented in Fig.~\\ref{param_study}.\n%\nWe can draw observations as below:\n%\n\\begin{itemize}[topsep=0pt]\n    \\item From the perspective of explaining fairness (i.e., identifying the edges whose existence can maximally alleviate the exhibited node-level bias), we observe that a relatively larger $\\alpha$ and a relatively smaller $\\beta$ help to achieve better performance, i.e., larger $\\Delta B_i$ (Reduced), in Fig.~\\ref{param-1}. This is because larger $\\alpha$ and smaller $\\beta$ help the framework better differentiate the edges between the two types of explanations given by the two explainers. In this way, the fairness explainer is able to identify an edge set that leads to more significant node-level bias reduction, i.e., to give a fairness explanation that brings higher $\\Delta B_i$ (Reduced) for any given node to be explained.\n\n    \\item From the perspective of debiasing the GNN predictions, we observe that a relatively larger $\\alpha$ and a relatively smaller $\\beta$ help to achieve better debiasing performance in Fig.~\\ref{param-2}. This is because: (1) Larger $\\alpha$ helps to better differentiate the edges between bias explanation and the fair explanation. This makes it easier for the framework to distinguish the edges that account for the exhibited bias and edges whose existence can alleviate the node-level bias. (2) Smaller $\\beta$ means that the constraint strength on prediction fidelity is weak. This enables the framework to focus more on explaining bias (fairness) for edges in any given computation graph.\n    \n    \\item From the perspective of maintaining GNN utility, we observe that a relatively smaller $\\alpha$ and a relatively larger $\\beta$ help achieve higher prediction accuracy in Fig.~\\ref{param-3}. This is because smaller $\\alpha$ and larger $\\beta$ enforce the framework to focus more on the fidelity of the explanation. Therefore, more critical information could be encoded in the identified edges. Such an advantage leads to higher prediction accuracy based on the identified edges for any given node.\n    \n    \\item Practically, it is necessary to balance the performance of bias reduction and model utility for any given GNN. In this regard, moderate values (e.g., values between 1e-4 and 1e0) for both $\\alpha$ and $\\beta$ are recommended.\n\\end{itemize}\n\n\n\n\n\n\n% \\begin{figure}[!t]\n% \t\\centering \n% % \t\\vspace{-0.3cm}\n%     \\subfloat[Delta statistical parity tendency]{\n%         \\includegraphics[width=0.23\\textwidth]{delta-b-1.pdf}\n%     }\n%     \\subfloat[Delta statistical parity tendency]{\n%         \\includegraphics[width=0.23\\textwidth]{sp-param-1.pdf}\n%     }\n\n%     \t\t\\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.}\n% \t\\vspace{-2mm}\n% \t\\label{tendency}\n% \\end{figure}\n\n% \\begin{figure*}[h]\n% \t\\centering \n% \t\\vspace{-0.6cm}\n%     \\subfloat[Delta statistical parity tendency]{\n%         \\includegraphics[width=0.32\\textwidth]{delta-b-1.pdf}\n%     }\n%         \\subfloat[Delta equal opportunity tendency]{\n%         \\includegraphics[width=0.32\\textwidth]{acc-param-1.pdf}\n%     }\n%     \t\\subfloat[Accuracy performance tendency]{\n%         \\includegraphics[width=0.32\\textwidth]{sp-param-1.pdf}\n%     } \n%     \t\t\\vspace{-3mm}\n% \\caption{Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.}\n% \t\\vspace{-2mm}\n% \t\\label{tendency}\n% \\end{figure*}\n\n\n\n\n\n% \\begin{table*}[]\n% \\caption{Evaluation of the GCN performance based on the three real-world datasets when the sampled edges that contribute the most to bias are removed from the original networks.}\n% \\label{datasets}\n% \\centering\n% % \\large\n% \\begin{tabular}{cccccccccccc}\n% \\hline\n%               & \\multicolumn{3}{c}{\\textbf{German}}          &  & \\multicolumn{3}{c}{\\textbf{Bail}}            &  & \\multicolumn{3}{c}{\\textbf{Credit}}          \\\\\n%               \\cline{2-4}   \\cline{6-8}  \\cline{10-12} \n%               & \\textbf{ACC} & $\\bm{\\Delta}_{SP}$ & $\\bm{\\Delta}_{EO}$ &  & \\textbf{ACC} & $\\bm{\\Delta}_{SP}$ & $\\bm{\\Delta}_{EO}$ &  & \\textbf{ACC} & $\\bm{\\Delta}_{SP}$ & $\\bm{\\Delta}_{EO}$ \\\\\n%               \\hline\n% \\textbf{Att.}          & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  \\\\\n% \\textbf{Grad.}         & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  \\\\\n% \\textbf{GNN Explainer} & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  \\\\\n% \\textbf{PGExplainer}   & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  \\\\\n% \\textbf{GE-REFEREE}     & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  \\\\\n% \\textbf{PGE-REFEREE}    & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  &  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  & xx.x $\\pm$ x.x  \\\\\n%               \\hline\n% \\end{tabular}\n% \\end{table*}\n\n"
                }
            }
        },
        "tables": {
            "edge_identification": "\\begin{table*}[]\n\\vspace{-1mm}\n\\caption{$\\Delta B_i$ (Promoted) and  $\\Delta B_i$ (Reduced) present how much Wasserstein-1 distance between the outcome distribution of two sensitive subgroups improves and reduces on average.\n%\nAbsolute values of normalized promotion and reduction are given in $\\times 10^{-4}$ scale.\n%\nLarger values indicate better effectiveness in explaining bias (fairness). \n%\nGE- and PGE- prefixes indicate the backbone of both explainers in REFEREE as GNN Explainer and PGExplainer, respectively.\n%\nThe best results are in Bold.\n}\n\\vspace{-3mm}\n\\label{edge_identification}\n\\centering\n% \\large\n\\small\n\\begin{tabular}{ccccccccc}\n\\hline\n              & \\multicolumn{2}{c}{\\textbf{German}} &  & \\multicolumn{2}{c}{\\textbf{Recidivism}} &  & \\multicolumn{2}{c}{\\textbf{Credit}} \\\\\n              \\cline{2-3}  \\cline{5-6}  \\cline{8-9}\n              & \\textbf{$\\Delta B_i$ (Promoted)}          & \\textbf{$\\Delta B_i$ (Reduced)}          &  & \\textbf{$\\Delta B_i$ (Promoted)}          & \\textbf{$\\Delta B_i$ (Reduced)}         &  & \\textbf{$\\Delta B_i$ (Promoted)}          & \\textbf{$\\Delta B_i$ (Reduced)}          \\\\\n              \\hline\n\\textbf{Att.}          &   6.11 $\\pm$ 2.51           &  7.84 $\\pm$ 3.48           &  &  4.58 $\\pm$ 1.67           & 7.18 $\\pm$ 2.24           &  & 6.72 $\\pm$ 0.75             &  8.48 $\\pm$  3.29           \\\\\n\\textbf{Grad.}         &   4.27 $\\pm$ 0.98           &  5.60 $\\pm$ 1.85           &  &  3.59 $\\pm$ 2.02          & 4.42 $\\pm$ 2.01           &  & 5.97 $\\pm$ 1.07             & 9.79 $\\pm$ 1.78            \\\\\n\\textbf{GNN Explainer} &   5.17 $\\pm$ 1.20           &  3.37 $\\pm$ 1.53           &  &  1.74 $\\pm$ 0.72           & 3.55 $\\pm$ 2.08           &  & 7.41 $\\pm$ 1.75             & 9.24 $\\pm$ 2.66            \\\\\n\\textbf{PGExplainer}   &   8.73 $\\pm$ 0.74           &  9.37 $\\pm$ 1.87          &  &  6.36 $\\pm$ 2.39           & 8.66 $\\pm$ 1.82           &  & 7.48 $\\pm$ 2.70             & 10.54 $\\pm$ 3.22            \\\\\n\\textbf{GE-REFEREE}     &   14.29 $\\pm$ 2.73           &  \\textbf{14.45 $\\pm$ 2.29}           &  &  \\textbf{13.94 $\\pm$ 3.74}         & 12.05 $\\pm$ 2.79           &  & 10.30 $\\pm$ 2.64            & \\textbf{15.07 $\\pm$ 3.35}            \\\\\n\\textbf{PGE-REFEREE}    &   \\textbf{15.72 $\\pm$ 2.31}           &  11.97 $\\pm$ 2.62           &  &  10.39 $\\pm$ 4.08           & \\textbf{12.57 $\\pm$ 3.12}           &  & \\textbf{11.57 $\\pm$ 2.91}             & 14.67 $\\pm$ 3.49            \\\\\n\\hline\n\\end{tabular}\n\\vspace{-1.4em}\n\\end{table*}",
            "utility_table": "\\begin{table}[]\n\\caption{Explanation fidelity evaluation for different GNNs. \n%\n% GNN Explainer is adopted as the explanation backbone model. \n%\nNumerical results are in percentage. Vanilla denotes the explanation results given by the vanilla GNN Explainer. B. Explainer and F. Explainer represent the Bias Explainer and Fairness Explainer, respectively. The best results are in Bold.}\n\\vspace{-3mm}\n\\label{utility_table}\n\\centering\n\\small\n\\begin{tabular}{ccccc}\n                     \\hline\n                     &              & \\textbf{German} & \\textbf{Recidivism} & \\textbf{Credit} \\\\\n                     \\hline\n\\multirow{3}{*}{\\textbf{GCN}} & \\textbf{Vanilla}      & 88.02 $\\pm$ 1.48      & 90.04 $\\pm$ 1.43       & 85.26 $\\pm$ 1.67     \\\\\n                     & \\textbf{B. Explainer}          & \\textbf{92.20 $\\pm$ 1.39}       & 90.26 $\\pm$ 3.24       & 87.60 $\\pm$ 2.79     \\\\\n                     & \\textbf{F. Explainer}          & 89.17 $\\pm$ 0.85       & \\textbf{92.08} $\\pm$ 2.44       & \\textbf{89.41} $\\pm$ 4.08     \\\\\n                     \\hline\n\\multirow{3}{*}{\\textbf{GAT}} & \\textbf{Vanilla}      & 83.65 $\\pm$ 3.02       & 87.91 $\\pm$ 2.04       & \\textbf{88.64 $\\pm$ 3.41}     \\\\\n                     & \\textbf{B. Explainer}          & \\textbf{85.71 $\\pm$ 2.31}       & 90.51 $\\pm$ 4.58       & 86.09 $\\pm$ 2.07     \\\\\n                     & \\textbf{F. Explainer}          & 84.40 $\\pm$ 1.57       & \\textbf{91.98 $\\pm$ 3.95}       & 87.04 $\\pm$ 3.10     \\\\\n                     \\hline\n\\multirow{3}{*}{\\textbf{GIN}} & \\textbf{Vanilla}      & 88.58 $\\pm$ 2.50       & \\textbf{91.77 $\\pm$ 1.42}       & 87.62 $\\pm$ 2.60    \\\\\n                     & \\textbf{B. Explainer}          & 88.11 $\\pm$ 1.78       & 90.26 $\\pm$ 4.13       & 86.47 $\\pm$ 2.13    \\\\\n                     & \\textbf{F. Explainer}         & \\textbf{89.67  $\\pm$ 2.23}       & 91.45 $\\pm$ 1.78       & \\textbf{88.17 $\\pm$ 2.98}   \\\\\n                     \\hline\n\\end{tabular}\n\\vspace{-1.8em}\n\\end{table}",
            "datasets": "\\begin{table}[tbp]\n% \\vspace{3mm}\n\\caption{Statistics of the three real-world graph data. \\textit{Sens.} denotes the sensitive feature.}\n\\label{datasets}\n\\small\n\\centering\n\\begin{tabular}{lccc}\n\\hline\n\\hline\n\\textbf{Dataset}             & \\textbf{German Credit}        & \\textbf{Recidivism}   & \\textbf{Credit Defaulter}         \\\\\n\\hline\n\\hline\n\\textbf{\\# Nodes}               & 1,000                      & 18,876             & 30,000                         \\\\\n\\textbf{\\# Edges}               & 22,242                     & 321,308            & 1,436,858                      \\\\\n\\textbf{\\# Attributes} & 27                         & 18                 & 13                             \\\\\n\\textbf{Avg. degree}           & 44.5                      & 34.0              & 95.8                      \\\\\n\\textbf{Sens.} & Gender       & Race  & Age     \\\\\n\\textbf{Label}         & Good / Bad & Bail / No Bail   & Default / No Default  \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\end{table}",
            "tb:symbols": "\\begin{table}[!t]\n\\small\n\\vspace{0.3cm}\n\\caption{Notations commonly used in this paper and the corresponding descriptions.} \n\\vspace{-0.2cm}\n\\label{tb:symbols}\n\\begin{tabular}{cc}\n\\hline\n\\hline\n\\textbf{Notations}       & \\textbf{Definitions or Descriptions} \\\\\n\\hline\n\\hline\n$\\mathcal{G}$   &  input graph\\\\\n$\\mathcal{V}$, $\\mathcal{E}$, $\\mathcal{X}$   &  node, edge set, and attribute set \\\\\n$\\mathcal{\\hat{Y}}_i$           &  probabilistic GNN outcome set for all nodes \\\\\n$\\mathcal{G}_i$   &  computation graph centered on the $i$-th node\\\\\n$\\mathcal{\\tilde{E}}_i$   &  bias explanation for the $i$-th node\\\\\n$\\mathcal{\\tilde{E}}_i'$   &  fairness explanation for the $i$-th node\\\\\n% $\\mathcal{\\tilde{Y}}_i$           &  outcome set for all nodes based on bias explanation\\\\\n% $\\mathcal{\\tilde{Y}}_i'$           &  outcome set for all nodes based on fairness explanation\\\\\n$\\mathbf{\\hat{y}}_i$           &  probabilistic GNN outcome of the $i$-th node \\\\\n$\\mathbf{\\tilde{y}}_i$           &  outcome of the $i$-th node based on bias explanation\\\\\n$\\mathbf{\\tilde{y}}_i'$           &  outcome of the $i$-th node based on fairness explanation\\\\\n$\\hat{Y}_i$           &  predicted label of the $i$-th node \\\\\n$N$    & number of nodes in the input graph  \\\\\n$M$    & number of nodes to be explained   \\\\\n$C$    & number of classes for node classification   \\\\\n% $L$    & layer number of the studied GNN model  \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\vspace{-0.4cm}\n\\end{table}"
        },
        "figures": {
            "framework": "\\begin{figure*}[]\n    \\centering\n    \\vspace{-5mm}\n    \\includegraphics[width=0.73\\textwidth]{images-renewed/structure-4-renewed.pdf}\n    \\vspace{-4mm}\n    \\caption{Framework structure of REFEREE: the edges in the edge set given by Bias Explainer maximally account for the node-level bias, while the edges in the edge set given by Fairness Explainer maximally alleviates the node-level bias.}\n    \\vspace{-3.5mm}\n    \\label{framework}\n\\end{figure*}",
            "tendency": "\\begin{figure*}[!t]\n\t\\centering \n\t\\vspace{-0.35cm}\n    \\subfloat[Changes of $\\Delta_{SP}$]{\n        \\includegraphics[width=0.25\\textwidth]{sp-1.pdf}\n    }\n    \\hspace{7mm}\n        \\subfloat[Changes of $\\Delta_{EO}$]{\n        \\includegraphics[width=0.25\\textwidth]{eo-1.pdf}\n    }\n        \\hspace{7mm}\n    \t\\subfloat[Changes of accuracy]{\n        \\includegraphics[width=0.25\\textwidth]{acc-1.pdf}\n    } \n    \t\t\\vspace{-4mm}\n\\caption{Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.}\n\t\\vspace{-4mm}\n\t\\label{tendency}\n\\end{figure*}",
            "param_study": "\\begin{figure}[htbp]\n\t\\centering \n% \t\\vspace{-0.6cm}\n        \\subfloat[The changes of relative $\\Delta B_i$ (Reduced) w.r.t. $\\alpha$ and $\\beta$]{\n        \\includegraphics[width=0.48\\textwidth]{delta-b-param-3.pdf}\n        \\label{param-1}\n    } \\\\\n    \t\\subfloat[The changes of $\\Delta_{SP}$ w.r.t. $\\alpha$ and $\\beta$]{\n        \\includegraphics[width=0.48\\textwidth]{sp-param-3.pdf}\n                \\label{param-2}\n    }  \\\\\n        \\subfloat[The changes of accuracy w.r.t. $\\alpha$ and $\\beta$]{\n        \\includegraphics[width=0.48\\textwidth]{acc-param-3.pdf}\n                \\label{param-3}\n    }\\\\\n\n    \t\t\\vspace{-3mm}\n\\caption{A parameter study of the proposed framework REFEREE based on hyper-parameter $\\alpha$ and $\\beta$. In (a), higher $\\Delta B_i$ (Reduced) indicates better performance on explaining fairness for the Fairness Explainer in REFEREE. In (b), lower $\\Delta_{SP}$ indicates a higher level of fairness is achieved based on the obtained explanations in terms of debiasing GNNs for the whole population. In (c), higher accuracy represents better GNN utility performance.}\n\t\\vspace{-5mm}\n\t\\label{param_study}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\label{ws_distance}  \n    \\max_{\\mathcal{\\tilde{E}}_i} \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1)),\n\\end{align}",
            "eq:2": "\\begin{align}\n\\begin{small}\n\\label{distance_expression}  \n    W_1(P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1)) = \n    \\inf \\mathbb{E}_{ (\\tilde{\\mathbf{y}}_{(0)}, \\tilde{\\mathbf{y}}_{(1)}) \\sim \\kappa} [\\| \\tilde{\\mathbf{y}}_{(0)} - \\tilde{\\mathbf{y}}_{(1)} \\|_1].\n\\end{small}\n\\end{align}",
            "eq:3": "\\begin{align}\n\\label{ws_distance2}  \n\\min_{\\mathcal{\\tilde{E}}_i'} \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0'), P(\\mathcal{\\tilde{Y}}_1')),\n\\end{align}",
            "eq:4": "\\begin{align}\n\\label{l3}  \n    \\mathscr{L}_1(\\bm{\\Phi}, \\bm{\\Phi}') = \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0'), P(\\mathcal{\\tilde{Y}}_1')) - \\text{W}_1 (P(\\mathcal{\\tilde{Y}}_0), P(\\mathcal{\\tilde{Y}}_1)).\n\\end{align}",
            "eq:5": "\\begin{align}\n\\label{KLD}  \n    \\max_{\\bm{\\Phi}, \\bm{\\Phi}'} \\text{Dist\\_Diff} (    P_{\\bm{\\Phi}'}(\\mathcal{\\tilde{E}}_i' | \\mathcal{E}_i) \\|  P_{\\bm{\\Phi}}(\\mathcal{\\tilde{E}}_i| \\mathcal{E}_i) ),\n\\end{align}",
            "eq:6": "\\begin{align}\n\\label{l4}  \n    \\mathscr{L}_2(\\bm{\\Phi}, \\bm{\\Phi}') = - \\text{Dist\\_Diff} (    P_{\\bm{\\Phi}'}(\\mathcal{\\tilde{E}}_i' | \\mathcal{E}_i) \\|  P_{\\bm{\\Phi}}(\\mathcal{\\tilde{E}}_i| \\mathcal{E}_i) ).\n\\end{align}",
            "eq:7": "\\begin{align}\n\\label{mi_term}\n    \\max_{\\mathcal{\\tilde{G}}_i} \\text{MI}(\\hat{Y}_i, \\mathcal{\\tilde{G}}_i) = \\text{H}(\\hat{Y}_i) - \\text{H}(\\hat{Y}_i| \\mathcal{\\tilde{G}}_i).\n\\end{align}",
            "eq:8": "\\begin{align}\n\\label{entropy_term}  \n    \\text{H}(\\hat{Y}_i| \\mathcal{\\tilde{G}}_i) = - \\mathbb{E}_{\\hat{Y}_i | \\mathcal{\\tilde{G}}_i} [\\log P_{\\bm{\\Theta}}(\\hat{Y}_i | \\mathcal{\\tilde{G}}_i)].\n\\end{align}",
            "eq:9": "\\begin{align}\n\\label{loss_1}  \n\\small\n    \\mathscr{L}_3 (\\bm{\\Phi}, \\bm{\\Phi}') = - \\mathbb{E}_{\\hat{Y}_i | \\mathcal{\\tilde{G}}_i} [\\log P_{\\bm{\\Theta}}(\\hat{Y}_i | \\mathcal{\\tilde{G}}_i)] - \\mathbb{E}_{\\hat{Y}_i | \\mathcal{\\tilde{G}}_i'} [\\log P_{\\bm{\\Theta}}(\\hat{Y}_i | \\mathcal{\\tilde{G}}_i')].\n\\end{align}",
            "eq:10": "\\begin{align}\n\\label{loss_2}  \n    \\mathscr{L}_4(\\bm{\\Phi}, \\bm{\\Phi}') = \\| \\mathbf{M}\\|_1 + \\| \\mathbf{M}'\\|_1\n\\end{align}",
            "eq:11": "\\begin{align}\n\\label{loss_budget}  \n    \\mathscr{L}_4(\\bm{\\Phi}, \\bm{\\Phi}', T, T') = \\text{ReLU}(\\| \\mathbf{M}\\|_1 -T) + \\text{ReLU}(\\| \\mathbf{M}'\\|_1 -T')\n\\end{align}",
            "eq:12": "\\begin{align}\n\\label{l_total}  \n    \\mathscr{L} =  \\mathscr{L}_1 +  \\alpha \\mathscr{L}_2 + \\beta  \\mathscr{L}_3 +  \\gamma \\mathscr{L}_4.\n\\end{align}"
        },
        "git_link": "https://github.com/yushundong/REFEREE"
    }
}