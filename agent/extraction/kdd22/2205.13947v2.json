{
    "meta_info": {
        "title": "Spatio-Temporal Graph Few-Shot Learning with Cross-City Knowledge  Transfer",
        "abstract": "Spatio-temporal graph learning is a key method for urban computing tasks,\nsuch as traffic flow, taxi demand and air quality forecasting. Due to the high\ncost of data collection, some developing cities have few available data, which\nmakes it infeasible to train a well-performed model. To address this challenge,\ncross-city knowledge transfer has shown its promise, where the model learned\nfrom data-sufficient cities is leveraged to benefit the learning process of\ndata-scarce cities. However, the spatio-temporal graphs among different cities\nshow irregular structures and varied features, which limits the feasibility of\nexisting Few-Shot Learning (\\emph{FSL}) methods. Therefore, we propose a\nmodel-agnostic few-shot learning framework for spatio-temporal graph called\nST-GFSL. Specifically, to enhance feature extraction by transfering cross-city\nknowledge, ST-GFSL proposes to generate non-shared parameters based on\nnode-level meta knowledge. The nodes in target city transfer the knowledge via\nparameter matching, retrieving from similar spatio-temporal characteristics.\nFurthermore, we propose to reconstruct the graph structure during\nmeta-learning. The graph reconstruction loss is defined to guide\nstructure-aware learning, avoiding structure deviation among different\ndatasets. We conduct comprehensive experiments on four traffic speed prediction\nbenchmarks and the results demonstrate the effectiveness of ST-GFSL compared\nwith state-of-the-art methods.",
        "author": "Bin Lu, Xiaoying Gan, Weinan Zhang, Huaxiu Yao, Luoyi Fu, Xinbing Wang",
        "link": "http://arxiv.org/abs/2205.13947v2",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Accepted to KDD2022"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nWith the rapid development of urbanization, humans, vehicles, and devices in the city have generated a considerable spatio-temporal data, which significantly change the urban management with a bunch of urban-related machine learning applications, such as traffic flow~\\cite{DBLP:conf/ijcai/WuPLJZ19,DBLP:conf/cikm/LuGJFZ20}, taxi demand~\\cite{DBLP:conf/aaai/YeSDF021,10.1145/3331184.3331368} and air quality forecasting~\\cite{DBLP:conf/wsdm/WangZZLY21}. However, existing machine learning algorithms require sufficient samples to learn effective models, which may not be applicable to cities without sufficient data. \n% \\weinan{can we add a general statistics about the ratio of data-scarce cities in the world?}\nThe similarity of cities inspires us to consider the cross-city knowledge transfer, which reduces the burden of data collection and improves the efficiency of smart city construction.\n%Many developed cities are at the forefront of building smart cities. \n% Recent advances of spatio-temporal graph learning lead to solving a series of urban computing problems, like traffic flow~\\cite{DBLP:conf/ijcai/WuPLJZ19,DBLP:conf/cikm/LuGJFZ20}, taxi demand~\\cite{DBLP:conf/aaai/YeSDF021,10.1145/3331184.3331368} and air quality forecasting~\\cite{DBLP:conf/wsdm/WangZZLY21}. However, their hunger for large amount of structured and labeled data limits the real-world application for developing cities where only few data are available for training. \n\n\n\n% 随着城市数字化的发展，一些数字化程度比较高的城市收集了大量的人、机、物的时空数据。\n% 最近的一系列promising的研究采用时空图学习实现了对交通流量、空气质量、人流的预测。\n% 然而上述这些方法都需要采集大量的数据进行训练才能得到不错的表现。\n% 而由于城市发展的不均匀性，一些欠发达的城市仅有很少的数据可供训练。\n% 城市运行规律的相似性启发了我们通过跨城市的知识迁移实现来减少新的目标城市的数据采集，提升智慧城市的建设效率。\n\n% Currently, much research progress has been made in Few-Shot Learning (\\emph{FSL}), like transfer learning~\\cite{ying2018transfer}, meta learning~\\cite{vanschoren2018meta}, multi-task learning~\\cite{zhang2021survey}, etc. \n\n% in Few-Shot Learning (\\emph{FSL})~\\cite{ying2018transfer,vanschoren2018meta,zhang2021survey}\n\nCurrently, much research progress has been made for solving urban computing tasks in few-shot scenarios.\nWang et al. ~\\cite{DBLP:conf/ijcai/WangGMLY19} model the cities as \\emph{grids} and first propose to facilitate spatio-temporal prediction in data-scarce cities. In order to achieve better similar region-to-region matching, they introduce \\emph{large-scale auxiliary data} (social media check-ins data). Whereas, finding and collecting the appropriate auxiliary data is inherently costly, and face the potential of risk leakage. \nIn ~\\cite{10.1145/2939672.2939830}, the authors propose \\emph{FLORAL} to classify air quality by transfering the semantically-related dictionary from one data-rich city. \nHowever, knowledge transfer from one single source city faces the risk of negative transfer due to the great difference between two cities.\nYao et al.~\\cite{10.1145/3308558.3313577} combine meta-learning method to learn a good initialization model from multiple source cities in target domain, however without considering the varied feature differences across cities and within cities.\nMore importantly, above methods are only applicable to grid-based data, but not compatible with graph-based modeling. \nActually, the graph-based model has aroused extensive attention recently and achieved great success in spatio-temporal learning of road-network, metro-network, sensor-network data, etc. \n\n\n% Wang et al. ~\\cite{DBLP:conf/ijcai/WangGMLY19} first propose to facilitate spatio-temporal prediction in data-scarce cities by transferring knowledge from one data-rich city. They introduce \\emph{large-scale auxiliary data} (social media check-ins data) for better similar region-to-region matching. Yao et al. ~\\cite{10.1145/3308558.3313577} model the cities as \\emph{grids} and combine meta-learning method to learn a good initialization model for adaptation in target domain.\n% Wei et al.~\\cite{10.1145/2939672.2939830} propose \\emph{FLORAL} to classify air quality by transfering the semantically-related dictionary.\n\n\nIn this paper, our goal is to transfer the cross-city knowledge in graph-based few-shot learning scenarios, simultaneously exploring the impact of knowledge transfer across multiple cities. However, there exists following two challenges: \n\\textbf{(i)} \\emph{How to adapt feature extraction in target city via the knowledge from multiple source cities?} Current meta-learning methods assume the transferable knowledge to be globally shared across multiple cities. However, even in different areas of one city, the spatio-temporal characteristic varies greatly. Existing methods fail to handle the knowledge transfer among complicated scenarios effectively.\n\\textbf{(ii)} \\emph{How to alleviate the impacts of varied graph structure on transferring among different cities?} Compared with grid-based data, graph-based modeling shows irregular structure information among cities. The edges between nodes explicitly depict various feature interactions. Existing FSL methods ignore the importance of structure when knowledge transferring, which causes unstable results and may even increase the risk of structure deviation. \n\n%%HY.2.07: It seems all above methods address small data problem. Why we need \"However, xxx\"?\n% However, the requirement of external dataset greatly limits the application of many real world scenarios.\n% Meanwhile, recent graph-based modeling has aroused extensive attention and achieved great success in road-network, metro-network, sensor-network data, etc. Whereas, the above methods are only applicable to grid-based data, and most of them focus on knowledge transfer from only one single city.\n%%HY.2.07: mentiond \"the above methods\" first and then discuss the application of graph-based modeling\n% Therefore, in this paper, our goal is to transfer the cross-city knowledge to graph-based few-shot learning scenarios without introducing additional information, and simultaneously explore the impact of knowledge transfer across multiple cities. \n\n%%HY.1.30: I do not think we need to use one paragraph to discuss Wang et al. You can use one paragraph to discuss all related literature and their corresponding cons.\n\n% leverage current well-designed spatio-temporal graph models\n% and transfer the cross-city knowledge of implied spatio-temporal characteristics to more data-scarce scenarios without introducing additional information. \n\n% 跨城市的知识迁移最早由Wang等人提出。他们将城市建模为网格，并通过引入额外的大规模数据实现网格区域的匹配。通过引入额外的大规模数据限制了更多实际场景的应用。同时，有许多基于图的时空学习问题在近年来得到了广泛的关注，并产生了很多先进的时空图模型。因此，在这篇论文中，我们的目标是在能够充分利用已有的时空图计算的成果，在不引入额外数据的情况下实现跨城市的知识迁移和小样本学习。\n\n%%HY.2.07: I do not think there are any relations between this sentence and the last paragraph. You have mentioned the progress of few-shot learning on smart city.\n% However, these advanced methods and positive results cannot be directly grafted into our research. Specically, there are two main \\emph{challenges}: \n% \\textbf{(i)} The graph structures are different across cities, and the edges between nodes explicitly depict various feature interactions. \n% Existing \\emph{FSL} methods are infeasible to capture the irregular structure information among multiple cities.\n%%HY.2.07: make it more clear. Why exiting FSL methods can not capture the irregular structure? I think there are a lot of graph few-shot learning methods.\n% These advanced methods are mainly studied on data with Euclidean properties (e.g., images and text). \n% Whereas, graph data is non-Euclidean, and node instances with explicit interactions do not satisfy the i.i.d assumption. Existing \\emph{FSL} methods are infeasible to capture the structure information on graph data.\n% \\textbf{(ii)} Compared with knowledge transfer from one single city, multi-source city data brings more information. However, it is more challenging to find the relevant knowledge related to the target city. Directly training and finetuning across the cities may causes negative transfer problem.\n%%HY.2.07: I do not think you have any negative transfer problem in the experiment. It is overclaimed. You may say \"causes instable results and may even increase the risk of negative transfer\"\n\n%lubin comments: i have revised two challenges compared with previous version.\n\n% Urban data has complex and dynamic spatio-temporal dependencies. Moreover, the characteristics and scale of training samples vary greatly among multiple cities. Directly training and finetuning across the cities faces the dilemma of underfitting large-scale samples (source cities) and overfitting few-shot samples (target city).\n\n%% HY.1.30: I do not think applying few-shot learning on graph is a key challenge of this paper. \n%% HY.1.30: The second challenge is unclear. Do you mean you want to improve the generalization ability of existing algorithms? If so, this is a general challenge in meta-learning.\n\nTo address the aforementioned challenges, we propose a novel and model-agnostic \\textbf{S}patio-\\textbf{T}emporal \\textbf{G}raph \\textbf{F}ew-\\textbf{S}hot \\textbf{L}earning framework called \\textbf{ST-GFSL}. \nTo the best of our knowledge, our work is the first to investigate the few-shot scenario in spatio-temporal graph learning. \nIn order to adapt to the diversity of multiple cities, ST-GFSL no longer learns a globally shared model as usual. We propose to generate non-shared model parameters based on node-level meta knowledge to enhance specific feature extraction. \nThe novel-level knowledge transfer is realized through parameter matching, retrieving from nodes with similar spatio-temporal characteristics across source cities and target city. In addition, we propose the ST-Meta Learner to learn node-level meta knowledge from both local graph structure and time series correlations. \nDuring the process of meta-learning, ST-GFSL proposes to reconstruct the \ngraph structures of different cities based on meta knowledge.\nGraph reconstruction loss is defined to guide structure-aware learning, so as to avoid the structure deviation across multiple source cities.\n\n% ------------------------\n% To transfer the knowledge to the target data-scarce city, ST-GFSL learns a well-generalized model from multiple source cities with sufficient training samples. \n% In order to adapt to different data scales and diverse characteristics among multiple cities, \n% Meanwhile, we propose to reconstruct a \\emph{ST-Meta graph} and calculate \\emph{graph reconstruction loss} to measure the structural deviation in cross-city knowledge transfer for structure-awareness learning. \n% ------------------------\n\n% The training process of ST-GFSL follows MAML-based episode learning, which mimics the few-shot scenario in testing.\n\n% to eliminates the impact of structure deviation caused by simultaneously learning on multiple graph datasets, we propose to reconstruct an \\emph{ST-Meta graph} and \\emph{graph reconstruction loss} for structure-awareness learning. \n\n% ST-GFSL generates the \\emph{non-shared} model parameters to enhance feature extraction among different city datasets. Meanwhile, we introduce \\emph{ST-Meta graph} and \\emph{graph reconstruction loss}, which brings the benefits of structure-awareness and eliminates the impact of structure deviation on various source datasets. It is worth noting that ST-GFSL is a model-agnostic framework for tackling spatio-temporal graph few-shot learning, which can be easily combined with recent breakthroughs of spatio-temporal graph learning models. \n\nIn summary, the main contributions of our work are as follows:\n\\begin{itemize}[left=1em]\n    \\item We investigate a challenging but practical problem of spatio-temporal graph few-shot learning in data-scarce cities. To our knowledge, we are the first to explore the few-shot scenario in spatio-temporal graph learning tasks.\n    \\item To address this problem, we propose a model-agnostic learning framework called ST-GFSL. ST-GFSL generates non-shared parameters by learning node-level meta knowledge to enhance the feature extraction. The novel-level knowledge transfer is achieved via parameter matching from similar spatio-temporal meta knowledge. \n    \\item We propose to reconstruct the graph structure of different cities based on meta-knowledge. The graph reconstruction loss is further combined to guide structure-aware few-shot learning, which avoids the structure deviation among multiple cities.\n\\end{itemize}\n\nWe demonstrate the superiority of our proposed framework by the application of traffic speed prediction on four public urban datasets. Extensive experiments on METR-LA, PEMS-BAY, Didi-Chengdu and Didi-Shenzhen datasets validate the effectiveness and versatility of our approach over state-of-the-arts baselines.\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\nIn this section, we briefly introduce the relevant research lines to our work.\n% , namely spatio-temporal graph learning, graph few-shot learning and knowledge transfer across cities.\n\n",
                "subsection 2.1": {
                    "name": "Spatio-Temporal Graph Learning",
                    "content": "\nSpatio-Temporal Graph Learning is a fundamental and widely studied problem in urban computing tasks. In the early, researchers studied this problem from the perspective of time series analysis and put forward a series of methods such as ARIMA, VAR and Kalman Filtering~\\cite{moreira2013predicting}. With the rise of deep learning and graph neural networks, graph, as an effective data structure to describe spatial structure relations, is applied to analyze a series of urban problems.\nBai et al.~\\cite{DBLP:conf/ijcai/BaiYK0S19} propose STG2Seq to model multi-step citywide passenger demand based on a graph. Lu et al.~\\cite{DBLP:conf/cikm/LuGJFZ20} propose spatial neighbors and semantic neighbors of road segments to capture the dynamic features of urban traffic flow. Do et al.~\\cite{do2020graph} employ IoT devices on vehicles to sense city air quality and estimated unknown air pollutants by variational graph autoencoders. In particular, Pan et al.~\\cite{DBLP:conf/kdd/PanLW00Z19, pan_tkde_2020} propose to leverage deep meta learning to improve the traffic prediction performance by generalizing the learning ability of different regions. \n\nHowever, the researches in the above papers are all based on a city with large-scale training data. The data-scarce scenario is not within the scope of the research, but it is an issue that is well worth investigating. In this paper, we aim to achieve spatio-temporal graph few-shot learning through cross-city knowledge transfer. In addition, we are committed to come up with a model-agnostic architecture that can be combined with latest spatio-temporal graph learning models to further improve the performance.\n% As far as we know, ST-GFSL is the first to incorporate meta learning into cross-city spatio-temporal graph learning. \n\n"
                },
                "subsection 2.2": {
                    "name": "Graph Few-Shot Learning",
                    "content": "\n\nFew-Shot learning has yielded significant progress in the field of computer vision and natural language processing. \n% Several models and algorithms have been recently proposed and achieved positive results\n, e.g., MAML~\\cite{DBLP:conf/icml/FinnAL17}, Prototypical Networks~\\cite{DBLP:conf/nips/SnellSZ17}, and Meta-Transfer Learning~\\cite{DBLP:conf/cvpr/SunLCS19}. However, few-shot learning on non-Euclidean domains, like graph few-shot learning, has not been fully explored. Among recent few-shot learning on graphs, Meta-GNN~\\cite{DBLP:conf/cikm/0002CZTZG19} is the first to incorporate meta-learning paradigm, MAML, into node classification in graphs. Nevertheless, it does not fully describe the interrelation between nodes. Liu et al.~\\cite{liu2021relative} propose to assign the relative location and absolute location of nodes on graph to further capture the dependencies between nodes. Yao et al. \\cite{DBLP:conf/aaai/YaoZWJWHCL20} and Ding et al. \\cite{DBLP:conf/cikm/DingWLSLL20} adopt the idea of prototypical network and conduct few-shot node classification by finding the nearest class prototypes.\n\nThrough the analysis of above work, it can be found that existing methods mainly focus on few-shot node classification, while many urban computing problems are regression problems. Furthermore, compared with general attribute network, spatio-temporal graph has more complex and dynamic node characteristics. Directly combining few-shot learning methods with vanilla GNN model is infeasible to capture the complicated node correlations.\n\n% Through the analysis of the above work, it can be found that they all focus on the classification of small sample nodes, while many urban computing problems are regression problems. Furthermore, compared with the general attribute network, the spatio-temporal graph has more complex and dynamic node characteristics, and the sampling general graph neural network cannot capture the correlation between nodes.\n\n% However, few-shot learning on non-Euclidean domains, like graph few-shot learning, is under-investigated. \n% Among recent few-shot learning on graphs, Meta-GNN~\\cite{DBLP:conf/cikm/0002CZTZG19} is the first to incorporate meta-learning paradigm, MAML, into node classification task of graph neural network, but it lacks consideration of graph characteristic. Yao et al. \\cite{DBLP:conf/aaai/YaoZWJWHCL20}, Huang et al.\\cite{DBLP:conf/nips/HuangZ20} and Ding et al. \\cite{DBLP:conf/cikm/DingWLSLL20} adopted the idea of prototype network on graph node classification tasks. However, most urban computing problems are node regression problem, the method of metric-based few-shot learning framework is not suitable.\n% Bose et al. \\cite{bose2020metagraph} adapts gradient-based meta learning to optimize a shared parameter initialization for link prediction, but the parameter-shared model are not effective for capturing dynamic spatio-temporal features.\n\n"
                },
                "subsection 2.3": {
                    "name": "Knowledge Transfer Across Cities",
                    "content": "\n\nKnowledge transfer addresses machine learning problems in data-scarce scenarios. Especially in urban computing tasks, how to realize knowledge transfer across cities to reduce the cost of data collection and improve learning efficiency is an ongoing research problem. \\emph{FLORAL}~\\cite{10.1145/2939672.2939830} is an early work that implements air quality classification by transfering knowledge from a city existing sufficient multimodal data and labels.\n\\emph{RegionTrans} ~\\cite{DBLP:conf/ijcai/WangGMLY19} studies knowledge transfer cross-cities by dividing cities into different grids for spatio-temporal feature matching. Yao et al.~\\cite{10.1145/3308558.3313577} first propose \\emph{MetaST} to transfer knowledge from multiple cities. \n\nNeverthelss, the above research can not be directly applied to our task, mainly for the following reasons: (1) Many urban computing problems are regression problems, while \\emph{FLORAL} is designed for a classification problem. (2) \\emph{RegionTrans} and \\emph{MetaST} are designed for grid-based data, which is incompatible for graph-based modeling in our tasks. Meanwhile, during matching process, \\emph{RegionTrans} introduces additional social media check-ins data, which reduced its versatility. \n(3) \\emph{FLORAL} and \\emph{RegionTrans} only focus on knowledge transfer from a single source city. How to make use of the data from multiple cities and avoid negative transfer is a problem worthy of study.\nIn this paper, we aim to learn the cross-city meta knowledge from multiple graph-based datasets and transfer to a data-scarce city without introducing auxiliary datasets.\n\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Preliminary",
                "content": "\n\n% \\subsection{Problem Formulation}\nWe denote the spatio-temporal graph as $\\mathcal{G}_{ST}=(\\mathcal{V}, \\mathcal{E}, \\mathbf{A}, \\mathbf{X})$. (1) $\\mathcal{V}=\\{v_1, v_2, \\cdots, v_N\\}$ denotes the nodes set, and $N=\\left|\\mathcal{V}\\right|$ is the number of nodes. (2) $\\mathcal{E}=\\{e_{ij}=(v_i, v_j)\\} \\subseteq (\\mathcal{V} \\times \\mathcal{V})$ denotes the edges set. (3) $\\mathbf{A}=\\{a_{ij}\\}\\in \\mathbb{R}^{N \\times N}$ is the adjacency matrix of spatio-temporal graph. $a_{ij}=1$ indicates that there is an edge between node $v_i$ and $v_j$; otherwise, $a_{ij}=0$. \n(4) $\\mathbf{X}$ is the node feature matrix, refering to the message passing on the graph, such as traffic speed, concentration of air pollutants and passenger flow of taxis over a period of time. We take the node feature observed at time $t$ as a graph signal $\\mathbf{X}^{t} \\in \\mathbb{R}^{N \\times d}$, where $d$ is the dimension of node feature. \n\n% (4) $\\mathbf{X}\\in \\mathbb{R}^{N \\times d}$ is node feature matrix with $x_i \\in \\mathbb{R}^d$ representing the spatio-temporal characteristics of a given node $v_i$.\n\n\\begin{myProb}\n\\textbf{Spatio-Temporal Graph Forecasting}\\,\nSuppose we have $T$ historical spatio-temporal graph signals, and we want to predict future $M$ graph signals. The forecasting task is formulated as learning a function $f(\\cdot)$ given a spatio-temporal graph $\\mathcal{G}_{ST}$:\n\\begin{equation}\n    [\\mathbf{X}^{t-T+1}, \\cdot\\cdot\\cdot, \\mathbf{X}^{t}; \\mathcal{G}_{ST}] \\xrightarrow{f(\\cdot)} [\\mathbf{X}^{t+1}, \\cdot\\cdot\\cdot, \\mathbf{X}^{t+M}]\\text{.}\n\\end{equation}\n\\end{myProb}\n\n% $\\mathbf{X} = [\\mathbf{X^{V_m}}, \\mathbf{X^{V_a}}]$ denotes the node features, where node message feature $\\mathbf{X^{V_m}}=[x_1^{V_m}, x_2^{V_m}, \\cdots, x_N^{V_m}]\\in \\mathbb{R}^{N \\times d_m}$ and node attribute feature $\\mathbf{X^{V_a}}=[x_1^{V_a}, x_2^{V_a}, \\cdots, x_N^{V_a}]\\in \\mathbb{R}^{N \\times d_a}$. Node message feature refers to the messgae passing on the graph, such as traffic speed, concentration of air pollutants and passenger flow of taxis over a period of time. Node attribute feature refers to the attribute information of the node itself, such as the geographical location, POI information, etc.\n\n\\begin{myProb}\n\\textbf{Spatio-Temporal Graph Few-Shot Learning}\\,\nSuppose we have a set of $P$ source spatio-temporal graphs of data-rich cities  $\\mathcal{G}^{source}_{1:P}=\\{\\mathcal{G}_1^{source}, \\cdots, \\mathcal{G}_P^{source}\\}$ and a target spatio-temporal graph of data-scarce city $\\mathcal{G}^{target}$. After training on $\\mathcal{G}^{source}_{1:P}$, the model is capable of leveraging the meta knowledge from multiple source graphs and is tasked to predict on a disjoint target scenario, where only few-shot structured data of $\\mathcal{G}^{target}$ is available.\n\\end{myProb}\n\n\n% \\subsection{spatio-temporal Graph Neural Network}\n\n% \\subsection{Graph Neural Network}\n% We treat graph convolutions as a message-passing process in which information can be passed from one node to another along edges directly. Formally, the message-passing GNN is defined as follow:\n% \\begin{equation}\n%     h_{v}^{l} = f(h_{v}^{l-1}, \\sum_{u\\inN(v)}g(h_{u}^{l-1}, x_{vu}^e))\n% \\end{equation}\n% where $h_v^l$ is the $l$-th layer representation of node $v$, $N(v)$ contains the neighborhoods of node $v$, and $x_{vu}^e$ is the edge feature vector of node $(v,u)$. $f(\\cdot)$ and $g(\\cdot)$ are functions with learnable parameters. We initialize $h^0=X$ and the final representation $h^L$ will be used for downstream tasks.\n\n\n"
            },
            "section 4": {
                "name": "Methodology",
                "content": "\n\nIn this section, we describe the proposed ST-GFSL framework in detail. We first give an overview of ST-GFSL, as shown in Figure \\ref{fig:framework}. The left side of the figure shows the input of ST-GFSL. We transfer the knowledge from multiple cities, and the target city only has few-shot training samples. The right side of the figure is mainly composed of two parts: \\emph{Spatio-Temporal Neural Network (STNN)} and \\emph{Cross-city Knowledge Transfer}. Specifically, STNN served as the base feature extractor in ST-GFSL, where any spatial-temporal learning architecture can be used, such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs) and other state-of-the-art spatio-temporal graph learning models. Second, \\emph{Cross-City Knowledge Transfer} module transfers knowledge from multiple source cities, which are depicted in the grey dotted box of Figure \\ref{fig:framework}. Concretely, we first design the \\emph{ST-Meta Learner} to obtain the node-level meta knowledge in both spatial and temporal domain. \nThe non-shared feature extractor parameters $\\theta_{ST}$ are generated to customize the feature extraction among source cities data and target city data.\n\\emph{ST-Meta Graph Reconstruction} is further designed for structure-aware meta training by reconstucting structure relations of different cities. The end-to-end learning process of ST-GFSL follows the MAML-based~\\cite{DBLP:conf/icml/FinnAL17} episode learning.\n%%H.Y.2.07: explain what is maml-based episode learning.\nBy mimicking few-shot scenarios in the target city, batches of few-shot training tasks are sampled to obtain a base model with strong adaptability.\n\n",
                "subsection 4.1": {
                    "name": "Spatio-Temporal Neural Network",
                    "content": "\n\nThe Spatio-Temporal Neural Networks (STNN) can be divided into feature extractor and multi-step predictor, as shown in the bottom dotted box of Figure \\ref{fig:framework}. The multi-step predictors often use one or more layers of fully connected networks ~\\cite{DBLP:conf/ijcai/WuPLJZ19, DBLP:conf/cikm/LuGJFZ20,DBLP:conf/ijcai/YuYZ18} in literature. The feature extractor is designed according to different tasks and data characteristics, like RNN-based, CNN-based and GNN-based models. \nFor example, in the experiment, we select the classical time series analysis networks GRU~\\cite{DBLP:journals/corr/ChungGCB14} and TCN~\\cite{DBLP:conf/cvpr/LeaFVRH17}, as well as the superior spatio-temporal graph neural network models STGCN~\\cite{DBLP:conf/ijcai/YuYZ18} and GWN~\\cite{DBLP:conf/ijcai/WuPLJZ19}.\nSince ST-GFSL is designed for a model-agnostic framework, the parameter generation method adaptively generates non-shared feature extractor parameters $\\theta_{ST}$ according to corresponding model structure. Therefore, our proposed ST-GFSL enables to benefit data-scarce scenarios from recent technique breakthroughs in spatio-temporal graph learning.\n%% HY.2.07: This paragraph is not clear. You could detail one type of base model that is actually used in the experiments.\n\n% In this section, we describe the framework of ST-GFSL as shown in Figure \\ref{fig:framework}. The left side of Figure \\ref{fig:framework} is the model diagram of Meta-STNN. Compared with vanilla Spatio-Temporal Neural Network (abbreviated as STNN), we propose the \\emph{spatio-temporal graph meta knowledge transfer} to generate non-shared model parameters and guide structure-aware meta training. The right side of ST-GFSL framework is the learning process of Meta-STNN. It is trained by sampling batches of few-shot tasks to obtain a base model with strong adaptability.\n\n% First, we will discuss how to extract the node spatio-temporal meta knowledge and the design of Spatio-Temporal Meta Knowledge Learner. Secondly, the parameter generator utilized the learned meta knowledge to generate model parameters that are adaptive to different nodes at different times. Finally, inspired by MAML~\\cite{DBLP:conf/icml/FinnAL17}, we designed a general spatio-temporal graph few-shot learning framework to efficiently perform meta knowledge transfer and model learning.\n\n% , includes two main components: (1) spatio-temporal Graph meta knowledge Transfer: we design the ST-Meta Learner to extract spatio-temporal meta knowledge from different dataset. Then, the Parameter Generator use the ST-Meta Knowledge to generate the hetegenous model parameters. (2) Vanilla spatio-temporal Neural Network: a component can be replaced by different state-of-the-art deep learning model after some changes in parameter generator. The two parts are combined to get the entire network, which we denote as \\emph{Meta-STNN}.\n\n"
                },
                "subsection 4.2": {
                    "name": "ST-Meta Learner",
                    "content": "\n\n% Therefore, we propose spatio-temporal graph meta knowledge transfer mechanism. Instead of using parameter-shared feature extractor, we generate non-shared model parameters according to node-level spatio-temporal meta knowledge. \n% Specifically, we first design the ST-Meta Learner to extract spatio-temporal meta knowledge $\\mathbf{Z}^{MK}$ and construct ST-meta graph $A_{meta}$. Afterwards, for different network structures of feature extractor, we design corresponding parameter generation strategies to generate node-level non-shared parameters.\n%  \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{figures/stmklearner.pdf}\n%     \\caption{Structure of ST-Meta Learner.}\n%     \\label{fig:mklearner}\n% \\end{figure}\n \n% \\subsubsection{ST-Meta Learner}\n\n% GRU derives the representations of different hidden state, which is expressed as:\nSpatio-Temporal Meta Knowledge Learner (ST-Meta Learner) extracts the node-level meta knowledge in both spatial and temporal domains. \nTo encode the temporal dynamics of spatio-temporal graph, we employ Gated Recurrent Unit (GRU) ~\\cite{DBLP:journals/corr/ChungGCB14}, which is widely used in time series modeling. Compared with classical RNN model, GRU retains the ability to extract long sequences and reduces the problem of gradient vanishing or exploding. \nTake node $v_i$ as an example, the node-level temporal meta knowledge $z_{i}^{tp}$ is expressed as the final state of $h_{i,t}$:\n\\begin{equation}\n    \\begin{aligned}\n    z &=\\sigma\\left(U^{z} x_{i, t} + W^{z} h_{i, t-1} \\right) \\\\\n    r &=\\sigma\\left(U^{r} x_{i, t} + W^{r}h_{i, t-1} \\right) \\\\\n    c &=\\phi \\left(U^{c} x_{i, t} +W^{c} \\left(h_{i, t-1} \\circ r\\right) \\right) \\\\\n    h_{i, t} &=(1-z) \\circ c+z \\circ h_{i, t-1}, \n    \\end{aligned}\n\\end{equation}\nwhere $x_{i, t} \\in \\mathbb{R}^d$ is the input vector of node $v_i$ at time $t$, and $h_{i, t-1}$ is the hidden state at time $t-1$. $U^{z}, U^{r}, U^{c} \\in \\mathbb{R}^{d\\times d^{\\prime}}$ and $W^{z}, W^{r}, W^{c} \\in \\mathbb{R}^{d^{\\prime}\\times d^{\\prime}}$ are weight matrices. $\\circ$ is the element-wise multiplication, $\\sigma$ is the nonlinear activation function \\emph{sigmoid}, and $\\phi$ is \\emph{tanh}. Thus, we derive the temporal meta knowledge of one city with GRU model, denoted as $\\mathbf{Z}^{tp} = (z_1^{tp}, z_2^{tp}, \\cdots, z_N^{tp}) \\in \\mathbb{R}^{N \\times d^{\\prime}}$. \n\nTo encode the spatial correlations of spatio-temporal graph, we utilize spatial-based graph attention network (GAT)~\\cite{DBLP:conf/iclr/VelickovicCCRLB18} for feature extraction. Graph attention is treated as a message-passing process in which information can be passed from one node to another along edges directly.\n% due to the dynamic correlations of different node neighbors, \n% GAT is capable of capturing dynamic correlations of different nodes.\n%%HY.2.7: This sentence is not precise. GCN is also a spectral-based method, which can be easily adapt to various structures.\nCompared with spectral-based graph neural network~\\cite{DBLP:conf/nips/DefferrardBV16, DBLP:conf/iclr/XuSCQC19}, GAT can adapt to various network structures. \nTherefore, it is suitable to learn spatial meta knowledge among multiple datasets. \n\nSpecifically, we first apply a shared linear transformation to each group of interconnected nodes and compute the attention coefficients $e_{ij}$:\n\\begin{equation}\n    e_{ij} = attention(W h_i, W h_j), j\\in \\mathcal{N}_{i} \\text{,}\n\\end{equation}\nwhere $W \\in \\mathbb{R}^{d \\times O}$ is the weight matrix and $\\mathcal{N}_{i}$ is a set of neighbor nodes of node $v_{i}$. The attention mechanism makes $\\mathbb{R}^{O} \\times \\mathbb{R}^{O}\\rightarrow \\mathbb{R}$. After that, the attention score is normalized across all choices of $j$ using the $softmax$ function:\n\\begin{equation}\n    \\alpha_{ij} = \\mathop{softmax}\\nolimits_{j} (e_{ij}) = \\frac{exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_{i}}exp(e_{ik})} \\text{.}\n\\end{equation}\nIn order to obtain more abundant representation, we execute the attention mechanism for $K$ times independently and employ averaging to achieve the spatial meta knowledge of node $v_i$:\n\\begin{equation}\n    z_{i}^{sp} = \\sigma(\\frac{1}{K}\\sum\\nolimits_{k=1}^{K}\\sum\\nolimits_{j\\in \\mathcal{N}_{i}}\\alpha_{ij}W^{k}h_{j})\\text{.}\n\\end{equation}\nThus, we derive the spatial meta knowledge of one city $\\mathbf{Z}^{sp} = (z_1^{sp}, z_2^{sp}, \\cdots, z_N^{sp}) \\in \\mathbb{R}^{N \\times d^{\\prime}}$. \n\n%%HY.2.7: how to integrate, be more specific\nBy integrating spatio-temporal features, we obtain the meta knowledge denoted as $\\mathbf{Z}^{MK}=(z_1^{MK}, z_2^{MK}, \\cdots, z_N^{MK}) \\in \\mathbb{R}^{N \\times d_{MK}}$. We weighted-sum the temporal and spatial meta knowledge through a learnable ratio $\\gamma$, and $\\gamma \\in \\mathbb{R}^{d^{\\prime}}$. It learns the impact from spatial or temporal domain in a data-driven manner. Compared with the classical concatenation method, it is easier to adapt the spatio-temporal characteristics of cross city data through a learnable ratio. Meanwhile, it reduces the amount of parameters for data generation, which will be further discussed in \\emph{Parameter Generation}. To be specific, the meta knowledge $\\mathbf{Z}^{MK}$ is calculated as follows:\n\\begin{equation}\n    \\mathbf{Z}^{MK} = W^{\\gamma}(\\gamma \\circ \\mathbf{Z}^{tp} + (1-\\gamma) \\circ \\mathbf{Z}^{sp}) \\text{,}\n\\end{equation}\nwhere $W^{\\gamma} \\in \\mathbb{R}^{d^{\\prime} \\times d_{MK}}$ is the weight matrix for meta knowledge output layer, and $d_{MK}$ is the dimension of meta knowledge.\n\n% meta knowledge is utilized to reconstruct the ST-Meta Graph for structure-aware learning and generate non-shared model parameters for spatio-temporal feature extraction.\n\n"
                },
                "subsection 4.3": {
                    "name": "ST-Meta Graph Reconstruction",
                    "content": "\n\nIn order to express the structural information of graphs and reduces structure deviation caused by different source data distribution, ST-Meta Graph is reconstructed by meta knowledge for structure-aware learning. We predict the likelihood of an edge existing between nodes $v_i$ and $v_j$, by multiplying learned meta knowledge $z_i^{MK}$ and $z_j^{MK}$ as follows:\n\\begin{equation}\n    p(a_{ij} | z_i^{MK},z_j^{MK}) = sigmoid((z_i^{MK})^T, z_j^{MK}).\n\\end{equation}\nAs such, the ST-meta graph $\\mathbf{A}_{meta}$ can be constructed as\n\\begin{equation}\n    \\mathbf{A}_{meta} = sigmoid[(\\mathbf{Z^{MK}})^T \\cdot {\\mathbf{Z^{MK}}}],\n\\end{equation}\nwhere $(\\cdot)^T$ is the transpose of meta knowledge matrix. \n\nIn order to guide the structure-aware learning of meta knowledge, we introduce \\emph{graph reconstruction loss} $\\mathcal{L}_g$ between the ST-meta graph $A_{meta}$ and the original adjacency matrix $A$ in training process, which is calculated as follows:\n\\begin{equation}\n    \\label{Lg}\n    \\mathcal{L}_g = \\| \\mathbf{A}_{meta} - \\mathbf{A} \\|^2.\n\\end{equation}\n\n\n% For the training of meta knowledge representation, on one hand, it is end-to-end optimized through back propagation of the error loss function between the prediction value and the ground truth. On the other hand, we hope that the meta knowledge can fully express the structural information of graph and reduce feature deviation caused by different source data distribution. Therefore, we introduce \\emph{ST-Meta Graph} in learning process. \n% We can predict the likelihood of an edge existing between two nodes, $v_i$ and $v_j$, by multiplying the learned meta knowledge of two nodes as follows:\n% \\begin{equation}\n%     p(a_{ij} | z_i,z_j) = sigmoid(z_i^T, z_j)\n% \\end{equation}\n% Therefore, the ST-meta graph $A_{meta}$ can be constructed as\n% \\begin{equation}\n%     \\mathbf{A}_{meta} = sigmoid[(\\mathbf{Z^{MK}})^T \\cdot {\\mathbf{Z^{MK}}}]\n% \\end{equation}\n% where $T$ is the transpose of meta knowledge matrix. \n% ST-Meta graph represents the structure information based on node-level spatio-temporal meta knowledge. The graph \n\n\n"
                },
                "subsection 4.4": {
                    "name": "Parameter Generation",
                    "content": "\n\n% Since different cities have different temporal and spatial characteristics, shallow models cannot effectively learn the complex features among multiple cities, and under-fitting problem occurs. Meanwhile, deep models are prone to overfitting during the adaptation to the target domain with few-shot data. Therefore, we propose parameter generation to obtain the non-shared parameters of feature extractors according to different node input features. The nodes in target city transfer the knowledge of source cities via parameter matching, retrieving from similar spatio-temporal characteristics.\n\nAfter we obtain the node-level meta knowledge, due to the great difference across cities and within cities, we propose parameter generation to obtain the non-shared parameters of feature extractors for different scenarios.\nWhen the meta-knowledge of a node in target domain is similar to that of a node in multiple source domains, approximate model parameters will be obtained. \nIn other words, the nodes in target city transfer the knowledge of source cities via parameter matching, retrieving from similar spatio-temporal characteristics.\n\n\n% \\weinan{I think how you retrieve `similar' nodes is quite intersting but you seems not to include the details here.}\n% lubin comments: i have revised and added the details about retrieving similar spatio-temporal features for parameter matching in above paragraph.\n\n\nOur parameter generation is a function $\\mathcal{F}$ that takes node-level meta knowledge as input and outputs the non-shared feature extractor parameters $\\theta_{ST}$. Specifically, \\emph{linear layer} and \\emph{convolutional layer} are two basic neural network units. The following introduces how to generate the parameters of these two units respectively.\n\n% Since different cities have different temporal and spatial characteristics, simple models cannot effectively learn the complex features, and under-fitting problem occurs. At the same time, complex models are prone to be overfitting during the adaptation to the target domain with few-shot data. Therefore, the parameter generation method can obtain the parameters of feature extractors according to different node input features, and achieve the balance between the source domain and the target domain.\n% In ST-GFSL, different model parameters can be generated according to the structure of feature extractor.\n\n\\emph{Linear Layer} \\, The expression of the linear layer is $\\mathbf{Y}=\\mathbf{WX}+\\mathbf{b}$, where $\\mathbf{W} \\in \\mathbb{R}^{d_{out}\\times d_{in}}$ is the weight matrix and $\\mathbf{b} \\in \\mathbb{R}^{d_{out}}$ is the bias. We can generate the model parameters $\\mathbf{W}$ and $\\mathbf{b}$ based on meta knowledge $\\mathbf{Z}^{MK}$.\n\nTake node $v_i$ as an example, based on its meta knowledge $z_i^{MK} \\in \\mathbb{R}^{d_{MK}}$, the non-shared weight matrix $\\mathbf{W_i}$ is generated through a two-step method as shown in Figure \\ref{fig:param_w}. \nFirst, we perform a linear transformtion through $\\mathcal{F}_W^1: \\mathbb{R}^{d_{MK}} \\rightarrow \\mathbb{R}^{{d_{in} \\cdot d_{MK}}}$ and conduct a dimension transformation: $\\mathbb{R}^{{d_{in} \\cdot d_{MK}}} \\rightarrow \\mathbb{R}^{d_{in} \\times d_{MK}}$. Secondly, we perform the second linear transformation $\\mathcal{F}_W^2: \\mathbb{R}^{d_{in} \\times d_{MK}} \\rightarrow \\mathbb{R}^{d_{in} \\times d_{out}}$ and achieve the weight matrix $\\mathbf{W_i}$ of node $v_i$. The parameter generation of bias $\\mathbf{b}$ can be obtained directly by once linear transformation $\\mathcal{F}_b$: $\\mathbb{R}^{d_{MK}} \\rightarrow \\mathbb{R}^{d_{out}}$. \n\nThe parameter number of the weight matrix $\\mathbf{W_{i}}$ generated by two-step methods is $d_{MK} \\cdot [d_{in} \\times d_{MK} + d_{out}]$. Compared with direct generation of one linear layer ($\\mathbb{R}^{d_{MK}} \\rightarrow \\mathbb{R}^{d_{in} \\times d_{out}}$, the parameter number is $d_{MK} \\times d_{in} \\times d_{out}$). Obviously, two-step generation has less parameters. For example, in our experiment, when $d_{MK}=16$, $d_{in}=8$, $d_{out}=32$, the parameter number of using the two-step method is 2560. Whereas, the parameter number of directly using the one-step method is 4096. Our parameter generation method reduces the number of parameters by 37.5\\%.\n\n\n\n\\emph{Convolutional Layer} \\,\nSimilar to the linear layer, the parameter generation of the convolutional layer adopts the two-step method shown in Figure \\ref{fig:param_conv}, where $C_{in}$ is the number of channels of input data, $C_{out}$ is the number of channels of output data, and $(K_{H}, K_{W})$ is the size of the convolution kernel. Through the two-step generation, the convolutional kernel $\\mathbf{W}_{i}^{conv}$ of node $v_i$ can be obtained after dimension reshape: $\\mathbb{R}^{C_{in} \\times (C_{out} \\cdot K_H \\cdot K_W)} \\rightarrow \\mathbb{R}^{C_{in} \\times C_{out} \\times K_H \\times K_W}$. For 1D-convolution, the model parameters can be obtained only by adjusting the dimension of the convolutional kernel.\n\n%%HY.2.7: Do you reduce the number of parameters in convolutional layers? If yes, you can point it out.\n\n\n\n% Spatio-Temporal Neural Networks (STNN) can mainly divide the network model into feature extractor and multi-step predictor, as shown in the bottom dotted box of Figure \\ref{fig:framework}. Multi-step predictors often use one or more layers of fully connected networks ~\\cite{DBLP:conf/ijcai/WuPLJZ19, DBLP:conf/cikm/LuGJFZ20}. The feature extractor is designed according to different tasks and data characteristics, like RNN-based model ~\\cite{DBLP:conf/iclr/LiYS018}, CNN-based model~\\cite{DBLP:conf/ijcai/YuYZ18} and GNN-based models~\\cite{DBLP:conf/aaai/ZhengFW020}. In ST-GFSL, the non-shared feature extractor parameters $\\theta_{ST}$ are adaptively generated using parameter generation method. Since most feature extractors network can be obtained through linear layers or convolutional networks, ST-GFSL is suitable for any feature extraction network. Therefore, ST-GFSL enables data-scarce scenarios to benefit from recent technique breakthroughs in spatio-temporal graph learning. \n\n% For commonly used deep learning models, most of them can be obtained through linear layer and convolutional layer, so we can use any feature extractor in our proposed framework ST-GFSL in principle. \n\n% In the experiment part, we use Gated Recurrent Unit (GRU) ~\\cite{DBLP:journals/corr/ChungGCB14}, graph attention network (GAT) ~\\cite{DBLP:conf/iclr/VelickovicCCRLB18}, 1D dilated convolution network-based temporal convolution network (TCN) ~\\cite{DBLP:conf/cvpr/LeaFVRH17}, and the hybrid models combining the above spatial and temporal models for fully validation. We also introduce the superior model STGCN ~\\cite{DBLP:conf/ijcai/YuYZ18} in spatio-temporal data mining, which is a more complex feature extractor. \n\n"
                },
                "subsection 4.5": {
                    "name": "ST-GFSL Learning Process",
                    "content": "\n\nTo handle adaptation with few-shot scenarios, the learning process of ST-GFSL follows the MAML-based episode learning process. ST-GFSL trains the spatio-temporal graph learning model with two stage: \\emph{base-model meta training} and \\emph{adaptation}. In the \\emph{base-model meta training} stage, inspired by MAML~\\cite{DBLP:conf/icml/FinnAL17}, ST-GFSL imitates the adaptation process when encountering a new few-shot scene, and optimizes the adaptation capability.\nDifferent graph sequences are sampled from multiple large-scale datasets (source datasets) to form a batch of training tasks $\\mathcal{T}_{ST}$. Each task $\\mathcal{T}_i \\in \\mathcal{T}_{ST}$ includes $K_\\mathcal{S}$ support sets $\\mathcal{S}_{i}$ and $K_\\mathcal{Q}$ query sets $\\mathcal{Q}_{i}$. In the \\emph{adaptation} stage, the ST-GFSL model updates the parameters via several gradient descent steps on target domain data.\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=0.8\\linewidth]{figures/framework_meta_train.pdf}\n%     \\caption{ST-GFSL Meta Training Stage Framework.}\n%     \\label{fig:st_maml}\n% \\end{figure}\n\nTo be specific, ST-GFSL first samples batches of task sets $\\mathcal{T}_{ST}$ from source datasets. Each task $\\mathcal{T}_i \\in \\mathcal{T}_{ST}$ belongs to one single city, and\nis divided into support set $\\mathcal{S}_{\\mathcal{T}_i}$, query set $\\mathcal{Q}_{\\mathcal{T}_i}$ and $\\mathcal{S}_{\\mathcal{T}_i} \\cap \\mathcal{Q}_{\\mathcal{T}_i} = \\emptyset$. \nWhen learning a task $\\mathcal{T}_i$, ST-GFSL considers a joint loss function\ncombining prediction error loss $\\mathcal{L}_e$ and graph reconstruction loss $\\mathcal{L}_g$. The prediction error loss $\\mathcal{L}_e$ is the root mean square error between multi-step prediction and ground truth of support set $\\mathcal{S}_{\\mathcal{T}_i}$:\n\\begin{equation}\n\\label{Le}\n    \\mathcal{L}_e = \\frac{1}{\\|\\mathcal{S}_{\\mathcal{T}_i}\\|}\\sum_{(x_j, y_j)\\in \\mathcal{S}_{\\mathcal{T}_i}} (f_{\\theta}(x_j) - y_j)^2.\n\\end{equation}\n\nAs given in Equation \\ref{Lg}, graph reconstruction loss $\\mathcal{L}_g$ represents the structure-aware capability of meta knowledge. \n% When learning a task $\\mathcal{T}_i$, ST-GFSL considers a joint loss function\n% combining graph reconstruction loss $\\mathcal{L}_g$ and prediction error loss $\\mathcal{L}_e$. As given in Equation \\ref{Lg}, graph reconstruction loss represents the structure-aware capability of meta knowledge. \n% $\\mathcal{L}_e$ is the root mean square error between multi-step prediction and ground truth of support set $\\mathcal{S}_{\\mathcal{T}_i}$:\n% \\begin{equation}\n% \\label{Le}\n%     \\mathcal{L}_e = \\frac{1}{\\|\\mathcal{S}_{\\mathcal{T}_i}\\|}\\sum_{(x_j, y_j)\\in \\mathcal{S}_{\\mathcal{T}_i}} (f_{\\theta}(x_j) - y_j)^2.\n% \\end{equation}\nConsequently, the joint loss funtion $\\mathcal{L}$ is:\n% The graph reconstruction loss $\\mathcal{L}_g$ between ST-Meta graph and input graph is calculated as Equation \\ref{Lg}. \n% Combining previous graph reconstruction loss $\\mathcal{L}_g$ in Equation \\ref{Lg} and prediction error loss $\\mathcal{L}_e$ in Equation \\ref{Le}, we obtain the joint loss function $\\mathcal{L}$:\n\\begin{equation}\n    \\label{loss_func}\n    \\mathcal{L} = \\mathcal{L}_e + \\lambda \\mathcal{L}_g,\n\\end{equation}\nwhere $\\lambda$ is the sum scale factor of two loss functions. \nTo be specific, the meta objective is to minimize the sum of task loss on query sets, which is expressed as follows:\n\\begin{equation}\n    \\theta^{*} = \\mathop{\\arg\\min}_{\\theta} \\sum\\limits_{\\mathcal{T}_i \\in \\mathcal{T}_{ST}}\\mathcal{L}_{\\mathcal{T}_i}(f_{\\theta^{\\prime}_i}) .\n\\end{equation}\nIn order to achieve the optimal model parameter $\\theta^{*}$, Algorithm \\ref{algorithm} outlines the \\emph{base-model meta training} process of ST-GFSL. First of all, we iteratively sample a batch of task sets $\\mathcal{T}_{ST}$ from source datasets (line 4). With regard to training task $\\mathcal{T}_i \\in \\mathcal{T}_{ST}$, task-specific model parameter $\\theta_{\\mathcal{T}_i}^{\\prime}$ is updated by gradient descents for several steps (line 8-9). The gradient of $f_{\\theta_i^{\\prime}}$ based on query set $Q_{\\mathcal{T}_i}$ is given in line 10. Finally, the general model parameters $\\theta$ are trained based on the summation across all the meta-training tasks (line 11).\n\n\\begin{algorithm}\n  \\KwIn{Source spatio-temporal Graph Dataset $\\mathcal{G}_{ST}$, learning rate hyperparameter $\\alpha, \\beta$}\n  \n  \\KwOut{Trained ST-GFSL model parameters $\\theta^{*}$}\n  $\\theta \\leftarrow$ random initalization\\; // $\\theta=\\theta_{ST}+\\theta_{\\text{predictor}}$\n  \n  \\While{not done}{\n        Sample a batch of tasks $\\mathcal{T}_{ST} \\leftarrow \\textsc{SampleTask} (\\mathcal{G}_{ST})$ \\;\n        \n        \\For{$\\mathcal{T}_i \\in \\mathcal{T}_{ST}$}{\n            $\\mathcal{S}_{\\mathcal{T}_i} \\leftarrow K_\\mathcal{S}$ support set sample from $\\mathcal{T}_i$ \\;\n            \n            $\\mathcal{Q}_{\\mathcal{T}_i} \\leftarrow K_\\mathcal{Q}$ query set sample from $\\mathcal{T}_i$ \\;\n            \n            Evaluate $\\nabla_\\theta \\mathcal{L}_{\\mathcal{T}_i}(f_{\\theta})$ with $S_{\\mathcal{T}_i}$ via Equation (\\ref{loss_func}) \\;\n            \n            Compute adapted parameters with gradient descent: $\\theta^{\\prime}_{\\mathcal{T}_i} = \\theta - \\alpha \\nabla_\\theta \\mathcal{L}_{\\mathcal{T}_i}(f_{\\theta})$ \\;\n            \n            Evaluate $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}_i}(f(\\theta^{\\prime}_{\\mathcal{T}_i}))$ with $\\mathcal{Q}_{\\mathcal{T}_i}$ via Equation (\\ref{loss_func}) \\;\n        }\n        Update $\\theta^{*} \\leftarrow \\theta - \\beta \\nabla_{\\theta} \\Sigma_{\\mathcal{T}_i}\\mathcal{L}_{\\mathcal{T}_i}(f_{\\theta^{\\prime}_i})$\n  }\n  \\caption{ST-GFSL base-model meta training}\n  \\label{algorithm}\n\\end{algorithm}\n\n"
                }
            },
            "section 5": {
                "name": "Experiment",
                "content": "\n\nIn this section, we evaluate ST-GFSL in various aspects through extensive experiments. Specifically, we try to answer the following research questions through our evaluation:\n\\begin{itemize}\n    \\item [\\textbf{RQ1}] How well does ST-GFSL perform against other baseline methods on different datasets?\n\\item [\\textbf{RQ2}] How well are different spatio-temporal prediction models adaptable under the ST-GFSL framework?\n\\item [\\textbf{RQ3}] How does each proposed component of model contribute to the performance of ST-GFSL?\n\\item [\\textbf{RQ4}] How does each major hyperparameter affect the performance of ST-GFSL?\n\\end{itemize}\n",
                "subsection 5.1": {
                    "name": "Experiment Settings",
                    "content": "\n",
                    "subsubsection 5.1.1": {
                        "name": "Dataset",
                        "content": " In the experiment, we take traffic flow prediction as an example to verify our proposed framework. We evaluate the performance of ST-GFSL on four traffic flow datasets: \\emph{METR-LA}, \\emph{PEMS-BAY}, \\emph{Didi-Chengdu}, \\emph{Didi-Shenzhen} ~\\cite{DBLP:conf/iclr/LiYS018, Didi_dataset}.\n\nIn the experiment, the datasets are divided into source datasets, target dataset and test dataset. Take METR-LA as an example. When it is set as the target city, three-day data (a very small amount of data, compared with most works requiring several months of data) are selected as target data for \\emph{adaptation} and the rest are regarded as test data. Other three datasets (PEMS-BAY, Didi-Shenzhen and Didi-Chengdu) are used as source datasets for \\emph{meta training}. The same division method is used for other datasets and Z-score normalization are applied for data preprocessing.\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Metrics",
                        "content": "\nIn order to fully verify the performance of our framework, we predict the traffic flow in the next 6 time steps with 12 historical time steps. Accordingly, the time step of METR-LA and PEMS-BAY datasets are 5 minutes, while Didi-Chengdu and Didi-Shenzhen datasets are 10 minutes due to the availability. Two widely used metrics are applied between the multi-step prediction and the ground truth for evaluation: Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n% \\begin{itemize}[left=1em]\n%     \\item Mean Absolute Error (MAE)\n%         \\begin{equation*}\n%             MAE(y,\\hat{y}) = \\frac{1}{N}\\sum\\nolimits_{i=1}^{N}\\left|\\hat{y_{i}}-y_{i} \\right|\\text{.}\n%         \\end{equation*}\n%     % \\item Mean Absolute Percentage Error (MAPE)\n%     %     \\begin{equation*}\n%     %         MAPE(y,\\hat{y}) = \\frac{100\\%}{N}\\sum\\nolimits_{i=1}^{N}\\left|\\frac{\\hat{y_{i}}-y_{i}}{y_{i}} \\right|\\text{.}\n%     %     \\end{equation*}\n%     \\item Root Mean Squared Error (RMSE)\n%         \\begin{equation*}\n%             RMSE(y,\\hat{y}) = \\sqrt{\\frac{1}{N}\\sum\\nolimits_{i=1}^{N}(\\hat{y_{i}}-y_{i})^{2}}\\text{.}\n%         \\end{equation*}\n% \\end{itemize}\n\n% \\subsubsection{Implementation} We implement ST-GFSL based on \\emph{Pytorch} framework\\footnote{The implementation code and details of our model is available at anonymous Github repo: https://github.com/RobinLu1209/ST-GFSL}. Totally, there are several important hyperparameters in our model, and we set them as: the dimension of meta knowledge $d_{MK} = 16$, task learning rate $\\alpha = 0.01$, meta-training rate $\\beta = 0.001$, task batch number $\\|\\mathcal{T}\\| = 5$, and sum scale factor of two loss function $\\lambda = 1.5$.\n\n\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Performance Comparison",
                    "content": "\n\n",
                    "subsubsection 5.2.1": {
                        "name": "Superiority of ST-GFSL over Baselines",
                        "content": "\n\nFirst, we compare the performance of ST-GFSL with a series of baselines on four datasets:\n% [leftmargin=*]\n\\begin{itemize}[left=1em]\n    \\item \\textbf{HA}: Historical Average, which formulates the traffic flow as a seasonal process, and uses average of previous seasons as the prediction.\n    \\item \\textbf{ARIMA}: Auto-regressive integrated moving average is a well-known model that can understand and predict future values in a time series.\n    \\item \\textbf{Target-only}: Directly training the model on few-shot data in target domain.\n    \\item \\textbf{Fine-tuned (Vanilla)}: We first train the model on source datasets, and then fine-tune the model on few-shot data in target domain.\n    \\item \\textbf{Fine-tuned (ST-Meta)}: Compared with ``Fine-tuned (Vanilla)'' method, we combine the proposed parameter generation based on meta knowledge to generate non-shared parameters for the model.\n    \\item \\textbf{AdaRNN~\\cite{Du2021ADARNN}}: A state-of-the-art transfer learning framework for non-stationary time series. This paper aims to reduce the distribution mismatch in the time series to learn an adaptive RNN-based model.\n    \\item \\textbf{MAML~\\cite{DBLP:conf/icml/FinnAL17}}: Model-Agnostic Meta \n    Learning (MAML), a superior meta-learning method that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task.\n\\end{itemize}\n\nWe can divide the above compared baselines into two categories: non-transfer methods and transfer methods. Non-transfer methods (HA, ARIMA, Target-only) only use the few-shot training data in target city. The rest are transfer methods, and these methods transfer the knowledge learned from multiple source datasets. Since AdaRNN uses GRU model for feature extraction, in order to maintain the fairness and consistency of comparison, other deep learning methods also use GRU model as feature extractor.\n\nTable \\ref{tab:performance} shows the performance comparison under various methods. We make the following observations: (1) Our proposed framework ST-GFSL obtains the best results on multiple datasets in both short-term and long-term predictions, demonstating the superiority of our method.\n(2) In respect of AdaRNN, it performs better on some indicators, especially the mid- and long-term prediction on METR-LA dataset. The possible reason is that METR-LA is a relatively small-scale dataset with only 207 sensor nodes, and there is a strong temporal correlation between nodes. The AdaRNN's design for temporal covariate shift makes it have a better performance in long-term prediction. However, AdaRNN is unstable when adapting to other cities (e.g. PEMS-BAY), and the experiment performance fluctuates greatly on different datasets.\n(3) For two fine-tuning methods, Fine-tuned (ST-Meta) based on parameter generation has a significant improvement compared to vanilla method. The results imply that our proposed non-shared parameters can better extract spatio-temporal features across the cities. \n(4) MAML shows good performance in the experiment. Meanwhile, MAML can be regarded as an ablation study of ST-GFSL, which will be further discussed in Section 5.3.\n\n% MAML shows good performance in the experiment and the effectiveness of episode learning. At the same time, MAML can be regarded as an ablation experimental model after the degradation of our method, which will be introduced in Section 4.2.\n\n% Secondly, our proposed framework ST-GFSL obtains the superior results on three datasets (PEMS-BAY, Didi-Shenzhen and Didi-Chengdu) in both short-term and long-term predictions, demonstating the benefits of meta knowledge transfer mechanism and structure-aware training process. In respect of the recently-proposed model, AdaRNN performs better on METR-LA dataset for mid- and long-term prediction. The possible reason is that METR-LA is a relatively small-scale dataset with only 207 sensor nodes, \n% and there is a strong temporal correlation between nodes. The AdaRNN design for temporal covariate shift makes it have a better performance in long-term prediction. However, AdaRNN is unstable when adapting to new cities (e.g. PEMS-BAY), and the experiment performance fluctuates greatly on different datasets. Lastly, MAML in performance comparison adds the graph reconstruction loss in meta-training process. Therefore, it can also be regarded as an ablation study of ST-GFSL and the details are further described in Section 5.3.\n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "ST-GFSL for different feature extractors",
                        "content": "\n\nST-GFSL is a model-agnostic framework. In order to verify the versatility of the framework and further improve the prediction performance from STNN model aspect, we apply some advanced spatio-temporal data graph learning algorithms to our ST-GFSL framework. At the same time, we also train the following model directly on few-shot data in target domain to compare these two methods.\n\n\\begin{itemize}[left=1em]\n    \\item \\textbf{TCN~\\cite{DBLP:conf/cvpr/LeaFVRH17}}: 1D dilated convolution network-based temporal convolution network.\n    \\item \\textbf{STGCN~\\cite{DBLP:conf/ijcai/YuYZ18}}: Spatial temporal graph convolution network, which combines graph convolution with 1D convolution.\n    \\item \\textbf{GWN~\\cite{DBLP:conf/ijcai/WuPLJZ19}}: A convolution network structure combines graph convolution with dilated casual convolution and a self-adaptive graph.\n\\end{itemize}\n\n\n% The RMSE of the 6-th step prediction of GWN on Didi-Shenzhen dataset is reduced from 4.170 to 4.088 compared with TCN, and from 7.073 to 6.778 on METR-LA dataset.\nFigure \\ref{fig:model_compare} shows the performance of different feature extractors on Didi-Shenzhen and METR-LA datasets. STGCN and Graph WaveNet, as the representative work of traffic speed prediction, still maintain their strong feature extraction capabilities under the ST-GFSL framework. \nCompared with direct training on few-shot target domain, ST-GFSL improves the performance greatly. Especially for STGCN, direct training with few-shot samples has over-fitting, which makes it impossible to predict accurately.\n\n% This shows that ST-GFSL is adapted to different network structures, such as RNN, CNN, GNN, etc., and has strong scalability and versatility.\n\n% In view of the length of the paper, Figure \\ref{fig:performance_diff_model} shows the performance of different base models on PEMS-BAY and Didi-Shenzhen datasets in the next several steps. It can be seen that the prediction error decreases with the upgrading of feature extractors. STGCN and Graph WaveNet, as the representative work of traffic speed prediction, still maintain their strong feature extraction capabilities under the ST-GFSL framework. This shows that ST-GFSL is adapted to different network structures, such as RNN, CNN, GNN, etc., and has strong scalability and versatility.\n\n% \\begin{figure}\n%     \\centering\n%     \\subfigure[Didi-Shenzhen Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/performance_model_shenzhen.pdf}\n%     }\n%     \\\\\n%     \\subfigure[METR-LA Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/performance_model_metr.pdf}\n%     }\n%     \\caption{Performance comparison of different feature extractor with ST-GFSL on Didi-Shenzhen and METR-LA dataset.} \n%     \\label{fig:performance_diff_model}\n% \\end{figure}\n"
                    },
                    "subsubsection 5.2.3": {
                        "name": "Performance across the cities",
                        "content": "\nIn order to explore the performance of transfering across multiple cities, we conduct experiments on different source cities, and obtain the experimental results in Figure \\ref{fig:source}. For simplicity, METR-LA dataset is denoted as ``M'', PEMS-BAY dataset is denoted as ``P'', Didi-Chengdu dataset is denoted as ``C'', and Didi-Shenzhen dataset is denoted as ``S''. \n\nAs shown in Figure \\ref{fig:source}(a), in Step \\#1, only use Didi-Chengdu dataset as the source city obtain the best performance. In Step \\#3 and Step \\#6, the best results are obtained by transfering the knowledge of three cities. In the short-term prediction, since Shenzhen and Chengdu are both first tier cities in China, they have more similar spatio-temporal characteristics. In long-term prediction, the knowledge transfer of multiple cities can help better predict long-term correlations. For METR-LA dataset, better performance is usually achieved when ``P+S'' is used as the source domain. In Step \\#3, the best performance can be obtained by using all datasets.\n\n\n\n\n\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Ablation Study",
                    "content": "\n\nIn this section, we verify the effectiveness of each module in ST-GFSL through ablation study. First of all, we only use time domain features (M1a) or only spatial domain features (M1b) as meta knowledge for parameter generation. \nWe can see that the performance has decreased to a certain extent, which proves that the spatio-temporal joint features are more accurate for parameter generation. \nIt is worth noting that the short-time prediction of METR-LA datasets achieves the best performance by using only temporal meta knowledge. This is because its temporal characteristics are more prominent.\nFurthermore, we directly use trainable random parameters to replace the learned spatio-temporal features (M1c). Since the spatio-temporal attributes are not explicitly defined, it is difficult for the random initialized vector to capture the complex and dynamic characteristics, so the performance has been greatly reduced. \nWhen we remove the parameter generator directly (M2), it degenerates into a vanilla neural network model trained by proposed ST-GFSL framework. Compared with the non-shared model parameters generated by meta knowledge, the performance has more degradation. When we subtract the graph reconstruction loss function during the training process (M3), the model has a severe performance degradation. The reason lies in that meta knowledge is not only expected to be able to extract effective spatio-temporal features, but more importantly, these features are compatible with structural information and can be generalized to more scenarios.\n% Due to space limitations, Table \\ref{tab:ablation} shows the results on METR-LA and Didi-Chengdu datasets.The other two datasets also have similar results.\n\n% For the spatio-temporal meta knowledge, first of all, we only use time domain features (M1a) or only spatial domain features (M1b) as meta knowledge for parameter generation. We can see that the model performance has decreased to a certain extent, which proves that the spatio-temporal joint features are more accurate for parameter generation. \n% % However, we see the advantages of several indicators in (M1a) and (M1b). This is because the temporal characteristics of METR-LA are more prominent, and the spatial characteristics of Didi-Chengdu are more dominant. Nevertheless, in general, the combination of temporal and spatial features can have a better effect. \n% In addition, we directly use trainable random parameters to replace the learned spatio-temporal features (M1c). Since the spatio-temporal attributes are not explicitly defined, it is difficult for the random initialized vector to capture the complex and dynamic characteristics, so the performance has been greatly reduced. \n\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{multirow}\n% \\usepackage{graphicx}\n% \\usepackage[table,xcdraw]{xcolor}\n% If you use beamer only pass \"xcolor=table\" option, i.e. \\documentclass[xcolor=table]{beamer}\n\n\n"
                },
                "subsection 5.4": {
                    "name": "Case Study",
                    "content": "\n\nIn order to further explore the influence of \\emph{graph reconstruction loss} in cross-city knowledge transfer, we perform visual analysis on the original adjacency matrix and reconstructed ST-Meta graph. For clarity, we select the adjacency relationship of the first 30 nodes as shown in Figure \\ref{fig:case_study}. It can be seen that in the process of graph reconstruction, some key structural relationships (represented by red boxes in the figure) are better reconstructed. This shows that the proposed ST-meta graph reconstruction achieves expected effects, and it greatly improves the prediction performance by avoiding structural deviation as shown in the ablation study.\nIndeed, in the process of knowledge transfer learning across multiple cities, some structural information has not been well captured. This is a great challenge in graph data transfer learning, i.e., how to avoid structure deviation among multiple source datasets, which will become our future work.\n\n\n\n"
                },
                "subsection 5.5": {
                    "name": "Hyperparameter Analysis",
                    "content": "\n\nWe select the hyperparameters by analyzing the experimental results as shown in Figure \\ref{fig:hyper}. (1) First, we change the dimension of meta knowledge $d_{MK}$. The dimension of meta knowledge directly affects the efficiency of parameter generation. Overall, when $d_{MK}=16$, there are better performance results in both short and long term prediction. (2) Besides, the trade-off between the two loss functions is an important consideration. By observing the changes of two loss functions, we set $\\lambda$ to change from 0.5 to 2.0 in the hyperparameter experiments. When we set $\\lambda \\textgreater 1$, we can often get better results, which also reflects the importance of graph reconstruction loss. (3) In the experiment, we all use 3-days target domain data as few-shot samples. When we adjust from 1 day to 7 days, the improvement of model performance is gradually significant in general.\n\n\n\n\n% All the following experiments are conducted with ST-GFSL based GRU and test on PEMS-BAY dataset. \n\n% First, we change the dimension of meta knowledge $d_{MK}$ as shown in Figure \\ref{fig:hyper_meta_dim}. The dimension of meta knowledge directly affects the efficiency of parameter generation. When we adjust the dimension from 8 to 20, it can be seen that the impact of $d_{MK}$ is small in short-term prediction, and the best effect can be obtained when the dimension is 16 in the 30-minute and 60-minute predictions. Therefore, we chose $d_{MK} = 16$ in the experiment.\n\n% % Hyper-param: d_MK\n% \\begin{figure}\n%     \\centering\n%     \\subfigure[Didi-Shenzhen Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/hyper_meta_dim_shenzhen.pdf}\n%     }\n%     \\\\\n%     \\subfigure[METR-LA Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/hyper_meta_dim_metr.pdf}\n%     }\n%     \\caption{Performance comparison with different hyperparameter meta knowledge dimension $d_{MK}$} \\label{fig:hyper_meta_dim}\n% \\end{figure}\n\n% Besides, the trade-off between the two loss functions is an important consideration. By observing the changes of two loss functions, we set $\\lambda$ to change from 0.5 to 2.0 in the hyperparameter experiments. As shown in Figure \\ref{fig:hyper_lambda}, the model has the best performance when $\\lambda=1.5$, which highlights that the graph reconstruction loss plays a vital role in model optimization.\n\n% % Hyper-param: \\lambda_loss\n% \\begin{figure}\n%     \\centering\n%     \\subfigure[Didi-Shenzhen Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/hyper_loss_shenzhen.pdf}\n%     }\n%     \\\\\n%     \\subfigure[METR-LA Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/hyper_loss_metr.pdf}\n%     }\n%     \\caption{Performance comparison with different hyperparameter sum scale factor $\\lambda$} \\label{fig:hyper_loss}\n% \\end{figure}\n\n% In the experiment, we all use 3-days target domain data as few-shot samples. When we adjust from 1 day to 7 days, the improvement of model performance is gradually significant in general as shown in Figure \\ref{fig:hyper_tg_data}. \n\n% % Hyper-param: \\target\n% \\begin{figure}\n%     \\centering\n%     \\subfigure[Didi-Shenzhen Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/hyper_target_shenzhen.pdf}\n%     }\n%     \\\\\n%     \\subfigure[METR-LA Dataset]{\n%         \\includegraphics[width=\\linewidth]{figures/hyper_target_metr.pdf}\n%     }\n%     \\caption{Performance comparison with different hyperparameter training sample days of target city} \\label{fig:hyper_target}\n% \\end{figure}\n\n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this paper, we first propose a spatio-temporal graph few-shot learning framework called ST-GFSL for cross-city knowledge transfer. \nNon-shared feature extractor parameters based on node-level meta knowledge improve the effectiveness of spatio-temporal representation on multiple datasets and transfer the cross-city knowledge via parameter matching from similar spatio-temporal meta knowledge.  \nST-GFSL integrates the graph reconstruction loss to achieve structure-aware learning.\nExtensive experimental results in the running case of traffic speed prediction demonstrate the superiority of ST-GFSL over other baseline methods. Besides traffic speed prediction, ST-GFSL could be applied to other\nfew-shot scenarios equipped with spatio-temporal graph learning, such as taxi demand prediction in different cities, indoor environment monitoring in different warehouses, etc. In the future, we will further explore the problem of structure deviation in knowledge transfer among graph data.\n\n% the knowledge transfer considering the heterogeneity of graphs.\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nThis work was partially supported by National Key R\\&D Program of China (No.2018AAA0101200), NSF China (No. 42050105, 62020106005, 62061146002, 61960206002, 61829201, 61832013), the Science and Technology Innovation Program of Shanghai (No.19YF1424500), and the Program of Shanghai Academic/Technology Research Leader under Grant No.18XD1401800.\n\n\\bibliographystyle{unsrt}\n\\bibliography{reference}\n\n\\appendix\n\n"
            },
            "section 8": {
                "name": "Appendix",
                "content": "\n\nTo support the reproducibility of the results in this paper, we have released our code and data.\\footnote{The implementation code and details of our model is available at Github repo: https://github.com/RobinLu1209/ST-GFSL}. Our implementation is based on torch 1.8.1, and torch-geometric 1.7.2.\nAll the evaluated models are implemented on a server with two CPUs (Intel Xeon E5-2630 $\\times$ 2) and four GPUs (NVIDIA GTX 2080 $\\times$ 4).\nHere, we detail the datasets, evaluation metrics, baselines, and the training settings of ST-GFSL.\n\n",
                "subsection 8.1": {
                    "name": "Datasets",
                    "content": "\n\\label{appendix-dataset}\n\nWe take four public dataset of traffic flow for evaluation. We adopt the same data preprocessing procedures in literature. In all those datasets, we apply Z-Score normalization for data preprocessing. The missing values are filled by the linear interpolation. To construct the road network graph, each traffic sensor or road segment is considered as a vertex and we compute the pairwise road network distances between sensors. \nThe adjacency matrix of the nodes is constructed by\nroad network distance with a thresholded Gaussian kernel. The detailed information of each dataset is shown in Table \\ref{tab:datasets}.\n\n% Please add the following required packages to your document preamble:\n% \\usepackage{graphicx}\n\n\n\\begin{itemize}[left=1em]\n    \\item \\textbf{METR-LA}~\\cite{DBLP:conf/iclr/LiYS018}: Traffic data are collected from observation sensors in the highway of Los Angeles County. We use 207 sensors and 4 months of data dated from 1st Mar 2012 until 30th Jun 2012 in the experiment. The readings of the sensors are aggregated into 5-minutes windows.\n    \\item \\textbf{PEMS-BAY}~\\cite{DBLP:conf/iclr/LiYS018}: PEMS-BAY dataset contains 6 months of data recorded by 325 traffic sensors ranging from January 1st, 2017 to June 30th, 2017 in the Bay Area. \n    \\item \\textbf{Didi-Chengdu}~\\cite{Didi_dataset}: Traffic index dataset of Chengdu, China, provided by Didi Chuxing GAIA Initiative. We select data from January to April 2018, with 524 roads in the core urban area of Chengdu. The data collection interval is 10 minutes.\n    \\item \\textbf{Didi-Shenzhen}~\\cite{Didi_dataset}: Traffic index dataset of Shenzhen, China, provided by Didi Chuxing GAIA Initiative. We select data from January to April 2018, and it contains 627 roads in downtown Shenzhen. The data collection interval is 10 minutes.\n\\end{itemize}\n\n\n\n"
                },
                "subsection 8.2": {
                    "name": "Evaluation Metrics",
                    "content": "\n\nTo verify the effectiveness of the proposed algorithm, we evaluate the results of multi-step prediction. \n% For each of the four data sets, we use historical data of 12 time steps to predict the future results of 6 time steps. Due to the different time resolutions of different datasets, Didi-Shenzhen and Didi-Chengdu datasets will predict the results in the next 60 minutes, while METR-LA and PEMS-BAY datasets will predict the results in the next 30 minutes.\nTwo widely used metrics are applied between the multi-step prediction $\\hat{y}$ and the ground truth $y$ for evaluation: Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).\n\\begin{itemize}[left=1em]\n    \\item Mean Absolute Error (MAE)\n        \\begin{equation*}\n            MAE(y,\\hat{y}) = \\frac{1}{N}\\sum\\nolimits_{i=1}^{N}\\left|\\hat{y_{i}}-y_{i} \\right|\\text{.}\n        \\end{equation*}\n    \\item Root Mean Squared Error (RMSE)\n        \\begin{equation*}\n            RMSE(y,\\hat{y}) = \\sqrt{\\frac{1}{N}\\sum\\nolimits_{i=1}^{N}(\\hat{y_{i}}-y_{i})^{2}}\\text{.}\n        \\end{equation*}\n\\end{itemize}\n\n"
                },
                "subsection 8.3": {
                    "name": "Baselines",
                    "content": "\nThe details of the baselines are as follows:\n\\begin{itemize}[left=1em]\n    \\item \\textbf{HA}: Historical Average, which formulates the traffic flow as a seasonal process, and uses average of previous seasons as the prediction. We use the few-shot sample of target city to calculate the average traffic speed of each node in one day, and use this as the baseline to predict the future values.\n    \\item \\textbf{ARIMA}: Auto-regressive integrated moving average is a well-known model that can understand and predict future values in a time series. The model are implemented using the statsmodel python package. The orders are (3,0,1).\n    \\item \\textbf{Target-only}: We directly use the few-shot training samples of target domain to train the model, without utilizing the training data of other source cities for knowledge transfer.\n    \\item \\textbf{Fine-tuned (Vanilla)}: We first train the model on source datasets, and then fine-tune the model on few-shot data in target domain. Here we used all the multiple source cities for training.\n    \\item \\textbf{Fine-tuned (ST-Meta)}: Compared with ``Fine-tuned (Vanilla)'' method, we combine the proposed parameter generation based on meta knowledge to generate non-shared parameters for the model. \n    \\item \\textbf{AdaRNN~\\cite{Du2021ADARNN}}: A state-of-the-art transfer learning framework for non-stationary time series. This paper aims to reduce the distribution mismatch in the time series to learn an adaptive RNN-based model. In the experiment, we predict each node independently, and the traffic speed of a single node can be analyzed as a typical time series as the setting in AdaRNN. \n    \\item \\textbf{MAML~\\cite{DBLP:conf/icml/FinnAL17}}: Model-Agnostic Meta \n    Learning (MAML), a superior meta-learning method that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task. It learns a better initialization model from multiple tasks to supervise the target task. \n\\end{itemize}\n\n"
                },
                "subsection 8.4": {
                    "name": "Training settings",
                    "content": "\nIn the experiment, we use $T=12$ historical time steps to predict the traffic speed of the next $M=6$ time steps. Due to different time resolutions, Didi-Shenzhen and Didi-Chengdu datasets will predict the results in the next 60 minutes, while METR-LA and PEMS-BAY datasets will predict the next 30 minutes’.\nThe ST-GFSL framework is trained by Adam optimizer with learning rate decay in both inner loop and outer loop. In ST-Meta Learner, the number of GAT is set to 2, and the the GRU layer is set to 1.\nTotally, there are several important hyperparameters in our model, and we set them as: the dimension of meta knowledge $d_{MK} = 16$, task learning rate $\\alpha = 0.01$, meta-training rate $\\beta = 0.001$, task batch number $\\|\\mathcal{T}\\| = 5$, and sum scale factor of two loss function $\\lambda = 1.5$.\n\nIn order to compare the performance of different feature extractors in the ST-GFSL framework, we also used three advanced models in this paper, which are described in detail as follows:\n\\begin{itemize}[left=1em]\n    \\item \\textbf{TCN~\\cite{DBLP:conf/cvpr/LeaFVRH17}}: 1D dilated convolution network-based temporal convolution network. We used the addition of two TCN models as the feature extractor. The size of the dilated convolution kernel is set to 2 and 3 respectively.\n    \\item \\textbf{STGCN~\\cite{DBLP:conf/ijcai/YuYZ18}}: Spatial temporal graph convolution network, which combines graph convolution with 1D causal convolution. The layer of STGCN block is set to 2, and use one-layer FC as the multi-step predictor.\n    \\item \\textbf{GWN~\\cite{DBLP:conf/ijcai/WuPLJZ19}}: A convolution network structure combines graph convolution with dilated casual convolution, which introduces a self-adaptive graph to capture the hidden spatial dependency. The block of GWN is set to 4, and the layer of GWN blocks is set to 2.\n\\end{itemize}\n\n"
                }
            }
        },
        "tables": {
            "tab:performance": "\\begin{table*}[h]\n\\centering\n\\caption{Performance comparison of few-shot learning on four traffic speed datasets. In each column, the best results are highlighted in bold and grey, and the second best is underlined.}\n\\label{tab:performance}\n% \\renewcommand\\arraystretch{1.1}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{ccccccccccccc}\n\\hline\n &\n  \\multicolumn{6}{c}{\\textbf{PEMS-BAY Dataset}} &\n  \\multicolumn{6}{c}{\\textbf{METR-LA Dataset}} \\\\ \\cline{2-13} \n &\n  \\multicolumn{3}{c}{\\textbf{MAE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{RMSE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{MAE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{RMSE ($\\downarrow$)}} \\\\ \\cline{2-13} \n\\multirow{-3}{*}{\\textbf{Baselines}} &\n  \\textbf{5 min} &\n  \\textbf{15 min} &\n  \\textbf{30 min} &\n  \\textbf{5 min} &\n  \\textbf{15 min} &\n  \\textbf{30 min} &\n  \\textbf{5 min} &\n  \\textbf{15 min} &\n  \\textbf{30 min} &\n  \\textbf{5 min} &\n  \\textbf{15 min} &\n  \\textbf{30 min} \\\\ \\hline\nHA &\n  4.373 &\n  4.373 &\n  4.373 &\n  6.745 &\n  6.745 &\n  6.745 &\n  6.021 &\n  6.021 &\n  6.021 &\n  9.483 &\n  9.483 &\n  9.483 \\\\\nARIMA &\n  2.019 &\n  2.307 &\n  2.429 &\n  3.929 &\n  4.648 &\n  5.360 &\n  2.900 &\n  3.058 &\n  3.369 &\n  4.179 &\n  5.279 &\n  7.670 \\\\\nTarget-only &\n  1.556 &\n  1.920 &\n  2.368 &\n  3.092 &\n  4.043 &\n  5.153 &\n  2.740 &\n  3.229 &\n  3.860 &\n  4.924 &\n  6.118 &\n  7.417 \\\\\nFine-tuned (Vanilla) &\n  1.823 &\n  2.166 &\n  2.590 &\n  3.434 &\n  4.280 &\n  5.276 &\n  2.757 &\n  3.277 &\n  3.900 &\n  4.883 &\n  6.123 &\n  7.413 \\\\\nFine-tuned (ST-Meta) &\n  1.371 &\n  1.791 &\n  2.277 &\n  2.699 &\n  3.747 &\n  4.920 &\n  2.647 &\n  3.188 &\n  3.800 &\n  4.368 &\n  5.759 &\n  7.110 \\\\\nAdaRNN~\\cite{Du2021ADARNN} &\n  1.248 &\n  1.928 &\n  2.749 &\n  2.084 &\n  3.796 &\n  5.725 &\n  2.513 &\n  {\\ul 2.897} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.312} &\n  4.298 &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{5.567} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{6.732} \\\\\nMAML~\\cite{DBLP:conf/icml/FinnAL17} &\n  {\\ul 1.081} &\n  {\\ul 1.600} &\n  {\\ul 2.141} &\n  {\\ul 1.906} &\n  {\\ul 3.291} &\n  {\\ul 4.708} &\n  {\\ul 2.405} &\n  2.960 &\n  3.639 &\n  {\\ul 4.159} &\n  5.710 &\n  7.124 \\\\ \\hline\nST-GFSL (ours) &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{1.073} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{1.560} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.073} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{1.865} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.180} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{4.584} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.355} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.896} &\n  {\\ul 3.557} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{4.099} &\n  {\\ul 5.588} &\n  {\\ul 6.961} \\\\ \\hline\n\\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} &\n  \\multicolumn{1}{l}{} \\\\ \\hline\n &\n  \\multicolumn{6}{c}{\\textbf{Didi-Chengdu Dataset}} &\n  \\multicolumn{6}{c}{\\textbf{Didi-Shenzhen Dataset}} \\\\ \\cline{2-13} \n &\n  \\multicolumn{3}{c}{\\textbf{MAE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{RMSE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{MAE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{RMSE ($\\downarrow$)}} \\\\ \\cline{2-13} \n\\multirow{-3}{*}{\\textbf{Baselines}} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} \\\\ \\hline\nHA &\n  3.438 &\n  3.438 &\n  3.438 &\n  4.879 &\n  4.879 &\n  4.879 &\n  2.955 &\n  2.955 &\n  2.955 &\n  4.342 &\n  4.342 &\n  4.342 \\\\\nARIMA &\n  2.825 &\n  3.305 &\n  4.317 &\n  3.889 &\n  4.253 &\n  5.597 &\n  2.888 &\n  3.056 &\n  3.596 &\n  4.489 &\n  4.764 &\n  5.575 \\\\\nTarget-only &\n  2.386 &\n  2.700 &\n  3.085 &\n  3.516 &\n  4.017 &\n  4.569 &\n  2.071 &\n  2.454 &\n  2.834 &\n  3.154 &\n  3.793 &\n  4.422 \\\\\nFine-tuned (Vanilla) &\n  2.586 &\n  2.877 &\n  3.246 &\n  3.746 &\n  4.213 &\n  4.751 &\n  2.117 &\n  2.490 &\n  2.867 &\n  3.196 &\n  3.831 &\n  4.442 \\\\\nFine-tuned (ST-Meta) &\n  2.240 &\n  2.693 &\n  3.083 &\n  3.249 &\n  3.956 &\n  4.519 &\n  2.033 &\n  2.454 &\n  2.850 &\n  2.989 &\n  3.719 &\n  4.385 \\\\\nAdaRNN~\\cite{Du2021ADARNN} &\n  2.260 &\n  2.724 &\n  3.036 &\n  3.231 &\n  3.942 &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{4.324} &\n  2.107 &\n  2.473 &\n  2.807 &\n  3.041 &\n  3.674 &\n  4.231 \\\\\nMAML~\\cite{DBLP:conf/icml/FinnAL17} &\n  {\\ul 2.215} &\n  {\\ul 2.599} &\n  {\\ul 2.956} &\n  {\\ul 3.215} &\n  {\\ul 3.858} &\n  4.399 &\n  {\\ul 1.917} &\n  2.330 &\n  {\\ul 2.673} &\n  {\\ul 2.825} &\n  {\\ul 3.546} &\n  {\\ul 4.158} \\\\ \\hline\nST-GFSL (ours) &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.188} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.579} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.927} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.190} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.820} &\n  {\\ul 4.339} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{1.890} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.288} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.644} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.763} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.477} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{4.100} \\\\ \\hline\n\\end{tabular}%\n}\n\\end{table*}",
            "tab:ablation": "\\begin{table*}[]\n\\centering\n\\caption{Ablation Studies of ST-GFSL on Didi-Shenzhen and METR-LA dataset.}\n\\label{tab:ablation}\n% \\renewcommand\\arraystretch{1.1}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{lllcccccccccccc}\n\\hline\n\\multicolumn{3}{l}{} &\n  \\multicolumn{6}{c}{\\textbf{Didi-Shenzhen Dataset}} &\n  \\multicolumn{6}{c}{\\textbf{METR-LA Dataset}} \\\\ \\cline{4-15} \n\\multicolumn{3}{l}{} &\n  \\multicolumn{3}{c}{\\textbf{MAE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{RMSE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{MAE ($\\downarrow$)}} &\n  \\multicolumn{3}{c}{\\textbf{RMSE ($\\downarrow$)}} \\\\ \\cline{4-15} \n\\multicolumn{3}{l}{\\multirow{-3}{*}{\\textbf{Ablation Methods}}} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} &\n  \\textbf{10 min} &\n  \\textbf{30 min} &\n  \\textbf{60 min} \\\\ \\hline\n\\multicolumn{3}{l}{(M1a): Use temporal meta knowledge only} &\n  1.910 &\n  2.317 &\n  2.668 &\n  2.834 &\n  3.530 &\n  4.139 &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.332} &\n  { 2.905} &\n  3.616 &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{4.077} &\n  5.619 &\n  6.991 \\\\\n\\multicolumn{3}{l}{(M1b): Use spatial meta knowledge only} &\n  { 1.872} &\n  { 2.300} &\n  { 2.649} &\n  { 2.756} &\n  { 3.495} &\n  { 4.108} &\n  { 2.364} &\n  2.915 &\n  { 3.604} &\n  { 4.112} &\n  { 5.612} &\n  { 6.970} \\\\\n\\multicolumn{3}{l}{(M1c): Use random initialized vectors} &\n  1.937 &\n  2.332 &\n  2.680 &\n  2.841 &\n  3.531 &\n  4.142 &\n  2.422 &\n  2.949 &\n  3.697 &\n  4.182 &\n  5.624 &\n  7.047 \\\\\n\\multicolumn{3}{l}{(M2): Remove parameter generator} &\n  1.917 &\n  2.330 &\n  2.673 &\n  2.825 &\n  3.546 &\n  4.158 &\n  2.405 &\n  2.960 &\n  3.639 &\n  4.129 &\n  5.655 &\n  7.075 \\\\\n\\multicolumn{3}{l}{(M3): Remove graph reconstruction loss $\\mathcal{L}_{g}$} &\n  2.286 &\n  2.652 &\n  3.000 &\n  3.309 &\n  3.979 &\n  4.604 &\n  3.087 &\n  3.585 &\n  4.140 &\n  4.960 &\n  6.209 &\n  7.464 \\\\ \\hline\n\\multicolumn{3}{l}{\\textbf{ST-GFSL (Ours)}} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{1.856} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.290} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.634} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.737} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.471} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{4.052} &\n  2.387 &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{2.895} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{3.546} &\n  4.140 &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{5.603} &\n  \\cellcolor[HTML]{EFEFEF}\\textbf{6.963} \\\\ \\hline\n\\end{tabular}%\n}\n\\end{table*}",
            "tab:datasets": "\\begin{table}[h]\n\\centering\n\\caption{Statistics of the traffic datasets in experiment.}\n\\label{tab:datasets}\n% \\renewcommand\\arraystretch{1.05}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{ccccc}\n\\hline\n\\textbf{Datasets}        & \\textbf{METR-LA}     & \\textbf{PEMS-BAY}    & \\textbf{Didi-Chengdu}  & \\textbf{Didi-Shenzhen} \\\\ \\hline\n\\# Nodes        & 207         & 325         & 524          & 627           \\\\\n\\# Edges        & 1,722       & 2,694       & 1,120        & 4,845         \\\\\nInterval  & 5 min       & 5 min       & 10 min       & 10 min        \\\\\nTime span   & 34,272      & 52,116      & 17,280       & 17,280        \\\\\nMean  & 58.274      & 61.776      & 29.023       & 31.001        \\\\\nStd & 13.128      & 9.285       & 9.662        & 10.969        \\\\ \\hline\n\\end{tabular}%\n}\n\\end{table}"
        },
        "figures": {
            "fig:framework": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.92\\linewidth]{figures/framework_KDD.pdf}\n    \\caption{The framework of proposed ST-GFSL. The left side is the input of the model, in which the source cities have large-scale training samples and the target city's is few-shot. The right side shows two main parts of ST-GFSL: (1) Cross-City Knowledge Transfer and (2) Spatio-Temporal Neural Network (STNN).}\n    \\label{fig:framework}\n\\end{figure*}",
            "fig:param_w": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/param_w.pdf}\n    \\caption{Parameter generation of linear weight matrix $W$.}\n    \\label{fig:param_w}\n\\end{figure}",
            "fig:param_conv": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/param_conv.pdf}\n    \\caption{Parameter generation of 2D-conv weight matrix.}\n    \\label{fig:param_conv}\n\\end{figure}",
            "fig:model_compare": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figures/model_comparison.pdf}\n    \\caption{Performance comparison of different feature extractors on Didi-Shenzhen and METR-LA datasets.}\n    \\label{fig:model_compare}\n\\end{figure}",
            "fig:source": "\\begin{figure}\n    \\centering\n    \\subfigure[Didi-Shenzhen Dataset]{\n        \\includegraphics[width=\\linewidth]{figures/source_performance_sz.pdf}\n    }\n    \\\\\n    \\subfigure[METR-LA Dataset]{\n        \\includegraphics[width=\\linewidth]{figures/source_performance_metr.pdf}\n    }\n    \\caption{Performance comparison across different source cities studied on Didi-Shenzhen and METR-LA dataset.} \n    \\label{fig:source}\n\\end{figure}",
            "fig:case_study": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/case_study.pdf}\n    \\caption{Case study on visualization of adjacency matrix and reconstructed ST-Meta graph.}\n    \\label{fig:case_study}\n\\end{figure}",
            "fig:hyper": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/hyper_all.pdf}\n    \\caption{Hyperparameter study on Didi-Shenzhen dataset: meta knowledge dimension $d_{MK}$, sum scale factor $\\lambda$ and training sample days of target city.}\n    \\label{fig:hyper}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\begin{aligned}\n    z &=\\sigma\\left(U^{z} x_{i, t} + W^{z} h_{i, t-1} \\right) \\\\\n    r &=\\sigma\\left(U^{r} x_{i, t} + W^{r}h_{i, t-1} \\right) \\\\\n    c &=\\phi \\left(U^{c} x_{i, t} +W^{c} \\left(h_{i, t-1} \\circ r\\right) \\right) \\\\\n    h_{i, t} &=(1-z) \\circ c+z \\circ h_{i, t-1}, \n    \\end{aligned}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    e_{ij} = attention(W h_i, W h_j), j\\in \\mathcal{N}_{i} \\text{,}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\alpha_{ij} = \\mathop{softmax}\\nolimits_{j} (e_{ij}) = \\frac{exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_{i}}exp(e_{ik})} \\text{.}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    z_{i}^{sp} = \\sigma(\\frac{1}{K}\\sum\\nolimits_{k=1}^{K}\\sum\\nolimits_{j\\in \\mathcal{N}_{i}}\\alpha_{ij}W^{k}h_{j})\\text{.}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\mathbf{Z}^{MK} = W^{\\gamma}(\\gamma \\circ \\mathbf{Z}^{tp} + (1-\\gamma) \\circ \\mathbf{Z}^{sp}) \\text{,}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    p(a_{ij} | z_i^{MK},z_j^{MK}) = sigmoid((z_i^{MK})^T, z_j^{MK}).\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathbf{A}_{meta} = sigmoid[(\\mathbf{Z^{MK}})^T \\cdot {\\mathbf{Z^{MK}}}],\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\label{Lg}\n    \\mathcal{L}_g = \\| \\mathbf{A}_{meta} - \\mathbf{A} \\|^2.\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\label{Le}\n    \\mathcal{L}_e = \\frac{1}{\\|\\mathcal{S}_{\\mathcal{T}_i}\\|}\\sum_{(x_j, y_j)\\in \\mathcal{S}_{\\mathcal{T}_i}} (f_{\\theta}(x_j) - y_j)^2.\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\label{loss_func}\n    \\mathcal{L} = \\mathcal{L}_e + \\lambda \\mathcal{L}_g,\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\theta^{*} = \\mathop{\\arg\\min}_{\\theta} \\sum\\limits_{\\mathcal{T}_i \\in \\mathcal{T}_{ST}}\\mathcal{L}_{\\mathcal{T}_i}(f_{\\theta^{\\prime}_i}) .\n\\end{equation}"
        },
        "git_link": "https://github.com/RobinLu1209/ST-GFSL"
    }
}