{
    "meta_info": {
        "title": "FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks",
        "abstract": "Modelling spatio-temporal processes on road networks is a task of growing\nimportance. While significant progress has been made on developing\nspatio-temporal graph neural networks (Gnns), existing works are built upon\nthree assumptions that are not practical on real-world road networks. First,\nthey assume sensing on every node of a road network. In reality, due to\nbudget-constraints or sensor failures, all locations (nodes) may not be\nequipped with sensors. Second, they assume that sensing history is available at\nall installed sensors. This is unrealistic as well due to sensor failures, loss\nof packets during communication, etc. Finally, there is an assumption of static\nroad networks. Connectivity within networks change due to road closures,\nconstructions of new roads, etc. In this work, we develop FRIGATE to address\nall these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that\nintegrates positional, topological, and temporal information into rich\ninductive node representations. The joint fusion of this diverse information is\nmade feasible through a novel combination of gated Lipschitz embeddings with\nLstms. We prove that the proposed Gnn architecture is provably more expressive\nthan message-passing Gnns used in state-of-the-art algorithms. The higher\nexpressivity of FRIGATE naturally translates to superior empirical performance\nconducted on real-world network-constrained traffic data. In addition, FRIGATE\nis robust to frugal sensor deployment, changes in road network connectivity,\nand temporal irregularity in sensing.",
        "author": "Mridul Gupta, Hariprasad Kodamana, Sayan Ranu",
        "link": "http://arxiv.org/abs/2306.08277v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction and Related Work",
                "content": "\n\\begin{comment}\n\\begin{figure}\n    \\begin{center}\n        \\includegraphics[width=.9\\linewidth]{plots/harbin_map.png}\n        \\caption{\\label{fig:harbin_map}Traffic prediction error for the city of Harbin after just seeing 30\\% of the map}\n    \\end{center}\n\\end{figure}\n\\end{comment}\n\n%\\setlipsum{default-range=5-}\n%\\lipsum[5-10]\nA road network can be modeled as a graph where nodes indicate important intersections in a region and edges correspond to streets connecting two intersections~\\cite{skygraph,skyroute,netclus,tops,foodmatch,medya2018noticeable}. \nModeling the evolution of spatio-temporal events on road networks has witnessed significant interest in the recent past~\\cite{dcrnn,stgcn,stgode, stnn,wavenet}. %Some concrete examples include forecasting of traffic speeds at various nodes/edges in a network~\\cite{dcrnn}, availability prediction of taxis~\\cite{yao2018deep}, and human activity recognition~\\cite{yan2018spatial}. \n%Broadly, the above events come under the umbrella of network-dependent spatio-temporal processes. \nSpecifically, each node (or edge) participates in a time-series. This time-series is a function of not only the time of the day, but also the events unfolding in other nodes of the network. The objective of the forecasting task is to model the time-series evolution in each node and predict the values in the immediate future, for e.g., the next one hour. As examples, in Fig.~\\ref{fig:nodetimeseries}, we present the number of cars passing through two randomly selected intersections in the cities of Beijing and Chengdu. As can be seen, there is significant variation in traffic through out the day. Furthermore, the time-series vary across nodes making the forecasting problem non-trivial.\n\\looseness=-1\n\nOne may learn an auto-regressive model independently at each node to fit the time-series data. This strategy, however, is limited by two critical factors. First, this ignores the connectivity-induced time-series dependency among nodes. For example, if one particular intersection (node) is observing traffic congestion, it is likely for neighboring nodes connected through an outgoing edge to be affected as well. Second, the number of parameters grows linearly with the number of nodes in the graph, making it non-scalable. \n\n",
                "subsection 1.1": {
                    "name": "Existing work",
                    "content": "\n\\label{sec:existingwork}\nTo address these specific needs of modeling network-dependent spatio-temporal processes, several algorithms merging models for structural data such as \\gnns~\\cite{stgcn,dstagnn,stfgnn} or Convolutional neural networks~\\cite{stnn} along with auto-regressive architectures have been developed. The proposed work is motivated based on some assumptions made by existing algorithms that are not realistic for real-world road networks. Table~\\ref{tbl:baselines} summarizes these. %We detail them below.\n\\begin{itemize}\n\\item \\textbf{Ability to extrapolate from partial sensing:} Several of the existing algorithms assume that a sensor is placed in each node of the road network~\\rev{\\cite{agcrn,gman,wavenet,dstagnn,stsgcn,zigzag,stgode,stfgnn,zhou2023towards,gmsdr,mtgnn}}. Forecasting is feasible \\textit{only} on these nodes. In reality, deploying and maintaining sensors across all intersections may be prohibitively expensive and cumbersome. Hence, it is important to also forecast accurately on nodes that do not have an explicit sensor placed on them. In this work, we show that this is indeed feasible by exploiting the network-induced dependency between nodes.\n\\item \\textbf{Ability to absorb network updates:} The connectivity within a road network may change with time. The directionality of edges may change based on time of the day, new roads may get constructed adding new nodes and edges to the network, and existing road may get removed (temporarily or permanently) to accommodate emergent needs such as street festivals,  construction activities, flooding, etc. Under these circumstances, it is important to absorb small changes in the network topology without the need to retrain from scratch. Several models~\\rev{\\cite{agcrn,gman,wavenet,dstagnn,stsgcn,zigzag,wang2018real,zhou2023towards,gmsdr,mtgnn}} fail to absorb any updates since they are \\textit{transductive} in nature, i.e., the number of parameters in their model is a function of the network size (number of nodes or edges). In this work, we develop an inductive model, which decouples the model parameter size from that of the network. Hence, accommodating changes to network structure does not require re-training.\n\\item \\textbf{Ability to predict without temporal history or regularity:} Several of the existing algorithms can forecast on the time-series of a node \\textit{only} if the data in the past $x$ time instants are available~\\cite{stgode,stfgnn}. This limitation often arises from a methodology where dependency between all nodes is learned by computing similarity between their past time-series information. If past data is not available, then this similarity cannot be computed. Furthermore, some algorithms model the spatio-temporal road network as a 3-dimensional tensor, under the implicit assumption that temporal data is collected at a regular granularity (such as every five minutes)~\\cite{stnn,stgode}. In reality, sensors may fail and may therefore provide data at irregular intervals or having no temporal history in the past $x$ time instants. For a model to be deployable in real workloads, it is pertinent to be robust to sensor failures.   \n\\end{itemize}\n\n\\vspace{-0.10in}\n"
                },
                "subsection 1.2": {
                    "name": "Contributions",
                    "content": "\n\\label{sec:contributions}\nMotivated by the above limitations, in this work, we ask the following questions: \\textit{Is it possible to design an accurate and inductive forecasting model across all nodes in a graph based on partial sensing through a small subset of nodes? In addition, can the model predict on nodes with irregular time-series visibility, or in the worst case, no visibility at all?} We show that this is indeed possible through \\name: \\underline{FR}ugal and \\underline{I}nductive Lipschitz-\\underline{GA}ted Spatio-\\underline{TE}mporal \\gnn. Specifically, we make the following contributions:\n\\begin{itemize}\n    \\item \\textbf{Problem formulation:} We formulate the problem of time-series forecasting on road networks. Taking a deviation from current works, the proposed formulation is cognizant of practical challenges such as limited sensor access, sensor failures, and noise in data (\\S~\\ref{sec:formulation}).\n    \\looseness=-1\n    \\item {\\bf Novel architecture:} To enable robust forecasting, we develop a novel spatio-temporal \\gnn called \\name. At a high-level, \\name is a joint architecture composed of stacks of \\textit{siamese} \\gnns and \\lstms in encoder-decoder format (\\S~\\ref{sec:frigate}). Under the hood, \\name incorporates several innovations. First, to succeed in accurate forecasting under frugal sensor deployment, \\name uses \\textit{Lipschitz-gated} attention over messages. Gating allows \\name to receive messages from far-away neighbors without over-smoothing the node neighborhoods. In addition, Lipschitz embedding allows \\name to inductively embed positional information in node representations. Second, \\name is inductive and hence does not require re-training on network updates. Finally, the coupling between the \\gnn and \\lstm is devoid of any assumption on the temporal granularity. Hence, it is robust to sensor failures or irregular data collection. \n   % \\item {\\bf Expressivity:} We establish that augmenting node representations with Lipschitz embeddings makes \\name provably more expressive than forecasting methodologies that rely solely on message-passing \\gnns (such as DCRNN, STGCN, etc.) (\\S~\\ref{sec:characterization}). \n    \\item {\\bf Empirical evaluation:} We perform extensive evaluation on real-world road network datasets from three large cities. The proposed evaluation clearly demonstrates the superiority of \\name over state-of-the-art algorithms and establishes its robustness to data noise and network changes (\\S~\\ref{sec:experiments}).\n\\end{itemize}"
                }
            },
            "section 2": {
                "name": "Problem Formulation",
                "content": "\n\\label{sec:formulation}\nIn this section, we define the concepts central to our work and formulate our problem. All key notations used in our work are summarized in Table~\\ref{tab:notation} in Appendix.\n\\vspace{-0.05in}\n\\begin{defn}[Road Network Snapshot]\n    \\label{def:graph_snapshot}\n    \\textit{A road network snapshot is represented as a directed graph $\\CG = (\\CV_t, \\CE_t, \\delta, \\tau_t)$, where $\\CV_t$ is the set of nodes representing road intersections at time $t$, $\\CE_t \\subseteq \\CV_t \\times \\CV_t$ is the set of edges representing road segments at time $t$, a distance function $\\delta : \\CE_t \\rightarrow \\mathbb{R}$ representing the length (weight) of each road segment (edge), and the sensor readings $\\tau_t =\\left\\{\\tau^v_t\\in \\mathbb{R} \\mid v\\in\\CV_t\\right\\}$ for each node at time $t \\in \\mathbb{N}$.}\n\\end{defn}\n\\vspace{-0.05in}\n%Intuitively, a road network snapshot characterizes the state of the road network at time $t$. We use $\\tau_t(v)$ to denote the sensor reading at node $v$. When $\\tau_t(v)$ is not available, either due to non-availability of a sensor at node $v$, or due to sensor failure, we assume $\\tau_t(v)$ is marked with a special label. We use $\\tau_t(v)=\\varnothing$ to denote a missing value. Furthermore, in the real world, the sensor value $\\tau_t(v)$ may not be recorded exactly at time $t$. We thus assume $\\tau_t(v)$ to be the latest value since the last snapshot; $\\tau_t(v)=\\varnothing$ if nothing has been recorded since the last snapshot. Note that we also make the vertex and edge sets time-dependent to account for the fact that minor changes are possible on the topology over time. Examples include changing directionality of traffic on a particular road (edge), addition of new roads and intersections, temporary road closures due to street festivals, etc. We use the notation $e=(u,v)$ to denote a road segment (edge) from node $u$ to $v$ and its length is denoted by $\\delta(e)$. The length $\\delta(e)$ of an edge $e$ is the \\emph{Haversine} distance from the locations represented by $u$ and $v$. \n\nIntuitively, a road network snapshot characterizes the state of the road network at time $t$. We use $\\tau_t^v$ to denote the sensor reading at node $v$. When $\\tau_t^v$ is not available, either due to non-availability of a sensor at node $v$, or due to sensor failure, we assume $\\tau_t^v$ is marked with a special label. We use $\\tau_t^v=\\varnothing$ to denote a missing value. Furthermore, in the real world, the sensor value $\\tau_t^v$ may not be recorded exactly at time $t$. We thus assume $\\tau_t^v$ to be the latest value since the last snapshot; $\\tau_t^v=\\varnothing$ if nothing has been recorded since the last snapshot. Note that we also make the vertex and edge sets time-dependent to account for the fact that minor changes are possible on the topology over time. Examples include changing directionality of traffic on a particular road (edge), addition of new roads and intersections, temporary road closures due to street festivals, etc. We use the notation $e=(u,v)$ to denote a road segment (edge) from node $u$ to $v$ and its length is denoted by $\\delta(e)$. The length $\\delta(e)$ of an edge $e$ is the \\emph{Haversine} distance from the locations represented by $u$ and $v$.\n\\vspace{-0.05in}\n\\begin{defn}[Road Network Stream]\n\\label{def:graph}\n\\textit{A road network stream is the chronologically ordered set of snapshots of the road network taken at various time instances, $\\VG = \\{\\CG_{1},\\CG_{2},\\dotsc, \\CG_{T}\\}$, where %\\(\\{t_i\\;\\lvert\\;i\\in\\mathbb{N}\\}\\) is the index set such that $i<j\\iff t_i<t_j$ and \n\\(\\CG_{t}\\) is the road network snapshot at time instant $t$.}\n\\end{defn}\n\\vspace{-0.05in}\n    \n\n\nWe assume, that $\\forall t,\\;\\CV_{t}\\approx\\CV_{t+1}$ and similarly $\\CE_{t}\\approx\\CE_{t+1}$. This assumptions are realistic based on real-world knowledge that change events on roads are rare over both space and time.\n\nOur goal is to learn the dynamics of the time-series at each node and forecast future values. Towards that, the modeling problem is defined as follows.\n\\vspace{-0.05in}\n\\begin{prob}[Forecasting on Road Network]\\hfill\n\\label{prob:frigate}\n\n\\noindent\n\\textbf{Training:} \\textit{Given a road network stream $\\VG = \\left\\{\\CG_{1},\\dotsc, \\CG_{T}\\right\\}$, a forecasting horizon $\\Delta$, and a timestamp $t$ where $\\Delta\\leq t\\leq T-\\Delta$, learn a function $\\Psi$, parameterized by $\\Theta$, to minimize the mean absolute prediction error over sensor values. Specifically,\n\\vspace{-0.05in}\n\\begin{equation}\n\\label{eq:mae}\n{\\small\n\\displaystyle{\\minimize\\left\\{\\frac{1}{|\\CV^{tr}| \\Delta }\\sum_{k=1}^{\\Delta} \\sum_{v\\in \\CV^{tr}_{t+k}} \\left\\lvert\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\overrightarrow\\CG_{[1,t]}\\right)- \\tau^v_{t+k}\\right\\rvert\\right\\}}\n}\n\\end{equation}\nHere, $\\CV^{tr}_{t+k}=\\left\\{v\\in\\CV_{t+k}\\mid\\tau^v_{t+k}\\neq\\varnothing\\right\\}$ is the set of training nodes with ground truth sensor values at time $t+k$, $\\overrightarrow\\CG_{[1,t]}=\\{\\CG_1,\\cdots,\\CG_{t}\\}$ denotes the subset of snapshots till $t$, and $\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\overrightarrow\\CG_{[1,t]}\\right)$ predicts the sensor value for node $v$ at time $t+k$ when conditioned on all snapshots till now, i.e., $\\overrightarrow\\CG_{[1,t]}$. We assume that to predict on a time horizon of $\\Delta$ snapshots, we must train on a history of at least $\\Delta$ snapshots.}\n\n\\noindent\n\\textbf{Inference:} \\textit{Let the last recorded snapshot be at time $t$. Given node $v\\in\\CV_{t}$ and forecasting horizon $\\Delta$, compute:} \n\\vspace{-0.05in}\n\\begin{equation}\n\\forall k\\in [1,\\Delta],\\;\\left\\{\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\VG_{[1,t]}\\right)\\right\\}\n\\end{equation}% Note that in an unseen snapshot, $\\tau_{t'}(v)$ is not available and hence arises the need to forecast.}\n\\end{prob}\n\\noindent\nIn addition to predicting accurately, $\\Psi_{\\Theta}$, must also satisfy the following properties:\n\\looseness=-1\n\\begin{itemize}\n\\item \\textbf{Inductivity:} The number of parameters in model $\\Psi_{\\Theta}$, denoted as $\\lvert \\Theta \\rvert$, should be independent of the number of nodes in the road network at any given timestamp. Inductivity enables the ability to predict on unseen nodes without retraining from scratch. \n\\item \\textbf{Permutation invariance:} Given any \\textit{permutation function} \\\\$\\mathcal{P}\\left(\\VG_{[1,t]}\\right)$ that randomly permutes the node set of each graph $\\CG\\in\\VG_{[1,t]}$, we require: \n\\vspace{-0.10in}\n\\begin{equation}\n\\nonumber\n\\forall k\\in[1,\\Delta],\\;\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\VG_{[1,t]}\\right)=\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\mathcal{P}\\left(\\VG_{[1,t]}\\right)\\right)\n\\end{equation}\nMore simply, if a graph contains $n$ nodes, then there are $n!$ possible permutations over its node set, and hence that many adjacency matrix representations of $\\CG_t$. Permutation invariance ensures that the output is dependent only on the topology of the graph and not coupled to one of its specific adjacency matrix representations. Hence, it aids in generalizability while also ensuring that the location of change in the adjacency matrix owing to a node/edge insertion or deletion is inconsequential to the output.\n\\looseness=-1\n\n\\item {\\bf Semi-supervised learning: } %Temporal features, i.e., $\\tau^v_t$, have a \\textit{dual} role in this problem. Their values on past snapshots act as input features, while the value in the current snapshot serves as the ground truth. \nTowards our objective of frugal forecasting,  \n$\\Psi_{\\Theta}(v,t\\mid \\VG_{[1,t]})$ should be computable even if temporal features from past snapshots are not available on $v$ as long as \\textit{some} nodes in the graph contain temporal information.\n\\begin{comment}\n\\item \\textbf{Permutation equivariance:} Given any permutation function $\\mathcal{P}\\left\\{\\Psi_{\\Theta}\\left(\\CG_t,v\\right))\\mid v\\in\\CV_t\\;\\right\\}$ over the output set of $\\Psi_{\\Theta}\\left(\\CG_t,v\\right)$, we require: \n\\begin{equation}\n\\forall t, \\mathcal{P}\\{\\forall v\\in\\CG_t,\\;\\Psi_{\\Theta}\\left(\\CG_t,v\\right))\\}=\\{\\Psi_{\\Theta}\\mathcal{P}\\{\\forall v\\in\\CG_t,\\;\\Psi_{\\Theta}\\left(\\CG_t,v\\right))\\}\n\\end{equation}\n\\end{comment}\n\\end{itemize}\n\n\\noindent\n%To learn $\\Psi_{\\Theta}$ in an inductive, permutation-invariant and semi-supervised manner, we develop \\name.\n% $\\tau_t(e)$ can be retrieved from can either be fetched from Google Maps API, or extracted from historical trajectory data.\n%The goal of zero shot traffic prediction problem is to predict the future traffic volume on roads given the traffic at their neighborhood without ever training specifically on them. The idea is to learn to use the traffic information in neighboring nodes.\n\\begin{comment}\nGiven a graph \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E}), \\mathcal{E}\\subseteq \\mathcal{V}\\times\\mathcal{V}\\) describing the road network's structure. Here, \\(\\mathcal{V}\\mbox{ and }\\mathcal{E}\\) are the set of nodes and the set of edges respectively. Further, each node \\(u\\in\\mathcal{V}\\) has time varying features like traffic speed, traffic volume, time of day, etc. encapsulated in a vector, \\(\\mathbf{x}^t_u\\). This traffic information for the whole graph is represented by a matrix \\(\\mathbf{X}^t=\\begin{bmatrix}\\mathbf{x}^t_0&\\mathbf{x}^t_1&\\cdots&\\mathbf{x}^t_{N}\\end{bmatrix}\\) where \\(N=\\lvert\\mathcal{V}\\rvert\\). Then the task is to learn a function \\(F(\\cdot)\\) that maps past \\(p-\\mbox{time units}\\) of traffic information to future \\(f-\\mbox{time units}\\) of traffic information.\n%\n\\begin{equation}\n        \\mathbf{X}^{t+1}, \\mathbf{X}^{t+2}, \\dotsc, \\mathbf{X}^{t+f} = \n        F\\left(\\mathbf{X}^t, \\mathbf{X}^{t-1}, \\dotsc, \\mathbf{X}^{t-(p-1)}\\right)\n\\label{eqn:prev_problem}\n\\end{equation}\n%\nThis is the problem being solved in existing literature. But, we distinguish our problem from the above problem by allowing \\(\\mathbf{X}^t,\\dotsc,\\mathbf{X}^{t-(p-1)}\\) to have zero columns for some \\textit{unseen} nodes, but we still want to predict on them. The problem is of generalization.\\par\n%\nSo, we segment the node set \\(\\mathcal{V}\\) into two disjoint sets \\(\\mathcal{V}=\\mathcal{V}_{\\text{seen}}\\cup\\mathcal{V}_{\\text{unseen}}\\) and \\(\\mathcal{V}_{\\text{seen}}\\cap\\mathcal{V}_{\\text{unseen}}=\\varnothing\\). We, then define \\(\\mathbf{\\tilde{x}}^t_u=0\\;\\forall\\;u\\in\\mathcal{V}_{\\text{unseen}}\\) and \\(\\mathbf{\\tilde{x}}^t_v=\\mathbf{x}^t_v\\;\\forall\\;v\\in\\mathcal{V}_{\\text{seen}}\\). Then, define \\(\\mathbf{\\tilde{X}}^t=\\begin{bmatrix}\\mathbf{\\tilde{x}}^t_0&\\mathbf{\\tilde{x}}^t_1&\\cdots&\\mathbf{\\tilde{x}}^t_{N}\\end{bmatrix}\\). Then, our task is to learn a function \\(\\tilde{F}(\\cdot)\\) that maps past \\(p-\\mbox{time units}\\) of \\textit{partial} traffic information to future \\(f-\\mbox{time units}\\) of \\textit{full} traffic information.\n%\n\\begin{equation}\n        \\mathbf{X}^{t+1}, \\mathbf{X}^{t+2}, \\dotsc, \\mathbf{X}^{t+f} = \n        F\\left(\\mathbf{\\tilde{X}}^t, \\mathbf{\\tilde{X}}^{t-1}, \\dotsc, \\mathbf{\\tilde{X}}^{t-(p-1)}\\right)\n\\label{eqn:our_problem}\n\\end{equation}\n%\n%\n\\end{comment}%\\input{related.tex}\n"
            },
            "section 3": {
                "name": "\\name: Proposed Methodology",
                "content": "\n\\label{sec:frigate}\n%\n%\nAt an individual node level, the proposed problem is a \\textit{sequence-to-sequence} regression task, wherein we feed the time-series over past snapshots and forecast the future time-series on the target node. To model this problem, we use an \\textit{Encoder-Decoder} architecture as outlined in Fig.~\\ref{fig:model}. To jointly capture the spatio-temporal dependency among nodes, the encoder is composed of a stack of \\textit{siamese} \\gnns and \\lstms, i.e., the weights and architecture are identical across each stack. The number of stacks corresponds to the number of historical snapshots one wishes to train on. Each stack is assigned an index depending on how far back it is from the current time $t$. Due to the siamese architecture, the number of stacks does not affect the number of parameters. In addition, the siamese design allows one to dynamically specify the stack size at inference time in a query-specific manner depending on the amount of data available. For simplicity, we will assume the number of historical snapshots to be the same as the forecasting horizon, which is denoted as $\\Delta$.\n\nIn the $k^{th}$ stack, where $0\\leq k\\leq \\Delta$, graph snapshot $\\CG_{t-\\Delta+k}$ is passed through the \\gnn to learn node representations. The node representation not only characterises its own state, but also the state in its ``neighborhood''. The ``neighborhood'' that affects the time-series of the target node is automatically learned through \\textit{Lipschitz-gating}. Each stack of siamese \\lstm in the encoder receives two inputs: the node embedding of the target node corresponding to its timestamp and the \\lstm output from the previous stack. \n\\looseness=-1\n\nThe decoder has a stack of siamese \\lstms as well. However, the decoder \\lstms have separate weights than the encoder \\lstms. The stack size is the same as the forecasting horizon $\\Delta$. The output of a decoder \\lstm is augmented with the \\textit{moments}~\\cite{moments} of the sensor value distribution in neighborhood of the target node, which injects a strong prior to the model and elevates its performance. Finally, this augmented representation is passed through an \\mlp to predict the time-series value. The entire architecture is trained \\textit{end-to-end} with \\textit{mean absolute error (MAE)} loss function, as defined in Eq.~\\ref{eq:mae}. The $\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\overrightarrow\\CG_{[1,t]}\\right)$ term in Eq.~\\ref{eq:mae} is computed as:\n\\vspace{-0.05in}\n\\begin{equation}\n\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\overrightarrow\\CG_{[1,t]}\\right)=\\y_v^k\n\\end{equation}\nwhere $k\\in[1,\\Delta]$ and $\\y_v^k$, as depicted in Fig.~\\ref{fig:model}, is the predicted output generated through the $k^{th}$ \\lstm in the decoder. The next sections detail these individual components.\n\\looseness=-1 \n%of \\name and elaborate its design.\n\n%The input time-series data is augmented with \\textit{positional} and \\textit{statistical} features that  %to model the spatial distribution and to make use of neighborhood time-series, and an \\lstm to model temporal dependency. The encoder also finds the mean of the traffic values in the neighborhood time-series which is then used in the decoder. This provides a strong prior which helps the model perform better. \n%The decoder is an auto-regressive \\lstm. % followed by an addition of the mean as calculated previously.\n%\n%\n\n%\n\\begin{comment}\nIn order to enable the \\gnn operating on the road network graph to distinguish and identify its neighborhood, we augment the raw traffic features with Lipschitz embeddings for each node. $k$-dimensional Lipschitz embeddings are generated by randomly selecting $d$ nodes in the graph, called anchor nodes $\\{a_1, a_2, \\dotsc, a_k\\}$ and then for each node $u$ in the graph finding the shortest distance to each of the anchor nodes and concatenating them in a vector.\n\\begin{defn}[Lipschitz Embeddings]\nIn lipschitz\n\\end{defn}\n\n\\begin{equation}\nL_u=\\mbox{Lipschitz}(u)=\\begin{bmatrix}\nd(u, a_1)\\\\\nd(u, a_2)\\\\\n\\vdots\\\\\nd(u, a_k)\n\\end{bmatrix}\n\\end{equation}\nLet $L=\\begin{bmatrix}L_{u_1}^T&L_{u_2}^T&\\cdots&L_{u_N}^T\\end{bmatrix}$.\n%\nSo, after the augmentation, the features of the nodes are given by\n\\begin{equation}\nX_{\\mbox{aug}}=\\begin{bmatrix}X&L\\end{bmatrix}\n\\end{equation}\nNote that the Lipschitz Embeddings are normalized before being used in a neural network.\n\\end{comment}\n%\n%\n",
                "subsection 3.1": {
                    "name": "\\gnn Module of \\name",
                    "content": "\n\\label{sec:gnn}\n\\begin{comment}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.6\\linewidth]{figures/gnn_layer.pdf}\\hfil\n    \\caption{\\label{fig:layer}Layer}\n\\end{figure}\n\\end{comment}\nIn this section, we discuss the architecture of a single stack of \\gnn. We use an $L$-layered message-passing \\gnn to model node dependencies. In each layer, each node $v$ receives messages from its neighbors. These messages are aggregated and passed through a neural network, typically an \\mlp, to construct the representation of $v$ in the next layer. \n\n The simplest aggregation framework would be a $\\textsc{MeanPool}$ layer where the aggregated message at $v$ from its neighbors is simply the mean of their representations. However, this approach is too simplistic for the proposed problem. Specifically, a road network is directed, and hence drawing messages only through incoming (or outgoing) edges is not enough. On the other hand, a direction-agnostic message passing scheme by uniformly drawing messages through all incident edges fails to capture traffic semantics. Furthermore, not all edges are equally important. An incoming edge corresponding to an arterial road is likely to have much more impact on an intersection (node) than a road from a small residential neighborhood. Hence, the importance of a road (edge) towards an intersection (node) must be learned from the data, and accordingly, its message should be weighted. To capture these semantics, we formulate our message-passing scheme as follows.\n\n Let $\\h^{\\ell}_v$ denote the representation of node $v$ in layer $\\ell\\in [0,L]$ of the $k^{th}$ \\gnn. Furthermore, the outgoing and incoming neighbors of $v$ are defined as $\\CN^{out}_v=\\{u\\mid (v,u)\\in\\CE\\}$ and $\\CN^{in}_v=\\{u\\mid (u,v)\\in\\CE\\}$ respectively. Now, $h^{\\ell+1}_v$ is constructed as follows:\n \\begin{align}\n \\label{eq:hl}\n \\h^{\\ell+1}_v&= \\sigma_1\\left(\\W^{\\ell}_1 \\left(h_v^{\\ell}\\parallel\\m^{\\ell,out}_v\\parallel \\m^{\\ell,in}_v\\right)\\right) \\text{, where}\\\\\n \\m^{\\ell,out}_v&=\\textsc{Aggr}^{out}\\left(\\left\\{\\h_u^{\\ell}\\mid u\\in N^{out}_v\\right\\}\\right)\\\\\n\\m^{\\ell,in}_v&=\\textsc{Aggr}^{in}\\left(\\left\\{\\h_u^{\\ell}\\mid u\\in N^{in}_v\\right\\}\\right)\n \\end{align}\n$\\parallel$ represents the \\textit{concatenation} operation, $\\W^{\\ell}_1\\in\\mathbb{R}^{3d\\times d}$ is a learnable weight matrix; $d$ is the dimension of node representations. More simply, we perform two separate aggregations over the messages received from the incoming and outgoing neighbors. The aggregated vectors are concatenated and then passed through a linear transformation followed by non-linearity through an activation function $\\sigma_1$. In our implementation, $\\sigma_1$ is \\textsc{ReLU}. By performing two separate aggregations over the incoming and outgoing edges and then concatenating them, we capture both directionality as well as the topology. \n\\looseness=-1\n\nThe aggregation functions perform a weighted summation, where the weight of a message-passing edge is learned through \\textit{sigmoid gating} over its \\textit{positional embedding}. More formally,\n\n\\vspace{-0.05in}\n\\begin{footnotesize}\n\\begin{align}\n\\label{eq:aggregation_out}\n\\textsc{Aggr}^{out}\\left(\\left\\{\\h_u^{\\ell}\\mid u\\in N^{out}_v\\right\\}\\right)&=\\sum_{\\forall u \\in N^{out}_v} \\beta_{v,u}\\h^{\\ell}_u\\\\\n\\label{eq:aggregation_in}\n\\textsc{Aggr}^{in}\\left(\\left\\{\\h_u^{\\ell}\\mid u\\in N^{in}_v\\right\\}\\right)&=\\sum_{\\forall u \\in N^{in}_v} \\beta_{u,v}\\h^{\\ell}_u\\\\\n\\label{eq:sigmoid}\n\\beta_{v_i,v_j}&=\\frac{1}{1+e^{-\\omega_{v_i,v_j}}}\\text{, where}\\\\\n\\label{eq:edgescalar}\n\\omega_{v_i,v_j}&=\\w^{\\ell}\\cdot\\CL_{v_i,v_j}+b \\text{, where}\\\\\n\\label{eq:edgeposition}\n\\CL_{v_i,v_j}&=\\sigma_2\\left(\\W^{\\ell}_{\\CL}\\left(\\left(\\w_{\\delta}^T\\cdot \\delta(v_i,v_j)\\right)\\parallel \\CL_{v_i}\\parallel \\CL_{v_j}\\right)\\right)%\n\\end{align}\n\\end{footnotesize}\n\nHere, $\\CL_v$ denotes the positional embedding of node $v$, whose construction we will discuss in Section~\\ref{sec:lipschitz}. Intuitively, the positional embedding represents the location of a node such that if two nodes are within close proximity in terms of shortest path distance, then their positional embeddings should also be similar. Since, $\\CL_{v_i,v_j}$ is a function of the positional embeddings of its two endpoints and the spatial distance between them, $\\CL_{v_i,v_j}$ may be interpreted as the positional embedding of the edge $(v_i,v_j)$. The importance of an edge is computed by passing its positional representation through an \\mlp (Eq.~\\ref{eq:edgeposition}), which is subsequently converted to a scalar (Eq.~\\ref{eq:edgescalar}). Finally, the scalar is passed through a sigmoid gate (Eq.~\\ref{eq:sigmoid}) to obtain its weight. Here, $\\w_{\\delta}\\in\\mathbb{R}^{d_{\\delta}}$, $\\W^{\\ell}_{\\CL}\\in\\mathbb{R}^{2d_L+d_{\\delta}\\times d_{\\CL_{e}}}$ and $\\w^{\\ell}\\in\\mathbb{R}^{d_{L_e}}$ are learnable weight parameters; $d_L$ and $d_{L_e}$ are the dimensionality of the positional embeddings of nodes and edges respectively, and $d_\\delta$ is the dimension of the projection created over edge distance $\\delta_{v_i,v_j}$.\\footnote{We project the edge distance to a higher dimensional representation since otherwise the significantly larger number of dimensions allocated to positional embeddings of the endpoints may dominate the single dimension allocated for edge distance.}. $\\sigma_2$ in Eq.~\\ref{eq:edgeposition} is an activation function to apply non-linearity. $b$ in Eq~\\ref{eq:edgescalar} is the learnable bias term.\n\\looseness=-1\n\n\n%The aggregation over incoming neighbors is defined in an analogous manner with its own set of weight parameters.\nWe now discuss the rationale behind this design. In a road network, the distribution of traffic over edges resemble a \\textit{power-law} (Fig.~\\ref{fig:traffic}). While arterial roads carry a lot of traffic and therefore is a strong determinant of the state of the downstream roads, local roads, which forms the majority, have less impact. Hence, we need to learn this importance of roads from the data. Furthermore, we would like the \\gnn to be deep without \\textit{over-smoothing} or \\textit{over-squashing} the representations~\\cite{gnnbenchmark}. Sigmoid gating serves these dual requirements. First, it assigns a weight to every edge to determine its importance. Second, it enables us to go deep since as more layers are added, we receive messages only from the edges with high weights and therefore avoid over-squashing. Semantically, the roads with high weights should correspond to the arterial roads. Note that unlike Graph attention networks~\\cite{gat}, where edges compete among each other for importance (due to normalization in \\textsc{SoftMax}), we do not have that effect in a Sigmoid gating. This is more natural for road networks since the magnitude of representations on a busy intersection should be higher than an intersection with no incoming arterial roads. In addition, as we will formally prove in Section~\\ref{sec:characterization}, Sigmoid gating provides higher expressive power. Finally, it is worth noting that the attention weights are directional since it is a function of the positional edge embedding where $\\CL_{v_i,v_j}\\neq \\CL_{v_j,v_i}$.\n\\looseness=-1\n\n\\noindent\n\\textbf{Initial embedding of nodes:} Let $t$ be the current time and $\\Delta$ the number of historical snapshots we are training on. Thus, we will have $\\Delta$ stacks of \\gnn \n where the $k^{th}$ \\gnn corresponds to snapshot of time $t-\\Delta+k,\\; k\\in[0,\\Delta]$ (Recall Fig.~\\ref{fig:model}). The initial embedding of all nodes $v\\in \\CV_{t-\\Delta+k}$ in the $k^{th}$ \\gnn is defined as:\n\\begin{equation}\n\\label{eq:h0}\n\\h^{0,k}_v=\\left(\\w_{\\tau}^T\\cdot\\tau^v_{t-\\Delta+k} \\right)\\parallel\\CL_v\n\\end{equation}\n$\\w_{\\tau}$ is a learnable vector to map the sensor value to a higher dimensional space. $\\CL_v$ is the positional embedding of $v$. Note we do not use the stack index in the notation for $\\h^{\\ell}_v$ (Eq.~\\ref{eq:hl}) since all computations are identical except the time-dependent input in Eq.~\\ref{eq:h0}. The final output after the $L^{th}$ layer in the $k^{th}$ \\gnn is denoted as $\\z^k_v=\\h^{L,k}_v$.\n%Furthermore, to denote $\\z_v=\\h^L_v$ for the \\gnn in the $k^{th}$ stack, which correspond to time $t-k$, we use $\\z^k_v$ (Recall Fig.~\\ref{fig:model}).\n\\begin{comment}\n%We will first define the key components of a message passing neural network.\n%\\begin{defn}[Message passing neural network]\n    A message passing neural network can be defined as\n    \\begin{equation}\n        \\x_u^{k}= \\phi\\left(\\x_u^{k-1},\\operatorname{Aggr}\\left(\\left\\{\\varphi(\\x_u^{k-1},\\x_v^{k-1},\\delta_{u,v})\\mid u\\in\\CN(v)\\right\\}\\right)\\right)\n    \\end{equation}\n    where, \\(\\x_u^{k}\\) are the representation of node \\(u\\) at layer \\(k\\), \\(\\phi(\\cdot)\\) is called the update function and can be parameterized as an MLP, \\(\\varphi(\\cdot)\\) is called the message function which can also be parameterized as any generic neural network, \\(\\operatorname{Aggr}\\) is a differentiable, permutation invariant aggregation function that aggregates the message \\(u\\) receives from the all nodes \\(v\\) in its neighborhood \\(\\CN(v)\\).\n\n\n%\\begin{defn}[name of layer]\n%We define the \\{name of layer\\} by the following equations:\n\n\\begin{align}\n    \\label{eqn:features}\\x_u^0&=[\\tau_t\\parallel t\\parallel L^T_u]^T\\;\\forall\\,u\\in\\CV_t\\\\\n    \\mbd{m}_v^{k-1}&=\\W^T_{\\CN(u)}\\x_v^{k-1}+\\bb_{\\CN(u)}\\\\\n    \\varphi \\left(\\x_v^{k-1}, \\delta_{u,v}\\right) &= \\mbd{m}_v^{k-1} \\odot \\sigma\\left(\\operatorname{MLP}^\\text{G}\\left(\\W_\\delta^T\\delta_{u,v}\\parallel L_u\\parallel L_v\\right)\\right)\\label{eqn:msg}\\\\\n    \\operatorname{Aggr}&=\\operatorname{mean}\\\\\n    \\varphi_v&\\mydefine \\varphi \\left(\\x_v^{k-1},\\delta_{u,v}\\right)\\\\\n    \\mbd{A}&\\mydefine \\operatorname{mean} \\left(\\left\\{\\varphi_v\\mid v\\in\\CN(u)\\right\\}\\right)\\\\\n    \\phi(\\x_u^{k-1},\\mbd{A})&=\\operatorname{normalize}_2\\left(\\gamma\\left(\\W_u^T\\x_u^{k-1}+\\bb_u\\right)+\\mbd{A}\\right)\n\\end{align}\nwhere \\(\\parallel\\) is the concatenation operation, \\(\\operatorname{normalize}_2\\) is the \\(L_2\\) normalization, \\(\\operatorname{normalize}_2(\\mbd{z})=\\frac{\\mbd{z}}{\\lVert\\mbd{z}\\rVert_2}\\)\nand \\(\\sigma(\\cdot)\\) is the sigmoid non-linearity while \\(\\gamma(\\cdot)\\) is any non-linearity. \\(\\W_{\\CN(u)}, \\W_u, \\W_\\delta, \\bb_{\\CN(u)}, \\bb_u\\) are learnable parameters along with any parameters of the MLP\\textsuperscript{G} used in equation~\\ref{eqn:msg}.\n%\\end{defn}\n\nThe \\(L-\\)layered \\gnn is defined by the equation~\\ref{eqn:gnn}.\n\\begin{equation}\n    \\label{eqn:gnn}\n    \\text{\\gnn}=\\text{GLayer}^L\\circ\\cdots\\circ\\text{GLayer}^2\\circ\\text{GLayer}^1\n\\end{equation}\nGating uses the distance of the edge along with Lipschitz embeddings of the two nodes on the ends of the edge. This is to allow the neural network to adaptively learn a distribution function over the distances, conditioned on their location in the edge, a distribution of how much traffic information should be allowed to flow conditioned on the distance. This function represents the answer to the question \\textit{how far does the influence of traffic travel over a road?} So coupled with the gated graph neural network, we believe increasing the number of layers should help. Note, unlike graph attention networks~\\cite{gat}, we use features of the edge and in gating, messages from different neighbors do not compete with each other which is better suited for a road network.\\par\n%We use a second reverse GNN. % this is crude. work on this.\n% why no competition? \n\\end{comment}\n",
                    "subsubsection 3.1.1": {
                        "name": "Positional Embeddings",
                        "content": "\n\\label{sec:lipschitz}\nThe simplest approach to encode the position of a node is to embed its latitude and longitude into a higher-dimensional representation. This embedding, however, would not reflect the constraints imposed by the road network. Hence, to learn network-induced positional embeddings, we use \\textit{Lipschitz embeddings}.\n\\begin{defn}[Lipschitz Embedding]\n\\label{def:lipschitz}\n\\textit{Let $\\mathcal{A} = \\{a_1,\\cdots, a_m\\} \\subseteq \\CV$ be a randomly selected subset of nodes. We call them anchors. Each node $v$ is embedded into an $m$-dimensional feature vector $\\CL_v$ where $\\CL_v[i]=\\frac{sp(a_i,v)+sp(v,a_i)}{2}$, where $sp(u,v)$ is the shortest path distance from $u$ to $v$.} %We embed all nodes in $\\CV$ in a $k$-dimensional feature space $\\bm{\\nu_L}(u)=[x_1,\\cdots, x_k]$, where $ x_i = d(u, a_i)$.\n\\end{defn}\n\\vspace{-0.05in}\n\\noindent\n The efficacy of the Lipschitz embedding lies in how well it preserves the shortest path distances among nodes. A well-accepted convention to measure the preservation of distances between the original space and the transformed space is the notion of \\textit{distortion}~\\cite{linial1995geometry}.\n \\looseness=-1\n\n%Several geometric problems over general metric spaces have been solved by embedding the metric space into another metric space such that this embedding has low distortion and the problem is easier to solve in this space~\\cite{linial1995geometry}. The graph nodes equipped with shortest path distance also form a metric space. We exploit this by embedding the graph's nodes into a real coordinate space as defined below.\n\\begin{defn}[Distortion]\n\\label{def:distortion}\n    \\textit{Let $X$ be a set of points embedded in a metric space with distance function $d_X$. Given a function \\(f:X\\rightarrow Y\\) that embeds objects in $X$ from the finite metric space \\((X,d_X)\\) into another finite metric space \\((Y,d_Y)\\). We define:}\n    \\begin{align}\n        \\text{expansion}(f) &= \\max_{x_1,x_2\\in X} \\frac{d_Y(f(x_1),f(x_1))}{d_X(x_1,x_2)}\\\\\n        \\text{contraction}(f) &= \\max_{x_1,x_2\\in X} \\frac{d_X(x_1,x_2)}{d_Y(f(x_1),f(x_2))}\n    \\end{align}\n    \\textit{The \\textit{distortion} of an embedding \\(f\\) is defined as the product of its expansion and contraction. If the distortion is $\\alpha$, this means that $\\forall\\,x,y\\in X,\\;\\frac{1}{\\alpha}d_X(x,y)\\le d_Y(f(x), f(y))\\le d_X(x,y)$.}\n\\end{defn}\nIn the context of a road network, $X=\\CV$, and $d_X(x_1,x_2)$ is the average two-way shortest path distance between $x_1$ and $x_2$ as defined in Def.~\\ref{def:lipschitz}. Furthermore, the mapping $f:X\\rightarrow Y$ is simply the Lipschitz embedding of $X$. Now, to define $d_Y(f(x_1),f(x_2))$, along with dimensionality $m$, we use \\emph{Bourgain's Theorem}~\\cite{linial1995geometry}.% establishes that a low \\emph{distortion} Lipschitz embedding exists for any metric space. \n\\begin{thm}[Bourgain's Theorem~\\cite{linial1995geometry}]\n    \\label{thm:bourgain}\n    \\textit{Given any finite metric space \\((X, d_X)\\), there exists a Lipschitz embedding of \\((X, d_X)\\) into \\(\\mathbb{R}^m\\) under any \\(L_p\\) norm such that \\(m=O(\\log^2 n)\\) and the distortion of the embedding is \\(O(\\log n)\\), where \\(n=\\lvert X\\rvert=|\\CV|\\).}\n\\end{thm}\nBased on Bourgain's theorem, if we can show that $d_X(u,v)=\\frac{sp(u,v)+sp(v,u)}{2}$, as defined in Def.~\\ref{def:lipschitz}, is metric, then choosing \\(m=O(\\log^2 |\\CV|)\\) anchors and any $L_p$ distance in the embedded space would provide a distortion of \\(O(\\log |\\CV|)\\). We next show that $d_X(u,v)$ is indeed a metric.\n\n%\\vspace{-0.05in}\n\n\\vspace{-0.05in}\n\\renewcommand{\\qedsymbol}{}\n \\begin{proof} \\vskip-6pt\\rev{\\textit{See App.~\\ref{app:proof_lem_one}.}}\n \\end{proof}\n \\looseness=-1\n \n The exact algorithm to choose the anchors is described in \\cite{linial1995geometry}. Note that we use the same set of anchors across all snapshots. Since the topology may change across snapshots, the positional embeddings are time-varying as well. Although unlikely, it is possible for an anchor node to get deleted in a particular snapshot. In such a scenario, we denote the distance corresponding to this dimension as $\\infty$ for all nodes. %We note that although Lipschitz embedding has been used within the framework of a \\gnn in the literature, they have \n\n\\begin{comment}\nSpecifically, such an embedding can be constructed as follows~\\cite{linial1995geometry}.\n\\begin{defn}[Lipschitz embeddings]\n    Let \\(\\mathcal{A}=\\{a_1,a_2,\\dotsc,a_k\\}\\subseteq\\CV\\) be a random subset of the nodeset \\(\\CV\\). The elements of \\(\\mathcal{A}\\) are called anchors. Then the Lipschitz embedding of node \\(u\\in\\CV\\) is defined as:\n    \\begin{equation}\n        L_u=\\begin{bmatrix}\n                d(a_1,u)\\\\\n                d(a_2,u)\\\\\n                \\vdots\\\\\n                d(a_k,u)\n            \\end{bmatrix}\n    \\end{equation}\n    where \\(d(a_i,u)\\) is the shortest path distance of node \\(u\\) from anchor \\(a_i\\).\n\\end{defn}\nIntuitively, the Lipschitz embeddings of the nodes of graph embed the nodes into a real coordinate space such that the distances of the embedding vectors is close to the actual distance of the nodes on the road network. We attach the Lipschitz embeddings of each node to the node as features.\n\\end{comment}\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Encoding Temporal Dependency",
                    "content": "\nTo encode long-term temporal dependencies over the node representations, we use an \\lstm encoder. Specifically, the final-layer outputs of the \\gnn in the $k^{th}$ stack, denoted as $\\z^k_v$, is fed to the $k^{th}$ \\lstm stack. In addition, the $k^{th}$ \\lstm also receives the hidden state of the $(k-1)^{th}$ \\lstm (Recall Fig.~\\ref{fig:model}). Mathematically, the output of the $k^{th}$ \\lstm on node $v$, denoted as $\\s^k_v\\in\\mathbb{R}^{d_{Lenc}}$, is computed as follows. \n\\vspace{-0.10in}\n\\begin{equation}\n\\label{eq:lstm}\n\\s^{k}_v=\n\\begin{cases}\n    \\lstm_{Enc}\\left(\\s_v^{k-1},\\z_v^{k}\\right)&\\text{if k>1}\\\\\n    \\lstm_{Enc}\\left(\\mlp_1\\left(t,\\z^k_v\\right),\\z_v^{k}\\right) & \\text{if k=1}\n    \\end{cases}\n\\end{equation}\nSince $\\s^{0}_v$ is undefined for the first \\lstm, i.e., $k=1$, we make it a learnable vector through the $\\mlp_1$. The output of the final \\lstm stack corresponding to current time $t$ (See Fig.~\\ref{fig:model}), feeds into the decoder. $d_{L_{enc}}$ is the dimension of the \\lstm representations. \n\nLines \\ref{algl:enc_start}--\\ref{algl:enc_end} in Alg.~\\ref{alg:forwardpass} summarize the encoder component. Given stream $\\VG$, current time $t$, and a target node $v$, we extract the subset $\\VG_{[t-\\Delta,t]}\\subseteq\\VG$ where $k$ denotes the number of \\gnn-\\lstm stacks. The processing starts at $\\CG_{t-\\Delta}$ where the $k^{th}$ \\gnn stack embeds $v$ into $\\z_v^k$. Next, $\\z_v^k$ is passed to the $k^{th}$ \\lstm. This completes one stack of computation. Iteratively, we move to the $(k+1)^{th}$ stack and the same operations are repeated till $k=\\Delta$, after which $\\s^{\\Delta}_v$~(Eq.~\\ref{eq:lstm}) is fed to the decoder.\n\n\\renewcommand{\\algorithmicensure}{\\textbf{Input:}}\n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}} % https://mirror.niser.ac.in/ctan/macros/latex/contrib/algorithms/algorithms.pdf   p. 13\n\\begin{algorithm}[t]\n    \\caption{\\name forward pass}\n    \\label{alg:forwardpass}\n    {\\scriptsize\n    \\begin{algorithmic}[1]\n        \\REQUIRE \\name, stream $\\rns$, target node $v$, current timestamp $t$, prediction horizon $\\Delta$\n        \\ENSURE Predicted sensor values $\\hat{y}^v_{t+1}\\dotsc\\hat{y}^v_{t+\\Delta}$\n        \\STATE $\\s_v^0\\gets\\mlp_1(t,\\z_v^1)$\\label{algl:enc_start}\n        \\STATE $k\\gets 1$\n        \\FORALL{$\\CG\\in\\rns_{[t-\\Delta,t]}$}\n            \\STATE $\\z_v^k\\gets \\gnn(v)$\n            \\STATE $\\s_v^k \\gets \\oplstm_\\text{Enc}(\\s_v^{k-1},\\z_v^{k})$\n            \\STATE $k\\gets k+1$ \\label{algl:enc_end}\n        \\ENDFOR\n        \\STATE $\\m_v^t\\gets\\mlp_3\\left(moments\\left(\\left\\{\\tau^{t'}_u \\neq \\varnothing| t'\\in[t-\\Delta,t], (u,v)\\text{ or } (v,u)\\in \\CE^{t'}\\right\\}\\right)\\right)$ \\label{algl:dec_begin}\n        \\STATE $\\hat{y}_0\\gets \\mlp_2(\\s_v^\\Delta,\\m_v^t)$\n        \\STATE $\\q_v^0\\gets\\s_v^\\Delta$\n        \\FORALL{$k\\in[1 \\dotsc \\Delta]$}\n            \\STATE $\\q_v^k\\gets\\oplstm_{\\text{Dec}}(\\q_v^{k-1},\\hat{y}^{k-1}_v)$\n            \\STATE $\\hat{y}_v^k\\gets\\mlp_4(\\q_v^k,\\m_v^t)$ \\label{algl:dec_end}\n        \\ENDFOR\n        \\STATE Re-index $\\hat{y}^v_k\\mapsto\\hat{y}^v_{t+k}$\n        \\RETURN $\\hat{y}^v_{t+1}, \\dots, \\hat{y}^v_{t+\\Delta}$\n    \\end{algorithmic}}\n\\end{algorithm}\n\n\\begin{comment}\nThus, the full encoder equation that encodes the input sequence is:\n\\begin{equation}\n    \\label{eqn:enc}\n    \\text{ENC}\\left(\\rns_{[t-k,t]}\\right)=\\left\\{\\lstm_\\text{enc}(\\text{\\gnn}(\\CG_{t_i}))\\;\\forall\\,v\\in\\CV_t,\\forall\\CG_{t_i}\\in\\rns\\right\\}\n\\end{equation}\nThe parameters of the encoder are \\[\\text{ENC}_\\Theta=\\{\\text{Glayer}^1_\\Theta\\dotsc,\\text{Glayer}^L_\\Theta,\\oplstm^1_\\Theta,\\dotsc,\\oplstm^\\Lambda_\\Theta\\}\\]\n\nAn \\lstm defined by the following equations:\n\\begin{align}\n    \\ii_t&=\\sigma(\\W_{ii}\\x_t+\\bb_{ii}+\\W_{hi}\\h_{t-1}+\\bb_{hi})\\label{eqn:ii}\\\\\n    \\f_t&=\\sigma(\\W_{if}\\x_t+\\bb_{if}+\\W_{hf}\\h_{t-1}+\\bb_{hf})\\label{eqn:ff}\\\\\n    \\g_t&=\\tanh(\\W_{ig}\\x_t+\\bb_{ig}+\\W_{hg}\\h_{t-1}+\\bb_{hg})\\label{eqn:gg}\\\\\n    \\oo_t&=\\sigma(\\W_{io}\\x_t+\\bb_{io}+\\W_{ho}\\h_{t-1}+\\bb_{ho})\\label{eqn:oo}\\\\\n    \\bc_t&=\\f_t\\odot\\bc_{t-1}+\\ii_t\\odot\\g_t\\label{eqn:cc}\\\\\n    \\h_t&=\\oo_t\\odot\\tanh(\\bc_t)\\label{eqn:hh}\n\\end{align}\nLet \\(\\oplstm(\\{\\x_1, \\x_2, \\dotsc, \\x_T\\},(\\h_0, \\bc_0))\\) denote the operations in the equations~\\ref{eqn:ii}--\\ref{eqn:hh} and its output is \\(\\{\\h_1,\\h_2,\\dotsc,\\h_T\\}\\). The parameters of the \\lstm are \\(\\W\\)s and \\(\\bb\\)s in the equations~\\ref{eqn:ii}--\\ref{eqn:hh} along with \\((\\h_0,\\bc_0)\\).\n\\end{comment}\n\\begin{comment}\nMultilayered \\lstms are just functional compositions of more than one \\lstms. Concretely, if there are \\(\\Lambda\\) layers of \\lstm, then the inputs to \\lstm\\(^{l+1}\\) are \\(\\{\\h^l_1,\\h^l_2,\\dotsc,\\h^l_T\\}\\), \\(l\\geq 1\\), and input to \\lstm\\(^1\\) is just the original sequence \\(\\{\\x_1,\\x_2,\\dotsc,\\x_T\\}\\). The output of multilayered \\lstms are the hidden states of the last layer $\\{\\h_1^\\Lambda,\\h_2^\\Lambda,\\dotsc,\\h_T^\\Lambda\\}$.\n\nWe propose to use a multilayered \\lstm as encoder with \\(\\Lambda\\) as a parameter.\n\\begin{equation}\n    \\oplstm_\\text{enc}=\\oplstm^\\Lambda\\circ\\oplstm^{\\Lambda-1}\\circ\\cdots\\circ\\oplstm^1\n\\end{equation}\n\\end{comment}\n\n%\n"
                },
                "subsection 3.3": {
                    "name": "Decoder",
                    "content": "\n\\label{sec:decoder}\n\nThe decoder is composed of a stack of $\\Delta$ (forecasting horizon) siamese \\lstms. Assuming $t$ to be the current time, the output of the $k^{th}$ \\lstm in the decoder corresponds to the predicted sensor reading at time $t+k$. Each \\lstm receives two inputs: (1) the predicted sensor value in the previous timestamp denoted as $\\y_v^{k-1}\\in\\mathbb{R}$ and (2) the hidden state of the previous \\lstm, denoted as $\\q_v^{k-1}\\in\\mathbb{R}^{d_{dec}}$. Thus, the output of the $k^{th}$ \\lstm is expressed as:\n\\vspace{-0.05in}\n\\begin{equation}\n\\label{eq:decoder}\n\\q^{k}_v=\n\\begin{cases}\n    \\lstm_{Dec}\\left(\\y_v^{k-1},\\q_v^{k-1}\\right)&\\text{if k>1}\\\\\n    \\lstm_{Dec}\\left(\\mlp_2\\left(\\s^{\\Delta}_v,\\m_v^t\\right),\\s_v^\\Delta\\right) & \\text{if k=1}\n    \\end{cases}\n\\end{equation}\n\nWe have a special case for $k=1$, since both $\\y_v^{0}$ and $\\q_v^{0}$ are undefined. Since we assume a setting of partial sensing across nodes, $\\tau^t_v$ may not be available and hence cannot be used to substitute $\\y_v^{0}$. To mitigate this situation, $\\q_v^{k-1}$ is replaced with the output $\\s^{\\Delta}_v$ of the last \\lstm in the encoder and $\\y_v^{0}$ is estimated through the $\\mlp_4$. This \\mlp takes as input $\\s^{\\Delta}_v$ and a representation of the \\textit{moments}~\\cite{moments} of the observed sensor values in the neighborhood of $v$, denoted as $\\m^t_v$. Formally,\n\\begin{align}\n\\nonumber\n\\resizebox{.97\\linewidth}{!}{%\n$\\m^t_v=\\mlp_3\\left(moments\\left(\\left\\{\\tau^{t'}_u \\neq \\varnothing| t'\\in[t-\\Delta,t], (u,v)\\text{ or } (v,u)\\in \\CE^{t'}\\right\\}\\right)\\right)$%\n}\n\\end{align}\nFinally, the predicted sensor value $\\forall k\\in [1,\\Delta],\\:\\y_v^{k}$ is computed as:\n\\begin{equation}\n\\y_v^{k}=\\mlp_4\\left(\\q_v^k,\\m^t_v\\right)\n\\end{equation}\n\nNote that instead of directly predicting the sensor value from the \\lstm hidden state $\\q^k_v$, we augment this information with statistical information $\\m^t_v$ on the time-series at $v$. This provides a strong inductive bias to the model and elevates its performance, which we will substantiate empirically during our ablation study in Section~\\ref{sec:experiments}. Lines \\ref{algl:dec_begin}--\\ref{algl:dec_end} in Alg.~\\ref{alg:forwardpass} summarize the computations in the decoder.\n\\begin{comment}\nThe first \\lstm receives $\\tau_v^t$ and $s_v^\\Delta$ (Eq.~\\ref{eq:lstm}).\n%In this section we describe how we propose to decode outputs from the encoded representations as part of \\name. \nFor decoding temporally coherent sequences, we use an auto-regressive multi-layered \\lstm, referred as \\(\\oplstm_\\text{dec}\\) here onwards. To prompt the decoder to start producing outputs, it needs two things: (1) a start symbol \\(S_\\text{dec}\\), which is a trainable parameter. (2) An encoded summary of input sequence. The encoded summary when producing output token \\(\\hat{y}_\\kappa\\) is calculated from the outputs of the final layer of the encoder using attention, specifically using the equations~\\ref{eqn:summary_begin}--\\ref{eqn:summary_end}.\n\\begin{align}\n    \\label{eqn:summary_begin}\n    a_i&=\\operatorname{MLP}^\\text{attn}(\\h^\\Lambda_i,\\mbd{d}_{\\kappa-1})\\\\\n    \\alpha_i&=\\operatorname{Softmax}(\\mbd{a})\\\\\n    \\mbd{g}_\\kappa&=\\sum_i\\alpha_i\\h^\\Lambda_i\\label{eqn:summary_end}\n\\end{align}\n%\nwhere \\(\\mbd{d}_{\\kappa-1}\\) is the hidden state of the final \\(\\oplstm\\) layer of \\(\\oplstm_\\text{dec}\\). The input of \\(\\oplstm_\\text{dec}\\), represented as \\(\\x^\\text{dec}_\\kappa\\; \\forall\\,\\kappa\\in[1,\\dotsc,\\Delta]\\) is a sequence defined as:\n%\n\\begin{equation*}\n    \\x^\\text{dec}_\\kappa=\n    \\begin{cases}\n        \\hat{y}_{\\kappa-1}\\parallel\\mbd{g}_\\kappa,&\\text{if }\\kappa > 1\\\\\n        S_\\text{dec}\\parallel\\mbd{g}_\\kappa,&\\text{if }\\kappa = 1\n    \\end{cases}\n\\end{equation*}\n%\nLet the output of decoder be \\(\\h^\\text{dec}_\\kappa\\), then the final traffic prediction \\(\\hat{y}_\\kappa\\) is calculated as:\n\\begin{equation}\n    \\hat{y}_\\kappa = \\operatorname{MLP}\\left( \\h^\\text{dec}_\\kappa\\parallel \\mbd{M} \\left(\\tau_{[t-\\Delta^-,t]}^v\\mid v\\in\\CN(u)\\right) \\right) %note: this is like a local filter, like a gaussian kernel looking N(u) in space and k steps in time. Same idea as STNN.\n\\end{equation}\n % of the current target node!\n\\textcolor{red}{include the fact that all the $\\tau$ values that participate in this are $\\neq \\varnothing$}\n\\end{comment}\n\\begin{comment}\nThe decoder is then defined by equations~\\ref{dec_begin}--\\ref{dec_end}.\n\\begin{align}\n    \\label{dec_begin}\\x^\\text{dec}_{[t,t+\\Delta]}&\\mydefine\\{\\x^\\text{dec}_{t},\\x^\\text{dec}_{t+1},\\dotsc \\x^\\text{dec}_{t+\\Delta}\\}\\\\\n    \\h^\\text{dec}_{[t,t+\\Delta]}&=\\oplstm_\\text{dec}(\\x^\\text{dec}_{[t,t+\\Delta]},\\mbd{d}_0)\\\\\n    \\hat{y}_{[t,t+\\Delta]}&=\\operatorname{MLP}(\\h^\\text{dec}_{[t,t+\\Delta]}\\parallel \\mbd{M}(\\tau^v_{[t-k,t]}\\mid v\\in\\CN(u)))\\label{dec_end}\n\\end{align}\nWe define the composition of the operations in equations~\\ref{dec_begin}--\\ref{dec_end} as \\(\\text{DEC}(\\cdot)\\). The parameters of the decoder are\n\\[\\text{DEC}_\\Theta=\\{\\operatorname{MLP}^\\text{attn}_\\Theta,\\operatorname{MLP}_\\Theta,\\oplstm_{\\text{dec},\\Theta}\\}\\]\n\\begin{defn}[\\name]\n    \\name is the model defined by the following equation\n    \\begin{equation}\n        \\name(\\rns)=\\text{DEC}(\\text{ENC}(\\rns))\n    \\end{equation}\n    and $\\Psi_\\Theta=\\name_\\Theta=\\text{DEC}_\\Theta\\cup\\text{ENC}_\\Theta$.\n\\end{defn}\n\\end{comment}\n"
                },
                "subsection 3.4": {
                    "name": "Theoretical Characterization of \\name",
                    "content": "\n\\label{sec:characterization}\n\n\\textbf{Fact 1.} \\textit{\\name is inductive and permuation-invariant.}\n\n\\noindent \n\\textsc{Discussion.} As outlined in Sections~\\ref{sec:gnn}-\\ref{sec:decoder} and summarized in Table~\\ref{tab:parameters} in Appendix, the parameter-size is independent of the network size, number of snapshots being used for training, forecasting horizon, and the temporal granularity across snapshots. Furthermore, since \\name uses sigmoid-weighted sum-pool, it is permutation invariant to node set. Consequently, \\name can inherently adapt to changes in topology or forecast on unseen nodes without the need to re-train from scratch. As outlined in Table~\\ref{tbl:baselines} and discussed in Sec.~\\ref{sec:existingwork}, majority of existing works on spatio-temporal network-constrained forecasting do not satisfy the needs of being inductive and permutation-invariant. $\\hfill\\square$\n\nWe now focus on the expressive power of \\name. Message-passing \\gnns that aggregate messages from their neighbors are no more powerful than 1-WL~\\cite{gin}. Hence, given nodes $v$ and $u$, if their $\\ell$-hop neighborhoods are isomorphic, they will get identical embeddings. Thus, the position of a node plays no role. Methodologies such as DCRNN~\\cite{dcrnn}, STGCN~\\cite{stgcn}, or \\rev{STGODE~\\cite{stgode}}, that build on top of these message-passing \\gnns will consequently inherit the same limitation. \\name does not suffer from this limitation.\n\n\\renewcommand{\\qedsymbol}{$\\square$}\n\n\\begin{proof}\n    Consider nodes $v_1$ and $v_2$ in Fig.~\\ref{fig:g3}. If we use a 1-layered message-passing \\gnn, then their embeddings would be identical since the 1-hop neighborhoods (color coded in green and pink) are isomorphic. Hence, DCRNN, STGCN, \\rev{or STGODE} would not be able to distinguish between them. \\name augments initial node features with Lipschitz embeddings. Assuming  $a_1$ and $a_2$ to be the anchor nodes, $v_1$ would have a Lipschitz embedding of $[2,2]$ vs. $[5,5]$ for $v_2$. Hence, \\name would generate different embeddings for $v_1$ and $v_2$.\n    \\looseness=-1\n\\end{proof}\n\\renewcommand{\\qedsymbol}{}\n\n\\vskip-6pt\n%The above expressivity result has strong implications for forecasting on road-networks. Traffic patterns have strong positional context. For example, while in the morning the generic traffic flow may be from south to north, in evening this can reverse (due to distribution of office and residential districts for example). Positional embedding enables \\name to also model these global patterns in addition to the local neighborhood features. \nAs we will show in our ablation study, removing positional embeddings have a significant impact on the performance.\n\n%We now further note that \\name remains as powerful as 1-WL. \n\n\\begin{proof}\n \\vskip-6pt\\rev{\\textit{See App.~\\ref{app:proof_of_lemmathree}.}}\n\\end{proof}\n%We show \\name is more expressive than 1-WL due to positional embeddings.\n\\begin{cor}\n\\vskip-6pt\\name is strictly more expressive than DCRNN, STGCN, \\rev{and STGODE.}\n\\end{cor}\n\\renewcommand{\\qedsymbol}{$\\square$}\n\\begin{proof} \\vskip-6pt This follows naturally by combining Lemmas~\\ref{lem:position} and \\ref{lem:1wl} since \\name retains the 1-WL expressivity of DCRNN, STGCN, \\rev{and STGODE}, while also being capable of distinguishing between isomorphic topologies through positional embeddings.\n\\end{proof}\n\nWe note that while positional embeddings have been used in the context of \\gnns~\\cite{pgnn,graphreach}, they do not provide 1-WL expressivity since messages are exchanged only among anchor nodes. The methodology proposed in this work is therefore unique.\n    \n    \n\\begin{comment}\n\\begin{thm}\nThe \\gnn in \\name is more powerful than 1-WL.\n\\end{thm}\n\\vspace{-0.10in}\n\\begin{proof} \n   Now, we show that there exists a graph that 1-WL cannot distinguish (and hence message-passing \\gnns cannot either), but \\name can. Specifically, $\\CG_1$  and $\\CG_2$, corresponding to Fig.~\\ref{fig:g1} and Fig.~\\ref{fig:g2} respectively, are two graphs that 1-WL cannot distinguish~\\cite{graphormer}. Since the nodes in $\\CG_1$ and $\\CG_2$ are unlabeled, they will all be assigned the same ``color'' in the first iteration of 1-WL. \n\n   We next show that 1-WL can distinguish between these graphs if the initial ``color'' is chosen by hashing both the node label and the shortest path distances to some randomly chosen anchor nodes as done in Lipschitz embedding (Def.~\\ref{def:lipschitz}). We call this version \\textit{1-LWL}.\n  % In \\name, we augment the initial representation with the shortest path distances to each anchor node (Eq.~\\ref{eq:h0} and Def.~\\ref{def:lipschitz}).\n  Specifically, let $v_1$ be the randomly selected anchor nodes in both $\\CG_1$ and $\\CG_2$. We now observe that while $v_4$ and $v_5$ are at a distance of 3 from $v_1$ in $\\CG_2$, only one node, $v_4$, is at distance of $3$ from $v_1$ in $\\CG_2$. Thus, when we assign color to nodes by hashing the set of shortest path distance to anchors, the color histogram will be different across $\\CG_1$ and $\\CG_2$, and thereby enabling 1-WL to distinguish these graphs if the node labels are augmented with shortest path distances to some anchor nodes. \n\n  Since \\name augments the initial representations with the shortest path distances to each anchor node (Eq.~\\ref{eq:h0}) and is capable of performing sum-pool, \\name $\\equiv$ 1-LipWL $\\geq$ 1-WL. \n\\end{proof}\n\\end{comment}\n \\begin{comment}\n Anchor nodes in Lipschitz embeddings are chosen randomly. The additional efficacy of 1-LipWL over 1-WL is dependent on whether the anchors chosen across graphs is distinguishing enough in terms of shortest path distances. Furthermore, the larger the number of anchors, denoted as $m$, the more powerful 1-LipWL is. We next summarize the relationship between 1-LipWL and 1-WL as a function of $m$.% We observe, that it the choice of the anchor node and the number of anchors nodes\n\n\\begin{thm}\n${Pr}(\\text{1-LipWL}>\\text{1-WL})\\geq \\frac{m}{n}$ where $n=|\\CV_1|=|\\CV_2|$ is the node set size.\\footnote{1-WL test is trivial when $|\\CV_1|\\neq|\\CV_2|$.}\n\\end{thm}\n\\begin{proof}\n    We assume the worst case scenario where two non-isomorphic graphs $\\CG_1$ and $\\CG_2$ differ in their\n    shortest path representations only in one node in each graph and 1-WL$(\\CG_1)$=1-WL$(\\CG_2)$. Let these nodes be $v_1$ and $v_2$. We further assume that\n    the shortest path representations $\\mbd{sp}^i_{v_i}\\;i\\in\\{1,2\\}$ differ in just\n    one position $p$ that can be distributed uniformly in $\\mbd{s}^i_{v_i}$. The probability\n    of not including the node corresponding to this position in set of anchor nodes,\n    $\\mathcal{A}$ of size $m$ is then:\n    \\begin{equation*}\n        \\mathrm{Pr}(v_p\\notin\\mathcal{A})=\\frac{n-1}{n}\\cdot\\frac{n-2}{n-1}\\cdots\\frac{n-m}{n-m+1}=\\frac{n-m}{n}\n    \\end{equation*}\n    And thus $\\mathrm{Pr}(v_p\\in\\mathcal{A})=\\frac{m}{n}$\n\\end{proof}\n\\end{comment}\n\n\\noindent\n%\\textbf{Implications:}\n\\begin{comment}\nThe Weisfeiler-Lehman (WL) graph isomorphism test may be used to measure the\nexpressive power of graph neural networks~\\cite{expressive}. The test shows\nthat the popular \\gnns including GCN~\\cite{graphsage}, GCN~\\cite{gcn}, GAT~\\cite{gat}\nand many others cannot distinguish between graphs that are indistinguishable\nby the 1-WL test. This is crucial because for universal function approximation\non graphs. Consider two substructures $S_1$ and $S_2$ (nodes, edges, subgraphs, etc.),\nand say a function on graph $F$ maps them as $S_1\\mapsto F(S_1)$,\n$S_2\\mapsto F(S_2)$. Since $F$ is a proper function,\n$S_1\\neq S_2\\iff F(S_1)\\neq F(S_2)$. It is easy to see that\ndistinguishing between substructures is crucial to learn this function.\n\\end{comment}\n%\\begin{wrapfigure}{l}{0.3\\linewidth}\n    %\\centering\n\n%\\end{wrapfigure}\n\n\\begin{comment}\nFirstly, in a static undirected graph, if the full vertex set\n$\\CV$ is used as the set of anchors to compute Lipschitz embeddings, then this\nreduces to a vector of shortest path distances of a node to all the nodes\nin the graph. This representation can be used to perform isomorphism\ntest on graphs that 1-WL fails on~\\cite{graphormer,degnn,provably}.\nWe can formalize this as\n\\begin{defn}[Shortest Path Isomorphism Test]\n    Given two unlabelled, undirected graphs $G_1=(\\CV_1,\\CE_1)$ and $G_2=(\\CV_2,\\CE_2)$ define\n    $\\mbd{s}^1_{u_1}=\\{d_1(u_1,v_1)\\mid\\forall\\,v_1\\in\\CV_1\\}\\;\\forall\\,u_1\\in\\CV_1$ and similarly\n    compute $\\mbd{s}^2_{u_2}=\\{d_2(u_2,v_2)\\mid\\forall\\,v_2\\in\\CV_2\\}\\;\\forall\\,u_2\\in\\CV_2$\n    where $d_1$ and $d_2$ are \\textit{shortest path distance} metric on\n    $G_1$ and $G_2$, respectively. Then, graphs $G_1$ and $G_2$ are not\n    isomorphic if $\\exists v_1\\in\\CV_1,\\forall\\,v_2\\in\\CV_2,\\mbd{s}^1_{v_1} \\neq \\mbd{s}^2_{v_2}$\n    or $\\exists v'_2\\in\\CV_2,\\forall\\,v'_1\\in\\CV_1,\\mbd{s}^2_{v'_2}\\neq\\mbd{s}^1_{v'_1}$.\n\\end{defn}\n\nWe know that shortest path isomorphism test$>$1-WL~\\cite{graphormer,degnn,provably}, we now try to find the place of Lipschitz embeddings\non this scale.\n\n\nWe observe, that it is possible to provide a probabilistic bound\nover the power of Lipschitz embeddings.\n\n\\begin{thm}\n    \\textit{The isomorphism test based on $k-$dimensional Lipschitz embeddings (LIT) is\n    more powerful than 1-WL with\n    \\begin{equation*}\n        \\mathrm{Pr}(\\text{LIT}>\\text{1-WL})\\geq \\frac{k}{\\lvert\\CV\\rvert}\n    \\end{equation*}\n    }\n\\end{thm}\n\\begin{proof}\n    We assume that the two non-isomorphic graphs $G_1$ and $G_2$ differ in their\n    shortest path representations only in one node in each graph. This is the\n    hardest condition. Let these nodes be $v_1$ and $v_2$. We further assume that\n    the shortest path representations $\\mbd{s}^i_{v_i}\\;i\\in\\{1,2\\}$ differ in just\n    one position $p$ that can be distributed uniformly in $\\mbd{s}^i_{v_i}$. The probability\n    of not including the node corresponding to this position in set of anchor nodes,\n    $\\mathcal{A}$ of size $k$ is then:\n    \\begin{equation*}\n        \\mathrm{Pr}(v_p\\notin\\mathcal{A})=\\frac{n-1}{n}\\cdot\\frac{n-2}{n-1}\\cdots\\frac{n-k}{n-k+1}=\\frac{n-k}{n}\n    \\end{equation*}\n    And thus $\\mathrm{Pr}(v_p\\in\\mathcal{A})=\\frac{k}{n}$\n\\end{proof}\n\nFurther, in the road network setting the set of anchor nodes for computing\nLipschitz embeddings is selected based on the results of theorem~\\ref{thm:bourgain}.\nThen we can show that Lipschitz embeddings allow to distinguish two distinct isomorphic\ninduced subgraphs within the graph based on their position in the graph.\n\\begin{thm}\n    Given an unlabelled, undirected graph $G$, any two distinct isomorphic induced\n    subgraphs $G_1$ and $G_2$ of $G$ are distinguishable by Lipschitz embeddings\n    test as long as the anchor nodes are not symmetrically distributed around\n    the two subgraphs.\n\\end{thm}\n\n\\begin{proof}\n    This is easily shown by observing that:\n         first, the two distinct subgraphs $G_1$ and $G_2$\n        are made up of separate set of nodes, i.e. there is at least one node in $u\\in G_1$ that\n        is not in $G_2$ and vice versa. The shortest paths $\\{d(u,v)\\mid\\forall\\,v\\in G_2\\}$\n        will be all non-zeros.\n        Second, given any two nodes $v_1$ and $v_2$, their Lipschitz\n        embeddings satisfy that $\\frac{1}{\\alpha}d(v_1,v_2)\\le \\lVert L_{v_1}-L_{v_2}\\rVert_2\\le d(v_1,v_2)$\n        (theorem~\\ref{thm:bourgain}), where $\\alpha$ is the distortion. Note both the lower and upper\n        bound are positive when $d(v_1,v_2)\\neq 0$, implying that $\\lVert L_{v_1}-L_{v_2}\\rVert_2>0$.\n\n        Thus,\n        \\begin{align*}\n        \\{d(u,v)>0\\mid\\forall\\,v\\in G_2\\}&\\implies \\{\\lVert L_{u}-L_v\\rVert_2>0\\mid\\forall\\,v \\in G_2\\}\\\\\n        &\\implies \\{L_u\\neq L_v\\mid\\forall\\,v \\in G_2\\}\n        \\end{align*}\n\\end{proof}\nNote, the above fails for the case when the anchor nodes are symmetrically placed\nwith respect to all the nodes in the two isomorphic induced subgraphs $G_1$ and $G_2$.\nWe assume this situation is rare in road networks. The underlying intuition is that for large graphs,\nlocal subgraphs (within L-hop neighbors) around two distinct nodes are almost independent.\n\nThis is a desirable property in road network predictions with missing sensor readings\nsince in the case when all of the sensor readings of a node is missing we still want\nto learn a different representation for it based on its location in the network (high traffic region, etc.)\nas compared to another such node.\n\\end{comment}%\n%\n\\begin{comment}\nIn designing the graph convolution operation, the motivation was that contemporary graph neural networks look upto $k$-hops, where $k$ is a fixed depth and equal to the number of layers used in the \\gnn. But, in road networks, it is logical to cut off the flow of information not based on the number of hops away a node is but based on how far it is physically on the road. But this is challenging to implement, and deciding what the distance should be itself is a daunting task. So, we simplify this problem by assuming that the information about the strength of the message that should be passed is contained in the length of the edge, along with where the edge is located. With this in mind we design a gated graph convolution operation, where the gating is calculated based on the edge weight and Lipschitz embeddings of the two nodes involved in the edge.\\par\nIn terms of the message passing neural network, it is defined as\n\\begin{equation}\n\\varphi\\left(x_u^l,x_v^l,e_{u,v}\\right) = x_v^l\\odot\\sigma\\left(\\mbox{MLP}_1\\left(\\mbox{MLP}_2\\left(w(e_{u,v})\\right)\\lVert L_u\\rVert L_v)\\right)\\right)\n\\end{equation}\nwhere $w(\\cdot)$ maps an edge to its weight (which is the length of the road segment here), MLP$_2$ is used to map the scalar edge weight to a dimension comparable to that of the Lipschitz embedding, and MLP$_1$ is used to calculate the gate value, $\\lVert$ is concatenation operation and $\\sigma(\\cdot)$ is the sigmoid non-linearity. This gate value is then multiplied element-wise with the neighbor node representation. Note as in equation~\\ref{eqn:mpnn}, $u$ is the target node and $v$ represents a node in its neighborhood. Same message is calculated for all nodes in the neighborhood of $u$ and a mean of them is taken (that is $\\sum$ is mean) which is then passed to the updater network $\\phi(\\cdot)$.%\n\\par\nInspired by DCRNN, to make use of information coming from both directions in the directed road network, we also run the \\gnn on the network with reversed edges and concatenate the representations together to represent the spatial distribution of a target node.\n%\n%\n\\subsection{Time series modeling}\nWe use two stacked \\lstms to extract temporal dynamics from the sequence of inputs provided by the graph neural network. We initialize the forget gate biases of the \\lstms to high constant value of 1 to encourage networks to pass on most of the information at least during the early phase of training. The hidden states are kept as trainable parameters and the state of the last unrolled \\lstm is passed on to the decoder as a summary of the input sequence.%\n\\par\nThe decoder consists of two stacked \\lstms as well, that is run auto-regressively. At each time instant, the decoder takes as input the previous prediction along with the final state of the encoder \\lstm. The output produced by the \\lstms is added with the neighborhood mean provided by \\gnn layer to produce a prediction.\\par\nThe addition of mean implies that our model is predicting deviations from the mean. It can also be seen as saying that the predictions are supposed to be around the mean, which acts as a prior that induces bias, and thus improves generalization by combating overfitting.\n\\end{comment}\n%\n%%\\input{thealgorithms.tex}\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiments}\nIn this section, we benchmark \\name and establish:\n\\begin{itemize}\n\\item \\textbf{Prediction Accuracy:} \\name is superior compared to baselines for the task of spatio-temporal forecasting on road networks. \n\\looseness=-1\n\\item \\textbf{Ablation study:} Through extensive ablation studies, we illuminate the significance of each component of \\name and provide insights into the critical role they play in the overall performance.\n\\item \\textbf{Robustness:} We test the limits of our model to frugality in sensing, graph structure modifications at inference time and resilience to non-uniform temporal granularity across snapshots. \n\\end{itemize}\nThe codebase of \\name and datasets are available at \\url{https://github.com/idea-iitd/Frigate}.\n\n%\\subsection{Experimental Setup}\n\n\\vspace{-0.05in}\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\nWe use three real-world datasets collected by tracking the GPS trajectory of taxis.\nAs summarized in Table~\\ref{tbl:dataset_details}, the three datasets correspond to the cities of Beijing, Chengdu and Harbin~\\cite{neuromlr}. We map-match~\\cite{grasshopper} the raw trajectories to their corresponding road networks obtained from Openstreetmap~\\cite{osm}. The traffic is aggregated into buckets of $5$ minutes and the sensor value corresponds to the number of vehicles passing through each node in a 5-minute interval. From the entire node-set, we select a subset of nodes uniformly at random as the ``seen'' nodes with installed sensors. Inference and loss calculation is performed only on the unseen nodes. The default percentage of nodes we retain as ``seen'' is shown in Table~\\ref{tbl:dataset_details}. \n% Nonetheless, we also have explicit experiments to vary the percentage of seen nodes across a broader range and evaluate performance. %Finally, we point the readers towards the high standard deviation observed in Harbin. Harbin \n\\looseness=-1 \n%Each dataset is physically divided into three files that contain the traffic features, adjacency matrix\n%and Lipschitz embeddings respectively. See table~\\ref{tbl:dataset_details} for details about the datasets.%\n\n\n\\vspace{-0.05in}\n"
                },
                "subsection 4.2": {
                    "name": "Experimental Setup",
                    "content": " \n% In this section, we outline the empirical setup used to benchmark \\name.\n\n\\noindent\n{\\bf $\\bullet$ Computational engine:} We use a system running on Intel Xeon 6248 processor with 96 cores and 1 NVIDIA A100 GPU with 40GB memory for our experiments. \n\\begin{comment}\nThe experiments require evaluation to be done on part of the graph that\nwas unseen during training. This can be done directly during training by specifying a set of nodes whose\ndata is allowed to be ``seen'', $\\mathcal{A}$, for STNN and our model. But, to enforce these constraints on DCRNN\nand STGCN, which cannot work with changing feature matrix size, we zeroed out those traffic features for\nall nodes $u\\notin \\mathcal{A}$. Loss calculation was also limited to nodes $v\\in \\mathcal{A}$.%\n\nThe set of allowed/seen nodes $A$ was created by randomly selecting $x\\%$ of nodes from all the available\nnodes in the graph. We tried variations in $x$, which is detailed out in the following section.%\n\\end{comment}\n\n\\noindent\n{\\bf $\\bullet$ Forecasting horizon:}  In all the experiments, we choose to predict one hour of traffic information from one hour of historical traffic information. Concretely, since in the datasets the duration between two snapshots is 5 minutes, we set $\\Delta=12$. Later, we also simulate what happens if the duration between snapshots is irregular, but even then we have past one hour of data and make predictions of traffic condition up to one hour in the future.\n\n\\noindent\n{\\bf $\\bullet$ Evaluation metric:} We use mean absolute error (MAE) as the primary metric across all methods (lower is better). We also use sMAPE and RMSE for our key results in Table~\\ref{tbl:giant}. In addition, we \nreport a $95\\%$ confidence interval around the mean by using bootstrapping on the distribution of the metric being used on the ``unseen'' nodes.%\n\n\\noindent\\textbf{ $\\bullet$ Training setting:} We use a $70$\\%-$20$\\%-$10$\\% train-val-test split for training. This split is on the time dimension. So, to be explicit, during training $70\\%$ of the total time series on seen nodes are used. Validation and test are both on different parts of the data both from node perspective\nand timestep perspective. We stop the training of the model if it does not improve the validation loss for more than 15 epochs. Further, we have  varied percentage of training nodes in various experiments to check the fidelity of the FRIGATE, as indicated in the results.\n\\looseness=-1\n\n\\noindent\n{\\bf $\\bullet$ Baselines:} \\rev{We consider DCRNN~\\cite{dcrnn}, STGCN~\\cite{stgcn}, LocaleGN~\\cite{li2022few} and STNN~\\cite{stnn} as our baselines.} \\rev{We adapt GraphWavenet~\\cite{wavenet} and STGODE~\\cite{stgode} to our setting by removing node embedding matrices or Dynamic Time Warping based graph computation so that they support the triple objectives outlined in Table~\\ref{tbl:baselines}.} For all \\rev{six} baselines, we obtain the official code-base released by the authors.% take the official codes of STNN and STGCN. We take the PyTorch implementation of DCRNN referred by the authors in their official code's repository. Also, STNN can inherently be trained and tested on a subset of the nodes of a graph. But STGCN and DCRNN cannot. So, we make the necessary changes so that the loss propagation is only through the nodes with valid sensor readings.\n\n\\noindent\n{\\bf $\\bullet$ Parameter settings:} We use $16$ anchor nodes to calculate Lipschitz embeddings for all graphs. We use $10$ layers of \\gnn in \\name. \n%\n\n\\vspace{-0.10in}\n"
                },
                "subsection 4.3": {
                    "name": "Inference Accuracy",
                    "content": "\n In Table~\\ref{tbl:giant}, we present the MAE obtained by \\name and all other baselines on all three datasets. We observe that \\name  consistently outperforms all baselines. On average, the MAE in \\name is more than 25\\% lower than the closest baseline. Among the baselines, \\rev{LocaleGN and STGODE perform the best, followed by DCRNN, followed by GraphWavenet}, followed by STNN and then STGCN. We further note that STGCN fails to train on Harbin and Beijing even after 48 hours (the largest dataset evaluated in STGCN~\\cite{stgcn} was of 1026 nodes and it only considered weekday traffic). \\rev{Likewise, GraphWavenet also fails to train on Harbin and Beijing (the largest dataset evaluated in GraphWavenet~\\cite{wavenet} was of 325 nodes).} Now, to better contextualize the results, let us compare the MAE with the standard deviation of sensor values reported in Table~\\ref{tbl:dataset_details}. We observe a clear correlation of MAE with the standard deviation, which explains why all techniques perform comparatively poorer in Harbin. Due to the substantially inferior performance of STGCN and its inability to scale on large datasets, subsequent experiments only consider DCRNN, STNN, \\rev{STGODE, and LocaleGN} as baselines.%Based on this, we prune STGCN out from further experiments and only compare against DCRNN and STNN.\n\nTo further diagnose the performance pattern among the benchmarked techniques, we segregate the nodes into three buckets based on the number of cars going through them and plot the distribution of errors within these three buckets. Fig.~\\ref{fig:freq_distrib} presents the results. Here, ``High'', ``Medium'' and ``Low'' corresponds to the top-33 percentile, 33-66 percentile, and bottom-33 percentile, respectively. We derive three key insights from this experiments. First, across all techniques, the MAE reduces as we move towards nodes handling lower traffic. This is natural, since these nodes have low variation in terms of traffic volume. In contrast, high-traffic nodes undergo more fluctuation depending on the time of the day, weekday vs. weekends, etc. Second, we note that \\name performs better than all other models on high frequency nodes, good on medium frequency nodes, and worse than the rest in low frequency nodes. Overall, it may be argued that doing better in high and medium-frequency nodes are more important since they handle a large majority of the traffic, where \\name mostly outperforms other techniques. Finally, one of the key reasons of under-performance in DCRNN, STNN, \\rev{and STGODE} is that they over-smooth the high-frequency nodes into inferring lower values as the majority of the nodes are of low-frequency (recall power-law distribution from Fig.~\\ref{fig:freq_distrib}). Positional embeddings and deep layering through gating enables \\name to avoid over-smoothing. As we established in Section~\\ref{sec:characterization}, these design choices make \\name provable more expressive. We further investigate the impact of these two components in our ablation study in Section~\\ref{sec:ablation}. \n\n%Finally, we highlight that this superior performance of \\name does not come at the cost of learning complexity. Specifically, in Table~\\ref{tab:params}, we present the number of parameters used by each technique. \\name is the most lightweight algorithm. %due to the majoroty being nodesthat have the most data, and the traffic at low frequency nodes $\\approx 0$. So, we claim that \\name is able to learn interesting traffic patterns, and models that perform better on low frequency nodes mostly learn to predict zeros (or values close to zero).\n%\n\n\n\\vspace{-0.10in}\n"
                },
                "subsection 4.4": {
                    "name": "Impact of Volume of ``Seen'' Nodes",
                    "content": "\n As in any machine learning task, we expect the accuracy to improve with larger volume of seen nodes. This increases training data, as well as induces a higher likelihood of an unseen node being close to a seen node. \n%We would like to understand the generalizability of various models as the amount of data they get to train on changes.\nTowards that objective, we vary the number of nodes that are ``seen'' by the models from $10\\%$ to $90\\%$ of the total\nnumber of nodes in the respective road networks and measure MAE against forecasting horizon. Fig.~\\ref{fig:pct_variation} presents the results. \nWe observe that \\name performs significantly better because of more informative\npriors and inherent inductivity. Furthermore, \\rev{the baselines} need considerably more data to reach the same performance. For Chengdu, DCRNN trained on 50\\% of the graph beats our model trained on only 10\\% of the graph while on Harbin, \\rev{none of} the models ever even surpass our model trained at only 10\\% of the graph. And as the data increases, the gap between our model and the baselines increases. This economic use of data without losing the ability to generalize is a desirable property that \\name contains. \n\nAnother interesting trend we note is that the performance slightly deteriorates in \\rev{the baselines} from 70\\% to 90\\%. It is hard to pinpoint the exact reason. We hypothesize two factors as possible reasons. First, with higher volumes of training data, there is a stronger chance of over-fitting. In addition, we note that the confidence interval expands as the percentage of seen nodes increases since the sample size of test nodes decreases. This trend, which is consistent with statistical theory, means that at low volumes of test sets, the results have higher variability. \\name does not suffer from this trend, which means it does not overfit. It is well-known in machine learning theory, that tendency to overfit is correlated to the number of parameters. In this regard, we draw attention to Table~\\ref{tab:params}, which shows that \\name has almost 50\\% and 33\\% smaller parameter set than DCRNN and STNN, respectively. Also, \\name utilizes moments as a robust inductive bias, reducing overfitting risks. \n\n%The above experiment has also been conducted on Beijing. However, DCRNN and STNN both consume more than 48 hours to train and hence, we only report \\name in Fig.~\\ref{fig:beijing_prediction} in Appendix.\n%\n%\n"
                },
                "subsection 4.5": {
                    "name": "Ablation Study",
                    "content": "\n\\label{sec:ablation}\nIn our ablation study, we systematically turn off various components of \\name and measure the impact on MAE. Fig.~\\ref{fig:ablation} presents the results on Chengdu and Harbin.\n\\begin{comment}\n\\begin{table}\n\\centering\n\\caption{\\label{tbl:ablation}Ablation over the various components of the model on Chengdu (50\\%) dataset}\n\\begin{tabular}{lrr}\n    \\toprule\n    Model & MAE\\\\\n    \\midrule\n    OURS & $3.547\\pm 0.20$\\\\\n    OURS (without Lipschitz) & $3.787\\pm 0.18$\\\\\n    OURS (without Gating) & $3.734\\pm 0.18$\\\\\n    \\bottomrule\n\\end{tabular}\n\\end{table}\n\\end{comment}\n\n\n\\noindent\\textbf{Gating:} %To establish the importance of the gated convolution layer proposed in this paper\nHere, we replace the gated convolution with a GraphSAGE layer~\\cite{graphsage}, leaving rest of the architecture intact. We see a clear\nincrease in MAE indicating the importance of gated convolution layer.\n\\looseness=-1\n\n\\noindent\\textbf{Positional embeddings:} %We use Lipschitz embeddings to encode positional information about nodes without losing inductivity of the model. \nTo understand its utility on performance, we remove Lipschitz embeddings as features of the node, and thus remove it as an input to the calculation of gating. We see a significant drop in performance, with Chengdu being more pronounced where Lipschitz has the highest drop. This result empirically demonstrates the value of positional embeddings in time-series forecasting on road networks. \n\n\\noindent\\textbf{In-Out aggregation:} In \\name, we separately aggregate messages from incoming and outgoing neighbors. Here, we measure the impact of aggregating into a single vector. We observe an increase in MAE, showcasing the need for direction-based aggregation.\n\\looseness=-1\n\n\\noindent\\textbf{Moments:} We remove the moments from the model and keep everything else in the architecture the same. We see a big drop in performance on both datasets, with Harbin being more pronounced. %For Chengdu, removing the moments drops the performance to almost as close to the model with everything else removed and for Harbin it drops even further. \nThis establishes the importance of having an informative prior.\n\n\\noindent\\textbf{Number of GNN layers:} To understand the effect of \\gnn layers on performance of \\name, we vary this parameter while training on Chengdu 50\\% dataset. As visible in Fig.~\\ref{fig:layer_variations}, the performance saturates at $10$, which we use as our default value. \n%with number of layers of GNN $\\in\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 15\\}$, constrained only by the hardware. At 16 layers\n%along with the two layered RNNs, one each in the encoder and the decoder, the computational graph becomes too to fit inside\n%GPU memory during forward pass.\n\\looseness=-1\n\n%What we observe here is that having stronger priors allows for deeper networks because strong priors make overfitting more difficult.\n%The trend continues well towards 15 layers of GNN and we never see a decline in performance. But we work with 10 layers because\n%larger datasets (like Harbin and Beijing) will constrain us and we want to be consistent to facilitate analysis.\n\n\n\n%\n%\\subsection{Weighted MAE, Frequency Study}\n%\n\\vspace{-0.10in}\n"
                },
                "subsection 4.6": {
                    "name": "Robustness and Resilience",
                    "content": "\nIn this section, we analyze the robustness and resilience of \\name to changes to network topology and irregular topological sensing. In these experiment, we train \\name on the original datasets, and then slightly perturb the topology or temporal granularity. We evaluate inference performance on this perturbed dataset. Note that we do not re-train the model after perturbations.\n\n\\noindent\n\\textbf{Topology change: } To simulate the real-world situation where the road network might change due to road blocks or opening of new roads, we\nfirst select the volume of perturbations to be introduced. Assuming this to be $X\\%$, we change the network by randomly dropping a maximum of $\\frac{X}{2}$\\% of the edges. In addition, we create an equal number edges among unconnected nodes whose distance is within a threshold. The distances for these edges are sampled, with replacement, from the original distribution of edge distance. We then vary $X$ and measure the impact on MAE. %where there were not any, while ensuring that new edges are created only among nodes. \n %This amounts to a total of 5\\% changes in the edges of the network. %This is roughly 400 ``roads'' in Chengdu and around 3000 ``roads'' in Harbin. \n%To create the new edges, edge length were sampled from the original distribution\n%with replacement. \n%The prediction is made on nodes the model was not trained on, the results are shown \nAs visible in Fig.~\\ref{fig:gchanges}, \\name is resilient to changes to the road network with minimal drop in accuracy.\n\\looseness=-1\n\n%However, the traffic features do not reflect the changes made to the road network. Thus, proper analysis of resilience to changing roads\n%cannot be done without an actual dataset. This can be a future work opportunity.\n%\n%\\subsection{Changing the Time Granularity During Inference}\n\n\\noindent\n{\\bf Irregular temporal sensing: }In this experiment, we try to break our model by changing the time step granularity at test time. We randomly drop 33.33\\% of snapshots in $\\rns_{[t-\\Delta,t]}$, effectively reducing the time sequence length and also changing the granularity at which the data is captured. The results are tabulated in table~\\ref{tbl:time_granularity}, where we observe negligible increase in MAE. The results indicate that the model is resilient to changes in the granularity of snapshots.\n%\\section{Conclusion}\nIn this paper, we have proposed a new spatio-temporal \\gnn, called \\name, to forecast network-constrained time-series processes on road networks. We show through experiments on real-world traffic datasets that \\name significantly outperforms existing baselines. In addition, \\name is robust to several practical challenges such as partial sensing across nodes, absorb minor updates to road network topology and resilience to irregular  temporal sensing.\n The core competency of \\name originates from its novel design that incorporates Lipschitz embedding to encode position, sigmoid gating to learn message importance and enable pathways for long-range communication across nodes, directional aggregation of messages, and strong priors in the form of moments of the time-series distribution. In addition, \\name is built from the ground-up keeping inductivity as a core objective. On the whole, \\name takes us closer to deployable technology for practical workloads. \n\n \\noindent\n"
                }
            },
            "section 5": {
                "name": "Acknowledgments",
                "content": " This research was supported by Yardi School of AI, IIT Delhi. Sayan Ranu acknowledges the Nick McKeown Chair position endowment. \n%For future directions, we plan to further increase resilience to partial sensing. Specifically, in this work, the seen nodes are assumed to be uniformly distributed. Can we install sensors strategically, so that the the ability to predict on unseen locations get further enhanced? We hope to study this. %Owing to this novel design, \\name is inductive and hence can absorb updates to network topology with retraining from scratch,  \\gnn without ovperformance with just a fraction of the data that the baselines need to reach that performance. We establish that our model is capable of generalising to parts of graph that it hasn't seen during training, and that may not have temporal features at all. We show that using gating based on edge features and incorporating both incoming and outgoing information enhances performance. We also show that using moments of the data to provide an inductive bias has a significant improvement on performance. Finally, we test how well can the model adapt to inference changes providing an empirical answer to the question \\textit{``how much change can my model handle without losing quality before it has to be retrained''} and we do not see significant deterioration within realistic bounds to road network changes. The resilience of \\name to changes improves the economy of the model. Even though we have simulated the situation where the road network changes over time the simulations are just approximations as we cannot simulate the change in traffic values that would have happened as a result of said edge removals or additions which can be a future work direction.\\noindent\n\n\\bibliographystyle{ACM-Reference-Format}\n\\FloatBarrier\n\\clearpage\n\\balance\n\\bibliography{ref}\n\\clearpage\n\\appendix\n\\counterwithin{figure}{section}\n\\counterwithin{table}{section}\n"
            },
            "section 6": {
                "name": "Notations",
                "content": "\n\n"
            },
            "section 7": {
                "name": "Inference time",
                "content": "\n\\FloatBarrier\nThe inference times of the three models are shown in Table~\\ref{tab:inference}. \\name is faster than STNN but slower than DCRNN. However, we note that even though \\name is slower than DCRNN it is still fast enough to be deployed for real-world workloads.\n\n"
            },
            "section 8": {
                "name": "Parameter size",
                "content": "\n\n"
            },
            "section 9": {
                "name": "Other results",
                "content": "\n\nFig.~\\ref{fig:beijing_prediction} shows the MAE of the three models on Beijing (5\\%) dataset against the prediction horizon.\n%\n\\renewcommand{\\qedsymbol}{$\\square$}\n"
            },
            "section 10": {
                "name": "lem:metric_dist",
                "content": "\n\\label{app:proof_lem_one}\n\\lemone*\n\\begin{proof}\nWe need to show \\textbf{(1)} Symmetry: $d_X(u,v)=d_X(v,u)$, \\textbf{(2)} Non-negativity: $d_X(u,v)\\geq 0$, \\textbf{(3)} $d_X(u,v)=0$ iff $u=v$ and \\textbf{(4)} Triangle inequality: $d_X(u,v)\\leq d_X(u,w)+d_X(w,v)$. \n \n We omit the proofs of first three properties since they are trivial. We use proof-by-contradiction to establish triangle inequality. \n \\vspace{-0.05in}\n \\begin{alignat}{2}\n \\label{eq:contradiction}\n \\text{Let us assume }d_X(u,v)&> d_X(u,w)+d_X(w,v)\\\\\n \\nonumber\n or, sp(u,v)+sp(v,u)&>sp(u,w)+sp(w,u)+sp(w,v)+sp(v,w)\n \\end{alignat}\n\\noindent\n From the definition of shortest paths, $sp(u,v)\\leq sp(u,w)+sp(w,v)$ and $sp(v,u)\\leq sp(v,w)+sp(w,u)$. Hence, Eq.~\\ref{eq:contradiction} is a contradiction.\n\\end{proof}\n\n"
            },
            "section 11": {
                "name": "lem:1wl",
                "content": "\n\\label{app:proof_of_lemmathree}\n\\lemthree*\n\\begin{proof}\n \\gin~\\cite{gin} is as powerful as 1-WL~\\cite{gin}. This power is induced by the \\textsc{Sum-Pool} aggregation since sum-pool is \\textit{injective} function, i.e., two separate aggregation over messages would be identical if and only if the input messages are identical~\\footnote{ As in \\gin~\\cite{gin}, we assume countable features on nodes.}. \\name can also model sum-pool, and in the more general case, an injective message aggregation, whenever the sigmoid gate over all edges is some value $x\\neq 0$ (Recall Eq.~\\ref{eq:sigmoid}). This happens if $\\w^{\\ell}$ in Eq.~\\ref{eq:edgescalar} is a zero vector, the bias $b$ in \n Eq.~\\ref{eq:edgescalar} is non-zero and the submatrix in $\\W^1_1$ of Eq.~\\ref{eq:hl} applying linear transformation to Lipschitz embeddings of nodes is $0$.\n\\end{proof}\n% \\section{Ablation Study}\n% \\label{sec:ablation}\n% \\begin{figure}\n% \\vspace{-0.05in}\n%     \\centering\n%     \\subfloat[]{\n%     \\includegraphics[width=0.5\\linewidth]{plots/ablation.pdf}\n%     \\label{fig:ablation}\n%     }\n%     \\subfloat[]{\n%      \\includegraphics[width=0.5\\linewidth]{plots/layer_variations.pdf}\n%     \\label{fig:layer_variations}\n%     }\n%     \\caption{(a) Ablation study. (b) Impact of \\gnn layer variations on Chengdu 50\\% }\n%     \\vspace{-0.20in}\n% \\end{figure}\n% In our ablation study, we systematically turn off various components of \\name and measure the impact on MAE. Fig.~\\ref{fig:ablation} presents the results on Chengdu and Harbin.\n% \\begin{comment}\n% \\begin{table}\n% \\centering\n% \\caption{\\label{tbl:ablation}Ablation over the various components of the model on Chengdu (50\\%) dataset}\n% \\begin{tabular}{lrr}\n%     \\toprule\n%     Model & MAE\\\\\n%     \\midrule\n%     OURS & $3.547\\pm 0.20$\\\\\n%     OURS (without Lipschitz) & $3.787\\pm 0.18$\\\\\n%     OURS (without Gating) & $3.734\\pm 0.18$\\\\\n%     \\bottomrule\n% \\end{tabular}\n% \\end{table}\n% \\end{comment}\n\n\n% \\noindent\\textbf{Gating:} %To establish the importance of the gated convolution layer proposed in this paper\n% Here, we replace the gated convolution with a GraphSAGE layer~\\cite{graphsage}, leaving rest of the architecture intact. We see a clear\n% increase in MAE indicating the importance of gated convolution layer.\n% \\looseness=-1\n\n% \\noindent\\textbf{Positional embeddings:} %We use Lipschitz embeddings to encode positional information about nodes without losing inductivity of the model. \n% To understand its utility on performance, we remove Lipschitz embeddings as features of the node, and thus remove it as an input to the calculation of gating. We see a significant drop in performance, with Chengdu being more pronounced where Lipschitz has the highest drop. This result empirically demonstrates the value of positional embeddings in time-series forecasting on road networks. \n\n% \\noindent\\textbf{In-Out aggregation:} In \\name, we separately aggregate messages from incoming and outgoing neighbors. Here, we measure the impact of aggregating into a single vector. We observe an increase in MAE, showcasing the need for direction-based aggregation.\n% \\looseness=-1\n\n% \\noindent\\textbf{Moments:} We remove the moments from the model and keep everything else in the architecture the same. We see a big drop in performance on both datasets, with Harbin being more pronounced. %For Chengdu, removing the moments drops the performance to almost as close to the model with everything else removed and for Harbin it drops even further. \n% This establishes the importance of having an informative prior.\n\n% \\noindent\\textbf{Number of GNN layers:} To understand the effect of \\gnn layers on performance of \\name, we vary this parameter while training on Chengdu 50\\% dataset. As visible in Fig.~\\ref{fig:layer_variations}, the performance saturates at $10$, which we use as our default value. \n% %with number of layers of GNN $\\in\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 15\\}$, constrained only by the hardware. At 16 layers\n% %along with the two layered RNNs, one each in the encoder and the decoder, the computational graph becomes too to fit inside\n% %GPU memory during forward pass.\n% \\looseness=-1\n\\begin{comment}\n\\begin{figure*}[!htbp]\n    \\subfloat[Chengdu]{\\includegraphics[width=.3\\linewidth]{plots/Chengdu_tod_mae.pdf}}\n    \\subfloat[Beijing]{\\includegraphics[width=.3\\linewidth]{plots/Beijing_tod_mae.pdf}}\n    \\subfloat[Harbin]{\\includegraphics[width=.3\\linewidth]{plots/Harbin_tod_mae.pdf}}\n\\end{figure*}\n\\end{comment}"
            }
        },
        "tables": {
            "lem:metric_dist": "\\begin{restatable}{lem}{lemone}\n\\label{lem:metric_dist}\n$d_X(u,v)=\\frac{sp(u,v)+sp(v,u)}{2}$ \\textit{is a metric.} \n\\end{restatable}",
            "lem:position": "\\begin{restatable}{lem}{lemtwo}\n\\label{lem:position}\n\\textit{\\name can distinguish between isomorphic neighborhoods based on positioning.}\n\\end{restatable}",
            "lem:1wl": "\\begin{restatable}{lem}{lemthree}\n\\label{lem:1wl}\n\\textit{\\name is at least as powerful as 1-WL.}\n\\end{restatable}",
            "tab:inference": "\\begin{table}[!htbp]\n    \\centering\n    \\caption{Inference times}\n    \\label{tab:inference}\n    \\begin{tabular}{lr}\n        \\toprule\n        Model & Time(s) \\\\\n        \\midrule\n         DCRNN &  0.0081\\\\\n         STGODE & 0.0164\\\\\n         \\name & 0.0207\\\\\n         STNN & 0.0375\\\\\n         LocaleGN & 0.0492\\\\\n         \\bottomrule\n    \\end{tabular}\n\\end{table}",
            "tab:parameters": "\\begin{table}[H]\n    \\centering\n    \\caption{Dimensionality of the parameters used in \\name.}\n    \\label{tab:parameters}\n    \\begin{tabular}{cc}\n        \\toprule\n        \\textbf{Parameter} & \\textbf{Shape}\\\\\n        \\midrule\n        $\\mbd{W}_1^\\ell$&$\\mathbb{R}^{3d\\times d}$\\\\\n        $\\mbd{w}_\\delta^T$&$\\mathbb{R}^{d_\\delta}$\\\\\n        $\\mbd{W}_\\CL^\\ell$&$\\mathbb{R}^{2d_L+d\\delta\\times d_{L_e}}$\\\\\n        $\\mbd{w}^\\ell$&$\\mathbb{R}^{d_{L_e}}$\\\\\n        $\\mbd{w}^T_\\tau$&$\\mathbb{R}^{d_\\tau}$\\\\\n        $\\mlp_1$&$\\mathbb{R}^{d_{Lenc}}$\\\\\n        $\\mlp_2$&$\\mathbb{R}^{d_{dec}}$\\\\\n        $\\mlp_3$&$\\mathbb{R}^{d_{moments}}$\\\\\n        $\\mlp_4$&$\\mathbb{R}^{d_{dec}+d_{moments}}$\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:g3": "\\begin{figure}\n    \\centering\n   % \\captionsetup[subfloat]\n    %\\subfloat[$\\CG_1$]{\n    %\\label{fig:g3}\n    \\includegraphics[width=0.4\\linewidth]{figures/lipschitz.pdf}\n    \\caption{Sample graph to illustrate expressivity of \\gnn in \\name.}\n    \\label{fig:g3}\n    \\end{figure}",
            "fig:beijing_prediction": "\\begin{figure}[!htbp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{plots/Beijing_timestep.pdf}\n    \\caption{Prediction on Beijing (5\\%) dataset}\n    \\label{fig:beijing_prediction}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\Psi_{\\Theta}\\left(v,t+k\\:|\\:\\overrightarrow\\CG_{[1,t]}\\right)=\\y_v^k\n\\end{equation}",
            "eq:2": "\\begin{align}\n \\label{eq:hl}\n \\h^{\\ell+1}_v&= \\sigma_1\\left(\\W^{\\ell}_1 \\left(h_v^{\\ell}\\parallel\\m^{\\ell,out}_v\\parallel \\m^{\\ell,in}_v\\right)\\right) \\text{, where}\\\\\n \\m^{\\ell,out}_v&=\\textsc{Aggr}^{out}\\left(\\left\\{\\h_u^{\\ell}\\mid u\\in N^{out}_v\\right\\}\\right)\\\\\n\\m^{\\ell,in}_v&=\\textsc{Aggr}^{in}\\left(\\left\\{\\h_u^{\\ell}\\mid u\\in N^{in}_v\\right\\}\\right)\n \\end{align}",
            "eq:3": "\\begin{equation}\n\\label{eq:h0}\n\\h^{0,k}_v=\\left(\\w_{\\tau}^T\\cdot\\tau^v_{t-\\Delta+k} \\right)\\parallel\\CL_v\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\label{eq:lstm}\n\\s^{k}_v=\n\\begin{cases}\n    \\lstm_{Enc}\\left(\\s_v^{k-1},\\z_v^{k}\\right)&\\text{if k>1}\\\\\n    \\lstm_{Enc}\\left(\\mlp_1\\left(t,\\z^k_v\\right),\\z_v^{k}\\right) & \\text{if k=1}\n    \\end{cases}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\label{eq:decoder}\n\\q^{k}_v=\n\\begin{cases}\n    \\lstm_{Dec}\\left(\\y_v^{k-1},\\q_v^{k-1}\\right)&\\text{if k>1}\\\\\n    \\lstm_{Dec}\\left(\\mlp_2\\left(\\s^{\\Delta}_v,\\m_v^t\\right),\\s_v^\\Delta\\right) & \\text{if k=1}\n    \\end{cases}\n\\end{equation}",
            "eq:6": "\\begin{align}\n\\nonumber\n\\resizebox{.97\\linewidth}{!}{%\n$\\m^t_v=\\mlp_3\\left(moments\\left(\\left\\{\\tau^{t'}_u \\neq \\varnothing| t'\\in[t-\\Delta,t], (u,v)\\text{ or } (v,u)\\in \\CE^{t'}\\right\\}\\right)\\right)$%\n}\n\\end{align}",
            "eq:7": "\\begin{equation}\n\\y_v^{k}=\\mlp_4\\left(\\q_v^k,\\m^t_v\\right)\n\\end{equation}"
        },
        "git_link": "https://github.com/idea-iitd/Frigate"
    }
}