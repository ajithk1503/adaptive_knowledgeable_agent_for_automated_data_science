{
    "meta_info": {
        "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than\n  In-Context Learning",
        "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning,\nsparse update methods, etc.) offers an alternative paradigm where a small set\nof parameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and PEFT and demonstrate that the\nlatter offers better accuracy as well as dramatically lower computational\ncosts. Along the way, we introduce a new PEFT method called (IA)$^3$ that\nscales activations by learned vectors, attaining stronger performance while\nonly introducing a relatively tiny amount of new parameters. We also propose a\nsimple recipe based on the T0 model called T-Few that can be applied to new\ntasks without task-specific tuning or modifications. We validate the\neffectiveness of T-Few on completely unseen tasks by applying it to the RAFT\nbenchmark, attaining super-human performance for the first time and\noutperforming the state-of-the-art by 6% absolute. All of the code used in our\nexperiments is publicly available.",
        "id": "http://arxiv.org/abs/2205.05638v2"
    },
    "latex_extraction": {
        "section 1": {
            "name": "Introduction",
            "content": "\n\nPre-trained language models have become a cornerstone of natural language processing, thanks to the fact that they can dramatically improve \\textit{data efficiency} on tasks of interest -- i.e., using a pre-trained language model for initialization often produces better results with less labeled data.\nA historically common approach has been to use the pre-trained model's parameters for initialization before performing gradient-based fine-tuning on a downstream task of interest.\nWhile fine-tuning has produced many state-of-the-art results \\cite{sanh2021multitask}, it results in a model that is specialized for a single task with an entirely new set of parameter values, which can become impractical when fine-tuning a model on many downstream tasks.\n\nAn alternative approach popularized by \\cite{radford2019language,brown2020language} is \\textit{in-context learning} (ICL), which induces a model to perform a downstream task by inputting \\textit{prompted} examples.\nFew-shot prompting converts a small collection of input-target pairs into (typically) human-understandable instructions and examples \\cite{radford2019language,brown2020language}, along with a single unlabeled example for which a prediction is desired.\nNotably, ICL requires no gradient-based training and therefore allows a single model to immediately perform a wide variety of tasks.\nPerforming ICL therefore solely relies on the capabilities that a model learned during pre-training.\nThese characteristics have led to a great deal of recent interest in ICL methods \\citep{chen2021meta,min2021metaicl,lampinen2022can,lazaridou2022internet,min2022rethinking,wang2022benchmarking}.\n\nDespite the practical benefits of ICL, it has several major drawbacks.\nFirst, processing all prompted input-target pairs every time the model makes a prediction incurs significant compute costs.\nSecond, ICL typically produces inferior performance compared to fine-tuning \\cite{brown2020language}.\nFinally, the exact formatting of the prompt (including the wording \\cite{webson2021prompt} and ordering of examples \\cite{zhao2021calibrate}) can have significant and unpredictable impact on the model's performance, far beyond inter-run variation of fine-tuning.\nRecent work has also demonstrated that ICL can perform well even when provided with incorrect labels, raising questions as to how much learning is taking place at all \\citep{min2022rethinking}.\n\n\n\nAn additional paradigm for enabling a model to perform a new task with minimal updates is \\textit{parameter-efficient fine-tuning} (PEFT), where a pre-trained model is fine-tuned by only updating a small number of added or selected parameters.\nRecent methods have matched the performance of fine-tuning the full model while only updating or adding a small fraction (e.g.\\ 0.01\\%) of the full model's parameters \\cite{hu2021lora,lester2021power}.\nFurthermore, certain PEFT methods allow \\textit{mixed-task batches} where different examples in a batch are processed differently \\citep{lester2021power}, making both PEFT and ICL viable for multitask models. \n\n\\looseness=-1\nWhile the benefits of PEFT address some shortcomings of fine-tuning (when compared to ICL), there has been relatively little focus on whether PEFT methods work well when very little labeled data is available.\nOur primary goal in this paper is to close this gap by proposing a recipe -- i.e., a model, a PEFT method, and a fixed set of hyperparameters -- that attains strong performance on novel, unseen tasks while only updating a tiny fraction of the model's parameters.\nSpecifically, we base our approach on the T0 model \\cite{sanh2021multitask}, a variant of T5 \\cite{raffel2019exploring} fine-tuned on a multitask mixture of prompted datasets.\nTo improve performance on classification and multiple-choice tasks, we add unlikelihood \\cite{tam2021improving,welleck2019neural} and length normalization-based \\citep{brown2020language} loss terms.\nIn addition, we develop \\ia, a PEFT method that multiplies intermediate activations by learned vectors.\n\\ia attains stronger performance than full-model fine-tuning while updating up to 10,000$\\times$ fewer parameters.\nFinally, we demonstrate the benefits of pre-training the \\ia parameters before fine-tuning \\citep{gu2021ppt, vu2021spot}.\nOur overall recipe, which we dub ``\\tfew'', performs significantly better than ICL (even against $16\\times$ larger models) and outperforms humans for the first time on the real-world few-shot learning benchmark RAFT \\citep{alex2021raft} while requiring dramatically less compute and allowing for mixed-task batches during inference.\nTo facilitate the use of \\tfew on new problems and future research on PEFT, we release our code.$^{\\ref{note:code}}$\n\nAfter providing background on ICL and PEFT in the following section, we discuss the design of \\tfew in \\cref{sec:tfew}.\nIn \\cref{sec:experiments}, we present experiments comparing \\tfew to strong ICL baselines.\nFinally, we discuss related work in \\cref{sec:related} and conclude in \\cref{sec:conclusion}.\n\n"
        },
        "section 2": {
            "name": "Background",
            "content": "\nIn this section, we provide am verview of ICL and PEFT with a focus on characterizing the computation, memory, and on-disk storage costs of making a prediction.\nReal-world costs depend on implementation and hardware, so we report costs in terms of FLOPs for computation and bytes for memory and storage, respectively.\nAdditional related work is discussed in \\cref{sec:related}.\n\n",
            "subsection 2.1": {
                "name": "Few-shot in-context learning (ICL)",
                "content": "\n\\label{sec:icl}\n\nICL \\citep{radford2019language,brown2020language} aims to induce a model to perform a task by feeding in concatenated and prompted input-target examples (called ``shots'') along with an unlabeled query example.\nTaking the cycled letter task from \\citet{brown2020language} as an example, a 4-shot input or \\textit{context} would be ``\\texttt{Please unscramble the letters into a word, and write that word: asinoc = casino, yfrogg = froggy, plesim = simple, iggestb = biggest, astedro =}'', for which the desired output would be ``\\texttt{roasted}''.\nICL induces an autoregressive language model to perform this task by feeding in the context and sampling from the model.\nFor classification tasks, each label is associated with a string (e.g.\\ ``\\texttt{positive}'' and ``\\texttt{negative}'' for sentiment analysis) and a label is assigned by choosing the label string that the model assigns the highest probability to.\nFor multiple-choice tasks (e.g.\\ choosing between $N$ possible answers to a question), the model's prediction is similarly determined by determining which choice is assigned the highest probability.\n\nThe primary advantage of ICL is that it enables a single model to perform many tasks immediately without fine-tuning.\nThis also enables \\textit{mixed-task batches}, where different examples in a batch of data correspond to different tasks by using different contexts in the input.\nICL is also typically performed with only a limited number of labeled examples -- called few-shot learning -- making it data-efficient.\n\nDespite these advantages, ICL comes with significant practical drawbacks:\nFirst, making a prediction is dramatically more expensive because the model needs to process all of the in-context labeled examples.\nSpecifically, ignoring the quadratic complexity of self-attention operations in Transformer language models (which are typically small compared to the costs of the rest of the model \\citep{kaplan2020scaling}), processing the $k$ training examples for $k$-shot ICL increases the computational cost by approximately $k + 1$ times compared to processing the unlabeled example alone.\nMemory costs similarly scale approximately linearly with $k$, though during inference the memory costs are typically dominated by storing the model's parameters.\nSeparately, there is a small amount of on-disk storage required for storing the in-context examples for a given task.\nFor example, storing $32$ examples for a task where the prompted input and target for each example is $512$ tokens long would require about $66$ kilobytes of storage on disk ($32$ examples $\\times\\;512$ tokens $\\times\\;32$ bits).\n\nBeyond the aforementioned costs, ICL also exhibits unintuitive behavior.\n\\citet{zhao2021calibrate} showed that the \\textit{ordering} of examples in the context heavily influences the model's predictions. \n\\citet{min2022rethinking} showed that ICL can still perform well even if the labels of the in-context examples are swapped (i.e.\\ made incorrect), which raises questions about whether ICL is really ``learning'' from the labeled examples. \n\nVarious approaches have been proposed to mitigate these issues.\nOne way to decrease computational costs is to cache the key and value vectors for in-context examples.\nThis is possible because decoder-only Transformer language models have a causal masking pattern, so the model's activations for the context do not do not depend on the unlabeled example.\nIn an extreme case, $32$-shot ICL with $512$ tokens per in-context example would result in over 144 gigabytes of cached key and value vectors for the GPT-3 model ($32$ examples $\\times\\;512$ tokens $\\times\\;96$ layers $\\times\\;12288\\;\\mathrm{d_{model}} \\times 32$ bits \\textit{each} for the key and value vectors).\n% Storing these cached values on disk would therefore incur nontrivial storage costs.\nSeparately, \\citet{min2021noisy} proposed \\textit{ensemble ICL}, where instead of using the output probability from concatenating the $k$ training examples, the output probabilities of the model on each training example (i.e.\\ $1$-shot ICL for each of the $k$ examples) are multiplied together.\nThis lowers the non-parameter memory cost by a factor of $k/2$ but increases the computational cost by a factor of $2$.\nIn terms of task performance, \\citet{min2021noisy} find that ensemble ICL outperforms the standard concatenative variant.\n\n"
            },
            "subsection 2.2": {
                "name": "Parameter-efficient fine-tuning",
                "content": "\n\nWhile standard fine-tuning updates all parameters of the pre-trained model, it has been demonstrated that it is possible to instead update or add a relatively small number of parameters.\nEarly methods proposed adding \\textit{adapters} \\cite{rebuffi2017learning,Houlsby2019ParameterEfficientTL,bapna2019simple}, which are small trainable feed-forward networks inserted between the layers in the fixed pre-trained model.\nSince then, various sophisticated PEFT methods have been proposed, including methods that choose a sparse subset of parameters to train \\cite{guo2020parameter,sung2021training}, produce low-rank updates \\cite{hu2021lora}, perform optimization in a lower-dimensional subspace \\cite{aghajanyan2020intrinsic}, add low-rank adapters using hypercomplex multiplication \\cite{mahabadi2021compacter}, and more.\nRelatedly, \\textit{prompt tuning} \\cite{lester2021power} and \\textit{prefix tuning} \\cite{li2021prefix} concatenate learned continuous embeddings to the model's input or activations to induce it to perform a task; this can be seen as a PEFT method \\cite{he2021towards}.\nState-of-the-art PEFT methods can match the performance of fine-tuning all of the model's parameters while updating only a tiny fraction (e.g.\\ 0.01\\%) of the model's parameters.\n\nPEFT drastically reduces the memory and storage requirements for training and saving the model.\nIn addition, certain PEFT methods straightforwardly allow mixed-task batches -- for example, prompt tuning enables a single model to perform many tasks simply by concatenating different prompt embeddings to each example in the batch \\cite{lester2021power}.\nOn the other hand, PEFT methods that re-parameterize the model (e.g.\\ \\citep{aghajanyan2020intrinsic,hu2021lora}) are costly or onerous for mixed-task batches.\nSeparately, different PEFT methods increase the computation and memory required to perform inference by different amounts.\nFor example, adapters effectively add additional (small) layers to the model, resulting in small but non-negligible increases in computational costs and memory.\nAn additional cost incurred by PEFT is the cost of fine-tuning itself, which must be performed once and is then amortized as the model is used for inference.\nHowever, we will show that PEFT can be dramatically more computationally efficient when considering both fine-tuning and inference while achieving better accuracy than ICL. \n\n"
            }
        },
        "section 3": {
            "name": "Designing the \\tfew Recipe",
            "content": "\n\\label{sec:tfew}\n\nGiven that PEFT allows a model to be adapted to a new task with relatively small storage requirements and computational cost, we argue that PEFT presents a promising alternative to ICL.\nOur goal is therefore to develop a recipe that allows a model to attain high accuracy on new tasks with limited labeled examples while allowing mixed-task batches during inference and incurring minimal computational and storage costs.\nBy \\textit{recipe}, we mean a specific model and hyperparameter setting that provides strong performance on any new task without manual tuning or per-task adjustments.\nIn this way, we can ensure that our approach is a realistic option in few-shot settings where limited labeled data is available for evaluation \\cite{perez2021true,oliver2018realistic}.\n\n",
            "subsection 3.1": {
                "name": "Model and Datasets",
                "content": "\n\\label{sec:model_datasets}\n\nAs a first step, we must choose a pre-trained model.\nIdeally, the model should attain high performance on new tasks after fine-tuning on a limited number of labeled examples.\nIn preliminary experiments applying PEFT methods to different pre-trained models, we attained the best performance with T0 \\cite{sanh2021multitask}.\nT0 is based on T5 \\citep{raffel2019exploring}, an encoder-decoder Transformer model \\citep{vaswani2017attention} that was pre-trained via a masked language modeling objective \\citep{devlin2018bert} on a large corpus of unlabeled text data.\nT0 was created by fine-tuning T5 on a multitask mixture of datasets in order to enable zero-shot generalization, i.e.\\ the ability to perform tasks without any additional gradient-based training.\nExamples in the datasets used to train T0 were prompted by applying the prompt templates from the Public Pool of Prompts (P3 \\citep{bach2022promptsource}), which convert each example in each dataset to a prompted text-to-text format where each label corresponds to a different string.\nFor brevity, we omit a detailed description of T0 and T5; interested readers can refer to \\citet{sanh2021multitask} and \\citet{raffel2019exploring}.\nT0 was released in three billion and eleven billion parameter variants, referred to as ``T0-3B'' and simply ``T0'' respectively.\nIn this section (where our goal is to design the \\tfew recipe through extensive experimentation), we use T0-3B to reduce computational costs.\nFor all models and experiments, we use Hugging Face Transformers \\citep{wolf2020transformers}.\n\nWhile T0 was designed for zero-shot generalization, we will demonstrate that it also attains strong performance after fine-tuning with only a few labeled examples.\nTo test T0's generalization, \\citet{sanh2021multitask} chose a set of tasks (and corresponding datasets) to hold out from the multitask training mixture -- specifically, sentence completion (COPA \\citep{copa}, H-SWAG \\citep{zellers2019hellaswag}, and Story Cloze \\citep{sharma2018tackling} datasets), natural language inference (ANLI \\citep{nie2019adversarial}, CB \\citep{cb}, and RTE \\citep{dagan2005pascal}), coreference resolution (WSC \\citep{wsc} and Winogrande \\citep{sakaguchi2020winogrande}), and word sense disambiguation (WiC \\citep{pilehvar2018wic}).\nEvaluation of generalization capabilities can then be straightforwardly done by measuring performance on these held-out datasets.\nWe also will later test \\tfew's abilities in the RAFT benchmark \\citep{alex2021raft} in \\cref{sec:raft}, a collection of unseen ``real-world'' few-shot tasks with no validation set and a held-out test set. \nANLI, WiC, WSC is licensed under a Creative Commons License. Winogrande is licnsed under an Apache license.  COPA is under a BSD-2 Clause license. We could not find the license of RTE and CB but they are part of SuperGLUE which mentions the datasets are allowed for use in research context.\n\nTo ease comparison, we use the same number of few-shot training examples for each dataset as \\citet{brown2020language}, which varies from 20 to 70.\nUnfortunately, the few-shot dataset subsets used by \\citet{brown2020language} have not been publicly disclosed.\nTo allow for a more robust comparison, we therefore constructed five few-shot datasets by sampling subsets with different seeds and report the median and interquartile range.\nWe prompt examples from each dataset using the prompt templates from P3 \\citet{bach2022promptsource}, using a randomly-sampled prompt template for each example at each step.\nUnless otherwise stated, we train our model for 1K steps with a batch size of 8 and report performance at the end of training.\n\nFor evaluation, we use ``rank classification'', where the model's log-probabilities for all possible label strings are ranked and the model's prediction is considered correct if the highest-ranked choice is the correct answer.\nRank classification evaluation is compatible with both classification and multiple-choice tasks.\nSince model performance can vary significantly depending on the prompt template used, we report the median accuracy across all prompt templates from P3 and across few-shot data subsets for each dataset.\nFor all datasets, we report the accuracy on the test set or validation set when the test labels are not public (e.g.\\ SuperGLUE datasets).\nIn the main text, we report median accuracy across the nine datasets mentioned above.\nDetailed results on each dataset are provided in the appendices.\n\n\n"
            },
            "subsection 3.2": {
                "name": "Unlikelihood Training and Length Normalization",
                "content": "\n\\label{sec:ul_ln}\n\nBefore investigating PEFT methods, we first explore two additional loss terms to improve the performance of few-shot fine-tuning of language models.\nLanguage models are normally trained with cross-entropy loss $L_{\\mathrm{LM}} = - \\frac{1}{T} \\sum_t \\log p(y_t|\\mathbf{x}, y_{<t})$ where the model is trained to increase the probability of the correct target sequence $\\mathbf{y} = (y_1, y_2, \\dots, y_T)$ given the input sequence $\\mathbf{x}$. \n\nFor evaluation, we use rank classification (described in \\cref{sec:model_datasets}) which depends on both the probability that the model assigns to the correct choice as well as the probabilities assigned by the model to the incorrect choices.\nTo account for this during training, we consider adding an unlikelihood loss \\cite{tam2021improving,welleck2019neural}:\n\\begin{equation}\n    L_{\\mathrm{UL}}=-\\frac{ \\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}} \\log (1 - p(\\hat{y}^{(n)}_i | \\mathbf{x}, \\hat{y}^{(n)}_{<t}))}{\\sum_{n=1}^{N} T^{(n)}}\n\\end{equation}\nwhich discourages the model from predicting tokens from incorrect target sequences, where  $\\mathbf{\\hat{y}}^{(n)} = (\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_{T^{(n)}})$ is the $n$-th of $N$ incorrect target sequences.\nWe hypothesize that adding $L_{\\mathrm{UL}}$ will improve results on rank classification because the model will be trained to assign lower probabilities to incorrect choices, thereby improving the chance that the correct choice is ranked highest.\n\nThe possible target sequences for a given training example can have significantly different lengths, especially in multiple-choice tasks.\nRanking each choice based on probability can therefore ``favor'' shorter choices because the model's assigned probability to each token is $\\le 1$.\nTo rectify this, we consider using length normalization when performing rank classification, which divides the model's score on each possible answer choice by the number of tokens in the choice (as used in GPT-3 \\citep{brown2020language}). \nWhen using length normalization during evaluation, we introduce an additional loss term during training that more closely reflects length-normalized evaluation.\nFirst, we compute the length-normalized log probability of a given output sequence $\\beta(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{T} \\sum_{t=1}^{T} \\log p(y_t|\\mathbf{x}, y_{<t})$.\nThen, we maximize the length-normalized log probability of the correct answer choice by minimizing the $\\mathrm{softmax}$ cross-entropy loss:\n\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}\nWhen training a model with $L_{\\mathrm{LM}}$, $L_{\\mathrm{UL}}$, and $L_{\\mathrm{LN}}$, we simply sum them.\nThis avoids introducing any hyperparameters that would be problematic to tune in the few-shot setting (where realistically-sized validation sets are tiny by necessity \\cite{perez2021true,oliver2018realistic}).\n\nWe report the results of fine-tuning all of T0-3B's parameters with and without length normalization on all datasets in \\cref{sec:full_ul_and_ln}.\nWe find that adding $L_{\\mathrm{LN}}$ improves the accuracy from 60.7\\% to 62.71\\% and including both $L_{\\mathrm{UL}}$ and $L_{\\mathrm{LN}}$ provides a further improvement to 63.3\\%.\nSince these loss terms improve performance without introducing any additional hyperparameters, we include them in our recipe and use them in all following experiments.\n\n"
            },
            "subsection 3.3": {
                "name": "Parameter-efficient fine-tuning with \\ia",
                "content": "\n\\label{sec:ia}\n\nIn order to compare favorably to few-shot ICL, we need a PEFT method that has the following properties:\nFirst, it must add or update as few parameters as possible to avoid incurring storage and memory costs.\nSecond, it should achieve strong accuracy after few-shot training on new tasks.\nFinally, it must allow for mixed-task batches, since that is a capability of ICL.\nIn order to easily enable mixed-task batches, a PEFT method should ideally not modify the model itself.\nOtherwise, each example in a batch would effectively need to be processed by a different model or computational graph.\nA more convenient alternative is provided by methods that directly modify the \\textit{activations} of the model since this can be done independently and cheaply to each example in the batch according to which task the example corresponds to.\nPrompt tuning and prefix tuning methods \\citep{lester2021power,li2021prefix} work by concatenating learned vectors to activation or embedding sequences and are therefore examples of activation-modifying PEFT methods that allow for mixed-task batches.\nHowever, as we will discuss later, we were unable to attain reasonable accuracy with prompt tuning and found that the more performant PEFT methods did not allow for mixed-task batches.\nWe therefore developed a new PEFT method that meets our desiderata.\n\nAs an alternative, we explored element-wise multiplication (i.e.\\ rescaling) of the model's activations against a learned vector.\nSpecifically, we consider adaptation of the form $l \\odot x$ where $l \\in \\mathbb{R}^d$ is a learned task-specific vector, $\\odot$ represents element-wise multiplication, and $x \\in \\mathbb{R}^{T \\times d}$ is a length-$T$ sequence of activations.\nWe use ``broadcasting notation'' \\cite{van2011numpy} so that the $(i, j)^\\mathrm{th}$ entry of $l \\odot x$ is $l_jx_{i, j}$.\nIn preliminary experiments, we found it was not necessary to introduce a learned rescaling vector for each set of activations in the Transformer model.\nInstead, we found it was sufficient to introduce rescaling vectors on the keys and values in self-attention and encoder-decoder attention mechanisms and on the intermediate activation of the position-wise feed-forward networks.\nSpecifically, using the notation from \\citet{vaswani2017attention}, we introduce three learned vectors $l_{\\mathrm{k}} \\in \\mathbb{R}^{d_\\mathrm{k}}, l_{\\mathrm{v}} \\in \\mathbb{R}^{d_\\mathrm{v}}$, and $l_{\\mathrm{ff}} \\in \\mathbb{R}^{d_\\mathrm{ff}}$, which are introduced into the attention mechanisms as:\n\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}\nand in the position-wise feed-forward networks as $(l_\\mathrm{ff} \\odot \\gamma(W_1x))W_2$, where $\\gamma$ is the feed-forward network nonlinearity.\nWe introduce a separate set of $l_{\\mathrm{k}}, l_{\\mathrm{v}}$, and $l_{\\mathrm{ff}}$ vectors in each Transformer layer block.\nThis adds a total of $L (d_k + d_v + d_\\mathrm{ff})$ new parameters for a $L$-layer-block Transformer encoder and $L (2d_k + 2d_v + d_\\mathrm{ff})$ (with factors of 2 accounting for the presence of both self-attention and encoder-decoder attention) for a $L$-layer-block decoder.\n$l_{\\mathrm{k}}, l_{\\mathrm{v}}$, and $l_{\\mathrm{ff}}$ are all initialized with ones so that the overall function computed by the model does not change when they are added.\nWe call our method \\ia, which stands for ``Infused Adapter by Inhibiting and Amplifying Inner Activations''.\n\n\\ia makes mixed-task batches possible because each sequence of activations in the batch can be separately and cheaply multiplied by its associated learned task vector.\nWe also note that, in the event that a model will only be used on a single task, the modifications introduced by \\ia can also be applied to weight matrices permanently so that no elementwise multiplication is required and the model's architecture remains unchanged.\nThis possible because element-wise multiplications performed in \\ia always co-occur with a matrix multiplication, and $l \\odot Wx = (l \\odot W)x$.\nIn this case, our method incurs no additional computational cost compared to the original model.\n\nTo validate \\ia, we compare it to a large variety of existing adaptation methods in our setting of fine-tuning T0-3B on few-shot datasets from held-out tasks.\nSpecifically, we compare with 9 strong PEFT methods: BitFit \\cite{zaken2021bitfit} which updates only the bias parameters; Adapters \\cite{Houlsby2019ParameterEfficientTL} which introduce task-specific layers after the self-attention and position-wise feed-forward networks; Compacter and Compacter++ \\cite{mahabadi2021compacter} which improve upon adapters by using low-rank matrices and hypercomplex multiplication; prompt tuning \\cite{lester2021power} which learns task-specific prompt embeddings that are concatenated to the model's input; FISH Mask \\cite{sung2021training} which chooses a subset of parameters to update based on their approximate Fisher information; Intrinsic SAID \\cite{aghajanyan2020intrinsic} which performs optimization in a low-dimensional subspace; prefix-tuning \\cite{li2021prefix} which learns task-specific vectors that are concatenated to the model's activations; and LoRA \\cite{hu2021lora} which assigns low-rank updates to parameter matrices.\nAdditionally, we include the baselines of full-model fine-tuning and updating only the layer normalization parameters.\nFor certain methods that allow changing the parameter efficiency, we report results for different budgets: 0.2\\% and 0.02\\% sparsity for FISH Mask, 10 and 100 learned prompt vectors for prompt tuning, and 20,000- or 500,000-dimensional subspaces for Intrinsic SAID.\n\nThe results are shown in \\cref{fig:peft}, with detailed per-dataset results in \\cref{sec:full_peft}.\nWe find that \\ia is the only method that attains higher accuracy than the full-model-fine-tuning baseline.\nWhile other PEFT methods (e.g.\\ Intrinsic SAID and prompt tuning) update or introduce fewer parameters, \\ia performs considerably better.\nOur results and setting differ with some past work on the PEFT methods we compare against.\n\\citet{mahabadi2021compacter} report that Compacter and Compacter++ outperform full-model fine-tuning, including in the few-shot setting.\n\\citet{lester2021power} found that prompt tuning could match full-model fine-tuning, and in subsequent work \\citet{wei2021finetuned} found that prompt tuning performed well when applied to a multitask fine-tuned model in the few-shot setting.\nIn both cases, we experimented with various hyperparameter choices to try to match past results.\nWe hypothesize the disagreement comes from us using a different model and different datasets.\nFor prompt tuning specifically, we noticed that the validation set performance could fluctuate wildly over the course of training, hinting at possible optimization issues.\n\n\\begin{minipage}[r]{0.48\\textwidth}\n\\vspace{1em}\n\\centering\n\\includegraphics[width=0.93\\textwidth]{peft.pdf}\n\\captionof{figure}{Accuracy of PEFT methods with $L_{\\mathrm{UL}}$ and $L_{\\mathrm{LN}}$  when applied to T0-3B. Methods that with variable parameter budgets are represented with larger and smaller markers for more or less parameters.}\n\\label{fig:peft}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[r]{0.48\\textwidth}\n\\vspace{0.8em}\n\\centering\n\\includegraphics[width=0.93\\textwidth]{main_results.pdf}\n\\captionof{figure}{Accuracy of different few-shot learning methods. \\tfew uses \\ia for PEFT methods of T0, T0 uses zero-shot learning, and T5+LM and the GPT-3 variants use few-shot ICL. The x-axis corresponds to inference costs; details are provided in \\cref{sec:costs}.}\n\\vspace{1em}\n\\label{fig:main}\n\\end{minipage}\n\n"
            },
            "subsection 3.4": {
                "name": "Pre-training \\ia",
                "content": "\n\nIn recent work, \\citet{gu2021ppt,vu2021spot} showed that \\textit{pre-training} the prompt embeddings in prompt tuning can improve performance when fine-tuning on downstream few-shot tasks.\nFor pre-training, \\citet{gu2021ppt} use a suite of self-supervised tasks applied to unlabeled text data, and \\citet{vu2021spot} consider using embeddings from a separate task or multitask mixture.\nWe follow \\citet{vu2021spot} and simply pre-train the new parameters introduced by \\ia on the same multitask mixture used to train T0.\nWe pre-train for 100,000 steps with a batch size of 16 before fine-tuning the \\ia parameters on each individual downstream dataset.\nA full comparison of accuracy with and without pre-training \\ia is detailed in \\cref{sec:full_pretraining}.\nWe find that pre-training improves fine-tuned accuracy from 64.6 to 65.8 and therefore add it to our recipe.\n\n"
            },
            "subsection 3.5": {
                "name": "Combining the ingredients",
                "content": "\n\nIn summary, the \\tfew recipe is defined as follows:\nWe use the T0 model as a backbone.\nWe add \\ia for downstream task adaptation and use parameters initialized from pre-training \\ia on the same multitask mixture for T0.\nAs an objective, we use the sum of a standard language modeling loss $L_\\mathrm{LM}$, an unlikelihood loss $L_\\mathrm{UL}$ for incorrect choices, and a length-normalized loss $L_\\mathrm{LN}$.\nWe train for 1,000 steps with a batch size of 8 sequences using the Adafactor optimizer \\cite{shazeer2018adafactor} with a learning rate of $3e^{-3}$ and a linear decay schedule with a 60-step warmup.\nWe apply prompt templates to downstream datasets during training and inference to convert each example into an instructive text-to-text format.\nImportantly, \\textit{we apply this recipe to every downstream dataset in exactly the same way} without per-dataset hyperparameter tuning or modifications.\nThis makes the recipe a realistic option for few-shot learning settings where validation sets are tiny by definition \\cite{perez2021true,oliver2018realistic}.\n\n"
            }
        },
        "section 4": {
            "name": "Outperforming ICL with \\tfew",
            "content": "\n\\label{sec:experiments}\n\nHaving designed and established the \\tfew recipe on T0-3B, we now apply it to T0 (with 11 billion parameters) and compare performance to strong few-shot ICL baselines.\nFrom this point onwards, we use exactly the same recipe and hyperparameters across all tasks.\n% To test the true few-shot performance of T-Few, we additionally test it on the RAFT benchmark (whose datasets have small training sets and no validation sets) in \\cref{sec:raft}.\n\n\n",
            "subsection 4.1": {
                "name": "Performance on T0 tasks",
                "content": "\n\nFirst, we evaluate \\tfew on the datasets that were held out from T0's training mixture. We compare against zero-shot learning with T0 \\cite{sanh2021multitask} (since we found few-shot ICL to performed worse than zero-shot for T0, see \\cref{sec:full_main}); few-shot ICL with T5+LM \\cite{lester2021power} (the next-step-prediction language model upon which T0 is based); and few-shot ICL with the 6.7, 13, and 175 billion parameter variants of GPT-3. See \\cref{sec:full_main} for more details on these baselines.\n%\nThe accuracy on the held-out T0 datasets (described in \\cref{sec:model_datasets}) is shown in \\cref{tab:main} and \\cref{fig:main}, with per-dataset results reported in \\cref{sec:full_main}.\nWe find that \\tfew outperforms all other methods by a substantial margin.\nNotably, \\tfew achieves a 6\\% higher accuracy than few-shot ICL with GPT-3 175B despite being about $16\\times$ smaller and outperforms the smaller GPT-3 variants by an even larger margin.\n\\tfew also attains significantly higher accuracy than both zero-shot learning with T0 and few-shot ICL with T5+LM.\n\n\\begin{minipage}[l]{0.64\\textwidth}\n    \\centering\n    \\begin{tabular}{p{2.2cm} p{1.25cm} p{1.25cm} p{1.1cm} p{0.9cm}}\n        \\toprule\n       \\phantom{placeholder} Method & Inference FLOPs & Training FLOPs & Disk space & \\phantom{plac} Acc. \\\\\n       \\midrule\n       \\tfew & 1.1e12 & 2.7e16 & 4.2 MB & 72.4\\% \\\\\n       T0 \\cite{sanh2021multitask} & 1.1e12 & 0 & 0 B & 66.9\\% \\\\\n       T5+LM \\cite{lester2021power} & 4.5e13 & 0 & 16 kB & 49.6\\% \\\\\n       GPT-3 6.7B \\cite{brown2020language} & 5.4e13 & 0 & 16 kB & 57.2\\% \\\\\n       GPT-3 13B \\cite{brown2020language} & 1.0e14 & 0 & 16 kB & 60.3\\% \\\\\n       GPT-3 175B \\cite{brown2020language} & 1.4e15 & 0 & 16 kB & 66.6\\% \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\captionof{table}{Accuracy on held-out T0 tasks and computational costs for different few-shot learning methods and models. \\tfew attains the highest accuracy with 1,000$\\times$ lower computational cost than ICL with GPT-3 175B. Fine-tuning with \\tfew costs about as much as ICL on 20 examples with GPT-3 175B.}\n    \\label{tab:main}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[r]{0.33\\textwidth}\n    \\centering\n    \\begin{tabular}{l c}\n        \\toprule\n       Method & Acc. \\\\\n       \\midrule\n       \\tfew  & 75.8\\% \\\\\n       Human baseline \\citep{alex2021raft} & 73.5\\% \\\\\n       PET \\citep{schick2021true} & 69.6\\% \\\\\n       SetFit \\citep{wasserblat2021sentence} & 66.9\\% \\\\\n       GPT-3 \\citep{brown2020language} & 62.7\\% \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\captionof{table}{Top-5 best methods on RAFT as of writing. \\tfew is the first method to outperform the human baseline and achieves over 6\\% higher accuracy than the next-best method.}\n    \\label{tab:raft}\n\\vspace{1em}\n\\end{minipage}\n\n"
            },
            "subsection 4.2": {
                "name": "Comparing computational costs",
                "content": "\n\\label{sec:costs}\n\nHaving established that \\tfew significantly outperforms ICL-based models, we now compare the relative costs of each few-shot learning approach.\nFor simplicity, we use the FLOPs-per-token estimates for Transformer-based language models introduced by \\citet{kaplan2020scaling}.\nSpecifically, we estimate that a decoder-only Transformer (e.g.\\ the GPT series) with $N$ parameters uses $2N$ FLOPs per token for inference and $6N$ FLOPs per token for training.\nEncoder-decoder models like T0 and T5 (where the encoder and decoder have the same number of layers and layer sizes) only process each token with either the encoder or decoder (each having roughly half the parameters of the full model), so the FLOPs per token estimates are halved to $N$ and $3N$ FLOPs per token for inference and training.\nWe note that FLOPs are not a direct measurement of real-world computational cost because latency, power usage, and other costs can vary significantly depending on hardware and other factors \\cite{dehgani2021efficiency}.\nHowever, we focus on FLOPs because it is a hardware-independent metric that closely with real-world costs the hardware setup used for running the different methods we consider would likely vary significantly across methods.\nWe summarize the costs in \\cref{tab:main} and discuss them below.\nFor all estimates, we use the median number of shots (41) across the datasets we consider.\nRank evaluation and our unlikelihood loss both require processing every possible output choice to attain a prediction for an unlabeled example.\nThe median combined tokenized sequence length for the input and all possible targets is 103 for the datasets we consider.\nFor in-context examples processed for few-shot ICL, only the correct target is required, producing a median sequence length of 98.\nAssuming that key and value vectors are cached, processing a single example with ICL therefore involves processing $41 \\times 98 + 103$ tokens.\nA summary of our cost estimates is provided in \\cref{tab:main}.\n\n\\paragraph{Inference cost.}\n\nBeyond improved accuracy, the primary advantage of avoiding few-shot ICL is dramatically lower inference costs.\nProcessing a single input and all target choices with \\tfew requires $11\\e9 \\times 103 = 1.1\\e12$ FLOPs, whereas few-shot ICL with GPT-3 175B requires $2 \\times 175\\e9 \\times (41 \\times 98 + 103) = 1.4\\e15$ FLOPs -- more than 3 orders of magnitude more.\nInference costs with ICL using the smaller GPT-3 variants are also dramatically higher than the inference cost of \\tfew.\nAs discussed in \\cref{sec:icl}, caching the key and value vectors when the same set of in-context examples is to be reused can reduce the computational cost of ICL.\nHowever, this would only result in an approximately $41\\times$ reduction, which is not nearly enough to make any of the GPT-3 ICL costs as low as \\tfew.\n\n\\paragraph{Training cost.}\n\nSince \\tfew is the only method that involves updating parameters, it is the only method that incurs a training cost.\nTraining an eleven billion parameter encoder-decoder model for 1,000 steps with a batch size of 8 length-103 sequences requires approximately $3 \\times 11\\e9 \\times 1,000 \\times 8 \\times 103 = 2.7\\e16$ FLOPs.\nWhile not insignificant, this is only about 20 times larger than the FLOPs required to process a \\textit{single} example with few-shot ICL using GPT-3 175B.\nIn other words, training \\tfew costs as much as using GPT-3 175B to process 20 examples with few-shot ICL.\nWe also found that fine-tuning T0 with \\tfew on a single dataset only takes about a half an hour on a single NVIDIA A100 GPU.\nAs of writing, this would cost about \\$2 USD using Microsoft Azure.\\footnote{\\url{https://docs.microsoft.com/en-us/azure/virtual-machines/ndm-a100-v4-series}}\n\n\\paragraph{Storage cost.}\n\n\\tfew also incurs the largest storage cost.\nWhen stored as single-precision floats, the parameters added by \\ia take up 4.2 MB of space on disk.\nIn contrast, ICL methods only require storing the tokenized in-context examples (typically stored as 32-bit integers), resulting in a smaller $41 \\times 98 \\times 32\\text{ bits} = 16\\text{ kB}$ disk space requirement.\nHowever, we note that 4.2 MB is dwarfed by the on-disk size of the model checkpoints themselves  -- storing the \\ia adaptation vectors for 10,000 tasks would take about as much space as the T0 checkpoint (41.5 GB).\n\n\\paragraph{Memory usage.}\n\nDuring inference, the primary memory cost is incurred by the model's parameters.\nThe only model smaller than T0 (used by \\tfew) is GPT-3 6.7B; otherwise, \\tfew will incur a lower memory cost during inference.\nAdditional memory costs are incurred when training \\tfew due to the need to cache intermediate activations for backpropagation and for the gradient accumulator variables in Adafactor.\nHowever, as mentioned above, it is possible to use the \\tfew recipe on a single 80GB A100 GPU.\n\n"
            },
            "subsection 4.3": {
                "name": "Performance on Real-world Few-shot Tasks (RAFT)",
                "content": "\n\\label{sec:raft}\n\nSo far, we have evaluated performance on a collection of datasets that were not explicitly designed for benchmarking few-shot learning.\nTo better evaluate \\tfew's performance in the real world, we evaluated our approach on the RAFT benchmark \\cite{alex2021raft}.\nRAFT consists of 11 ``economically valuable'' tasks that aim to mirror real-world applications.\nImportantly, each RAFT datasets has only 50 training examples with no validation set and a (larger) test set with no public labels, so it is impossible to ``cheat'' by tuning on an unrealistically-large validation set or by peeking at the test set \\citep{oliver2018realistic,perez2021true}.\nWe apply \\tfew to RAFT by using the standard prompts released alongside the dataset.\nThe accuracy of the current top-5 methods is shown in \\cref{tab:raft}, with further details provided in \\cref{sec:raft_details}.\n\\tfew attains a state-of-the-art accuracy of 75.8\\% and outperforms the human baseline (73.5\\% accuracy) for the first time.\nThe next-best model (from \\citet{schick2021true}) achieves 6\\% lower accuracy and GPT-3 175B attains only 62.7\\%.\nThese results validate that \\tfew can be readily applied as-is to novel real-world tasks to attain strong performance.\n\n"
            },
            "subsection 4.4": {
                "name": "Ablation experiments",
                "content": "\n\\label{sec:ablation}\n\nGiven that our \\tfew design experiments were on T0-3B, we perform an ablation of some of the ingredients of \\tfew on T0.\n% We experiment with omitting the step of pre-training \\ia and removing unlikelihood training and length normalization\nDetailed results are shown in \\cref{sec:full_ablation}.\nWhile the gains from adding each ingredient does not always significant increase the accuracy on each individual dataset, each ingredient consistently improves the average performance across datasets: Removing pre-training decreases accuracy by 1.6\\%, removing unlikelihood training and length normalization decreases accuracy by 4.1\\%, and removing both pre-training and our additional loss terms reduces accuracy by 2.5\\%.\n\n"
            }
        },
        "section 5": {
            "name": "Conclusion",
            "content": "\n\\label{sec:conclusion}\n\nWe introduced \\tfew, a parameter-efficient few-shot learning recipe that attains higher accuracy than few-shot ICL at a lower computational cost.\n\\tfew uses \\ia, a new PEFT method that rescales inner activations with learned vectors.\nUsing \\ia produces better performance than fine-tuning the full model while only introducing a tiny amount of additional parameters.\n\\tfew also uses two additional loss terms that encourage the model to output lower probabilities for incorrect choices and account for the length of different answer choices.\nWhen applying \\tfew as-is (with no task-specific hyperparameter tuning or other changes) to the RAFT benchmark, we attained super-human performance for the first time and outperformed prior submissions by a large margin.\nThrough detailed characterization of computational costs, we found that \\tfew uses over 1,000$\\times$ fewer FLOPs during inference than few-shot ICL with GPT-3 and only requires 30 minutes to train on a single NVIDIA A100 GPU.\nSince all of our experiments were on classification tasks, we are interested in applying \\tfew to generative tasks like as summarization and question answering in future work. \nWe hope our results provide a new perspective on how best to perform few-shot learning with large language models.\n\n% \\begin{ack}\n% We thank Brian Lester and Noah Constant for helpful discussion on debugging prompt tuning and Rabeeh Karimi Mahabadi for help with Compacter and Intrinsic SAID. We also thank Stella Biderman and the Google TPU Research Cloud who provided valuable computational resources to support this work. This work was supported by NSF-AI Engage Institute DRL-2112635.\n% \\end{ack}\n\n\\bibliographystyle{unsrtnat}\n\\bibliography{general.bib}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% \\section*{Checklist}\n\n% \\begin{enumerate}\n\n\n% \\item For all authors...\n% \\begin{enumerate}\n%   \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n%     \\answerYes{We compare different PEFT methods in Figure 2 and demonstate PEFT outperforms ICL in section 4.}\n%   \\item Did you describe the limitations of your work?\n%     \\answerYes{Section 5}\n%   \\item Did you discuss any potential negative societal impacts of your work?\n%     \\answerNo{}\n%   \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n%     \\answerYes{}\n% \\end{enumerate}\n\n\n% \\item If you are including theoretical results...\n% \\begin{enumerate}\n%   \\item Did you state the full set of assumptions of all theoretical results?\n%     \\answerNA{}\n%         \\item Did you include complete proofs of all theoretical results?\n%     \\answerNA{}\n% \\end{enumerate}\n\n\n% \\item If you ran experiments...\n% \\begin{enumerate}\n%   \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n%     \\answerYes{Included code with README}\n%   \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n%     \\answerYes{Section 3.1, Section 3.5, Appendix D, H}\n%         \\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? \n%     \\answerYes{Appendix C, D, E, F, G}\n%         \\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n%     \\answerYes{\\Cref{sec:compute_resources}}\n% \\end{enumerate}\n\n\n% \\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n% \\begin{enumerate}\n%   \\item If your work uses existing assets, did you cite the creators?\n%     \\answerYes{Section 3.1, Section 4.3}\n%   \\item Did you mention the license of the assets?\n%     \\answerYes{Section 3.1}\n%   \\item Did you include any new assets either in the supplemental material or as a URL?\n%     \\answerNA{}\n%   \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n%     \\answerNo{}{}\n%   \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n%     \\answerNo{}\n% \\end{enumerate}\n\n\n% \\item If you used crowdsourcing or conducted research with human subjects...\n% \\begin{enumerate}\n%   \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n%     \\answerNA{}\n%   \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n%     \\answerNA{}\n%   \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n%     \\answerNA{}\n% \\end{enumerate}\n\n\n% \\end{enumerate}\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\clearpage\n\\appendix\n\n\n"
        },
        "section 6": {
            "name": "Compute resources used",
            "content": "\n\\label{sec:compute_resources}\nAll T0-3B models were trained on 48GB A6000s. \nTraining T0-3B with different PEFT methods took about an hour to train, except for Intrinsic SAID and FishMask which each took about two hours to train. \nPre-training \\ia took 1 day on 4 A6000s. \nAll T0 models were trained 80GB A100s from DataCrunch \\footnote{\\url{https://cloud.datacrunch.io/}} and took about half an hour to train each.\nPre-training \\ia took about 1 day on 4 A100s.\n\n"
        },
        "section 7": {
            "name": "Related Work",
            "content": "\n\\label{sec:related}\n\nCurrently, prompt tuning is one of the most parameter-efficient methods for large language models \\cite{li2021prefix, lester2021power, qin2021learning}. \\citet{liu2021p} introduce several tricks to improve prompt tuning, \\citet{an2022input} tune prompts along with input embeddings for boost in performance, and \\citet{chen2022adaprompt} improve prompt embeddings through continued pre-training.\nGiven optimization difficulties when training prompt embeddings, \\citet{diao2022black} recently used black-box optimization to train prompt embeddings without requiring gradients. \nSeveral works have analyzed prompt tuning from the perspective of interpretability \\citet{khashabi2021prompt} and its similarity to other PEFT methods \\citet{he2021towards}.\nPrompt tuning has been applied to various applications for NLP including continual learning \\cite{wang2021learning}, model robustness \\cite{yang2022robust, yang2022prompting}, summarization \\cite{liu2022psp}, machine translation \\cite{garcia2022using}, co-training \\cite{lang2022co}, probing language models \\cite{wang2022shepherd, wang2022shepherd}, inverse prompting \\cite{zou2021controllable} and transfer learning \\cite{su2021transferability}. \n\\citet{he2022hyperprompt} recently proposed the use of a hypernetwork to predict prompts for new tasks (rather than training the prompt parameters with gradient descent).\nPrompt tuning and other PEFT methods have also been explored outside of the context of language models (e.g.\\ vision \\cite{rebuffi2017learning,jia2022visual} and vision-and-language models \\cite{sung2021training}).\n\nSeparately, various studies have considered few-shot full-model fine-tuning with discrete prompts \\cite{schick2020exploiting}. \nRecent work has analyzed training with discrete prompts, demonstrating a boost in performance with prompting when training on various numbers of examples \\cite{scao2021many}, finding that models perform similarly when trained on good and bad prompts \\cite{webson2021prompt}, and exploring which prompts work well for few-shot and full-shot setting \\cite{yang2022prompts}.\nThere have also been efforts to develop methods that find performant discrete prompts \\cite{shin2020autoprompt, gao2020making} and training prompts using methods similar to prompt tuning \\cite{zhang2021differentiable}.\n\nThere has also been a great deal of work on improving ICL. \\citet{chen2021meta, min2021metaicl} use ICL for meta-learning to perform few-shot learning on new tasks. \\citet{lampinen2022can} show ICL can improve when explanations are provided and \\cite{lazaridou2022internet} use ICL with text retrieved from the web for open-domain question-answering. \nMeanwhile, \\citet{min2022rethinking} analyze how ICL works and show that ICL can still perform well when incorrect labels are provided for the in-context examples. \n\nWith the advent of large language models with billions of parameters, there has been a great deal of recent interest in PEFT methods.\nA small amount of recent work has also begun to explore the compatibility of PEFT methods in the few-shot setting.\n\\citet{mahabadi2021compacter} found that PEFT can outperform standard fine-tuning in the low-resource setting.\nIn concurrent work, \\citet{mahabadi2022perfect} compare PEFT to the use of discrete prompts (e.g. PET \\cite{schick2020exploiting}) during few-shot fine-tuning and find that PEFT compares favorably.\nAlso concurrently, \\citet{moosavi2022adaptable} propose a framework for introducing adapters whose architecture and design vary from task to task and demonstrate improved results in few-shot settings.\n\\citet{gu2021ppt} and \\citet{vu2021spot} both explored how pre-training prompt tuning parameters can improve when limited labeled data is available.\nFor few-shot learning, \\citet{triantafillou2021universaltemplates} explore learning universal and dataset dependent parameters that can be blended for generalization. \n\\citet{reqeuima2019cnap} use conditional neural adaptive processes and \\citet{li2021unviersalrepresentations} leverage distillation from multiple feature extractors  for learning new classes or domains in few-shot learning. \n\n"
        },
        "section 8": {
            "name": "Full Unlikelihood Training and Length Normalization Results",
            "content": " \\label{sec:full_ul_and_ln}\n\n\\Cref{tab:full_ul_and_ln} shows the full results with unlikelihood training and length normalization. \n\n\n\n"
        },
        "section 9": {
            "name": "Full PEFT Results",
            "content": " \\label{sec:full_peft}\n\nWe compare against the following PEFT methods, using a linear decay with warmup scheduler with a warm-up ratio of $0.06$ and the Adafactor optimizer \\cite{shazeer2018adafactor}.\nWe show the full per-dataset result of all PEFT methods we considered and ablate the losses. \\Cref{tab:full_peft} includes all losses, \\Cref{tab:full_peft_with_ln} includes $L_{\\mathrm{LN}}$, \\Cref{tab:full_peft_with_ul} includes $L_{\\mathrm{UL}}$, and \\Cref{tab:full_peft_withou_ul_and_ln} does not include either loss. \n\n\\begin{description}\n    \\item[Full Model Fine-tuning] We train for 300 steps with a learning rate of $3e^{-4}$.\n    \\item[BitFit \\cite{zaken2021bitfit}] We train for 300 steps with a learning rate of $3e^{-4}$.\n    \\item[LayerNorm] We train for 300 steps with a learning rate of $3e^{-4}$.\n    \\item[Adapter \\cite{Houlsby2019ParameterEfficientTL}] We use a reduction factor of $32$, ReLU nonlinearity, and residual connections. We train for 500 steps with a learning rate of $3e^{-3}$.\n    \\item[Compacter \\cite{mahabadi2021compacter}] We train for 500 steps with a learning rate of $3e^{-3}$ and hyper complex division factor of 4 $(n=4)$.\n    \\item[Compacter++ \\cite{mahabadi2021compacter}] We train for 500 steps with a learning rate of $3e^{-3}$ and hyper complex division factor of 4 $(n=4)$.\n    \\item[Prompt tuning \\cite{lester2021power}] We train for 1000 steps with a learning rate of $3e^{-1}$ and use 10 and 100 prompt embeddings. \n    \\item[Prefix tuning \\cite{li2021prefix}] We train for 1000 steps with a learning rate of $3e^{-3}$ and adopt the two-layer MLP parameterization in the paper with hidden size 512. We use \"Question:\" and \"Answer:\" as initialization text for the prefixes attached to the input and target sequence, respectively. \n    \\item[FishMask \\cite{sung2021training}] The Fisher is first computed on the training examples and we keep $0.2\\%$ or $0.02\\%$ of the parameters. Then, these parameters are trained for 1500 steps with a learning rate of $3e^{-4}$.\n    \\item[Intrinsic SAID \\cite{aghajanyan2020intrinsic}] We train for 3000 steps with a learning rate of $3e^{-2}$. Due to large model size, we use Intrinsic SAID to produce rank-1 updates for 2D weights via an outer product of two vectors.\n    \\item[LoRA \\cite{hu2021lora}] We use a rank of $4$ with initialization scale of $0.01$ and update all the attention and feedforward module. We train for 1000 steps with a learning rate of $3e^{-3}$.\n\\end{description}\n\n\n\n\n\n\n\n\n\n\n\n"
        },
        "section 10": {
            "name": "Full Pre-training Results",
            "content": " \\label{sec:full_pretraining}\n\n\\Cref{tab:full_pt} shows the per-dataset results for of pre-training \\ia.\n\n\n\n\n"
        },
        "section 11": {
            "name": "Full Main Results",
            "content": "  \\label{sec:full_main}\n\nWe compare against the following baselines: \n\n\\paragraph{T0.} To measure the improvement in performance conferred through parameter-efficient few-shot learning, we compare to zero-shot evaluation using T0 itself. In preliminary experiments, we found that T0 was not able to perform few-shot ICL -- performance actually \\textit{decreased} as we increased the number of in-context examples. This is likely because of the zero-shot format used during multitask prompted fine-tuning and corroborates a recent finding by \\cite{wang2022benchmarking}.\n\\paragraph{T5+LM.} Since T0 is unable to perform ICL on its own, we also compare to T5+LM, the next-step-prediction language model upon which T0 is based. Specifically, we use the LM-adapted variant of T5.1.1.xxl released by \\citet{lester2021power}, which has the same architecture and number of parameters as T0. Due to memory constraints and because of its improved performance, we use ensemble ICL for T5+LM \\citep{min2021metaicl}. Specifically, we perform one-shot ICL using each example in the training set individually and average the predictions for a given query example. For fair comparison with GPT-3 models, we use the EleutherAI evaluation harness \\cite{eval-harness}, which was designed to replicate the evaluation setup done by \\citet{brown2020language}.\n\\paragraph{GPT-3.} For a strong ICL baseline, we consider models in the GPT-3 family \\citep{brown2020language}. Specifically, we compare to the 6.7, 13, and 175 billion parameter variants of GPT-3. Because these models have not been publicly released, we report numbers directly from \\citet{brown2020language}. While GPT-3 is available through the commercial OpenAI API, re-running evaluation through the API would be more than an order of magnitude more expensive than running all of the experiments performed for this paper.\n\n\n\n\n"
        },
        "section 12": {
            "name": "Full Ablation Results",
            "content": "  \\label{sec:full_ablation}\n\nTable \\cref{tab:full_ablation} shows the \\tfew ablation results.\n\n\n\n\n"
        },
        "section 13": {
            "name": "RAFT Experiment Details",
            "content": " \\label{sec:raft_details}\n\nRAFT consists of 11 tasks: Ade Corpus V2, Banking 77, NeurIps Impact Statement Risks, One Stop English, Overruling, Systematic Review Inclusion, Tai Safety Research, Terms of Service, Tweet Eval Hate, and Twitter Complaints. \nWe use the \\tfew recipe on all datasets without putting the labels into the input string except Banking 77. Since Banking 77 has 77 classes which causes memory issues for unlikelihood training, we turn off unlikelihood training for Banking 77. We also feed in all the labels as part of the input string for Banking 77 since there were some labels never seen during training and clean the labels by replacing \".\" with \",\". \n\nPer-dataset results of \\tfew and the other top-5 methods on RAFT are shown in \\cref{tab:full_raft}.\n\n\n\n"
        },
        "tables": {
            "tab:full_ul_and_ln": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{@{} l c c c c c c @{}}\n        \\toprule\n        & COPA & H-Swag & StoryCloze & Winogrande & WSC & WiC  \\\\\n       \\midrule\n        \\textsc{FT} & $78.0_{2.0}$ & $39.2_{0.2}$ & $91.5_{1.0}$ & $54.5_{0.9}$ & $66.4_{1.0}$ & $53.8_{1.7}$  \\\\\n       \\; + \\textsc{UL} & $81.0_{3.0}$ & $46.1_{4.8}$ & $93.6_{2.5}$ & $56.5_{2.2}$ & $61.5_{8.7}$ & $56.4_{4.1}$ \\\\\n       \\; + \\textsc{LN} & $86.0_{4.0}$ & $47.1_{22.4}$ & $94.0_{0.6}$ & $56.9_{3.8}$ & $65.4_{3.9}$ & $53.9_{2.0}$  \\\\\n       \\; + \\textsc{UL} + \\textsc{LN} & $81.0_{11.0}$ & $46.4_{8.8}$ & $93.8_{2.7}$ & $56.5_{1.5}$ & $65.4_{7.7}$  & $57.7_{3.9}$  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} l c c c c c c c @{}}\n         & RTE & CB & ANLI-R1 & ANLI-R2 & ANLI-R3 \\\\\n       \\midrule\n       \\textsc{FT}  & $75.8_{5.4}$ & $82.1_{5.4}$ & $47.8_{1.5}$ & $40.6_{0.8}$ & $37.8_{1.8}$ & \\\\\n       \\; + \\textsc{UL} & $77.6_{1.4}$ & $89.3_{1.8}$ & $47.9_{1.9}$ & $40.9_{1.9}$ & $38.8_{5.0}$ \\\\\n       \\; + \\textsc{LN} & $75.8_{4.3}$ & $89.3_{7.1}$ & $48.2_{0.6}$ & $40.9_{0.9}$ & $38.3_{1.6}$ \\\\\n       \\; + \\textsc{UL} + \\textsc{LN} & $79.8_{3.6}$ & $87.5_{5.4}$ & $46.6_{2.5}$ & $41.3_{0.9}$ & $40.2_{5.3}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Per-dataset results for comparing the effect of including the additional loss terms introduced in \\cref{sec:ul_ln}. Subscripts are IQR.}\n    \\label{tab:full_ul_and_ln}\n\\end{table}",
            "tab:full_peft": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} c c c c c c c c@{}}\n        \\toprule\n        & \\# of Param & COPA & H-Swag & StoryCloze & Winogrande \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B & $81.0_{11.0}$ & $46.4_{8.8}$ & $93.8_{2.7}$ & $56.5_{1.5}$ \\\\\n       BitFit (with LayerNorm) & 1.3M & $75.0_{2.0}$ & $29.5_{3.6}$ & $88.6_{0.7}$ & $49.6_{1.3}$ \\\\\n       LayerNorm & 250K & $76.0_{2.0}$ & $29.6_{3.4}$ & $88.7_{0.9}$ & $49.4_{1.4}$ \\\\\n       Adapter & 12.9M & $84.0_{3.0}$ & $41.9_{3.8}$ & $91.7_{3.7}$ & $54.7_{3.6}$  \\\\\n       Compacter & 807K & $84.0_{5.0}$ & $46.4_{2.5}$ & $93.5_{2.2}$  & $55.5_{2.9}$ \\\\\n       Compacter++ & 540K  & $86.0_{3.0}$ & $46.3_{3.0}$ & $93.5_{1.2}$  & $55.1_{1.1}$ \\\\\n       Prompt tuning (10)& 41K & $67.0_{5.0}$ & $29.9_{0.6}$ & $84.2_{0.8}$  & $51.9_{1.6}$ \\\\\n       Prompt tuning (100) & 409K & $60.0_{19.0}$ & $26.8_{0.6}$ & $74.0_{3.4}$ & $51.1_{0.8}$\\\\\n       Prefix tuning & 576K & $71.0_{8.0}$ & $42.1_{4.0}$ & $90.2_{3.1}$ & $52.0_{1.3}$\\\\\n       FishMask (0.2\\%) & 6M & $82.0_{5.0}$ & $44.1_{4.2}$ & $94.2_{1.8}$   & $54.5_{2.1}$ \\\\ \n        FishMask (0.02\\%) & 600K & $84.0_{6.0}$ & $38.2_{3.6}$ & $93.6_{0.7}$ & $53.9_{2.8}$  \\\\\n       Intrinsic SAID & 500K & $77.0_{4.0}$ & $36.7_{4.5}$ & $89.3_{2.3}$ & $52.7_{2.1}$ \\\\\n       Intrinsic SAID & 20K & $76.0_{4.0}$ & $38.3_{6.4}$ & $89.7_{2.7}$ & $50.9_{1.0}$  \\\\\n        LoRA  & 9.1M & $88.0_{5.0}$ & $47.1_{3.2}$ & $93.6_{2.1}$ & $56.8_{3.3}$\\\\\n        \\ia & 540K & $87.0_{3.0}$ & $49.4_{4.6}$ & $94.7_{2.7}$  & $59.8_{0.6}$  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param  & WSC & WiC & RTE & CB  \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B  & $65.4_{7.7}$ & $57.7_{3.9}$ & $79.8_{3.6}$ & $87.5_{5.4}$ \\\\\n       BitFit (with LayerNorm) & 1.3M  & $61.5_{11.5}$ & $51.7_{2.2}$ & $72.2_{1.1}$ & $57.1_{1.8}$  \\\\\n       LayerNorm & 250K  & $63.5_{12.5}$ & $52.2_{1.6}$ & $71.8_{0.4}$ & $57.1_{1.8}$\\\\\n       Adapter & 12.9M & $65.4_{1.0}$ & $55.5_{2.7}$ & $76.2_{3.6}$ & $87.5_{3.6}$\\\\\n       Compacter $(n = 4)$ & 807K & $64.4_{6.7}$ & $55.2_{3.8}$ & $75.8_{6.1}$ & $82.1_{3.6}$ \\\\\n       Compacter++ $(n = 4)$ & 540K  & $65.4_{3.9}$ & $54.1_{2.2}$ & $76.9_{0.4}$ & $82.1_{3.6}$  \\\\\n       Prompt tuning (10)& 41K & $54.8_{10.6}$ & $51.6_{2.0}$ & $52.7_{5.4}$ & $66.1_{1.8}$  \\\\\n       Prompt tuning (100) & 409K   & $60.6_{4.8}$ & $50.0_{1.1}$ & $48.0_{2.9}$ & $53.6_{17.9}$  \\\\\n       Prefix tuning & 576K & $56.7_{3.3}$ & $54.2_{3.3}$ & $68.6_{3.3}$ & $84.0_{1.8}$  \\\\\n       FishMask (0.2\\%) & 6M  & $63.5_{4.8}$ & $52.5_{3.3}$ & $76.9_{4.7}$ & $83.9_{3.6}$  \\\\ \n        FishMask (0.02\\%) & 600K & $61.5_{1.0}$ & $53.5_{1.3}$ & $75.5_{5.4}$ & $76.8_{3.6}$ \\\\\n       SAID & 500K  & $61.5_{8.7}$ & $55.0_{2.7}$ & $69.0_{7.6}$ & $80.4_{0.0}$\\\\\n       SAID & 20K & $55.8_{6.7}$ &  $55.3_{0.5}$ & $66.1_{5.4}$ & $83.9_{1.8}$\\\\\n        LoRA  & 9.1M  & $60.6_{5.8}$  &  $55.2_{5.0}$ & $78.3_{7.6}$ & $85.7_{1.8}$ \\\\\n        \\ia & 540K& $68.3_{6.7}$ & $56.0_{4.6}$ & $78.0_{2.5}$ & $87.5_{1.8}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param & ANLI-R1 & ANLI-R2 & ANLI-R3 \\\\\n        \\midrule\n        Full Model Fine-tuning & 3B & $46.6_{2.5}$ & $41.3_{0.9}$ & $40.2_{5.3}$ \\\\\n        BitFit (with LayerNorm) & 1.3M & $36.5_{0.8}$ & $35.3_{2.2}$ & $36.6_{0.8}$ \\\\\n       LayerNorm & 250K & $36.5_{0.7}$ & $35.1_{2.6}$ & $36.3_{1.0}$  \\\\\n       Adapter & 12.9M & $45.1_{2.6}$  & $40.4_{1.2}$ & $35.3_{1.3}$ \\\\\n       Compacter & 807K & $40.8_{3.3}$ & $37.4_{0.2}$ & $35.8_{3.3}$  \\\\\n       Compacter++ & 540K  & $41.7_{0.4}$ & $38.3_{1.8}$ & $36.9_{1.5}$ \\\\\n       Prompt tuning (10)& 41K  & $34.2_{1.9}$ & $33.5_{1.1}$ & $33.5_{1.3}$ \\\\\n       Prompt tuning (100) & 409K & $33.4_{1.2}$ & $33.8_{0.5}$ & $33.3_{0.8}$  \\\\\n       Prefix tuning & 576K & $43.3_{4.1}$ & $37.5_{1.2}$ & $36.5_{1.5}$  \\\\\n       FishMask (0.2\\%) & 6M & $43.7_{0.3}$ & $39.7_{1.4}$ & $37.2_{1.1}$ \\\\\n        FishMask (0.02\\%) & 600K & $39.9_{0.9}$ & $38.1_{2.0}$ & $36.2_{1.8}$  \\\\\n       SAID & 500K  & $40.4_{3.3}$ & $35.4_{4.1}$ & $35.5_{1.6}$ \\\\\n       SAID & 20K & $41.3_{1.3}$ & $38.5_{1.8}$ & $35.8_{2.0}$ \\\\\n        LoRA & 9.1M & $45.1_{2.5}$ & $41.0_{1.4}$ & $39.5_{4.8}$ \\\\\n        \\ia & 540K & $48.6_{2.0}$ & $40.8_{1.5}$ & $40.8_{2.3}$  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Per-dataset accuracies for the PEFT methods we consider when adding $L_{\\mathrm{UL}}$ and $L_{\\mathrm{LN}}$. Subscripts are IQR.}\n    \\label{tab:full_peft}\n\\end{table}",
            "tab:full_peft_with_ln": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} c c c c c c c c@{}}\n        \\toprule\n        & \\# of Param & COPA & H-Swag & StoryCloze & Winogrande \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B & $86.00_{4.00}$ & $47.12_{22.44}$ & $93.96_{0.59}$ & $56.91_{3.79}$ \\\\\n       BitFit (with LayerNorm) & 1.3M & $80.00_{6.00}$ & $31.33_{0.16}$ & $92.89_{0.27}$ & $51.38_{0.71}$ \\\\\n       LayerNorm & 250K & $82.00_{2.00}$ & $31.25_{0.64}$ & $92.84_{0.48}$ & $51.14_{0.39}$ \\\\\n       Adapter & 12.9M & $84.00_{5.00}$ & $44.05_{3.22}$ & $92.89_{2.35}$ & $52.64_{0.55}$ \\\\\n       Compacter $(n = 4)$  & 807K & $85.00_{3.00}$ & $47.20_{5.34}$ & $94.33_{1.23}$ & $53.91_{1.34}$ \\\\\n       Compacter++ $(n = 4)$  & 540K  & $85.00_{2.00}$ & $47.86_{1.65}$ & $94.55_{0.69}$ & $54.38_{2.92}$\\\\\n       Prompt tuning (10)& 41K & $72.00_{5.00}$ & $30.43_{1.07}$ & $90.38_{1.23}$ & $50.51_{0.95}$ \\\\\n       Prompt tuning (100) & 409K & $65.00_{1.00}$ & $27.93_{4.69}$ & $87.01_{3.05}$ & $51.93_{0.39}$\\\\\n       Prefix tuning & 576K & $79.00_{6.00}$ & $34.40_{9.71}$ & $90.33_{3.15}$ & $51.10_{1.72}$\\\\\n       FishMask (0.2\\%) & 6M  & $85.00_{4.00}$ & $26.65_{0.14}$ & $93.80_{0.90}$ & $54.38_{0.16}$ \\\\ \n        FishMask (0.02\\%) & 600K & $82.00_{2.00}$ & $26.65_{0.14}$ & $93.64_{1.12}$ & $53.91_{1.97}$\\\\\n       Intrinsic SAID & 500K &  \\\\\n       Intrinsic SAID & 20K &   \\\\\n        LoRA  & 9.1M & $86.00_{1.00}$ & $48.68_{2.62}$ & $94.44_{1.66}$ & $56.12_{1.03}$ \\\\\n        \\ia & 540K & $90.00_{2.00}$ & $50.03_{3.02}$ & $95.40_{1.12}$ & $58.25_{0.55}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param  & WSC & WiC & RTE & CB  \\\\\n        \\midrule\n        Full Model Fine-tuning & 3B & $65.38_{3.85}$ & $53.92_{2.04}$ & $75.81_{4.33}$ & $89.29_{7.14}$ \\\\\n        BitFit (with LayerNorm) & 1.3M & $63.46_{2.88}$ & $54.23_{3.13}$ & $75.45_{1.81}$ & $67.86_{0.00}$ \\\\\n       LayerNorm & 250K &  $60.58_{2.88}$ & $55.33_{1.88}$ & $76.17_{1.44}$ & $67.86_{1.79}$ \\\\\n       Adapter & 12.9M & $63.46_{3.85}$ & $55.49_{3.61}$ & $77.26_{3.97}$ & \n       $80.36_{3.57}$ \\\\\n       Compacter $(n = 4)$  & 807K &  $64.42_{3.85}$ & $53.29_{5.49}$ & $75.45_{2.89}$ & $82.14_{5.36}$ \\\\\n       Compacter++ $(n = 4)$  & 540K  & $65.38_{3.85}$ & $54.86_{3.45}$ & $77.26_{5.78}$ & $76.79_{7.14}$ \\\\\n       Prompt tuning (10) & 41K  & $53.85_{4.81}$ & $52.04_{1.72}$ & $55.23_{2.53}$ & $66.07_{3.57}$\\\\\n       Prompt tuning (100) & 409K &  $50.96_{6.73}$ & $51.88_{1.57}$ & $48.38_{3.69}$ & $62.50_{12.50}$ \\\\\n       Prefix tuning & 576K & $60.58_{3.85}$ & $68.95_{0.72}$ & $80.36_{12.50}$ &  $75.00_{8.93}$ \\\\\n        FishMask (0.2\\%) & 6M & $66.35_{2.88}$ & $54.23_{1.10}$ & $75.81_{3.61}$ & $83.93_{7.14}$\\\\ \n        FishMask (0.02\\%) & 600K & $60.58_{1.92}$ & $52.82_{1.10}$ & $75.09_{3.61}$ & $76.79_{3.57}$ \\\\\n       SAID & 500K  & \\\\\n       SAID & 20K &  \\\\\n        LoRA  & 9.1M  &  $61.54_{1.92}$ & $55.02_{4.70}$ & $74.73_{4.69}$ & $85.71_{1.79}$ \\\\\n        \\ia & 540K&  $66.35_{3.85}$ & $53.76_{0.63}$ & $76.90_{2.89}$ & $83.93_{0.00}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param & ANLI-R1 & ANLI-R2 & ANLI-R3 & \\textbf{Avg.} \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B  & $48.20_{0.60}$ & $40.90_{0.90}$ & $38.25_{1.58}$ & $63.25$ \\\\\n       BitFit (with LayerNorm) & 1.3M  & $36.10_{1.40}$ & $35.60_{1.40}$ & $35.42_{2.00}$ & $56.7$ \\\\\n       LayerNorm & 250K  & $37.30_{0.50}$ & $37.10_{0.70}$ & $36.25_{1.08}$ & $57.07$\\\\\n       Adapter & 12.9M & $42.40_{3.20}$ & $38.80_{0.60}$ & $36.50_{3.83}$ & $60.71$ \\\\\n       Compacter $(n = 4)$ & 807K & $42.90_{3.90}$ & $38.00_{0.80}$ & $37.33_{2.33}$ & $61.27$ \\\\\n       Compacter++ $(n = 4)$ & 540K  & $41.90_{0.50}$ & $38.50_{2.40}$ & $36.00_{0.58}$ & $61.13$ \\\\\n       Prompt tuning (10)& 41K & $34.20_{1.10}$ & $34.20_{1.30}$ & \n       $34.42_{0.83}$ & $52.12$  \\\\\n       Prompt tuning (100) & 409K & $34.10_{1.10}$ & $34.20_{0.20}$ & $34.08_{1.25}$ & $49.82$\\\\\n       Prefix tuning & 576K & $37.50_{3.60}$ & $34.17_{4.50}$ & $34.40_{9.71}$ & $58.71$  \\\\\n       FishMask (0.2\\%) & 6M & $43.40_{0.60}$ & $40.00_{0.90}$ & $36.75_{2.83}$ & $60.03$\\\\ \n        FishMask (0.02\\%) & 600K & $40.10_{0.90}$ & $38.00_{2.00}$ & \t$35.50_{0.75}$ & $57.73$ \\\\\n       SAID & 500K  & \\\\\n       SAID & 20K &  \\\\\n        LoRA & 9.1M &  $46.20_{1.70}$ &  $41.40_{0.90}$ & $38.42_{2.67}$ & $62.57$\\\\\n        \\ia & 540K & $49.20_{2.80}$ & $40.30_{2.30}$ & $40.42_{3.17}$ & \n        $64.05$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Per-dataset accuracies for the PEFT methods we consider when adding $L_{\\mathrm{LN}}$. Subscripts are IQR.}\n    \\label{tab:full_peft_with_ln}\n\\end{table}",
            "tab:full_peft_with_ul": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} c c c c c c c c@{}}\n        \\toprule\n        & \\# of Param & COPA & H-Swag & StoryCloze & Winogrande \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B & $81.00_{3.00}$ & $46.12_{4.82}$ & $93.64_{2.51}$ & $56.51_{2.21}$ \\\\\n       BitFit (with LayerNorm) & 1.3M & $81.00_{4.00}$ & $35.51_{2.34}$ & $92.78_{0.86}$ & $50.91_{0.08}$\\\\\n       LayerNorm & 250K & $82.00_{1.00}$ & $34.60_{2.31}$ & $92.68_{0.75}$ & $51.78_{1.26}$ \\\\\n       Adapter & 12.9M &  $83.00_{1.00}$ & $42.53_{5.35}$ & $90.49_{3.15}$ & $53.67_{3.63}$ \\\\\n       Compacter $(n = 4)$ & 807K & $88.00_{3.00}$ & $42.95_{4.06}$ & $92.89_{1.87}$ & $54.62_{1.50}$ \\\\\n       Compacter++ $(n = 4)$  & 540K  & $85.00_{2.00}$ & $48.26_{2.95}$ & $93.85_{1.60}$ & $54.85_{2.84}$\\\\\n       Prompt tuning (10)& 41K & $74.00_{5.00}$ & $29.24_{2.48}$ & $88.88_{1.12}$ & \t$51.38_{0.47}$  \\\\\n       Prompt tuning (100) & 409K & $68.00_{7.00}$ & $28.51_{2.43}$ & $86.91_{4.33}$ & $50.59_{0.16}$\\\\\n       Prefix tuning & 576K & $69.00_{2.00}$ &  $29.04_{10.83}$ & $86.44_{2.35}$ & $50.63_{1.41}$\\\\\n       FishMask (0.2\\%) & 6M & $85.00_{5.00}$ & $27.78_{0.51}$ & $94.01_{1.55}$ & $53.67_{2.60}$ \\\\ \n        FishMask (0.02\\%) & 600K & $84.00_{4.00}$ & $27.78_{0.51}$ & $93.16_{1.23}$ & $53.59_{2.21}$ \\\\\n       Intrinsic SAID & 500K &  \\\\\n       Intrinsic SAID & 20K &   \\\\\n        LoRA  & 9.1M & $87.00_{3.00}$ & $46.97_{1.98}$ & $93.11_{2.03}$ & $57.93_{3.63}$ \\\\\n        \\ia & 540K & $86.00_{4.00}$ & $48.78_{4.12}$ & $94.01_{2.83}$ & $58.72_{1.34}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param  & WSC & WiC & RTE & CB  \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B  & $61.54_{8.65}$ & $56.43_{4.08}$ & $77.62_{1.44}$ & $89.29_{1.79}$ \\\\\n       BitFit (with LayerNorm) & 1.3M  & $64.42_{3.85}$ & $53.61_{2.51}$ & $76.17_{3.61}$ & $60.71_{1.79}$\\\\\n       LayerNorm & 250K  & $60.58_{8.65}$ & $53.92_{2.35}$ & $75.09_{1.81}$ & $57.14_{3.57}$\\\\\n       Adapter & 12.9M & $65.38_{6.73}$ & $54.39_{3.13}$ & $79.06_{5.42}$ & $85.71_{3.57}$ \\\\\n       Compacter $(n = 4)$ & 807K & $65.38_{4.81}$ & $54.55_{3.61}$ & $75.45_{5.05}$ & $82.14_{0.00}$\\\\\n       Compacter++ $(n = 4)$ & 540K  & $64.42_{3.85}$ & $55.64_{3.61}$ & $77.62_{4.69}$ & $80.36_{7.14}$\\\\\n       Prompt tuning (10)& 41K & $54.81_{6.73}$ & $52.82_{3.29}$ & $52.71_{1.08}$ & $69.64_{5.36}$  \\\\\n       Prompt tuning (100) & 409K & $50.00_{3.85}$ & $50.16_{0.94}$ &\t$52.71_{4.33}$ & $58.93_{12.50}$\\\\\n       Prefix tuning & 576K &  $55.77_{1.92}$ & $71.12_{6.14}$ & $82.14_{5.36}$ & $83.93_{8.93}$\\\\\n       FishMask (0.2\\%) & 6M & $62.50_{3.85}$ & $53.61_{1.41}$ & $76.17_{2.17}$ & $83.93_{8.93}$\\\\ \n        FishMask (0.02\\%) & 600K & $59.62_{1.92}$ & $53.61_{0.47}$ & $74.37_{5.05}$ & $75.00_{1.79}$ \\\\\n       SAID & 500K  & \\\\\n       SAID & 20K &  \\\\\n        LoRA  & 9.1M  &  $59.62_{12.50}$ & $55.49_{4.86}$ & $79.06_{1.81}$ & $87.50_{1.79}$\\\\\n        \\ia & 540K & $65.38_{4.81}$ & $56.74_{4.39}$ & $77.26_{2.53}$ & $87.50_{1.79}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param & ANLI-R1 & ANLI-R2 & ANLI-R3 & \\textbf{Avg.}\\\\\n        \\midrule\n        Full Model Fine-tuning & 3B & $47.90_{1.90}$ & $40.90_{1.90}$ & $38.83_{5.00}$ & $62.71$ \\\\\n        BitFit (with LayerNorm) & 1.3M & $36.40_{1.10}$ & $34.00_{0.70}$ & $35.25_{2.42}$ & $56.43$ \\\\\n       LayerNorm & 250K & $37.00_{1.90}$ & $36.00_{2.10}$ & $35.58_{2.17}$ & $56.03$  \\\\\n       Adapter & 12.9M & $43.90_{1.10}$ & $38.60_{1.10}$ & $36.17_{2.17}$ & $61.17$ \\\\\n       Compacter $(n = 4)$  & 807K & $41.80_{1.30}$ & $37.60_{3.00}$ & $37.17_{1.92}$ & $61.14$  \\\\\n       Compacter++ $(n = 4)$  & 540K  & $41.70_{0.60}$ & $38.20_{2.50}$ & $35.58_{0.33}$ & $61.41$ \\\\\n       Prompt tuning (10)& 41K  & $35.00_{2.10}$ & $33.80_{0.60}$ & $33.67_{2.75}$ & $52.36$\\\\\n       Prompt tuning (100) & 409K &  $35.70_{0.90}$ & $33.80_{1.50}$ & $33.00_{2.17}$ &\t$49.85$ \\\\\n       Prefix tuning & 576K & $34.60_{1.60}$ & $36.83_{4.67}$ & $38.52_{3.00}$ & $58$ \\\\\n       FishMask (0.2\\%) & 6M & $44.10_{1.00}$ & $38.70_{1.50}$ & $38.25_{0.83}$ & $59.79$ \\\\ \n        FishMask (0.02\\%) & 600K & $40.50_{2.60}$ & $37.00_{1.20}$ & $35.58_{0.75}$ & $57.66$  \\\\\n       SAID & 500K  & \\\\\n       SAID & 20K &  \\\\\n        LoRA & 9.1M & $45.90_{2.20}$ & $41.10_{1.70}$ & $38.83_{1.08}$ & $62.96$ \\\\\n        \\ia & 540K & $49.80_{2.10}$ & $40.30_{0.30}$ & $40.17_{3.33}$ & $64.06$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Per-dataset accuracies for the PEFT methods we consider when adding $L_{\\mathrm{UL}}$. Subscripts are IQR.}\n    \\label{tab:full_peft_with_ul}\n\\end{table}",
            "tab:full_peft_withou_ul_and_ln": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} c c c c c c c c@{}}\n        \\toprule\n        & \\# of Param & COPA & H-Swag & StoryCloze & Winogrande \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B & $78.00_{2.00}$ & $39.16_{0.24}$ & $91.45_{0.96}$ & $54.46_{0.87}$ \\\\\n       BitFit (with LayerNorm) & 1.3M & $77.00_{7.00}$ & $33.76_{0.38}$ & $90.49_{0.27}$ & $51.54_{0.16}$\\\\\n       LayerNorm & 250K & $77.00_{7.00}$ & $33.58_{0.65}$ & $90.43_{0.21}$ & $51.38_{0.32}$ \\\\\n       Adapter & 12.9M & $76.00_{5.00}$ & $36.41_{2.27}$ & $90.59_{1.71}$ & $52.01_{0.47}$  \\\\\n       Compacter $(n = 4)$  & 807K & $81.00_{5.00}$ & $37.53_{0.67}$ & $91.50_{0.21}$ & $52.57_{0.87}$ \\\\\n       Compacter++ $(n = 4)$  & 540K  & $78.00_{2.00}$ & $37.00_{1.02}$ & $91.98_{0.91}$ & $53.12_{0.87}$ \\\\\n       Prompt tuning (10)& 41K & $73.00_{4.00}$ & $30.09_{1.67}$ & $88.88_{1.12}$ & $52.25_{0.32}$ \\\\\n       Prompt tuning (100) & 409K & $66.00_{4.00}$ & $26.31_{4.46}$ & $87.44_{0.21}$ & $51.14_{0.55}$\\\\\n       Prefix tuning & 576K & $70.00_{3.00}$ & $27.98_{6.62}$ & $86.75_{2.24}$ & $51.07_{1.10}$ \\\\\n       FishMask (0.2\\%) & 6M & $77.00_{3.00}$ & $35.45_{0.87}$ & $90.54_{1.07}$ & $52.96_{0.87}$ \\\\ \n        FishMask (0.02\\%) & 600K & $74.00_{2.00}$ & $31.15_{1.30}$ & $89.52_{1.28}$ & $52.57_{0.47}$  \\\\\n       Intrinsic SAID & 500K &  \\\\\n       Intrinsic SAID & 20K &   \\\\\n        LoRA  & 9.1M & $80.00_{5.00}$ & $39.14_{1.26}$ & $92.04_{1.07}$ & $53.75_{0.47}$ \\\\\n        \\ia & 540K & $82.00_{1.00}$ & $40.59_{0.56}$ & $92.57_{0.48}$ & $56.91_{2.53}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param  & WSC & WiC & RTE & CB  \\\\\n        \\midrule\n       Full Model Fine-tuning & 3B  & $66.35_{0.96}$ & $53.76_{1.72}$ & $75.81_{5.42}$ & $82.14_{5.36}$ \\\\\n       BitFit (with LayerNorm) & 1.3M  & $61.54_{3.85}$ & $53.13_{1.72}$ & $76.53_{1.08}$ & $64.29_{8.93}$ \\\\\n       LayerNorm & 250K  & $61.54_{3.85}$ & $53.29_{1.72}$ & $76.17_{2.17}$ & $62.50_{8.93}$ \\\\\n       Adapter & 12.9M & $65.38_{7.69}$ & $54.70_{1.72}$ & $77.26_{2.89}$ & $83.93_{1.79}$ \\\\\n       Compacter $(n = 4)$ & 807K & $61.54_{2.88}$ & $55.33_{3.61}$ & $76.17_{2.17}$ & $83.93_{0.00}$ \\\\\n       Compacter++ $(n = 4)$ & 540K & $61.54_{1.92}$ & $54.70_{4.23}$ & $73.65_{1.81}$ & $78.57_{5.36}$ \\\\\n       Prompt tuning (10)& 41K &  $53.85_{7.69}$ & $52.51_{1.88}$ & $57.40_{4.33}$ & $69.64_{10.71}$ \\\\\n       Prompt tuning (100) & 409K & $56.73_{6.73}$ & $52.35_{0.63}$ & $54.15_{3.97}$ & $53.57_{19.64}$\\\\\n       Prefix tuning & 576K &  $52.88_{7.69}$ & $52.51_{0.31}$ & $72.56_{11.91}$ & $75.00_{17.86}$ \\\\\n       FishMask (0.2\\%) & 6M  &  $62.50_{4.81}$ & $54.23_{2.04}$ & $77.26 _{5.42}$ & $82.14_{1.79}$ \\\\ \n        FishMask (0.02\\%) & 600K & $58.65_{2.88}$ & $54.39_{1.10}$ & $76.17_{5.05}$ & $75.00_{3.57}$\\\\\n       SAID & 500K  & \\\\\n       SAID & 20K &\\\\\n        LoRA  & 9.1M & $64.42_{12.50}$ & $54.86_{3.45}$ & $77.26_{4.33}$ & $87.50_{3.57}$  \\\\\n        \\ia & 540K & $64.42_{3.85}$ & $54.23_{1.57}$ & $77.98_{1.81}$ & $82.14_{5.36}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n        & \\# of Param & ANLI-R1 & ANLI-R2 & ANLI-R3 & \\textbf{Avg.}\\\\\n        \\midrule\n        Full Model Fine-tuning & 3B & $47.80_{1.50}$ & $40.60_{0.80}$ & $37.75_{1.83}$ & $60.66$ \\\\\n        BitFit (with LayerNorm) & 1.3M & $37.30_{1.80}$ & $36.10_{2.60}$ & $35.17_{3.67}$ & $56.08$ \\\\\n       LayerNorm & 250K & $37.50_{1.50}$ & $36.00_{2.80}$ & $35.08_{3.42}$ & $55.86$  \\\\\n       Adapter & 12.9M & $40.70_{3.70}$ & $39.20_{1.10}$ & $35.83_{1.92}$ & $59.27$ \\\\\n       Compacter $(n = 4)$  & 807K & $41.80_{2.70}$ & $38.00_{0.80}$ & $36.00_{2.75}$ & $59.58$  \\\\\n       Compacter++ $(n = 4)$ & 540K  & $41.10_{1.50}$ & $38.90_{2.50}$ & $36.92_{1.42}$ & $58.68$ \\\\\n       Prompt tuning (10)& 41K  & $33.60_{0.70}$ & $33.80_{1.10}$ & $34.83_{1.00}$ & $52.71$ \\\\\n       Prompt tuning (100) & 409K &  $35.60_{1.70}$ & $34.50_{0.70}$ & $34.75_{1.42}$ & $50.23$ \\\\\n       Prefix tuning & 576K & $37.60_{2.30}$ & $34.10_{3.50}$ & $35.08_{0.67}$ & $54.14$ \\\\\n       FishMask (0.2\\%) & 6M & $43.50_{0.30}$ & $40.30_{0.40}$ & $36.42_{2.25}$ & \t$59.3$\\\\ \n        FishMask (0.02\\%) & 600K & $40.40_{2.20}$ & $37.50_{1.00}$ & $36.42_{1.08}$ & $56.89$ \\\\\n       SAID & 500K  & \\\\\n       SAID & 20K &  \\\\\n        LoRA & 9.1M & $44.20_{2.60}$ & $40.40_{1.20}$ & $37.58_{0.58}$ & $61.01$ \\\\\n        \\ia & 540K & $48.50_{0.90}$ & $40.20_{1.80}$ & $39.42_{1.67}$ & $61.72$\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Per-dataset accuracies for the PEFT methods we consider without $L_{\\mathrm{UL}}$ or $L_{\\mathrm{LN}}$. Subscripts are IQR.}\n    \\label{tab:full_peft_withou_ul_and_ln}\n\\end{table}",
            "tab:full_pt": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} l c c c c c c @{}}\n        \\toprule\n        & COPA & H-Swag & StoryCloze & Winogrande & WSC  & WiC \\\\\n        \\midrule\n       \\ia  & $87.0_{3.0}$ & $49.4_{4.6}$ & $94.7_{2.7}$ & $59.8_{0.6}$ & $68.3_{6.7}$ & $56.0_{4.6}$  \\\\\n      \\; + \\textsc{PT} &  $89.0_{5.0}$ & $51.2_{4.6}$ & $95.1_{2.5}$ & $62.6_{1.1}$ & $70.2_{8.7}$ & $57.2_{2.5}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} l c c c c c c c @{}}\n        & RTE & CB & ANLI-R1 & ANLI-R2 & ANLI-R3 & \\textbf{Acc.} \\\\\n        \\midrule\n       \\ia & $78.0_{2.5}$ & $87.5_{1.8}$ & $48.6_{2.0}$ & $40.8_{1.5}$ & $40.83_{2.3}$ & 64.6 \\\\\n        \\; + \\textsc{PT}  & $80.9_{1.4}$ & $87.5_{1.8}$ & $49.3_{1.1}$ & $41.1_{0.5}$ & $39.8_{4.8}$ & 65.8\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Per-dataset results when pre-training (\\textsc{PT}) \\ia vs. not pre-training \\ia.  Subscripts are IQR.}\n    \\label{tab:full_pt}\n\\end{table}",
            "tab:full_main": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} c c c c c c c @{}}\n        \\toprule\n        & COPA & H-Swag & StoryCloze & Winogrande & WSC  & WiC  \\\\\n       \\midrule\n       \\tfew  & $93.0_{2.0}$ & $67.1_{6.0}$\t& $97.9_{0.3}$ & $74.3_{1.5}$ & $75.0_{5.5}$ & $62.2_{7.8}$\\\\\n       T0  & $90.8$ & $33.7$ & $94.7$ & $60.5$ & $64.4$ & $57.2$ \\\\\n       T5+LM & $68.0$ & $60.9$5 & $62.8$ & $56.9$ & $63.5$ & $50.0$ \\\\\n      GPT-3 (175B) & $92.0$ & $79.3$ & $87.7$ & $77.7$ & $75.0$ & $55.3$ \\\\\n      GPT-3 (13B) & $86.0$ & $71.3$ & $83.0$ & $70.0$ & $75.0$ & $51.1$ \\\\\n      GPT-3 (6.7B) & $83.0$ & $67.3$ & $81.2$ & $67.4$ & $67.3$ & $53.1$\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} c c c c c c c c @{}}\n       & RTE & CB & ANLI-R1 & ANLI-R2 & ANLI-R3  \\\\\n       \\midrule\n       \\tfew  & $85.6_{2.9}$ & $87.5_{3.6}$ & $59.3_{3.6}$ &  $49.8_{2.6}$ & $44.8_{8.0}$ \\\\\n        T0  & $81.2$ & $78.6$ & $44.7$ & $39.4$ & $42.4$ \\\\\n       T5 + LM & $53.4$ & $32.1$ & $33.3$ & $32.7$ & $34.1$ \\\\\n       GPT-3 (175B) & $72.9$ & $82.1$ & $36.8$ & $34.0$ & $40.2$  \\\\\n      GPT-3 (13B) & $60.6$ & $66.1$ & $33.3$ & $32.6$ & $34.5$ \\\\\n      GPT-3 (6.7B) & $49.5$ & $60.7$ & $33.1$ & $33.1$ & $33.9$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\caption{Comparing \\tfew with few-shot ICL methods. All GPT-3 numbers are from \\citet{brown2020language} and all T0 numbers are from \\citet{sanh2021multitask}.  Subscripts are IQR.}\n    \\label{tab:full_main}\n\\end{table}",
            "tab:full_ablation": "\\begin{table}[!h]\n    \\centering\n    \\begin{tabular}{@{} l c c c c c c @{}}\n        \\toprule\n        & COPA & H-Swag & StoryCloze & Winogrande & WSC  & WiC  \\\\\n        \\midrule\n       \\tfew  & $93.0_{2.0}$ & $67.1_{6.0}$\t& $97.9_{0.3}$ & $74.3_{1.5}$ & $75.0_{5.5}$ & $62.15_{7.8}$\\\\\n       \\; - \\textsc{PT} & $92.0_{2.0}$ & $64.5_{6.6}$ & $97.8_{0.8}$ & $72.7_{1.0}$ & $73.1_{6.3}$ & $60.8_{6.4}$ \\\\\n         \\; - $L_{\\mathrm{UL}}$ - $L_{\\mathrm{LN}}$ & $91.0_{2.0}$ & $52.1_{2.7}$ & $97.4_{0.5}$ & $71.9_{1.1}$ & $71.2_{1.0}$\t& $62.2_{2.4}$ \\\\\n        \\; - PT - $L_{\\mathrm{UL}}$ - $L_{\\mathrm{LN}}$ & $94.0_{2.3}$ & \t$52.7_{4.9}$ & $98.0_{0.3}$ & $74.0_{1.1}$ & $72.6_{4.8}$ & $62.6_{5.0}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{@{} l c c c c c c c @{}}\n       & RTE & CB & ANLI-R1 & ANLI-R2 & ANLI-R3 & \\textbf{Acc.}  \\\\\n        \\midrule\n       \\tfew  & $85.6_{2.9}$ & $87.5_{3.6}$ & $59.3_{3.6}$ &  $49.8_{2.6}$ & $44.8_{8.0}$ & 72.4 \\\\\n        \\; - \\textsc{PT} & $84.5_{2.8}$ & $83.9_{5.4}$ & $57.9_{3.2}$ & $48.6_{3.0}$ & $43.1_{5.7}$ & 70.8\\\\\n        \\; - $L_{\\mathrm{UL}}$ - $L_{\\mathrm{LN}}$ & $82.0_{0.7}$ & $82.1_{3.6}$ & $54.8_{0.4}$ & $46.1_{0.6}$ & $40.8_{5.2}$ & 68.3 \\\\\n        \\; - PT - $L_{\\mathrm{UL}}$ - $L_{\\mathrm{LN}}$ & $84.5_{2.9}$ & $80.4_{3.6}$ & $57.1_{3.1}$ & $47.1_{2.4}$ & $43.8_{5.9}$ & 69.7\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\captionof{table}{\\tfew ablation results when omitting \\ia pre-training (PT) and/or the $L_{\\mathrm{UL}}$ and $L_{\\mathrm{LN}}$ losses.  Subscripts are IQR.}\n    \\label{tab:full_ablation}\n    \n\\end{table}",
            "tab:full_raft": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{c l l l l l l l l l l l}\n        Method &  \\rot{Ade Corpus V2} & \\rot{Banking 77} & \\rot{Neurips Impact Statement Risks} & \\rot{One Stop English} & \\rot{Overruling} & \\rot{Semiconductor Org Types} & \\rot{Systematic Review Inclusion} & \\rot{Tai Safety Research} & \\rot{Terms Of Service} & \\rot{Tweet Eval Hate} & \\rot{Twitter Complaints} \\\\\n        \\midrule\n        \\tfew  & $80.4$ & $69.5$ & $83.3$ & $67.6$ & $95.0$ & $91.5$ & $50.8$ & $73.6$ & $75.0$ & $58.6$ & $87.9$ \\\\\n        Human baseline \\cite{alex2021raft}  & $83.0$ & $60.7$ & $85.7$ & $64.6$ & $91.7$ & $90.8$ & $46.8$ & $60.9$ & $62.7$ & $72.2$ & $89.7$ \\\\\n        PET \\cite{schick2021true}  & $82.2$ & $59.3$ & $85.7$ & $64.6$ & $90.8$ & $81.6$ & $49.3$ & $63.8$ & $57.6$ & $48.3$ & $82.4$ \\\\\n        SetFit \\cite{wasserblat2021sentence} & $72.6$ & $53.8$ & $87.2$ & $52.1$ & $90.7$ & $68.2$ & $49.3$ & $62.8$ & $62.0$ & $53.2$ & $83.7$ \\\\\n        GPT-3 \\cite{brown2020language} & $68.6$ & $29.9$ & $67.9$ & $43.1$ & $93.7$ & $76.9$ & $51.6$ & $65.6$ & $57.4$ & $52.6$ & $82.1$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{1em}\n    \\captionof{table}{Detailed per-dataset results for \\tfew and the other top-5 methods on RAFT.}\n    \\label{tab:full_raft}\n    \\end{table}"
        },
        "figures": {
            "fig:diagram": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\textwidth]{diagram.pdf}\n\\captionof{figure}{Diagram of \\ia and the loss terms used in the \\tfew recipe. \\textit{Left:} \\ia introduces the learned vectors $l_{\\mathrm{k}}, l_{\\mathrm{v}}$, and $l_{\\mathrm{ff}}$ which respectively rescale (via element-wise multiplication, visualized as $\\odot$) the keys and values in attention mechanisms and the inner activations in position-wise feed-forward networks. \\textit{Right:} In addition to a standard cross-entropy loss $L_\\mathrm{LM}$, we introduce an unlikelihood loss $L_\\mathrm{UL}$ that lowers the probability of incorrect outputs and a length-normalized loss $L_\\mathrm{LN}$ that applies a standard $\\mathrm{softmax}$ cross-entropy loss to length-normalized log-probabilities of all output choices.}\n\\label{fig:diagram}\n\\end{figure}"
        },
        "eqs": {
            "eq:1": "\\begin{equation}\n    L_{\\mathrm{UL}}=-\\frac{ \\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}} \\log (1 - p(\\hat{y}^{(n)}_i | \\mathbf{x}, \\hat{y}^{(n)}_{<t}))}{\\sum_{n=1}^{N} T^{(n)}}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    L_{\\mathrm{UL}}=-\\frac{ \\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}} \\log (1 - p(\\hat{y}^{(n)}_i | \\mathbf{x}, \\hat{y}^{(n)}_{<t}))}{\\sum_{n=1}^{N} T^{(n)}}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    L_{\\mathrm{UL}}=-\\frac{ \\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}} \\log (1 - p(\\hat{y}^{(n)}_i | \\mathbf{x}, \\hat{y}^{(n)}_{<t}))}{\\sum_{n=1}^{N} T^{(n)}}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    L_{\\mathrm{UL}}=-\\frac{ \\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}} \\log (1 - p(\\hat{y}^{(n)}_i | \\mathbf{x}, \\hat{y}^{(n)}_{<t}))}{\\sum_{n=1}^{N} T^{(n)}}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    L_{\\mathrm{UL}}=-\\frac{ \\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}} \\log (1 - p(\\hat{y}^{(n)}_i | \\mathbf{x}, \\hat{y}^{(n)}_{<t}))}{\\sum_{n=1}^{N} T^{(n)}}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    L_{\\mathrm{LN}}= -\\log \\frac{ \\exp(\\beta(\\mathbf{x}, \\mathbf{y}))}{\\exp(\\beta(\\mathbf{x}, \\mathbf{y})) + \\sum_{n=1}^{N} \\exp(\\beta(\\mathbf{x}, \\mathbf{\\hat{y}}^{(n)}))}\n\\end{equation}",
            "eq:12": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:13": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:14": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:15": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:16": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:17": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:18": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:19": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:20": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:21": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:22": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:23": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}",
            "eq:24": "\\begin{equation*}\n    \\mathrm{softmax}\\mathopen{}\\left(\\frac{Q(l_\\mathrm{k} \\odot K^T)}{\\sqrt{d_k}}\\right)(l_\\mathrm{v} \\odot V) \n\\end{equation*}"
        },
        "extracted_git_link": "https://github.com/r-three/t-few"
    },
    "LLM_extraction": {
        "Experiments": {
            "ablation study": "Despite the advantages of In-Context Learning (ICL), this paper investigates PEFT techniques for improving few-shot learning while maintaining computational efficiency.",
            "analysis": null,
            "baselines": [
                "T0",
                "GPT-3",
                "T5"
            ],
            "datasets": [
                "COPA",
                "H-SWAG",
                "ANLI",
                "WiC"
            ],
            "evaluation metric": "Accuracy and testing loss",
            "hyperparameters": "Learning rate, optimizer settings, batch size",
            "performance": "Overperformed compared to baselines by high margin",
            "results": "The method significantly improved few-shot learning performance with reduced computational costs.",
            "setup": "Utilized benchmarks like COPA and H-SWAG for testing and validation"
        },
        "Method": {
            "algorithm step": "Detailed in Section 3.5. Uses specialized loss functions and PEFT techniques.",
            "complexity": "The complexity is reduced computationally compared to traditional methods.",
            "description": "This paper proposes a novel parameter-efficient fine-tuning (PEFT) technique for enhancing few-shot learning.",
            "feature processing": null,
            "model": "T0 model with PEFT enhancements",
            "problem formultaion": "Improving few-shot learning efficiency in context of limited computational resources.",
            "tasks": [
                "Few-shot learning",
                "Classification"
            ],
            "theoretical analysis": null
        },
        "conclusion": {
            "future work": "Applying the method to generative tasks like summarization and question answering.",
            "summary": "Presented a PEFT technique that overperforms ICL in few-shot learning tasks while being computationally efficient."
        },
        "high_level_summary": {
            "conclusion": "The proposed techniques overperformed ICL significantly, enabling few-shot learning with lesser computational costs.",
            "method summary": "Introduced PEFT techniques like fine-tuning activations and specialized loss functions.",
            "research challenge": "Enhancing few-shot learning while ensuring parameter and computational efficiency.",
            "research purpose": "Investigating and implementing methods for PEFT compared to ICL for effective few-shot learning.",
            "summary of this paper": "This paper proposes and tests PEFT techniques for enabling efficient few-shot learning, achieving superior performance vis-a-vis traditional ICL methods."
        },
        "meta_data": {
            "abstract": "This research investigates methods to train large language models efficiently for few-shot learning. By integrating parameter-efficient fine-tuning and introducing new loss functions, it achieves better task performance compared to state-of-the-art models like GPT-3 in few-shot setups at significantly reduced computational costs.",
            "affiliations": [
                "Coherent AI, Research Labs"
            ],
            "authors": [
                "Bryan Lester",
                "Noah Constant"
            ],
            "doi link": "None",
            "keywords": [
                "Few-shot learning",
                "PEFT",
                "Language Models"
            ],
            "method name": "TFew Recipe",
            "title": "Improving Few-Shot Learning Efficiency through Parameter-Efficient Fine-Tuning",
            "venue": "NeurIPS 2023",
            "year": "2023"
        },
        "relate work": {
            "comparisons with related methods": "Improved accuracy compared to GPT-3 and achieved better computational efficiency.",
            "related papers": "[Refer adjacent text]",
            "related work category": [
                "Efficiency improvement",
                "Learning optimization"
            ]
        }
    }
}