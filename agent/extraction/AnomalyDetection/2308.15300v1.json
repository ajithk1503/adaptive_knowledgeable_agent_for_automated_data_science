{
    "meta_info": {
        "title": "MSFlow: Multi-Scale Flow-based Framework for Unsupervised Anomaly  Detection",
        "abstract": "Unsupervised anomaly detection (UAD) attracts a lot of research interest and\ndrives widespread applications, where only anomaly-free samples are available\nfor training. Some UAD applications intend to further locate the anomalous\nregions without any anomaly information.\n  Although the absence of anomalous samples and annotations deteriorates the\nUAD performance, an inconspicuous yet powerful statistics model, the\nnormalizing flows, is appropriate for anomaly detection and localization in an\nunsupervised fashion. The flow-based probabilistic models, only trained on\nanomaly-free data, can efficiently distinguish unpredictable anomalies by\nassigning them much lower likelihoods than normal data.\n  Nevertheless, the size variation of unpredictable anomalies introduces\nanother inconvenience to the flow-based methods for high-precision anomaly\ndetection and localization. To generalize the anomaly size variation, we\npropose a novel Multi-Scale Flow-based framework dubbed MSFlow composed of\nasymmetrical parallel flows followed by a fusion flow to exchange multi-scale\nperceptions. Moreover, different multi-scale aggregation strategies are adopted\nfor image-wise anomaly detection and pixel-wise anomaly localization according\nto the discrepancy between them. The proposed MSFlow is evaluated on three\nanomaly detection datasets, significantly outperforming existing methods.\nNotably, on the challenging MVTec AD benchmark, our MSFlow achieves a new\nstate-of-the-art with a detection AUORC score of up to 99.7%, localization\nAUCROC score of 98.8%, and PRO score of 97.1%. The reproducible code is\navailable at https://github.com/cool-xuan/msflow.",
        "author": "Yixuan Zhou, Xing Xu, Jingkuan Song, Fumin Shen, Heng Tao Shen",
        "link": "http://arxiv.org/abs/2308.15300v1",
        "category": [
            "cs.CV"
        ],
        "additionl_info": ""
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nAnomaly detection (AD) is a crucial and challenging field of research with broad applications such as fraud detection \\cite{dal2017credit}, violence detection \\cite{sultani2018real}, medical diagnostics \\cite{paluru2021anam} and industrial defect detection \\cite{bergmann2019mvtec}. \nFurthermore, many applications such as industrial defect detection are not only satisfied with just discriminating anomalies but also intend to locate the anomaly regions. Guided by anomaly localization, the manufacturers can optimize the industrial production process to improve the qualification rate and save costs. \nHowever, anomalies are rare, unpredictable, and diverse in real-world scenarios. It is impractical to collect a dataset containing all anomalous types. \nAnnotating anomalous data is also costly, especially for pixel-wise localization annotations. Therefore, it is unavailable to collect labeled data to support the full-supervision training for anomaly detection. \nTo address this inconvenience, \n% With the infeasibility of fully-supervision, \nunsupervised anomaly detection (UAD) collects easily accessible anomaly-free data for training, and anomalies are discriminated if there is any deviation from anomaly-free data. \n\n\n\nThe unsupervised fashion eases the data collection while intensifying the complication of anomaly localization. \nMost existing methods address this challenge via computing the pixel-wise reconstruction errors \\cite{bergmann2019mvtec, zavrtanik2021riad, marimont2021anomaly, ristea2021self, deng2022distillation} and clustering pixel-wise or region-wise anomaly-free samples to distinguish the anomalous regions \\cite{defard2021padim, reiss2021panda, cohen2020spade, roth2021patchcore}. Recently, \\cite{rudolph2021differnet} first proposed to apply the normalizing flows \\cite{rezende2015variational} to estimate the likelihood of the entire image for the image-wise anomaly detection. CFlow-AD \\cite{gudovskiy2022cflow} extended the normalizing flows to the pixel-wise anomaly localization by estimating the likelihood for each feature vector positioned in the feature map. These flow-based methods reveal impressive effectiveness in unsupervised anomaly detection and localization. \n\nHowever, the unpredictable size variation of diverse anomalies remains an obstacle for flow-based methods to achieve high-precision detection and localization. \nAs illustrated in Fig. \\ref{fig:defect-examples}, the defect size varies enormously across industrial classes in the MVTec AD benchmark specialized for industrial defect detection and localization. \nThe defect region of the \\textit{misplaced transistor} covers nearly half (49.3\\%) of the entire image, while the one of \\textit{pill}, \\textit{screw} or \\textit{toothbrush} only covers a few pixels. For this issue, CSFlow \\cite{rudolph2022csflow} proposed a cross-scale flow module to further exchange information captured on different scales, which is beneficial for the adaptation to anomalies with various sizes. However, the CSFlow only targets anomaly detection without considering localization, and its simple architecture is burdensome and inefficient in information exchange.\n\nTo tackle the size variation, we propose a novel \\textbf{M}ulti-\\textbf{S}cale \\textbf{Flow}-based framework dubbed \\textbf{MSFlow} tailored for unsupervised anomaly detection, as shown in Fig. \\ref{fig:framework}. We perform multi-scale optimization of the flow-based methods in the following three aspects: \n\n1) \\textit{Multi-scale Feature Pyramid Extraction}. Different from \\cite{rudolph2021differnet,rudolph2022csflow} using the featurized image pyramid or \\cite{gudovskiy2022cflow} extracting high-level but low-resolution feature maps, we build our multi-scale feature pyramid with the activation outputs of the low-level stages.\nThe pixel-wise anomaly localization focuses on spatial structure consistency, and global anomaly detection is highly dependent on the perception of local anomalous regions.\nTherefore, the low-level feature pyramid with a larger scale and more spatial details are more favorable for this specific task. \n\n2) \\textit{Multi-scale Flow Model.} Our multi-scale flow model not only independently transforms the feature map of each scale by respective \\textit{parallel flows} but also employs \\textit{a fusion flow} to fuse all feature maps for multi-scale perceptions exchange. In particular, our multi-scale parallel flows are stacked in an asymmetrical architecture, where the parallel flows applied on the higher-level feature map comprise more flow blocks. Our lightweight fusion flow shares a similar mechanism with CSFlow\\cite{rudolph2022csflow} but is much more efficient in information exchange.\n\n3) \\textit{Multi-scale Outputs Aggregation.} We propose different multi-scale aggregation strategies for image-wise anomaly detection and pixel-wise anomaly localization considering the inherent discrepancy between these two subtasks.\nSpecifically, the addition aggregation maintaining the multi-scale properties is adopted to calculate the pixel-wise anomaly score map, while the multi-scale likelihood maps are aggregated by multiplication to suppress the noise before the image-wise anomaly score calculation. Furthermore, the mean of the largest $K$ scores in the multiplication-aggregated anomaly score map is treated as the global anomaly score.\nCompared with the maximum \\cite{zavrtanik2021riad,gudovskiy2022cflow,roth2021patchcore} or the mean \\cite{rudolph2021differnet,rudolph2022csflow} widely used in existing methods, the mean of the top$K$ pixel-wise anomaly scores is more robust and sensitive to small defects.\n% 1) the multi-scale feature pyramid extracted by the pre-trained feature extractor, 2) the multi-scale parallel flows with a fusion flow to exchange multi-scale perceptions, and 3) post-processing for multi-scale likelihood maps. \n% Different from \\cite{rudolph2021differnet,rudolph2022csflow} using the featurized image pyramid or \\cite{gudovskiy2022cflow} where high-level feature pyramid are extracted, we build our multi-scale feature pyramid with the activation outputs of the low-level stages \\cite{he2016resnet,zagoruyko2016wide}.\n% % Compared with semantic segmentation where semantic comprehension is necessary, \n% The pixel-wise anomaly localization focuses on the spatial structure consistency, and the global anomaly detection is highly dependent on the perception of local anomalous regions.\n% Therefore, the low-level feature pyramid with a larger scale and more spatial details are more favorable for this specific task. \n% As for the flow model architecture, our multi-scale flow model not only independently transforms the feature map of each scale by respective parallel flows but also employs a fusion flow to fuse multi-scale information. In particular, our parallel flows are stacked in an asymmetrical architecture, where the parallel flows applied on the higher-level feature map are composed of more flow blocks. Our fusion flow shares the same mechanism with CSFlow but is much more lightweight and efficient in information exchange.\n% Finally, considering the discrepancy between image-wise anomaly detection and pixel-wise anomaly localization, we propose different multi-scale aggregation strategies for these two subtasks.\n% The mean of the largest $k$ scores in the entire anomaly score map is adopted as the image-wise anomaly score.\n% %  And we calculate the final image-wise anomaly score by taking the mean of the largest $k$ scores in the entire anomaly score map. \n% Compared with the maximum value \\cite{zavrtanik2021riad,gudovskiy2022cflow,roth2021patchcore} or the mean \\cite{rudolph2021differnet,rudolph2022csflow} widely used in existing methods, the mean of the largest $k$ pixel-wise anomaly scores is more robust and sensitive to small defects.\n\nTo showcase the superiority of our method, the proposed MSFlow is compared with existing methods\n% \\cite{rudolph2022csflow,gudovskiy2022cflow,roth2021patchcore,defard2021padim,cohen2020spade,rudolph2021differnet,deng2022distillation,ristea2021self,zavrtanik2021draem,li2021cutpaste}\non the challenging MVTec AD benchmark \\cite{bergmann2019mvtec}.\nOur MSFlow achieves state-of-the-art (SOTA) no matter either image-wise anomaly detection (AUCROC 99.7\\%) or pixel-wise anomaly localization (AUCROC 98.8\\% and PRO 97.1\\%) on this industrial defect detection benchmark. \n% Notably, the near-perfect image-wise anomaly detection score of our MSFlow based on the WideResNet-50 (WRN-50) \\cite{zagoruyko2016wide} is even higher than the ensemble performance (AUCROC 99.6\\%) of the previous SOTA PatchCore \\cite{roth2021patchcore} with larger backbones including WRN-101, DenseNet-201\\cite{huang2017densenet} and ResNext-101 \\cite{xie2017resnext}. \n% Besides, the inference speed of the MSFlow is fast because there is no time-consuming similarity calculation used in the PatchCore.\nThe proposed MSFlow is also applied to another image anomaly detection dataset Magnetic Tile Defects (MTD) dataset \\cite{huang2020mtd} and a non-image violence detection dataset mini Shanghai Tech Campus (mSTC) \\cite{luo2017stc} to verify its generality. Our MSFlow achieves comparable accuracy on these two datasets as well. The comprehensive ablation studies highlight the effectiveness of the proposed components in the MSFlow.\n\nOur main contributions can be summarized as follows:\n% \\vspace{-0.8\\topsep}\n\\noindent{\n\\begin{itemize}\n    \\setlength{\\itemsep}{0pt}\n    \\setlength{\\parsep}{0pt}\n    \\setlength{\\parskip}{3pt}\n    \\item To generalize the variation of anomaly size, we explore the multi-scale property of the flow-based method for unsupervised anomaly detection.\n    A \\textbf{M}ulti-\\textbf{S}cale \\textbf{Flow}-based framework (\\tb{MSFlow}) is presented as our implementation.\n    \\item The asymmetrical parallel flows architecture in our MSFlow achieves the trade-off of performance and efficiency. A lightweight fusion flow further exchanges information of different scales and receptive fields to boost the flow model's generalizability of the anomaly size variation.\n    % Our MSFlow not only separately encodes each of the multi-scale feature maps but also efficiently exchanges information of different scales and receptive fields by an additional fusion flow to boost the flow model's generalizability to the variation of anomaly size.\n    \\item Different multi-scale aggregation strategies are employed for image-wise anomaly detection and pixel-wise anomaly localization. Moreover, the mean of the top$K$ anomaly scores in the entire pixel-wise anomaly map is treated as the final image-wise anomaly score, which is robust and sensitive to small anomalous regions.\n\\end{itemize}}\n\nThe rest part of the paper is organized as follows. Section \\ref{sec:related-work} introduces the previous works related to ours, followed by a brief introduction of the normalizing flows in Section \\ref{sec:theory-background}. Section \\ref{sec:method} provides implementation details about our MSFlow. Extensive experiments and ablation studies are provided in Section \\ref{sec:experiment}, which exhibit the superiority of the proposed MSFlow. We conclude the paper in Section \\ref{sec:conclusion}.\n% \u5c3d\u7ba1\u73b0\u5728\u4e00\u4e9b\u7b97\u6cd5\u5728MVTec\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u7cbe\u5ea6\u5df2\u7ecf\u8fbe\u5230\u4e8698%\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u4f46\u662f\u5bf9\u4e8e\u90e8\u5206\u7c7b\u522b\u7684\u68c0\u6d4b\u7cbe\u5ea6\u8fd8\u4e0d\u591f\u9ad8\u3002\n% \u5bf9\u4e8e\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u8fd9\u79cd\u4efb\u52a1\uff0c\u5bf9\u68c0\u6d4b\u7cbe\u5ea6\u8981\u6c42\u975e\u5e38\u9ad8\uff0c\u57fa\u672c\u4e0a\u9700\u8981\u8fbe\u5230\u3002\u5e76\u4e14\u5b9e\u73b0\u5b9a\u4f4d\u80fd\u591f\u8d77\u5230\u5f88\u597d\u7684\u6307\u5bfc\u4f5c\u7528\u6765\u4f18\u5316\u5de5\u4e1a\u751f\u4ea7\u8fc7\u7a0b\u6765\u51cf\u5c11\u635f\u574f\uff0c\u63d0\u9ad8\u5408\u683c\u7387\uff0c\u8282\u7ea6\u6210\u672c\u3002\n% Because only normal samples are available during training, \n\n% The shapes and sizes of defect regions vary for different types of industrial parts, so a single-level feature with the only receptive field is \n\n% discriminative \u53ef\u5206\u8fa8\u7684\n% facilitate \u4fc3\u8fdb \u63d0\u5347\n% alleviated \u7f13\u89e3\n% probabilistic modeling\n% contextual \u4e0a\u4e0b\u6587\u76f8\u5173\u7684\n% inferior \u6b21\u7ea7\u7684\n% inherent \u56fa\u6709\u7684\n% derive \u5bfc\u51fa\uff0c\u884d\u751f\u51fa\n% trivial \u7410\u788e\u7684\n% engage in \u6d89\u53ca\u5230\n% \n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": " \\label{sec:related-work}\n\n",
                "subsection 2.1": {
                    "name": "Unsupervised Anomaly Detection",
                    "content": "\nIn real/practical scenarios, since only normal data are available, \\textit{unsupervised anomaly detection} (UAD) is also named \\textit{out-of-distribution} or \\textit{one-class-calssification}. In an unsupervised fashion, normal samples and anomalies are discriminated through the inductive bias for normal training data. Predominant UAD approaches can be broadly summarized into three categories: \\textit{reconstruction-based methods}, \\textit{clustering-based methods}, and \\textit{flow-based methods}, to which our method belongs.\n\n\\textit{Reconstruction-based methods} are the most widely used methodology not only in image anomaly detection but also in video violation detection \\cite{sabokrou2017deepcascade, ergen2019unsupervised, luo2017stc}. These methods encode and reconstruct the normal data via generative models such as AutoEncoders (AEs) \\cite{zhou2017anoae, akcay2018ganomaly,liu2020vevqe, zhou2021memorizing} and generative adversarial networks (GANs) \\cite{perera2019ocgan}. The anomalies are distinguished based on the high reconstruction error assuming that the generative models trained on normal data perform poorly on the anomalies. However, such an assumption is invalid because the models trained with the strict MSE loss focus on pixel reconstruction and generalize to anomalies. To address this problem, there are some extensions based on weak restrictions like SSIM loss \\cite{bergmann2018improving} or cosine similarity loss \\cite{deng2022distillation}, generality degradation by memory modules \\cite{gong2019memorizing} or codebook \\cite{marimont2021anomaly}, and self-supervision such as rotation prediction \\cite{hendrycks2019using}, transformed image restoration \\cite{fei2020attribute} and mask inpainting \\cite{zavrtanik2021riad}. Recently, some works \\cite{li2021cutpaste, zavrtanik2021draem} synthesize pseudo defects on the anomaly-free data and train the reconstruction networks in a supervised way to boost the performance of anomaly localization. However, there is still no guarantee that the trained reconstructors only reconstruct the normal regions but not the defective regions. Besides, the reconstruction-based method can only detect the damage defects like cracks, scratches, or dents but not structural deformations like the \\textit{misplaced transistor} in \\ref{fig:defect-examples}.\n\n\\textit{Clustering-based methods} \\cite{liu2021anomaly} build a reference gallery of normal data representations and then detect the anomalies based on the similarity with the reference gallery. \\cite{ruff2018deepsvdd} use a deep neural network to learn the discriminative representations of normal data by SVM. However, most recent works utilize the feature extractors \\cite{he2016resnet,zagoruyko2016wide,xie2017resnext} pre-trained on the large-scale external datasets \\cite{deng2009imagenet} to estimate the representative features. Based on such extracted representations, $k$ nearest neighbors clustering \\cite{bergman2020deepknn,reiss2021panda}, memory banks \\cite{cohen2020spade} and bag-of-features approach \\cite{defard2021padim} are used to build the reference gallery on normal data. The previous SOTA on the MVTec AD benchmark PatchCore \\cite{roth2021patchcore} employs the kNN algorithm on patch-wise features to boost inference efficiency and constrain generalization to anomalies before building the memory bank. Nonetheless, the inference speed of clustering-based methods is still slow due to the time-consuming kNN algorithm as post-processing. Similar to reconstruct-based methods, the methods of this category also perform inferiorly on structural deformations.\n\n\n\n\n"
                },
                "subsection 2.2": {
                    "name": "Normalizing Flows",
                    "content": "\nCompared with the well-known generative models VAEs \\cite{kingma2013vaes} and GANs \\cite{goodfellow2014gans}, the normalizing flows are inconspicuous but powerful. With the invertible property, the normalizing flows can transform any complex distribution into a tractable base distribution like the Gaussian distribution. To satisfy the invertibility of flow blocks, diverse implementations such as the autoregressive flow \\cite{germain2015made} and the reverse autoregressive flow \\cite{kingma2016improved} are proposed. Nevertheless, these two implementations are only effective in a single direction but costly in the other direction. The widespread Real-NVP \\cite{dinh2016realnvp} implements the normalizing flows based on the affine coupling layers, which are efficient in both directions. Following the Real-NVP, \\cite{kingma2018glow} further simplifies the flow architecture by introducing the invertible $1\\times1$ convolutions to generalize any channel-wise permutation. They construct a large-scale Glow with numerous flow blocks for high-quality human face generation, achieving comparable results to GANs.\n\nIn addition to probabilistic modeling like image generation, the normalizing flows perform well in exact density estimation \\cite{germain2015made,dinh2016realnvp} with invertibility. The training objective of normalizing flows for density estimation requires no annotation but only one pre-defined base distribution, commonly set as the Gaussian distribution. Therefore, the normalizing flows can be easily extended to UAD with their capability of explicitly estimating the probabilistic density of the normal data.\n\n"
                },
                "subsection 2.3": {
                    "name": "Flow-based Methods for UAD",
                    "content": " \nFlow-based methods have recently arisen in UAD, and our method belongs to this emerged methodology.\nThe methods of this category utilize the powerful normalizing flows to estimate the normal data's density, and the unseen anomalies are assigned with low likelihoods \\cite{serra2019input}. \nHowever, \\cite{kirichenko2020normalizing} reveals that the flow models trained on raw RGB images often assign even higher likelihoods to anomalies than the normal data. This puzzling result can be settled by applying the flows to the high-dimensional features instead of the raw image pixels.\nWith this revelation, DifferNet \\cite{rudolph2021differnet} implements normalizing flows for the image-wise anomaly detection based on the extracted features.\nTo handle the defect size variation, DifferNet rescales the images to 3 scales and extracts the multi-scale feature maps through the same extractor. CFlow-AD \\cite{rudolph2021differnet} builds its multi-scale feature pyramid with feature maps of different levels for efficiency and various receptive fields. Besides, CFlow-AD extends the normalizing flows to the pixel-wise anomaly localization by estimating the likelihood for the feature vector of each position.\n\nHowever, CFlow-AD separates each position's feature vector and estimates its likelihood independently, neglecting spatial contextual information. The lack of context-awareness harms the global perception and suffers disconnected localization results. To remedy that, our MSFlow estimates the features in all positions parallelly and builds the flows with $3\\times3$ convolutions to automatically learn the contextual information.\n\\rounda{\nThe detection-only method CSFlow \\cite{rudolph2022csflow} develops a fully-convolutional cross-scale flow module to jointly process multi-scale feature maps. To take full advantage of the multi-scale perceptions, we build a fusion flow module inspired by the lightweight module\\cite{yu2021lite} to exchange information across different scales. Our fusion flow plays the same role as the cross-scale flow module in CSFlow while is much more efficient in information exchange. }\n\n\n\n"
                }
            },
            "section 3": {
                "name": "Preliminary",
                "content": " \\label{sec:theory-background}\nBefore presenting our method, we first introduce the theoretical background of the normalizing flows and the most common implementation: the normalizing flows based on affine coupling layers.\n\nUnlike the widely-used generative models including GANs \\cite{goodfellow2014generative, esser2021taming} and VAEs \\cite{kingma2013vaes, razavi2019generating}, which learn the data distribution by a proxy adversarial task or maximizing the ELBO, the flow models explicitly estimate arbitrary data distribution through the bijective invertible normalizing flows. \nThe flow model $f$ can transform arbitrary complex distribution $p_X(\\boldsymbol{x})$ to a tractable base distribution $p_Z(\\boldsymbol{z})$.\nThus, a bijection is built between the data sample $\\boldsymbol{x}$ and the $\\boldsymbol{z}$ through the flow model $f$ comprising a chain of $K$ flow blocks $\\{f_1, f_2, ..., f_K\\}$:\n\\begin{equation}\n  \\begin{aligned}\n    \\boldsymbol{x}=f(\\boldsymbol{z})=& f_K \\circ f_{K-1} \\circ ... \\circ f_1(\\boldsymbol{z}); \\\\\n    \\boldsymbol{z}=f^{-1}(\\boldsymbol{x})=& {f_1}^{-1} \\circ {f_2}^{-1} \\circ ... \\circ {f_K}^{-1}(\\boldsymbol{x}),\n  \\end{aligned}\n\\end{equation}\nwhere $\\circ$ denotes the cascade of flow blocks.\nWith the invertible property of the normalizing flows, the likelihood of the input data $\\boldsymbol{x}$ can be estimated by a change of variables \\cite{halmos2013measure}:\n\\begin{equation}\n  p_X(\\boldsymbol{x})=p_Z(\\boldsymbol{z})\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{x}) \\right\\rvert,\n  \\, \\textrm{where} \\, J_{f^{-1}}(\\boldsymbol{x})= \\frac{\\partial f^{-1}}{\\partial \\boldsymbol{x}}.\n\\end{equation}\n$J_{f^{-1}}(\\boldsymbol{x})$ is the Jacobian of the reversed normalizing flows $f^{-1}$, and $\\left\\lvert\\textrm{det}(J)\\right\\rvert$ denotes the absolute determinant of the Jacobian matrix $J$.\n\\rounda{Therefore, the optimization target of maximizing the likelihood of the implicit distribution $p_X(\\boldsymbol{x})$ can be equivalently transformed to minimizing the negative log-likelihood $- \\textrm{log}p_Z(\\boldsymbol{z})$ of the accessible distribution $p_Z(\\boldsymbol{z})$ along with the Jacobian determinant as follows:\n\\begin{equation}\n  \\begin{aligned}\n  \\max p_X(\\boldsymbol{x}) &\\Leftrightarrow \\min \\, - \\textrm{log} \\,p_X(\\boldsymbol{x}) \\\\\n  &\\Leftrightarrow \\min - \\textrm{log}\\,p_Z(\\boldsymbol{z}) - \\textrm{log}\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{x}) \\right\\rvert.\n  \\end{aligned}\n\\end{equation}\nFor simplicity, the base distribution p$_Z(\\boldsymbol{z})$ is commonly selected as a multivariate Gaussian distribution. The corresponding optimization target is formulated as follows:\n\\begin{equation} \\label{equation:loss-function}\n  \\max p_X(\\boldsymbol{x}) \\Leftrightarrow \\min \\frac{\\left\\lVert \\boldsymbol{z} \\right\\rVert_2^2}{2} - \\textrm{log}\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{x}) \\right\\rvert.\n\\end{equation}\n}\n\nAccording to the above, the flow block $f_i$ in the normalizing flows $f$ should satisfy two basic properties: 1) it can be easily invertible, and 2) its Jacobian determinant is computationally efficient.\n% \\subsubsection{\\textbf{Affine coupling layers}}\nThere are diverse implementations \\cite{germain2015made, van2016pixel, van2016wavenet, papamakarios2017masked, kingma2016improved} of normalizing flows, where the normalizing flows based on affine coupling layers are flexible and most widely used \\cite{dinh2014nice, dinh2016realnvp,kingma2018glow,kirichenko2020normalizing}. % TIP reference of NF\nIn the normalizing flows of this type, as illustrated in Fig. \\ref{fig:fusion-block}, the bijective flow block $f_\\textrm{aff}$ is conveniently implemented by the affine coupling layer as follows:\n\\begin{equation}\n  \\begin{aligned}\n  \\textrm{forward:}&\n  \\begin{cases}\n  \\boldsymbol{x}_{1:d}  &= \\boldsymbol{z}_{1:d}, \\\\\n  \\boldsymbol{x}_{d+1:D}&= \\boldsymbol{z}_{d+1:D} \\odot \\textrm{exp}(s(\\boldsymbol{z}_{1:d})) +t(\\boldsymbol{z}_{1:d});\n  \\end{cases}\n  \\\\\n  \\textrm{reverse:}&\n  \\begin{cases}\n  \\boldsymbol{z}_{1:d}  &= \\boldsymbol{x}_{1:d},\\\\\n  \\boldsymbol{z}_{d+1:D}&= (\\boldsymbol{x}_{d+1:D}-t(\\boldsymbol{x}_{1:d}))\\odot \\textrm{exp}(-s(\\boldsymbol{x}_{1:d})). \\nonumber\n  \\end{cases}\n  \\end{aligned}\n\\end{equation}\n$\\boldsymbol{z}_{1:d}$ and $\\boldsymbol{z}_{d+1:D}$ are halved parts of $z$ with channel dimensions $D$, $\\boldsymbol{x}_{1:d}$ and $\\boldsymbol{x}_{d+1:D}$ are two corresponding parts of $x$. The $s(\\cdot )$ and $t(\\cdot )$ functions are commonly implemented by the neural network (st-network for short), yielding the \\textit{scale} weights $\\boldsymbol{w}_s$ and \\textit{shift} $\\boldsymbol{w}_t$ weights. \n\nWith the ingenious design of the affine coupling layer, its reversed Jacobian determinant can be easily calculated as follows:\n\\begin{equation}\n  \\textrm{log}\\left\\lvert \\textrm{det}J_{{f_\\textrm{aff}}^{-1}}(\\boldsymbol{x}) \\right\\rvert = \\sum s(\\boldsymbol{x}_{1:d}).\n\\end{equation}\nOur MSFlow also adopts such flexible normalizing flows and makes adaptive adjustments on the st-network according to the particularity of industrial defect detection with localization.\n\n"
            },
            "section 4": {
                "name": "Proposed Method",
                "content": " \\label{sec:method}\nAs shown in Fig. \\ref{fig:framework}, we first extract the multi-scale feature maps in the first $3$ stages as the input of the normalizing flows. \nThe extracted multi-scale feature maps are then fed into their respective series of fully-convolutional parallel flows, followed by a fusion flow to exchange information of different scales and perceptive fields.\nFinally, the multi-scale likelihood maps are aggregated by different aggregation strategies for the discrepancy between image-wise anomaly detection and pixel-wise anomaly localization. The mean of the top$K$ scores in the entire anomaly score map is treated as the image-wise anomaly score.\n\n% \\subsection{Preliminary Knowledge}\n\\rounda{\n\\subsection{Symbol Defintion}\nWe first introduce the formal definition of mathematical symbols used in the following statements. The bold lowercase letters $\\boldsymbol{x}$, $\\boldsymbol{y}$, $\\boldsymbol{h}$, and $\\boldsymbol{z}$ respectively refer to the input image, outputs of the extractor, the inputs and outputs of the flow model.\nThe normal lowercase letters $f$ and $g$ refer to the flow module and common network.\n$st$ denotes the st-network in flow modules.\nParticularly, the superscripts indicate the attributes like p(arallel) and fuse, and subscripts indicate the branch number.}\n\n",
                "subsection 4.1": {
                    "name": "Feature Extraction",
                    "content": " \\label{sec:feature-extraction}\n% \\ref{fig:feature-comparison} % image vs. low-level features vs. high-level features\n% \u4f7f\u7528feature \u800c\u4e0d\u662f\u4f7f\u7528 image\u4f5c\u4e3a\u8f93\u5165\nAs claimed in \\cite{kirichenko2020normalizing}, the normalizing flows trained on raw RGB images learn latent representations largely based on local pixel correlations. Such flow models simultaneously increase likelihood for both normal and anomalous images, which defames the distinguishing effectiveness of anomaly detection. \n% Therefore, we feed the feature maps extracted by the pre-trained feature extractors \\cite{he2016resnet, zagoruyko2016wide} into the flow model.\n%  unlike the generative flow-based models \\cite{dinh2014nice, dinh2016realnvp, kingma2018glow} that directly take the raw RGB images as the input. \nMoreover, the extracted features imbued with the prior knowledge of the large-scale dataset, such as ImageNet \\cite{deng2009imagenet}, are more representative than the simple RGB values in the raw images.\nTherefore, we propagate the feature maps extracted by the pre-trained feature extractors \\cite{he2016resnet, zagoruyko2016wide} into the flow model.\n\nSince the sizes of anomalies are varied dramatically, we extract multi-scale feature maps as the inputs of the flow model to generalize the anomaly size variation.\nExisting detection-only flow-based detection-only methods \\cite{rudolph2021differnet,rudolph2022csflow} extract the multi-scale image featurized pyramid upon the image pyramid by an identical extractor. Although the image featurized pyramid involves multi-scale perceptions, all feature maps share the same perceptive field and semantic.\nDifferent from them, we only forward the feature extractor on input images once and sample different stages' last feature maps to construct our multi-scale feature pyramid. The sampled feature maps of these stages are denoted as $\\{\\boldsymbol{h}_i \\in \\mathbb{R}^{D_i \\times H_i \\times W_i}\\}_{i=1}^L$, where $L$ is the number of the sample stage, and $L$ is set $3$ like other common methods \\cite{gudovskiy2022cflow,rudolph2022csflow}.\n\\roundb{Consequently, we can formulate our multi-scale feature extraction as follows:}\n\\begin{equation}\n  (\\boldsymbol{h}_1, \\boldsymbol{h}_2, \\boldsymbol{h}_3) = E(\\boldsymbol{x}),\n\\end{equation}\nwhere $E(\\cdot)$ denotes the \\roundb{pre-trained feature extractor following multi-stage architecture design}.\n\nNotably, we use the feature maps of the first three stages $\\{\\boldsymbol{h}_1, \\boldsymbol{h}_2, \\boldsymbol{h}_3\\}$ rather than those of the last three \\cite{gudovskiy2022cflow} to build our feature pyramid.\nCompared with the feature maps of the last $3$ stages with high-level semantic information, the first $3$ stages' feature maps contain more spatial details. \nNo matter for the pixel-wise anomaly localization that naturally focuses on spatial structure instead of semantic comprehension or the image-wise anomaly detection that highly depends on the local anomalous regions, the feature maps with abundant details are more applicable.\n\nThe low-level feature maps with high resolutions preserve structural details while introducing a high computational cost for the following flow model. To handle the heavy computational burden, we simply downsample them through the average pooling with kernel size 3 and stride 2 to build our feature pyramid $\\{\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\boldsymbol{y}_3\\}$ as follows:\n\\begin{equation}\n \\boldsymbol{y}_i = \\textrm{AvgPooling}(\\boldsymbol{h}_i), i = 1,2,3,\n\\end{equation}\nwhere $\\boldsymbol{y}_i \\in \\mathbb{R}^{D_i \\times \\frac{H_i}{2} \\times \\frac{W_i}{2}}$.\nMoreover, the weights of the pre-trained feature extractor are frozen and not updated during the training schedule.\n% When we employ ResNet \\cite{he2016resnet} as the feature extractor, the channel dimensions of our feature pyramid are only half of the ones in \\cite{gudovskiy2022cflow} while the spatial sizes are the same, which sharply cut the computational cost of the adjacent normalizing flows in half.\n\n"
                },
                "subsection 4.2": {
                    "name": "Multi-scale Normalizing Flows",
                    "content": "\nThe proposed multi-scale flow model in our MSFlow is composed of two parts: \n1) asymmetrical parallel flows for each feature map in the feature pyramid built as Section \\ref{sec:feature-extraction} to encode their intrinsic properties;\n2) a fusion flow to exchange the global and detailed information of multi-scale feature pyramid.\n\nIn the first part, each branch of parallel flows $f_{i}^\\textrm{p}$ comprising $k_{i}$ flow blocks respectively encode the extracted feature map $\\boldsymbol{y}_i$ and transform it to the latent feature map $\\boldsymbol{z}_i^\\textrm{p}=f_{i}^\\textrm{p}(\\boldsymbol{y}_i)$. \nIn particular, our parallel flows adopt an asymmetrical architecture as shown in Fig. \\ref{fig:framework}, where $k_{i+1} > k_{i}$.\nOur parallel flows build the st-network $st_\\textrm{p}(\\cdot )$ with the $3\\times3$ convolutions, which automatically capture the spatial contextual information neglected in \\cite{gudovskiy2022cflow}.\nFor details, the $st_\\textrm{p}(\\cdot )$ networks in all parallel flows share the same architecture:\n% For details, the $s_\\textrm{p}(\\cdot )$ and $t_\\textrm{p}(\\cdot )$ networks in all parallel flows share the same architecture:\n\\rounda{\\begin{equation} \\label{equation:st-network}\n  % s_\\textrm{p}(\\cdot ) = t_\\textrm{p}(\\cdot ) = \n  st_\\textrm{p}(\\cdot ) = \n  \\textrm{Conv3} \\circ \\textrm{ReLU} \\circ \\textrm{LN} \\circ \\textrm{Conv3} (\\cdot ),\n\\end{equation}}\nwhere Conv3 refers to the $3 \\times 3$ convolution, ReLU is the ReLU non-linear activation, and LN denotes the layer normalization \\cite{ba2016layer}.\n\\rounda{\nParticularly, the hidden dimension between two $3 \\times 3$ convolutions is set identical to the input dimension of the first one $3 \\times 3$ convolution for simplicity.}\n% two cascaded convolutional layers with the ReLU activation inserted between them. \nThe additional layer normalization not only stabilizes the optimization but also improves the performance of anomaly detection with the statistic on anomaly-free training data \\cite{song2019unsupervised}.\n%  Our $s(\\cdot )$ and $t(\\cdot )$ can be formulated as follows:\n% \\begin{equation} \\label{equation:st-network}\n%   s(\\cdot ) = t(\\cdot ) = \n%   \\textrm{Conv3} \\circ \\textrm{ReLU} \\circ \\textrm{LN} \\circ \\textrm{Conv3} (\\cdot ) ,\n% \\end{equation}\n% where $\\textrm{Conv3}$ refers to the convolution with kernel size (3,3).\nBesides, positional encodings \\cite{vaswani2017attention} are inserted into parallel flow blocks as the condition. Although the CNN layers capture the global perception, they fail to perceive the absolute position of each feature vector. For anomaly detection, especially industrial defect detection, where all industrial parts share the same photography pose, the perception of the absolute position is conducive to this specific task.\n\n\n\n\\rounda{\nA fusion flow $f_\\textrm{fuse}$ is further attached to the encoded feature pyramid\n$\\{\\boldsymbol{z}_i^\\textrm{p}\\}_{i=1}^3$\nand estimates the final multi-scale outputs\n$\\{\\boldsymbol{z}_i^\\textrm{fuse}\\}_{i=1}^3$.\nThe proposed fusion flow $f_\\textrm{fuse}$ efficiently exploits multi-scale perception by exchanging information captured on different scales.\nUnlike the common flow blocks \\cite{dinh2016realnvp,kingma2018glow} that only process on a single feature map, our fusion flow estimates the multi-scale feature maps simultaneously and fuses their information. \nParticularly, a fusion network $g^\\textrm{fuse}$ first fuses the multi-scale inputs $\\{\\boldsymbol{a}_i\\}_{i=1}^3$ and outputs fused feature maps $\\{\\boldsymbol{a}_i^\\textrm{fuse}\\}_{i=1}^3$ before learning the $scale$ weights $\\boldsymbol{w}^\\textrm{s}_i$ and $shift$ weights $\\boldsymbol{w}^\\textrm{t}_i$ in the st-network of our fusion flow. Such an st-network is formulated as follows:\n\\begin{equation}\n  \\begin{aligned}\n    (\\boldsymbol{a}_1^\\textrm{fuse}&, \\boldsymbol{a}_2^\\textrm{fuse}, \\boldsymbol{a}_3^\\textrm{fuse}) = g^\\textrm{fuse}(\\boldsymbol{a}_1, \\boldsymbol{a}_2, \\boldsymbol{a}_3); \\\\\n    \\boldsymbol{w}^\\textrm{s}_i &= g_i^\\textrm{s}(\\boldsymbol{a}_i^\\textrm{fuse}), \\textrm{where}\\, i =1, 2, 3; \\\\\n    \\boldsymbol{w}^\\textrm{t}_i &= g_i^\\textrm{t}(\\boldsymbol{a}_i^\\textrm{fuse}), \\textrm{where}\\, i =1, 2, 3.\n  \\end{aligned}\n\\end{equation}\n% Unlike the common st-network \\cite{dinh2016realnvp,kingma2018glow} fed with a single feature map, our cross-scale st-network takes $\\{\\boldsymbol{x}_i\\}_{i=1}^3$ as the inputs of the fusion network $g^\\textrm{fuse}$ to exchange information. \nInspired by the lightweight block \\cite{yu2021lite} for high-resolution pose estimation, the fusion network $g^\\textrm{fuse}$ in our fusion flow $f_\\textrm{fuse}$ is designed as illustrated in Fig. \\ref{fig:fuse-network}.\nThe fully-convolutional $g^\\textrm{fuse}$ provides a bridge for multi-scale feature maps to exchange information across all scales. \nThe $g^\\textrm{fuse}$ first shrinks all multi-scale inputs $\\{\\boldsymbol{a}_1,\\boldsymbol{a}_2,\\boldsymbol{a}_3\\}$ to the minimum size of them (size of $\\boldsymbol{a}_3$) by the average pooling and concatenate them together. The concatenated features are fused by two convolutional layers with a similar structure as Eq. \\ref{equation:st-network}, where the middle channels are narrowed by dividing 4. The fused single-scale feature map is split along the channel dimension and rescaled to multi-scale again. Finally, the fused multi-scale feature maps are added with their respective inputs to get the fused outputs $\\{\\boldsymbol{x}_i^\\textrm{fuse}\\}_{i=1}^3$.\nBased on the fused feature pyramid that receives the multi-scale perception, $3$ pairs of $3\\times3$ convolutions $g_i^\\textrm{s}$ and $g_i^\\textrm{t}$ independently learn the \\textit{scale} and \\textit{shift} weights $\\boldsymbol{w}^\\textrm{s}_i$ and $\\boldsymbol{w}^\\textrm{t}_i$ for each scale.\n}\n\n"
                },
                "subsection 4.3": {
                    "name": "Learning Objective",
                    "content": "\nAs introduced in Section \\ref{sec:theory-background}, the optimization target of the normalizing flows is uniform and simple. Because we employ the feature extracted maps $\\{\\boldsymbol{y}_i\\}_{i=1}^{3}$ as the input instead of the raw RGB image $\\boldsymbol{x}$, the optimization target shifts to the maximization of the likelihood $p_Y(\\boldsymbol{y})$ in the feature space $Y$.\nThe proposed MSFlow is composed of several flow submodels: 3 parallel flows $\\{f_{i}^\\textrm{p}\\}_{i=1}^3$ and one fusion flow $f_\\textrm{fuse}$, which are summarized as follows:\n\\begin{equation}\n  \\begin{aligned}\n    \\boldsymbol{z}_i^\\textrm{p}&=f_{i}^\\textrm{p}(\\boldsymbol{y}_i), \\, \\textrm{where} \\,\\, i=1,2,3;\\\\ \n    (\\boldsymbol{z}_1^\\textrm{fuse}&,\\boldsymbol{z}_2^\\textrm{fuse}, \\boldsymbol{z}_3^\\textrm{fuse}) = f_\\textrm{fuse}(\\boldsymbol{z}_1, \\boldsymbol{z}_2, \\boldsymbol{z}_3).\n  \\end{aligned}\n\\end{equation}\nAccordingly, there are 4 Jacobian determinants in our model: $\\{ \\left\\lvert \\textrm{det}J_{{f_{i}^\\textrm{p}}^{-1}}\\right\\rvert \\}_{i=1}^3$ and $\\left\\lvert \\textrm{det}J_{{f_\\textrm{fuse}}^{-1}} \\right\\rvert$. \n\\rounda{Correspondingly, the global learning objective $\\mathcal{L}$ can be formulated as the minimization of the negative log-likelihood $-\\textrm{log}p_Y(\\boldsymbol{y})$:\n\\begin{align} \n    \\mathcal{L} = & -\\textrm{log}\\,p_Y(\\boldsymbol{y})  \\\\\n    & \\quad \\,\\,\\,\\, \\frac{\\left\\lVert \\boldsymbol{z} \\right\\rVert_2^2}{2}  \\quad \\,\\,\\,- \\, \\quad  \\qquad  \\quad   \\textrm{log}\\left\\lvert \\textrm{det}J_{f^{-1}} \\right\\rvert  \\notag \\\\\n    = &\\overbrace{\\sum_{i = 1}^{3} \\frac{\\left\\lVert \\boldsymbol{z}_i^\\textrm{fuse} \\right\\rVert_2^2}{2}}\n    - \\overbrace{\\left[  \\sum_{i = 1}^{3} \\textrm{log}\\left\\lvert \\textrm{det}J_{{f_{i}^\\textrm{p}}^{-1}} \\right\\rvert \n    + \\textrm{log}\\left\\lvert \\textrm{det}J_{f_\\textrm{fuse}^{-1}}\\right\\rvert  \\right]}, \\notag \\label{equation:global-loss}\n\\end{align}\nwhere $f$ refers to the whole flow model containing all parallel flows $\\{f_{i}^\\textrm{p}\\}_{i=1}^3$ and the fusion flow $f_\\textrm{fuse}$. \n% In addition to the format similar to Eq. \\ref{equation:loss-function} from a global perspective, it can also be understood from the perspective of each feature layer\nFrom the global perspective, our learning objective $\\mathcal{L}$ is consistent with the Eq. \\ref{equation:loss-function}. }\n% Besides, it can be transformed into a level-wise format as follows:\n% \\begin{align} \n%   % \\mathcal{L} = & -\\textrm{log}\\,p_Y(\\boldsymbol{y}) \\notag \\\\\n%   % & \\quad \\,\\,\\,\\, \\frac{\\left\\lVert \\boldsymbol{z} \\right\\rVert_2^2}{2}  \\qquad  \\qquad  \\qquad  \\quad   \\textrm{log}\\left\\lvert \\textrm{det}J_{f^{-1}} \\right\\rvert  \\notag \\\\\n%   % = &\\overbrace{\\sum_{i = 1}^{L} \\frac{\\left\\lVert \\boldsymbol{z}_i^\\textrm{fuse} \\right\\rVert_2^2}{2}}\n%   % - \\overbrace{\\left[  \\sum_{i = 1}^{L} \\textrm{log}\\left\\lvert \\textrm{det}J_{{f_{i}^\\textrm{p}}^{-1}} \\right\\rvert \n%   % + \\textrm{log}\\left\\lvert \\textrm{det}J_{f_\\textrm{fuse}^{-1}}\\right\\rvert  \\right]} \\label{equation:global-loss} \\\\\n%   \\mathcal{L} = &\\sum_{i = 1}^{L} -\\textrm{log}\\,p_Y(\\boldsymbol{y}_i)  \\notag \\\\\n%   = &\\sum_{i = 1}^{L} \\left[ \\frac{\\left\\lVert \\boldsymbol{z}_i^\\textrm{fuse} \\right\\rVert_2^2}{2} - \n%   (\\textrm{log}\\left\\lvert \\textrm{det}J_{{f_{i}^\\textrm{p}}^{-1}} \\right\\rvert\n%   + \\frac{\\textrm{log}\\left\\lvert \\textrm{det}J_{f_\\textrm{fuse}^{-1}}\\right\\rvert}{L}) \\right]. \\label{equation:level-loss}\n% \\end{align}\n% In such level-wise learning objectives, the Jacobian determinant of the fusion flow is equally divided into each level because \n\n\n"
                },
                "subsection 4.4": {
                    "name": "Anomaly Score Calculations",
                    "content": " \\label{sec:aggregation}\nAfter training on the anomaly-free images, our MSFlow can transform the feature distribution of normal data to the base distribution. According to the Eq. \\ref{equation:loss-function} the pixel-wise log-likelihood $\\textrm{log} \\,\\hat{p}_Y (\\boldsymbol{y}_{ijk})$ located at $(j, k)$ on the $i$-th feature map $\\boldsymbol{y}_i$ can be estimated as:\n\\begin{equation}\n  \\textrm{log}\\,\\hat{p}_Y (\\boldsymbol{y}_{ijk}) = -\\frac{\\left\\lVert \\boldsymbol{z}_{ijk} \\right\\rVert_2^2}{2} + \\textrm{log}\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{y}_{ijk})\\right\\rvert.\n\\end{equation}\nUnlike \\cite{gudovskiy2022cflow} which independently estimates the likelihood of the features $\\boldsymbol{y}_{ijk}$ at each position, we globally estimate the entire feature map's likelihood and fail to calculate the pixel-wise Jacobian determinant $\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{y}_{ijk}) \\right\\rvert$. Instead, we directly compute the anomaly score based on the log-likelihood $\\textrm{log}\\,\\hat{p}_Z (\\boldsymbol{z}_{ijk})= -\\frac{\\left\\lVert \\boldsymbol{z}_{ijk} \\right\\rVert_2^2}{2}$, which is the one-to-one mapping of $\\textrm{log}\\,\\hat{p}_Y (\\boldsymbol{y}_{ijk})$.\n\nBefore aggregation, the multi-scale log-likelihood maps are first upsampled to the input image size $(H_{img}, W_{img})$ by the bilinear interpolating. Different aggregation strategies are employed for pixel-wise anomaly localization and image-wise anomaly detection. For the pixel-wise anomaly localization, the rescaled log-likelihood maps $\\{\\textrm{log}\\,\\hat{P}_i\\}_{i=1}^3$ are converted to multi-level probability maps $\\{\\hat{P}_i = e^{\\textrm{log}\\,\\hat{P}_i} \\}_{i=1}^3$, which are further aggregated through the element-wise addition as follows:\n\\rounda{\n\\begin{equation}\n  P_{add} = \\sum_{i = 1}^{3} e^{\\textrm{log}\\,\\hat{P}_i} = \\sum_{i = 1}^{3} \\hat{P}_i.\n\\end{equation}}\nDifferently, the rescaled log-likelihood maps $\\{\\textrm{log}\\,\\hat{P}_i\\}_{i=1}^3$ are summed first and then converted to the probability map $P_{mul}$ for the image-wise anomaly detection. It can also be viewed as the multiplication of multi-level probability maps $\\{\\hat{P}_i \\}_{i=1}^3$ as follows:\n\\rounda{\n\\begin{equation}\n  P_{mul} = e^{\\sum_{i = 1}^{3} \\textrm{log}\\,\\hat{P}_i} = \\prod_{i = 1}^{3} e^{\\textrm{log}\\,\\hat{P}_i} = \\prod_{i = 1}^{3} \\hat{P}_i.\n\\end{equation}}\nThe multiplication that suppresses the noise of a certain layer is more suitable for the global anomaly score calculation. However, pixel-wise anomaly localization benefits from the addition aggregation that preserves multi-scale information for various scale anomalies.\nThe final pixel-wise anomaly score map $S_{loc}$ and the image-wise anomaly score $s_{det}$ are respectively calculated as follows:\n\\begin{align}\n  S_{loc} &= \\textrm{max}(P_{add}) - P_{add}; \\\\\n  S_{mul} &= \\textrm{max}(P_{mul})-P_{mul}, \\notag \\\\\n  s_{det} &= \\frac{1}{K} \\sum_{1}^K \\textrm{top}K(S_{mul}),\n\\end{align}\nwhere the max$(\\cdot)$ and top$K(\\cdot)$ respectively sample the maximum and the largest $K$ values.\nThe previous methods either take the maximum \\cite{gudovskiy2022cflow, zavrtanik2021riad} or the mean \\cite{rudolph2022csflow,rudolph2021differnet} in the pixel-wise anomaly score map as the global anomaly score. The former as an image-wise anomaly score is too sensitive to the noise, the latter is robust while imperceptible to small anomalous regions. \nTo compensate for their individual drawbacks, we combine them together and take the mean of the top$K$ values as the global image-wise anomaly score.\n\\rounda{\nDifferent to DevNet \\cite{pang2019devnet} sampling top$K$ feature values along channel dimension, our MSFlow takes average on the top$K$ pixel-wise anomaly in the spatial dimensions.}\nThe maximum and mean are the special cases of our solution when $K$ is set to $1$ or $H_{img} \\times W_{img}$.\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Experiment",
                "content": " \\label{sec:experiment}\nWe evaluate the MSFlow on the two image anomaly detection datasets as well as one video violation detection dataset and present the comparisons with existing methods. We also perform comprehensive ablation studies to investigate the effectiveness of each proposed component in our MSFlow.\n\n",
                "subsection 5.1": {
                    "name": "Experimental Settings",
                    "content": "\n",
                    "subsubsection 5.1.1": {
                        "name": "Datasets",
                        "content": "\nTo highlight the superiority and generality of our method, we mainly conduct performance comparisons with other methods on two image anomaly detection datasets and a video violation detection dataset. For all these datasets, no data augmentation except resizing is applied.\n\n\\textit{MVTec Anomaly Detection (MVTec)}\\cite{bergmann2019mvtec}: The widely-used MVTec AD benchmark contains 15 industrial product classes, where 10 objects and 5 textures. \nThere are total 3,629 images for training and 1,725 images for testing, and the train set size varies and ranges from 60 for \\textit{toothbrush} to 391 for \\textit{screw} for each class. \nOnly defect-free samples are included in the train set, the test set includes both defect-free samples and anomalous samples of various defective types (up to 8 for \\textit{cable}). \nThe images in the MVTec AD benchmark are high-quality with resolutions from $700\\times700$ to $1024 \\time 1024$. We resize images of all classes except the \\textit{transistor} to $512\\times512$, and $256 \\times 256$ for \\textit{transistor} whose defective size of misplaced samples is large up to 42\\% average.\n\n% We also compare our method with others on the \\eat{\\textit{BeanTech Anomaly Detection (BTAD)} dataset \\cite{mishra2021btad} and} \\textit{Magnetic Tile Defects (MTD)} dataset \\cite{huang2020mtd}. \n\\textit{Magnetic Tile Defects (MTD)} \\cite{huang2020mtd}:\n% The \\textit{BTAD} dataset comprises 3 types of industrial products with 400, 1000, and 399 high-resolution defect-free images respectively for training. For consistency, all images are resized to $512\\times512$ as well before being fed into the neural network.\nThe MTD dataset is specialized for \\textit{tile} with 952 defect-free images and 382 defective images. For a fair comparison, we use 80\\% defect-free images for training and the rest are used for testing along with all defective images. In particular, the images in the MTD dataset are resized to $192\\times192$ since the images are low-resolution.\n\n\\textit{mini Shanghai Tech Campus (mSTC)}:\nBesides image anomaly detection datasets, we also extend our method to the video violation detection dataset mSTC \\cite{defard2021padim, roth2021patchcore}. The mSTC dataset comprises every fifth frame sampled from the training and test videos in the standard STC dataset \\cite{luo2017stc}. The videos for training only contain normal behaviors while the test set includes diverse violations such as chasing and brawling. All sampled frames are resized to $256\\times384$ during both training and testing.\n\n\n\n"
                    },
                    "subsubsection 5.1.2": {
                        "name": "Evaluation Metrics",
                        "content": "\nFollowing \\cite{bergmann2019mvtec}, the performance of both image-wise anomaly detection and pixel-wise anomaly localization are evaluated through the area under the receiver-operator curve (AUCROC) for each class. Furthermore, we compute the class-average AUCROC score as the result on the whole dataset for the multi-class datasets: the MVTec AD and mSTC datasets. Particularly on the challenging MVTec AD benchmark, we also measure the per-region-overlap (PRO) score \\cite{bergmann2020uninformed} for pixel-wise anomaly localization. \n\\rounda{The PRO curve is calculated as the overlap between each connected region within the ground truth mask. The PRO score is the area under the PRO curve, when an average per-pixel false-positive rate of $30\\%$ for the entire dataset is reached.}\nThe PRO score treats the defects with different sizes equally and attaches importance to the connectivity of localization results, which can be viewed as the region-wise AUCROC score.\n\n"
                    },
                    "subsubsection 5.1.3": {
                        "name": "Implementation Details",
                        "content": "\nFor a fair comparison with existing methods, we utilize the most widely-used WideResNet-50 (WRN-50) \\cite{zagoruyko2016wide} and ResNet-18 \\cite{he2016resnet} as the feature extractors.\nBefore feeding into our parallel flows, the activation outputs of the first 3 stages in these two feature extractors are downsampled by the average pooling for efficiency.\nOur parallel flows cascade more flow blocks for the higher-level feature map with more channels. Specifically, 2, 5, and 8 fully-convolutional flow blocks are respectively stacked for the feature maps of stage1, stage2, and stage3. In all flow blocks of these three parallel flows, the 2D positional encodings with channels=64 are inserted to capture the absolute position. For information exchange, a fusion flow is attached to fuse the outputs of our three parallel flows. \nOur flow model is trained from scratch on one 2080Ti GPU card with a batch size of 16. For optimization, we adopt the Adam optimizer with an initial learning rate of $1e^{-4}$, and the learning rate is dropped by $3$ at 70\\% and 90\\% of the whole training schedule. We set different training epochs for these 3 industrial anomaly detection datasets according to the amount of training data: 100 epochs for MVTec AD and 30 epochs for MTD. As for the large-scale mSTC, the MSFlow is only trained for 10 epochs. \nOur MSFlow is training-time-friendly, costing less than half the training schedule of the previous methods \\cite{gudovskiy2022cflow,rudolph2022csflow,rudolph2021differnet} yet achieving superior performance.\n\n\n\n"
                    }
                },
                "subsection 5.2": {
                    "name": "Comparisons with Other Methods",
                    "content": "\n",
                    "subsubsection 5.2.1": {
                        "name": "MVTec AD",
                        "content": "\nWe compare our MSFlow with the previous reconstruction-based methods \\cite{zavrtanik2021draem, cohen2020spade,li2021cutpaste,deng2022distillation}, clustering-based methods \\cite{cohen2020spade,defard2021padim,roth2021patchcore} and flow-based methods \\cite{rudolph2021differnet, gudovskiy2022cflow,rudolph2022csflow} on the MVTec AD benchmark. Our method exhibits consistently superior performance regardless of the image-wise anomaly detection or the pixel-wise anomaly localization.\n\nThe image-wise anomaly detection comparisons are presented in Table \\ref{tab:det-comparison-mvtec}. Since only the class average AUCROC scores are provided in  \nCompared with prior methods of all three categories, our MSFlow achieves the best or the second-best detection performance in all classes except the \\textit{grid} and the \\textit{screw}, whose accuracies are only 0.1\\% lower than the second-best performance. The reconstruction-based methods achieve the best detection accuracy on some classes such as \\textit{grid} and \\textit{pill}. However, their detection AUCROC scores are inferior on other classes like \\textit{transistor} including misplaced defects. The near-perfect detection AUCROC scores on all classes highlight the effectiveness and generality of anomaly size variation of the proposed MSFlow.\nNaturally, our MSFlow also achieves the best class-average detection performance on either \\textit{texture} classes or \\textit{object} classes with 100\\% and 99.6\\% AUCROC scores. The overall class-average detection accuracy of the MSFlow is up to 99.7\\%, which is 0.6\\% higher than that of the previous SOTA \\textit{PatchCore} with the same feature extractor (WRN-50). The proposed MSFlow even outperforms the ensemble version of PatchCore (99.6\\%) with large feature extractors including WRN-101 \\cite{he2016resnet}, DenseNet-201\\cite{huang2017densenet} and ResNext-101 \\cite{xie2017resnext}. \n\nThe performance comparisons of pixel-wise anomaly localization with prior methods are shown in Table \\ref{tab:loc-comparison-mvtec}. Among flow-based methods, DifferNet\\cite{rudolph2021differnet} and CSFlow\\cite{rudolph2022csflow} are designed for image-wise anomaly detection without localization, hence only CFlow-AD\\cite{gudovskiy2022cflow} is compared with our MSFlow in anomaly localization.\nThe proposed MSFlow achieves high localization accuracies in all industrial classes, summarized in the class-average AUCROC score of 98.8\\% and PRO score of 97.1\\%. \nThe PRO scores of our MSFlow are overwhelmingly superior to other methods, especially on the \\textit{carpet, grid, leather, transistor, and zipper} with higher than 99\\% PRO \\rounda{score}. The overall RPO score is dramatically improved by 2.5\\%, which demonstrates the high-precision localization and noise suppression capability of the proposed MSFlow.\n\n\n\n\n\n  \n\n"
                    },
                    "subsubsection 5.2.2": {
                        "name": "Other Datasets",
                        "content": "\nThe anomaly detection performance on MTD and the violation localization performance on mSTC are presented in Table \\ref{tab:det-comparison-mtd} and Table \\ref{tab:loc-comparison-mstc}, respectively. On the MTD dataset specialized for \\textit{Tile}, our MSFlow achieves the second-best detection AUCROC score of 99.2\\%, negligibly inferior to the detection-only CSFlow by 0.1\\%. On the non-image dataset mSTC where pedestrian violations are treated as anomalies.\nFollowing \\cite{roth2021patchcore}, the proposed MSFlow is compared with clustering-based SAPDE\\cite{cohen2020spade}, PaDiM\\cite{defard2021padim} and PatchCore\\cite{roth2021patchcore} on the mSTC dataset. Although the violation detection requires semantic comprehension, our MSFlow achieves the best violation localization AUCROC score of 93.0\\%, outperforming the clustering-based PatchCore \\cite{roth2021patchcore} by 1.2\\%.  \n  \n\n% \\subsubsection{Computational Complexity}\n\n"
                    }
                },
                "subsection 5.3": {
                    "name": "Ablation Study",
                    "content": "\nThe comprehensive ablation studies are conducted on the MVTec AD benchmark. If not specifically stated, all results are obtained on the MSFlow with WRN-50 as the feature extractor.\n\n",
                    "subsubsection 5.3.1": {
                        "name": "Multi-scale Feature Pyramid Extraction",
                        "content": "\nThe feature extraction is simple but essential for flow-based methods to achieve high anomaly detection performance. We not only explore the effect of feature maps of different levels but also reveal the influence of different feature extractors.\n\n\\vspace{3pt}\\textbf{The Effect of Feature Maps of Different Levels}. We only change the feature maps fed into the following flow model but keep other implementations identical. The overall average detection AUCROC scores, localization AUCROC scores, and localization PRO scores are presented in Table \\ref{as-feature}. To fairly compare the effect of feature maps of stage1 and stage4, the feature pyramid comprising feature maps of stage2 and stage3 is treated as the baseline. With these two feature maps, our MSFlow also achieves a high detection AUCROC score of 99.4\\% while performing inferiorly on anomaly localization. When added stage4's feature map, performance even becomes worse in both anomaly detection and localization. The feature map of stage4 contains high-level semantic information but lacks details. With such a feature map as input, the flow model is induced to focus on semantic understanding rather than spatial structure perception. \nBoth anomaly detection and localization performance are promoted with the help of the feature map of stage1 as the third row in Table \\ref{as-feature}, which verifies the significance of spatial details for anomaly detection and localization. \n\n\n\n\n\n\\rounda{\n  \\textbf{The Ablation on pooling operation}.\n  Although only the feature maps of the first 3 stages are extracted to feed into the flow-based model, the GFLOPs of the entire model is still high because of the large-scale feature maps to be estimated. \n\tTo alleviate the computational burden, pooling layers with stride 2 are used to $2\\times$ downsample the extracted feature maps. We also conduct ablation studies about different pooling strategies (max pooling and average pooling) with different kernel sizes (k=2, 3, 5).\n\tCompared with max pooling, applying the average pooling achieves the same computation reduction yet further boosts the performance, especially for region-wise anomaly localization ($2.4\\%$ PRO increment when k=3).\n\tAccording to our analysis, the max pooling wildly destroys contextual correlation considering its sample mechanism, which is intolerant to achieving a high region-wise PRO score.\n\tShifting the kernel size in the pooling operation also influences the detection performance. Pooling with a small kernel size (k=2) pays attention to details while a large kernel size (k=5) helps to capture global information. \n\tAdopting the pooling with kernel size $5$ raises the detection AUROC score to $99.8\\%$ but significantly reduces the PRO score to $94.1\\%$.\n\tThe detection AUROC score is even up to $99.8\\%$ when kernel size is set to $5$ but significantly cut down the PRO localization score. According to the ablation on kernel size as demonstrated in Table \\ref{as-feature}, kernel size in the pooling operation is set to $3$ to achieve the detection and localization trade-off.\n}\n\n\n\nFig. \\ref{fig:viz-ss-ms} straightforwardly displays the localization results of each single-scale and multi-scale on four industrial products, whose defective region sizes vary wildly. When only shage1's feature map is encoded, the flow model locates the minor defects while deriving adverse noises. The noises are alleviated on the feature map of stage2, but the localization results are incomplete. With the feature map of stage3 as the input, the flow model is equipped with the global perception but excessively overrun the localization to non-defect regions. When the feature maps of all scales are encoded, our MSFlow generates precise localization to defective regions regardless of their various sizes. The superior localization results based on feature maps at all scales underscore the significance of multi-scale property for anomaly detection and localization.\n% According to the localization result visualization of different scales as displayed in Fig. \\ref{fig:viz-ss-ms}, our MSflow benefits \n\n\n\n\\vspace{3pt}\\textbf{Different Feature Extractors.}\nWe also employ a lightweight backbone ResNet-18 (\\rounda{RN-18}) \\cite{he2016resnet} as the feature extractor and present a fair comparison with prior methods \\cite{deng2022distillation,defard2021padim,gudovskiy2022cflow}, which displayed in Table \\ref{tab:as-resnet18}. Based on the feature maps extracted by the RN-18, our MSFlow also remarkably outperforms other methods on both subtasks. Therefore, our MSFlow is flexible and can be treated as a plug-in module with diverse backbones according to application demands. The substantial performance gap between the MSFlow based on RN-18 and WN-50 also reflects the significance of representative features for the flow-based methods.\n\n\n"
                    },
                    "subsubsection 5.3.2": {
                        "name": "Multi-scale Flow Model",
                        "content": "\nAs for the flow model in our MSFlow, the ablation studies are conducted to verify the effectiveness of our asymmetrical parallel flow architecture and fusion flow.\n\n\n\n\\vspace{3pt}\\textbf{Asymmetrical Parallel Flows Architecture.} Different from existing flow-based methods \\cite{rudolph2022csflow,gudovskiy2022cflow,rudolph2021differnet} stacking as many flow blocks in different branches, our parallel flows adopt an asymmetrical structure to explore the trade-off in performance and efficiency. Table \\ref{tab:as-parallel-flow} highlights the advantage of our asymmetrical flow architecture. The obvious performance increments brought by the additional 3 blocks (1st row vs. 2nd row) reveal that the representative capability of flow models can be facilitated with more flow blocks. In our asymmetrical parallel flow architecture, more flow blocks are stacked for the feature map with more channel dimensions. There are two asymmetrical architectures instantiated in the 3rd and 4th row, whose accuracies are similar to the symmetrical architecture of 8 flow blocks (2nd row). According to the 4th row in Table \\ref{tab:as-parallel-flow}, it is noteworthy that just 2 flow blocks are enough for the feature map of stage1, and our MSFlow achieves the best trade-off of performance and efficiency in this setting. Our asymmetrical parallel flow architecture avoids the model overfitting on low-dimensional feature maps in symmetric architecture. \n% And such an asymmetrical architecture design is a general pattern in backbone design \\cite{he2016resnet,zagoruyko2016wide} and many applications such as object detection \\cite{redmon2018yolov3}.\n\n\n\n\n\n\n\\vspace{3pt}\\textbf{The Effect of Fusion Flow.}\nOur fusion flow plays an essential role in exchanging spatial perception of different scales and perceptive fields. To verify its effect, the flow model simply dropping the fusion flow is treated as the baseline. Furthermore, because adding a fusion flow to the baseline increases the number of flow blocks, we add one additional flow block for each sequence of parallel flows (3, 6, 9 parallel flow blocks for stage1, stage2, stage3, respectively) for a fair comparison. \n\\rounda{\nWe also conduct ablation studies on the adaptive size in the additional fusion block. The performance and GFLOPs of the flow model are presented in Table \\ref{tab:as-fusion-flow}. The additional parallel flow in each branch brings negligible influence on performance. In contrast, our fusion flow facilitates all evaluation metrics, especially the region-wise localization performance PRO score, without introducing heavy computational burdensome. The effect of our fusion flow achieves best when the adaptive size is set to $8$ which is exactly equal to the size of the minimum feature map in $3$ branches.}\nWith the bridge between different scales built by our fusion flow, the noise of the single scale is suppressed. Therefore, our fusion flow remarkably improves the PRO score that takes connectivity into account by 2.3\\%. \nFrom the effect of fusion flow on AUCROC and PRO scores of all classes in the MVTec AD benchmark as illustrated in Fig. \\ref{fig:fusion-all-classes}, our fusion flow is effective and generalized for diverse industrial products with various defect sizes.\n\\roundb{\n  Particularly, when incorporating our fusion flow, there is a discernible decrease in the detection accuracy specifically aimed at identifying \\emph{screw}. We conjecture that this decline in performance stems from the consistently minute defects present in \\emph{screw} products, as shown in Fig. \\ref{fig:defect-examples}. In this scenario, the effectiveness of our fusion flow in covering defect size variation instead becomes a hindrance when attempting to detect these diminutive defects, which are easily overshadowed by the features with the large receptive field.}\n  \n  \nFurthermore, we employ our fusion flow to replace all cross-scale flows in the detection-only CSFlow \\cite{rudolph2022csflow} without any other setting changes, and the resulting model is named FuseFlow. From the comparison displayed in Table \\ref{tab:as-fuseflow}, our FuseFlow reduces the computational cost of CSFlow by 93\\% without sacrificing detection accuracy. This comparison highlights that our fusion flow is much more efficient in information exchange than the cross-scale flow.\n\n\n\n\\vspace{3pt}\\textbf{The Effect of Normalizaion in Flow Blocks.}\nCompared with the prior flow-based methods \\cite{rudolph2022csflow,gudovskiy2022cflow,rudolph2021differnet}, we add a layer normalization (LN) between two $3\\times3$ convolutions in st-network of all flow blocks including parallel flows and the fusion flow. The effect of the layer normalization is highlighted in Table \\ref{tab:as-normalization}. Removing the normalization decreases the detection AUCROC score by 0.3\\% and the localization AUCROC score by 0.4\\%. However, the region-wise PRO score drops significantly by 2.2\\% without normalization. Although both detection and localization scores are improved when normalization is applied, there is still a performance gap between normalized by BN or LN. This phenomenon can be explained by the different mechanisms of BN and LN, where BN and LN perform statistics in data and model space, respectively.\nBecause the images are captured on the industrial production lines, the normal training samples of the same industrial class share similar \\roundb{pose and appearance}. Therefore, the statistical effect of BN is weakened by the narrow data space, while LN is still effective in the model space.\n\n"
                    },
                    "subsubsection 5.3.3": {
                        "name": "Multi-scale likelihood outputs aggregation",
                        "content": " This section introduces the ablation studies of different aggregation strategies of the multi-scale likelihood outputs and the image-wise anomaly score calculation. \n\n\n\n  \n\n\n\\vspace{3pt}\\textbf{Different Aggregation Strategies.}\nConsidering the inherent discrepancy between anomaly detection and localization, we aggregate multi-scale likelihood maps by common addition for pixel-wise anomaly localization and multiplication for image-wise anomaly localization. Table \\ref{tab:as-aggregation} demonstrates the effect of addition and multiplication aggregation on detection and localization performance. As illustrated in Fig. \\ref{fig:add-mul-viz}, the multiplication aggregation suppresses noise and only preserves the high likelihoods for regions with high likelihoods across all levels. Therefore, it is suitable for image-wise anomaly score calculation by averaging the largest $K$ anomaly scores and the region-wise PRO score, which takes region connectivity into account. As for the pixel-wise localization AUCROC score, the addition aggregation outperforms the multiplication by retaining more individuality of each scale. \nWhen using either single aggregation strategy, the proposed MSFlow still outperforms previous methods both in anomaly detection and localization, which demonstrates the superiority of our MSFlow.\n\n\n\n\\vspace{3pt}\\textbf{The Robustness of the Mean of top\\textit{K}.}\nTo determine the hyperparameter $K$ of top$K$ in image-wise anomaly score calculation, we conduct the ablation study of $K$. Instead of setting a specific value for $K$ directly, we implicitly determine $K$ through the occupancy ratio on the whole anomaly map. The $K$ is respectively set 1 (the maximum), 1\\%, 2\\%, 3\\%, 5\\%, 10\\%, 25\\%, 50\\% and 100\\% (the mean). Fig. \\ref{fig:as-topk} intuitively displays the detection performance tendency of different $K$.\nOur MSFlow achieves the best when $K$ is set 2\\% or 3\\%, and the mean of the top 3\\% is treated as the optimum. The detection AUCROC scores remain higher than 99.2\\% when $K$ varies from 1\\% to 10\\%, which reveals the performance is robust to the setting of $K$ in a large range. \n\n\\roundb{\nNotably, the performance is dramatically degraded to 86.46\\% when $K$ is set to 100\\%. The drastic performance degradation seems in conflict with the high performance (98.7\\%) achieved by the detection-only method CSFlow \\cite{rudolph2022csflow} where the mean is adopted as the image-wise anomaly score. According to our analysis, since our MSFlow achieves high localization performance, only defect regions are assigned with high anomaly scores. Therefore, for the images with minor defects, the mean of the entire anomaly map is misled by the low anomaly scores of most non-defect regions, resulting in such drastic image-wise detection performance degradation.}\n\n\n\nTo further validate the discriminative capability of the proposed aggregation strategy, we visualize the anomaly score distributions derived from the widely-used maximum (top 1) and our mean of the top 3\\%. As as illustrated in Fig. \\ref{fig:topk-viz}, although both aggregation approaches achieve 100\\% accuracy for for \\textit{bottle} class in the MVTec AD benchmark, there is an obvious gap separating defects and non-defects when employing our global anomaly score calculation.\n\n"
                    }
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": " \\label{sec:conclusion}\nThe normalizing flows are appropriate for anomaly detection in an unsupervised fashion. However, the flow-based methods perform inferiorly when facing the unpredictable size variation of anomalies, especially those intending to locate the anomalous regions further. To tackle this issue, this paper exploits the multi-scale potential of flow models and proposes the MSFlow to generalize the anomaly size variation. Our MSFlow achieves state-of-the-art unsupervised anomaly detection with near-perfect 99.7\\% detection AUCROC score and 98.8\\% localization AUCROC score on the challenging MVTec AD benchmark. The MSFlow also exhibits promising potential for video violation detection. We hope this work will inspire future work about this research interest.\n\n% \\begin{thebibliography}{1}\n\\bibliographystyle{IEEEtran}\n\\bibliography{ref}\n% \\end{thebibliography}\n\n\\vspace{-10pt}\n\\begin{IEEEbiography}\n  [{\\includegraphics[width=0.95in,height=1.25in]{./figures/photos/zyx.png}}]\n\t{Yixuan Zhou}\n  received the B.E. degree from the University of Electronic Science and Technology of China, China, in 2019. He is currently pursuing his Ph.D. degree at the Center for Future Media and the School of Computer Science and Engineering, University of Electronic Science and Technology of China, China. His current research interests include unsupervised anomaly detection and computer vision.\n\\end{IEEEbiography}\n\\vspace{-10pt}\n\\begin{IEEEbiography}\n  [{\\includegraphics[width=1.in,height=1.25in]{./figures/photos/Xingxu.jpg}}]\n\t{Xing Xu}\n\treceived the B.E. and M.E. degrees from Huazhong University of Science and Technology, China, in 2009 and 2012, respectively, and the Ph.D. degree from Kyushu University, Japan, in 2015. He is currently with the School of Computer Science and Engineering, University of Electronic Science and Technology of China, China. He is the recipient of six academic awards, including the IEEE Multimedia Prize Paper 2020, the Best Paper Award from ACM Multimedia 2017, and the World's FIRST 10K Best Paper Award-Platinum Award from IEEE ICME 2017. His current research interests mainly focus on multimedia information retrieval and computer vision.\n\\end{IEEEbiography}\n\\vspace{-10pt}\n\\begin{IEEEbiography}\n  [{\\includegraphics[width=1.in,height=1.25in]{./figures/photos/JingkuanSong.png}}]\n\t{Jingkuan Song}\n\tis a full professor at the University of Electronic Science and Technology of China (UESTC). He joined Columbia University as a Postdoc Research Scientist (2016-2017), and the University of Trento as a Research Fellow (2014-2016). He obtained his Ph.D. degree in 2014 from The University of Queensland (UQ), Australia (advised by Prof. Heng Tao Shen). His research interest includes large-scale multimedia retrieval, image/video segmentation and image/video understanding using hashing, graph learning and deep learning techniques. He was the winner of the Best Paper Award in ICPR (2016, Mexico), the Best Student Paper Award at Australian Database Conference (2017, Australia), and the Best Paper Honorable Mention Award (2017, Japan). He is the Guest Editor of TMM, and WWWJ and is a PC member of CVPR\u201918, MM'18, IJCAI'18, etc.\n\\end{IEEEbiography}\n\\vspace{-10pt}\n\\begin{IEEEbiography}\n  [{\\includegraphics[width=1.in,height=1.25in]{./figures/photos/FuminShen.jpg}}]\n\t{Fumin Shen}\n\treceived a bachelor's degree from Shandong University in 2007 and a Ph.D. degree from the Nanjing University of Science and Technology, China, in 2014. He is currently with the School of Computer Science and Engineering, University of Electronic Science and Technology of China. His major research interests include computer vision and machine learning. He was a recipient of the Best Paper Award Honorable Mention from ACM SIGIR 2016 and ACM SIGIR 2017 and the World's FIRST 10K Best Paper Award -- Platinum Award from the IEEE ICME 2017.\n\\end{IEEEbiography}\n\\vspace{-10pt}\n\\begin{IEEEbiography}\n  [{\\includegraphics[width=1.in,height=1.25in]{./figures/photos/HengtaoShen.jpg}}]\n\t{Heng Tao Shen} \n\tis the Dean of the School of Computer Science and Engineering, and the Executive Dean of the AI Research Institute at the University of Electronic Science and Technology of China (UESTC). He obtained his BSc with 1st class Honours and Ph.D. from the Department of Computer Science, National University of Singapore in 2000 and 2004 respectively. His research interests mainly include Multimedia Search, Computer Vision, Artificial Intelligence, and Big Data Management. He has published 360+ peer-reviewed papers, including 140+ IEEE/ACM Transactions, and received 8 Best Paper Awards from international conferences, including the Best Paper Award from ACM Multimedia 2017 and Best Paper Award - Honourable Mention from ACM SIGIR 2017. He is/was an Associate Editor of ACM Transactions of Data Science, IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Knowledge and Data Engineering, and Pattern Recognition. He is a Member of Academia Europaea, Fellow of ACM, IEEE and OSA.  \n\\end{IEEEbiography}\n\n"
            }
        },
        "tables": {
            "tab:det-comparison-mvtec": "\\begin{table*}[t] \n  \\centering\n  \\caption{The comparisons of image-wise anomaly detection performance (AUCROC in \\%) on the MVTec AD benchmark. The best results are highlighted in bold, and the second-best results are underlined.}\\label{tab:det-comparison-mvtec}\n  \\resizebox{\\textwidth}{!}{\n  \\begin{tabular}{cl|cccc|c|cccc}\n  \\myrule\n  \\multicolumn{2}{c|}{Taxonomy}                                 & \\multicolumn{4}{c|}{Reconstruction-based}                  & Clustering-based & \\multicolumn{4}{c}{Flow-based}                            \\\\ \\hline\n  % \\multicolumn{2}{c|}{Method}                                   & \\makecell{CutPaste\\\\ \\cite{li2021cutpaste}}     & \\makecell{DRAEM\\\\ \\cite{zavrtanik2021draem}  }     & \\makecell{SSPCAB\\\\ \\cite{ristea2021self} }      & \\makecell{RD4AD\\\\ \\cite{deng2022distillation}  }      & \\makecell{PatchCore\\\\ \\cite{roth2021patchcore}  }    & \\makecell{DifferNet\\\\ \\cite{rudolph2021differnet}}  & \\makecell{CFlow-AD\\\\ \\cite{gudovskiy2022cflow}}     & \\makecell{CSFlow\\\\ \\cite{rudolph2022csflow}}      & \\textbf{MSFlow} \\\\ \\hline\n  \\multicolumn{2}{c|}{Method}                                   & CutPaste \\cite{li2021cutpaste}     & DRAEM \\cite{zavrtanik2021draem}       & SSPCAB \\cite{ristea2021self}       & RD4AD \\cite{deng2022distillation}    & PatchCore \\cite{roth2021patchcore}      & DifferNet \\cite{rudolph2021differnet}  & CFlow-AD \\cite{gudovskiy2022cflow}     & CSFlow \\cite{rudolph2022csflow}      & \\textbf{MSFlow} \\\\ \\hline\n  \\multicolumn{2}{c|}{Venue}                                    & CVPR'21      & CVPR'21      & CVPR'22       & CVPR'22       & CVPR'22         & WACV'21    & WACV'22       & WACV'22      & \\textbf{Ours} \\\\ \\hline\n  \\multicolumn{1}{c|}{\\multirow{6}{*}{\\rotatebox{90}{Texture}}} & Carpet       & 93.9         & 97.0         & 98.2          & 98.9          & 98.7            & 92.9       & {\\ul 99.3}    & \\textbf{100} & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Grid         & \\textbf{100} & {\\ul 99.9}   & \\textbf{100}  & \\textbf{100}  & 98.2            & 84.0       & 99.0          & 99.0         & 99.8          \\\\\n  \\multicolumn{1}{c|}{}                          & Leather      & \\textbf{100} & \\textbf{100} & \\textbf{100}  & \\textbf{100}  & \\textbf{100}    & 97.1       & {\\ul 99.7}    & \\textbf{100} & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Tile         & 94.6         & {\\ul 99.6}   & \\textbf{100}  & 99.3          & 98.7            & 99.4       & 98.0          & \\textbf{100} & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Wood         & 99.1         & 99.1         & 99.5          & 99.2          & 99.2            & {\\ul 99.8} & 96.7          & \\textbf{100} & \\textbf{100}  \\\\ \\cline{2-11} \n  \\multicolumn{1}{c|}{}                          & \\tb{Average} & 97.5         & 99.1         & 99.5          & 99.5          & 99.0            & 94.6       & 98.5          & {\\ul 99.8}   & \\textbf{100}  \\\\ \\hline\n  \\multicolumn{1}{c|}{\\multirow{11}{*}{\\rotatebox{90}{Object}}} & Bottle       & 98.2         & 99.2         & 98.4          & \\textbf{100}  & \\textbf{100}    & 99.0       & 99.0          & {\\ul 99.8}   & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Cable        & 81.2         & 91.8         & 96.9          & 95.0          & \\textbf{99.5}   & 95.9       & 97.6          & {\\ul 99.1}   & \\textbf{99.5} \\\\\n  \\multicolumn{1}{c|}{}                          & Capsule      & 98.2         & 98.5         & \\textbf{99.3} & 96.3          & 98.1            & 86.9       & 99.0          & 97.1         & {\\ul 99.2}    \\\\\n  \\multicolumn{1}{c|}{}                          & Hazelnut     & 98.3         & \\textbf{100} & \\textbf{100}  & {\\ul 99.9}    & \\textbf{100}    & 99.3       & 98.9          & 99.6         & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Metal Nut    & {\\ul 99.9}   & 98.7         & \\textbf{100}  & \\textbf{100}  & \\textbf{100}    & 96.1       & 98.6          & 99.1         & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Pill         & 94.9         & 98.9         & \\textbf{99.8} & 96.6          & 96.6            & 88.8       & 99.0          & 98.6         & {\\ul 99.6}    \\\\\n  \\multicolumn{1}{c|}{}                          & Screw        & 88.7         & 93.9         & {\\ul 97.9}    & 97.0          & 98.1            & 96.3       & \\textbf{98.9} & 97.6         & 97.8          \\\\\n  \\multicolumn{1}{c|}{}                          & Toothbrush   & 99.4         & \\textbf{100} & \\textbf{100}  & {\\ul 99.5}    & \\textbf{100}    & 98.6       & 98.9          & 91.9         & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Transistor   & 96.1         & 93.1         & 92.9          & 96.7          & \\textbf{100}    & 91.1       & 93.3          & {\\ul 99.3}   & \\textbf{100}  \\\\\n  \\multicolumn{1}{c|}{}                          & Zipper       & 99.9         & \\textbf{100} & \\textbf{100}  & 98.5          & 98.8            & 95.1       & 99.1          & {\\ul 99.7}   & \\textbf{100}  \\\\ \\cline{2-11} \n  \\multicolumn{1}{c|}{}                          & \\tb{Average} & 95.5         & 97.4         & 98.5          & 98.0          & {\\ul 99.1}      & 94.7       & 98.2          & 98.2         & \\textbf{99.6} \\\\ \\hline\n  \\multicolumn{2}{c|}{\\textbf{Total Average}}                   & 96.1         & 98.0         & 98.9          & 98.5          & {\\ul 99.1}      & 94.7       & 98.3          & 98.7         & \\textbf{99.7} \\\\ \\myrule\n  \\end{tabular}}\n  \\end{table*}",
            "tab:loc-comparison-mvtec": "\\begin{table*}[t] \n  \\centering\n  \\caption{The comparisons of pixel-wise anomaly localization performance (AUCROC and RPO in \\%) on the MVTec AD benchmark. The best results are highlighted in bold, and the second-best results are underlined. The PRO scores are not evaluated in the DRAEM\\cite{zavrtanik2021draem} and SSPCAB\\cite{ristea2021self}, replaced with `-'.}\\label{tab:loc-comparison-mvtec}\n  \\begin{tabular}{cl|ccc|ccc|cc}\n  \\myrule\n  \\multicolumn{2}{c|}{Taxonomy}                               & \\multicolumn{3}{c|}{Reconstruction-based} & \\multicolumn{3}{c|}{Clustering-based}   & \\multicolumn{2}{c}{Flow-based} \\\\ \\hline\n  \\multicolumn{2}{c|}{Method}                                 & DRAEM \\cite{zavrtanik2021draem}         & SSPCAB \\cite{ristea2021self}        & RD4AD   \\cite{deng2022distillation}               & SPADE \\cite{cohen2020spade}            & PaDiM \\cite{defard2021padim}      & PatchCore \\cite{roth2021patchcore}        & CFlow-AD \\cite{gudovskiy2022cflow}               & \\tb{MSFlow}  \\\\ \\hline\n  \\multicolumn{2}{c|}{Venue}                                  & CVPR'21        & CVPR'22        & CVPR'22                & ArXiv'21          & ICPR'21     & CVPR'22           & WACV'22                 & \\tb{Ours}  \\\\ \\hline\n  \\multicolumn{1}{c|}{\\multirow{6}{*}{\\rotatebox{90}{Texture}}} & Carpet     & 95.5 / -       & 95.0 / -       & 98.9 / 97.0            & 97.5 / 94.7       & 99.1 / 96.2 & 99.1 / 96.6       & {\\ul 99.3} / {\\ul 97.7} & \\tb{99.4} / \\tb{99.6}   \\\\\n  \\multicolumn{1}{c|}{}                          & Grid       & \\tb{99.7} / -  & 99.5 / -       & 99.3 / {\\ul 97.6}      & 93.7 / 86.7       & 97.3 / 94.6 & 98.7 / 95.9       & 99.0 / 96.1             & {\\ul 99.4} / \\tb{99.1}  \\\\\n  \\multicolumn{1}{c|}{}                          & Leather    & 98.6 / -       & {\\ul 99.5} / - & 99.4 / 99.1            & 97.6 / 97.2       & 99.2 / 97.8 & 99.3 / 98.9       & \\tb{99.7} / {\\ul 99.4}  & \\tb{99.7} / \\tb{99.9}   \\\\\n  \\multicolumn{1}{c|}{}                          & Tile       & {\\ul 99.2} / - & \\tb{99.3} / -  & 95.6 / 90.6            & 87.4 / 75.9       & 94.1 / 86.0 & 95.9 / 87.4       & 98.0 / {\\ul 94.3}       & 98.2 / \\tb{95.3}        \\\\\n  \\multicolumn{1}{c|}{}                          & Wood       & 96.4 / -       & {\\ul 96.8} / - & 95.3 / 90.9            & 88.5 / 87.4       & 94.9 / 91.1 & 95.1 / 89.6       & 96.7 / {\\ul 95.8}       & \\tb{97.1} / \\tb{96.6}   \\\\ \\cline{2-10}      \n  \\multicolumn{1}{c|}{}                          & Average    & 97.9 / -       & 98.0 / -       & 97.7 / 95.0            & 92.9 / 88.4       & 96.9 / 93.1 & 97.6 / 93.7       & {\\ul 98.5} / {\\ul 96.7} & \\tb{98.8} / \\tb{98.1}   \\\\ \\hline\n  \\multicolumn{1}{c|}{\\multirow{11}{*}{\\rotatebox{90}{Texture}}} & Bottle     & \\tb{99.1} / -  & 98.8 / -       & 98.7 / 96.6            & 98.4 / 95.5       & 98.3 / 94.8 & 98.6 / 96.1       & {\\ul 99.0} / {\\ul 96.8} & {\\ul 99.0} / \\tb{98.5}  \\\\\n  \\multicolumn{1}{c|}{}                          & Cable      & 94.7 / -       & 96.0 / -       & 97.4 / 91.0            & 97.2 / 90.9       & 96.7 / 88.8 & \\tb{98.5} / 92.6  & {\\ul 97.6} / {\\ul 93.5} & \\tb{98.5} / \\tb{93.7}   \\\\\n  \\multicolumn{1}{c|}{}                          & Capsule    & 94.3 / -       & 93.1 / -       & 98.7 / {\\ul 95.8}      & {\\ul 99.0} / 93.7 & 98.5 / 93.5 & 98.9 / 95.5       & 99.0 / 93.4             & \\tb{99.1} / \\tb{98.4}   \\\\\n  \\multicolumn{1}{c|}{}                          & Hazelnut   & {\\ul 99.7} / - & \\tb{99.8} / -  & 98.9 / 95.5            & 99.1 / 95.4       & 98.2 / 92.6 & 98.7 / 93.9       & 98.9 / \\tb{96.7}        & 98.7 / {\\ul 96.6}       \\\\\n  \\multicolumn{1}{c|}{}                          & Metal Nut  & \\tb{99.5} / -  & 98.9 / -       & 97.3 / 92.3            & 98.1 / {\\ul 94.4} & 97.2 / 85.6 & 98.4 / 91.3       & 98.6 / 91.7             & {\\ul 99.3} / \\tb{97.6}  \\\\\n  \\multicolumn{1}{c|}{}                          & Pill       & 97.6 / -       & 97.5 / -       & 98.2 / \\tb{96.4}       & 96.5 / 94.6       & 95.7 / 92.7 & 97.6 / 94.1       & \\tb{99.0} / 95.4        & {\\ul 98.8} / {\\ul 96.0} \\\\\n  \\multicolumn{1}{c|}{}                          & Screw      & 97.6 / -       & \\tb{99.8} / -  & {\\ul 99.6} / \\tb{98.2} & 98.9 / 96.0       & 98.5 / 94.4 & 99.4 / {\\ul 97.9} & 98.9 / 95.3             & 99.1 / 94.2             \\\\\n  \\multicolumn{1}{c|}{}                          & Toothbrush & 98.1 / -       & 98.1 / -       & \\tb{99.1} / {\\ul 94.5} & 97.9 / 93.5       & 98.8 / 93.1 & 98.7 / 91.4       & {\\ul 98.9} / \\tb{95.1}  & 98.5 / 91.6             \\\\\n  \\multicolumn{1}{c|}{}                          & Transistor & 90.9 / -       & 87.0 / -       & 92.5 / 78.0            & 94.1 / {\\ul 87.4} & 97.5 / 84.5 & 96.4 / 83.5       & {\\ul 98.0} / 81.4       & \\tb{98.3} / \\tb{99.8}   \\\\\n  \\multicolumn{1}{c|}{}                          & Zipper     & 98.8 / -       & 99.0 / -       & 98.2 / 95.4            & 96.5 / 92.6       & 98.5 / 95.9 & 98.9 / {\\ul97.1}  & {\\ul 99.1} / 96.6       & \\tb{99.2} / \\tb{99.4}   \\\\ \\cline{2-10}      \n  \\multicolumn{1}{c|}{}                          & Average    & 97.0 / -       & 96.8 / -       & 97.9 / 93.4            & 97.6 / 93.4       & 97.8 / 91.6 & 98.4 / 93.3       & {\\ul 98.7} / {\\ul 93.6} & \\tb{98.8} / \\tb{96.6}   \\\\ \\hline\n  \\multicolumn{2}{c|}{Total Average}                          & 97.3 / -       & 97.2 / -       & 97.8 / 93.9            & 96.0 / 91.7       & 97.5 / 92.1 & 98.1 / 93.5       & {\\ul 98.6} / {\\ul 94.6} & \\tb{98.8} / \\tb{97.1}   \\\\ \\myrule\n  \\end{tabular}\n  \\end{table*}",
            "tab:det-comparison-mtd": "\\begin{table}[t]\n  \\caption{The comparisons of image-wise anomaly detection performance (AUCROC in \\%) on the MTD dataset. The best results are highlighted in bold, and the second-best results are underlined.} \\label{tab:det-comparison-mtd}\n  \\centering\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|c|c|c}\n  \\myrule\n  Method             & GANomaly \\cite{akcay2018ganomaly}  & DifferNet \\cite{rudolph2021differnet}    & PaDiM  \\cite{defard2021padim}       \\\\ \\hline\n  Det. AUCROC  & 76.6                               & 97.7                                     & 98.7          \\\\ \\hline \\hline\n  Method             & PatchCore \\cite{roth2021patchcore} & CSFlow  \\cite{rudolph2022csflow}         & \\textbf{MSFlow(Ours)} \\\\ \\hline\n  Det. AUCROC  & 97.9                               & \\textbf{99.3}                            & {\\ul 99.2}    \\\\ \\myrule\n  \\end{tabular}}\n  \\end{table}",
            "tab:loc-comparison-mstc": "\\begin{table}[t] \n  \\caption{The comparisons of pixel-wise anomaly localization performance (AUCROC in \\%) on the mSTC dataset. The best results are highlighted in bold, and the second-best results are underlined. } \\label{tab:loc-comparison-mstc}\n  \\centering\n  % \\resizebox{!}{width=\\linewidth}{\n  \\begin{tabular}{c|c|c|c|c}\n  \\myrule                \n  Method                     & \\makecell{SPADE \\\\ \\cite{cohen2020spade}}  & \\makecell{PaDiM \\\\ \\cite{defard2021padim}} & \\makecell{PatchCore \\\\ \\cite{roth2021patchcore}}    & \\makecell{\\textbf{MSFlow} \\\\ \\textbf{(Ours)}} \\\\ \\hline\n  Loc. AUCROC         & 89.9                         &  91.2                        & {\\ul 91.8}                          & \\textbf{93.0}    \\\\ \\myrule\n  \\end{tabular}\n  \\end{table}",
            "tab:as-resnet18": "\\begin{table}[!htb]\n  \\centering\n  \\caption{The comparisons of detection and localization performance on MVTec AD with the ResNet-18 as a feature extractor. Some unavailable results of other methods \\cite{defard2021padim,gudovskiy2022cflow} are filled by `-'. * refers to our MSFlow based on WRN-50. The best results are highlighted in bold.} \\label{tab:as-resnet18}\n  \\begin{tabular}{l|c|c|c}\n  \\myrule\n  Method                                & Det. AUCROC                  & Loc. AUCROC         & Loc. PRO                             \\\\ \\hline\n  RD4AD  \\cite{deng2022distillation}    & 97.9                        & 97.1               & 91.2                                 \\\\ \n  PaDiM  \\cite{defard2021padim}        & -                           & 97.1               & 90.8                                 \\\\ \n  CFlow-AD  \\cite{gudovskiy2022cflow}    & 97.1                        & 98.1               & -                                    \\\\ \\hline\n  \\textbf{MSFlow (Ours)}                    & \\textbf{98.9}               & \\textbf{98.4}      & \\textbf{94.3}                        \\\\ \\hline\\hline\n  \\textbf{MSFlow (Ours)*}               & \\textbf{99.7}                & \\textbf{98.8}        &  \\textbf{97.1} \\\\ \\myrule\n  % {\\color[HTML]{9B9B9B} \\textbf{MSFlow (Ours)*}} & {\\color[HTML]{9B9B9B} \\textbf{99.7}} & {\\color[HTML]{9B9B9B} \\textbf{98.8}} & {\\color[HTML]{9B9B9B} \\textbf{97.1}} \\\\ \\myrule\n  \\end{tabular}\n  \\end{table}",
            "tab:as-parallel-flow": "\\begin{table}[!htb]\n  \\centering\n  \\caption{The ablation study of different numbers of parallel flow blocks. `\\#Flow Blocks' denotes the number of flow blocks, where the $i$-th value refers to the flow block number for the feature map of stage $i$. The best results are highlighted in bold.} \\label{tab:as-parallel-flow}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{c|c|c|c|c}\n  \\myrule\n  \\#Flow Blocks & Det. AUCROC     & Loc. AUCROC     & Loc. PRO       & GFLOPs        \\\\ \\hline\n  (5, 5, 5)     & 99.47          & 98.71          & 95.45          & 47.9          \\\\ \n  (8, 8, 8)     & 99.68          & 98.75          & \\textbf{97.34} & 71.1          \\\\ \\hline\n  (4, 6, 8)     & 99.69          & \\textbf{98.82} & 97.11          & 52.5          \\\\  \n  \\textbf{(2, 5, 8)}     & \\textbf{99.71} & 98.79          & 97.07          & \\textbf{44.7} \\\\ \\myrule\n  \\end{tabular}}\n  \\end{table}",
            "tab:as-fuseflow": "\\begin{table}[!htb]\n    \\centering\n    \\caption{The detection performance (AUCROC in \\%) of CSFlow\\cite{rudolph2022csflow} and our FuseFlow. Only the GFLOPs of the flow model are displayed here. The best results are highlighted in bold.} \\label{tab:as-fuseflow}\n    \\rounda{\n    \\begin{tabular}{l|c|c|c}\n    \\myrule\n    Method   & Det. AUCROC     & \\#Params      & GFLOPs       \\\\ \\hline\n    CSFlow \\cite{rudolph2022csflow}   & \\textbf{98.7} & 275.2         & 65.9         \\\\ \n    \\textbf{FuseFlow (Ours)} & 98.6          & \\textbf{77.3} & \\textbf{4.9} \\\\ \\myrule\n    \\end{tabular}}\n    \\end{table}",
            "tab:as-normalization": "\\begin{table}[b]\n  \\centering\n  \\caption{The ablation study of normalization in flow blocks. w/o Norm refers to dropping the normalization. BN and LN denote batch normalization and layer normalization, respectively. The best results are highlighted in bold.} \\label{tab:as-normalization}\n  \\begin{tabular}{c|c|c|c}\n  \\myrule\n  Normalization & Det. AUCROC    & Loc. AUCROC    & Loc. PRO      \\\\ \\hline\n  w/o Norm      & 99.4          & 98.4          & 94.9          \\\\ \n  w BN          & 99.5          & 98.6          & 95.5          \\\\ \n  \\textbf{w LN}          & \\textbf{99.7} & \\textbf{98.8} & \\textbf{97.1} \\\\ \\myrule\n  \\end{tabular}\n  \\end{table}",
            "tab:as-aggregation": "\\begin{table}[t]\n  \\centering\n  \\caption{The ablation study of addition and multiplication aggregation. The best results are highlighted in bold.}  \\label{tab:as-aggregation}\n  \\begin{tabular}{l|c|c|c}\n  \\myrule\n  Aggregation    & Det. AUCROC    & Loc. AUCROC     & Loc. PRO      \\\\ \\hline\n  Addition       & 99.4          & \\textbf{98.8} & 95.3          \\\\\n  Multiplication & \\textbf{99.7} & 98.3          & \\textbf{97.1} \\\\ \\myrule\n  \\end{tabular}\n  \\end{table}"
        },
        "figures": {
            "fig:defect-examples": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\linewidth]{./figures/defect-examples.png} \n  \\caption{Defect examples of all classes in MVTec AD benchmark. The percentages represent the area proportion of the defective regions.}\n  \\label{fig:defect-examples}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{./figures/framework.pdf}\n  \\caption{The framework of the MSFlow. The feature maps of the first 3 stages are extracted and $2\\times$ downsampled by average pooling. The networks of stage4 are abandoned in our MSFlow, which is specifically drawn as a dashed box with a lighter color. The flow model in the MSFlow is composed of asymmetrical parallel flows and a fusion flow for information exchange. Finally, multiplication and addition aggregation are adopted for image-wise anomaly detection and pixel-wise anomaly localization, respectively.}\n  \\label{fig:framework}\n\\end{figure*}",
            "fig:fusion-block": "\\begin{figure}[t]\n  \\includegraphics[width=\\linewidth]{./figures/flow-block.png}\n  \\caption{The pipeline of the flow block including two affine coupling layers. Each affine coupling layer encodes the halved features, and the encoded features are concatenated together as the output.}\n  \\label{fig:fusion-block}\n\\end{figure}",
            "fig:fuse-network": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{./figures/fusion-network.png}\n  \\caption{The detailed architecture of the fusion network $g^\\textrm{fuse}$ in st-network of the fusion flow. $AP$ denotes the average pooling.} \\label{fig:fuse-network}\n  \\label{fig:fusion-network}\n\\end{figure}",
            "fig:fusion-network": "\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{./figures/fusion-network.png}\n  \\caption{The detailed architecture of the fusion network $g^\\textrm{fuse}$ in st-network of the fusion flow. $AP$ denotes the average pooling.} \\label{fig:fuse-network}\n  \\label{fig:fusion-network}\n\\end{figure}",
            "fig:viz-ss-ms": "\\begin{figure}[!htb]\n  \\centering\n  \\includegraphics[width=\\linewidth]{./figures/viz-ss-ms.png}\n  \\caption{The visualization comparisons of different single-scale and multi-scale outputs for four classes with different defective sizes. The percentages in ground truth images refer to the area proportion of the defects.}\n  \\label{fig:viz-ss-ms}\n\\end{figure}",
            "fig:fusion-all-classes": "\\begin{figure}[h]\n  % \\left\n  \\centering\n  \\subfloat[The effect of fusion flow on AUCROC scores.]{\\includegraphics[width=0.9\\linewidth]{./figures/as-fusion-auroc.pdf}}\n\n  \\subfloat[The effect of fusion flow on PRO scores.]{\\includegraphics[width=0.9\\linewidth]{./figures/as-fusion-pro.pdf}}\n  \\caption{The effect of fusion flow on AUCROC (a) and PRO (b) scores of all classes in the MVTec AD benchmark. The blue and yellow curves denote the smooth curves of AUCROC scores in all classes with or without the fusion flow.} \\label{fig:fusion-all-classes}\n\\end{figure}",
            "fig:add-mul-viz": "\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{./figures/add-mul-viz.png}\n  \\caption{The visualization for anomaly score maps based on multiplication and addition aggregation.}\n  \\label{fig:add-mul-viz}\n\\end{figure}",
            "fig:as-topk": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{./figures/as-topk.pdf}\n  \\vspace{-10.pt}\n  \\begin{center}\n    $\\quad K$\n  \\end{center}\n  \\vspace{-5.pt}\n  \\caption{The effect of different $K$ set in top$K$ on overall detection performance (AUCROC in \\%). The mean of top $K$ pixel-wise anomaly scores is viewed as the image-wise anomaly score.}\n  \\label{fig:as-topk}\n\\end{figure}",
            "fig:topk-viz": "\\begin{figure}[!htb]\n  \\vspace{-5pt}\n  \\centering\n  \\subfloat[Top 1 (the maximum).]{\n    \\includegraphics[width=0.4\\linewidth]{./figures/score_histogram_top1.pdf}\n  }\n  \\hspace{3mm}\n  \\subfloat[The mean of top 3\\%.]{\\includegraphics[width=0.4\\linewidth]{./figures/score_histogram_top3.pdf}}\n  \\caption{Image-wise anomaly score distributions based on the maximum and the mean of top 3\\% (ours) of `bottle' class in MVTec AD benchmark.} \\label{fig:topk-viz}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n  \\begin{aligned}\n    \\boldsymbol{x}=f(\\boldsymbol{z})=& f_K \\circ f_{K-1} \\circ ... \\circ f_1(\\boldsymbol{z}); \\\\\n    \\boldsymbol{z}=f^{-1}(\\boldsymbol{x})=& {f_1}^{-1} \\circ {f_2}^{-1} \\circ ... \\circ {f_K}^{-1}(\\boldsymbol{x}),\n  \\end{aligned}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n  p_X(\\boldsymbol{x})=p_Z(\\boldsymbol{z})\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{x}) \\right\\rvert,\n  \\, \\textrm{where} \\, J_{f^{-1}}(\\boldsymbol{x})= \\frac{\\partial f^{-1}}{\\partial \\boldsymbol{x}}.\n\\end{equation}",
            "eq:3": "\\begin{equation}\n  \\begin{aligned}\n  \\textrm{forward:}&\n  \\begin{cases}\n  \\boldsymbol{x}_{1:d}  &= \\boldsymbol{z}_{1:d}, \\\\\n  \\boldsymbol{x}_{d+1:D}&= \\boldsymbol{z}_{d+1:D} \\odot \\textrm{exp}(s(\\boldsymbol{z}_{1:d})) +t(\\boldsymbol{z}_{1:d});\n  \\end{cases}\n  \\\\\n  \\textrm{reverse:}&\n  \\begin{cases}\n  \\boldsymbol{z}_{1:d}  &= \\boldsymbol{x}_{1:d},\\\\\n  \\boldsymbol{z}_{d+1:D}&= (\\boldsymbol{x}_{d+1:D}-t(\\boldsymbol{x}_{1:d}))\\odot \\textrm{exp}(-s(\\boldsymbol{x}_{1:d})). \\nonumber\n  \\end{cases}\n  \\end{aligned}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n  \\textrm{log}\\left\\lvert \\textrm{det}J_{{f_\\textrm{aff}}^{-1}}(\\boldsymbol{x}) \\right\\rvert = \\sum s(\\boldsymbol{x}_{1:d}).\n\\end{equation}",
            "eq:5": "\\begin{equation}\n  (\\boldsymbol{h}_1, \\boldsymbol{h}_2, \\boldsymbol{h}_3) = E(\\boldsymbol{x}),\n\\end{equation}",
            "eq:6": "\\begin{equation}\n \\boldsymbol{y}_i = \\textrm{AvgPooling}(\\boldsymbol{h}_i), i = 1,2,3,\n\\end{equation}",
            "eq:7": "\\begin{equation}\n  \\begin{aligned}\n    \\boldsymbol{z}_i^\\textrm{p}&=f_{i}^\\textrm{p}(\\boldsymbol{y}_i), \\, \\textrm{where} \\,\\, i=1,2,3;\\\\ \n    (\\boldsymbol{z}_1^\\textrm{fuse}&,\\boldsymbol{z}_2^\\textrm{fuse}, \\boldsymbol{z}_3^\\textrm{fuse}) = f_\\textrm{fuse}(\\boldsymbol{z}_1, \\boldsymbol{z}_2, \\boldsymbol{z}_3).\n  \\end{aligned}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n  \\textrm{log}\\,\\hat{p}_Y (\\boldsymbol{y}_{ijk}) = -\\frac{\\left\\lVert \\boldsymbol{z}_{ijk} \\right\\rVert_2^2}{2} + \\textrm{log}\\left\\lvert \\textrm{det}J_{f^{-1}}(\\boldsymbol{y}_{ijk})\\right\\rvert.\n\\end{equation}",
            "eq:9": "\\begin{align}\n  S_{loc} &= \\textrm{max}(P_{add}) - P_{add}; \\\\\n  S_{mul} &= \\textrm{max}(P_{mul})-P_{mul}, \\notag \\\\\n  s_{det} &= \\frac{1}{K} \\sum_{1}^K \\textrm{top}K(S_{mul}),\n\\end{align}"
        },
        "git_link": "https://github.com/cool-xuan/msflow"
    }
}