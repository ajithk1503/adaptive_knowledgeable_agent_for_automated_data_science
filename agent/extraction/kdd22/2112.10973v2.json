{
    "meta_info": {
        "title": "Minimizing Congestion for Balanced Dominators",
        "abstract": "A primary challenge in metagenomics is reconstructing individual microbial\ngenomes from the mixture of short fragments created by sequencing. Recent work\nleverages the sparsity of the assembly graph to find $r$-dominating sets which\nenable rapid approximate queries through a dominator-centric graph partition.\nIn this paper, we consider two problems related to reducing uncertainty and\nimproving scalability in this setting.\n  First, we observe that nodes with multiple closest dominators necessitate\narbitrary tie-breaking in the existing pipeline. As such, we propose finding\n$\\textit{sparse}$ dominating sets which minimize this effect via a new\n$\\textit{congestion}$ parameter. We prove minimizing congestion is NP-hard, and\ngive an $\\mathcal{O}(\\sqrt{\\Delta^r})$ approximation algorithm, where $\\Delta$\nis the max degree.\n  To improve scalability, the graph should be partitioned into uniformly sized\npieces, subject to placing vertices with a closest dominator. This leads to\n$\\textit{balanced neighborhood partitioning}$: given an $r$-dominating set,\nfind a partition into connected subgraphs with optimal uniformity so that each\nvertex is co-assigned with some closest dominator. Using variance of piece\nsizes to measure uniformity, we show this problem is NP-hard iff $r$ is greater\nthan $1$. We design and analyze several algorithms, including a polynomial-time\napproach which is exact when $r=1$ (and heuristic otherwise).\n  We complement our theoretical results with computational experiments on a\ncorpus of real-world networks showing sparse dominating sets lead to more\nbalanced neighborhood partitionings. Further, on the metagenome\n$\\textsf{HuSB1}$, our approach maintains high query containment and similarity\nwhile reducing piece size variance.",
        "author": "Yosuke Mizutani, Annie Staker, Blair D. Sullivan",
        "link": "http://arxiv.org/abs/2112.10973v2",
        "category": [
            "cs.DS"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nMicrobial communities play a critical role in many aspects of human health\n (e.g. gut microbiomes) and ecosystems (e.g. marine ecology),\n and understanding their composition and function has been increasingly important in\n biological and medical research.\nMuch of the work on these communities focuses on analyzing the genomic material (DNA and RNA)\n of the constituent microorganisms, a research area called metagenomics.\nA primary challenge in the field is reconstructing individual genomes\n from the mixture of short fragments created by shotgun sequencing~\\cite{quince2017shotgun}.\n\nOne practical approach that has gathered significant recent attention utilizes\n a metagenome assembly graph to guide analyses.\nCommonly, this is done with a (compact) De Bruijn graph, or (c)DBG,\n where vertices correspond to DNA subsequences called $k$-mers and\n edges indicate potential compatibility in an assembly (almost complete overlap).\n%\nSince the graphs corresponding to real-world metagenomic datasets may have\n tens of millions of vertices, scalable methods for analysis are imperative.\n Recent work of Brown et al.~\\cite{Brown2020}, implemented in\n \\sgc \\footnote{https://github.com/spacegraphcats/spacegraphcats},\n leveraged the sparsity of these graphs to enable efficient indexing and querying\n using partial information about suspected constituent microbes.\nTheir approach relies on finding an $r$-dominating set\n (using Dvorak's approximation algorithm for sparse graphs~\\cite{dvorak2013constantfactor}),\n then partitioning the assembly graph into bounded-radius \\emph{pieces} by assigning each vertex to\n one of its closest dominators.\nThe process is repeated on the piece graph to form a hierarchy of dominating sets\n which enables effective navigation and categorization of the data.\n%\nInitial experiments demonstrated the approach can improve the completeness of\n partial genomes for microbes present in a community\n and also reveal significant strain variation in real-world microbiomes \\cite{Brown2020}.\\looseness-1\n\nDespite these promising results, several key challenges remain.\nHere, we focus on those related to partitioning the metagenome assembly graph\n into pieces around the dominators.\nIn particular, the current dominating set algorithm in~\\cite{Brown2020}\nmay choose dominators that are close together in the cDBG,\n leading to uncertainty in how a region should be ``carved up'' into pieces. Thus, the resulting piece graph may reflect the tie-breaking rules more than underlying ground-truth associations intrinsic to the cDBG/network.\nFurther, the sizes of the pieces may be imbalanced,\n counteracting any advantage gained by using the hierarchy to prune away irrelevant regions of the cDBG.\n\nIn this work, we tackle these challenges by (a) introducing a notion of\n \\textit{sparse dominating sets} which rewards ``scattering'' dominators,\n with the aim of generating pieces which are biologically meaningful and\n inherently more stable,\n and (b) considering algorithms for \\textit{balanced neighborhood partitioning}:\n assigning vertices to dominators which minimize variation in the resulting piece sizes.\n%\n \\looseness-1\n\nWhile we defer formal definitions to Section~\\ref{sec:sparse-domset},\n our approach to avoiding nodes with multiple closest dominators is\n based on minimizing \\textit{congestion}, which measures the average number\nof dominators appearing in an arbitrary vertex neighborhood.\nWe show that \\PrbMCDS is NP-hard and present an $\\mathcal{O}(\\sqrt{\\Delta^r})$-approximation algorithm running in $\\bigo{\\Delta^{2r} n \\log n}$ time, where $\\Delta$ denotes the maximum degree.\nWe compare this with the $\\mathcal{O}(r\\log{\\Delta})$ standard approximation algorithm\n for finding a (smallest) \\PrbRDom. We note that sparse dominating sets have no explicit\n size restriction, and discuss trade-offs between solution size and congestion.\n\nOnce we have an $r$-dominating set, the problem becomes one of partitioning\n the vertices into pieces so that (a) each vertex is assigned to a piece containing\n one of its closest dominators,\n and (b) the pieces are as equal in size as possible.\nFor the latter condition, we minimize the variance of the piece size distribution\n in \\PrbBNP.\nWe show this is polynomial-time solvable when $r=1$ and NP-hard when $r \\geq 2$,\n even when there are only two dominators.\nDespite this, \\AlgPrtBranch establishes the problem is fixed parameter tractable%\n\\footnote{%\n  A problem is called fixed parameter tractable (FPT) with respect to parameter $k\\in \\N$\n  if it can be solved in time $f(k)n^{O(1)}$ for some computable function $f$.\n} (FPT) in graphs of bounded degree when parameterized by the number of vertices equidistant\n from multiple dominators.\n %($k$), giving an $\\bigo{\\Delta^k n^4}$ FPT algorithm \\AlgPrtBranch.\nFurther, when $r=1$, we give an exact $\\bigo{n^4}$ algorithm \\AlgPrtLayer in general graphs\n using flow-based techniques; when $r \\geq 2$, this yields a heuristic.\nFinally, we compare with a linear-time greedy heuristic, \\AlgPrtWeight.\nThese algorithms are described in Section~\\ref{sec:nbr-prt}.\n\nWe implemented all of the above algorithms using C++ in an open-source repository\n and tested their performance on a large corpus of real-world networks,\n including a variety of metagenome graphs.\nExperimental results demonstrate that the choice of dominating set can significantly impact\n the runtime and solution quality of balanced neighborhood partitioning algorithms,\n with sparse sets out-performing their smaller but more congested analogues.\nFinally, we present preliminary results indicating that low-congestion dominating sets\n do not significantly degrade the fidelity of queries using partial genome bins on\n  \\scalebox{.85}{\\textsf{HuSB1}}, a metagenome analyzed in~\\cite{Brown2020},\n a critical requirement for their downstream adoption.\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\n",
                "subsection 2.1": {
                    "name": "Notation \\& Terminology",
                    "content": "\n\nGiven a graph $G=(V,E)$, we write $n=\\abs{G}=\\abs{V}$ for the number of vertices and\n$m=\\norm{G}=\\abs{E}$ for the number of edges. The \\textit{distance} between two vertices\n$x,y\\in V$, denoted $d_G(x,y)$, equals the minimum number of edges in an $x,y$-path in\n$G$; if no such path exists, we set $d(x,y) := \\infty$. The distance between a vertex\n$v \\in V$ and vertex set $X \\subseteq V$ is defined as\n$d_G(v,X) := \\min_{x\\in X}d_G(v,x)$. We use $N(v)$ and $N[v]$ to denote the open and closed\nneighborhoods of a vertex $v$, respectively.\nWe write $N^r[v]$ for the $r$-neighborhood: the set of all vertices at\ndistance at most $r$ from $v$. For a vertex set $X \\subseteq V$, $N^r[X]$ denotes the union\nof $N^r[x]$ for all $x \\in X$. The $r$-th power of $G$ is defined as\n$G^r := \\left(V,\\left\\{uv \\mid u,v \\in V,\\ d_G(u,v) \\leq r\\right\\}\\right)$.\n\nWe write $\\deg_G(v)=\\deg(v)$ for the degree of a vertex $v$,\n and $\\delta$ and $\\Delta$ for the minimum and maximum degree of $G$, respectively.\nA graph is called \\textit{regular} if $\\delta=\\Delta$.\n%\nWe denote the induced subgraph of $G$ on a set $X \\subseteq V$ by $G[X]$.\nFor an edge $e \\in E$, we write $G/e$ for the graph formed from $G$ by contracting the edge\n$e$; for a vertex set $X \\subseteq V$, $G/X$ denotes the graph obtained by contracting all edges in\n$G[X]$.\\looseness-1\n\n"
                },
                "subsection 2.2": {
                    "name": "Dominating Sets \\& Related Problems",
                    "content": "\n\nFor a graph $G=(V,E)$, a vertex set $D \\subseteq V$ is called a \\textit{dominating set}\nif $N[D]=V$.\n%\nAn \\textit{$r$-dominating set} generalizes this notion and is defined as a set $D \\subseteq V$\nsuch that $N^r[D]=V$.\n%\nThe problems asking for such sets of minimum cardinality are\ncalled \\PrbMDS (\\PrbMDSShort) and \\PrbMRDS, respectively, and are NP-complete,\neven for regular graphs of degree 4 \\cite{garey1979computers}.\n%\nAn \\textit{$r$-perfect code} is an $r$-dominating set $D$ such that every vertex $v \\in V$\nsatisfies $\\abs{N^r[v] \\cap D} = 1$.\n%\nIt is NP-complete to determine whether a graph has a perfect code \\cite{KRATOCHVIL1994191,marilynn1997}.\n\nThe problem \\PrbMRDS is further generalized by \\PrbMWDS (\\PrbMWDSShort), where each vertex has non-negative weight,\nand one seeks an $r$-dominating set of minimum total weight.\n%\nAlthough \\PrbMDSShort and \\PrbMWDSShort cannot be approximated within a factor $c \\log \\abs{V}$\nfor some constant $c>0$ in polynomial time (unless P=NP) \\cite{raz1997sub, alon2006algorithmic},\nit is known that \\PrbMWDSShort has a polynomial-time approximation scheme (PTAS)\nin growth-bounded graphs with bounded degree constraint \\cite{wang2012ptas}.\n%BDS: I don't understand what this means\n% \\PrbMWDSShort also has a polynomial-time constant approximation if restricted to unit disk graphs\n% \\cite{ambuhl2006constant,gao20086+,dai20095+,zou2011new}.\n"
                }
            },
            "section 3": {
                "name": "Sparse Dominating Sets",
                "content": "\n\\label{sec:sparse-domset}\n\nIn this section, we formulate the problem of finding \\textit{sparse dominating sets},\n establish its hardness, and describe several heuristics along with\n an integer linear programming (ILP) formulation.\n% Graph theory notation is detailed in Appendix~\\ref{sec:theory}.\n\n\nTo define a \\textit{sparse} dominating set, we first introduce the notion of \\textit{congestion}\n which measures how frequently a given set of vertices overlaps with the $r$-neighborhoods\n in a graph.\n\n\\begin{definition}\n    Given a graph $G=(V,E)$, vertex set $S \\subseteq V$, and radius $r \\in \\N$,\n    the \\textbf{$r$-congestion} of $S$ at a vertex $v \\in V$, denoted $\\rcong(S, v)$,\n    is $\\abs{N^r[v] \\cap S}$.\n    The \\textbf{average $r$-congestion} of $S$ in $G$ is then\n    $\\ravgcong(S) = \\frac{1}{|V|}\\sum_{v \\in V}{\\rcong(S, v)}$.\n\\end{definition}\n\nWe observe that the average congestion of a given set $S$ be computed directly\n from the neighborhood sizes of vertices in $S$.\n\n\\begin{lemma}\\label{lem:prop-avgcong}\n    Average congestion can be computed as\n    $\\ravgcong(S) = \\frac{1}{\\abs{V}}\\sum_{u \\in S}{\\abs{N^r[u]}}$.\n\\end{lemma}\n\nWe say an $r$-dominating set is \\textit{sparse} when it achieves low average $r$-congestion,\nnaturally leading to the following problem.\n\n\\begin{ProblemBox}{\\small \\PrbMCDS (\\PrbMCDSShort) \\normalsize}\n    \\Input & A graph $G=(V,E)$ and radius $r \\in \\N$.\\\\\n    \\Prob  & Find an $r$-dominating set $D \\subseteq V$ such that\n    $\\ravgcong(D)$ is minimized.\\\\\n\\end{ProblemBox}\n\nWe remark that this is distinct from the class of problems studied\n in~\\cite{JAFFKE2019216, einarson2020general} which put uniform local constraints\n on each vertex (e.g. that they are dominated at least $\\lambda$ and at most $\\mu$ times).\n\nWe write $\\mac^r(G)$ ($\\mac(G)$ when $r=1$) for the minimum average congestion attainable\n by any $r$-dominating set on $G$.\nBy Lemma~\\ref{lem:prop-avgcong}, $\\abs{V}\\cdot\\ravgcong(D)$\n equals the weighted sum over $D$ of $w(v)=\\abs{N^r(v)}$.\nThus, \\PrbMCDSShort is a specialization of \\PrbMWDSShort.\nFurthermore, like other dominating set problems~\\cite{slater1976}, \\PrbMCDSShort$(G,r)$ is\n equivalent to \\PrbMCDSShort$(G^r, 1)$, where $G^r$ denotes\n the $r$\\textsuperscript{th} power of $G$.\nWe now establish that minimizing average congestion is NP-hard.\n\n\\begin{theorem}\n    \\PrbMCDSShort is NP-hard.\n    \\label{thm:hardness-mac}\n\\end{theorem}\n\n\\begin{proof}\n    We show that \\PrbMCDSShort with $r=1$ is equivalent to \\PrbMDSShort when\n    $G$ is regular.\n    By Lemma~\\ref{lem:prop-avgcong}, if $G$ is $d$-regular, then\n    $\\ravgcong(S) = \\frac{1}{\\abs{V}}\\sum_{u \\in S} \\abs{N_{G}[u]} =\n    \\left(1+d\\right)\\abs{S}/\\abs{V}$.\n    Thus, any minimum congestion dominating set must also be minimum in size.\n    The result follows directly, since $\\PrbMDSShort$ is NP-hard in regular graphs \\cite{garey1979computers}.\n\\end{proof}\n\nAdditionally, we observe that determining the value of $\\mac_r(G)$ is hard\nvia its relationship to perfect codes. Specifically, $G$ admits an $r$-perfect code if and only if $\\mac_r(G)=1$, but determining the existence of a perfect code is NP-hard \\cite{KRATOCHVIL1994191,marilynn1997}.\n\n% An \\textit{$r$-perfect code} is a dominating set $D$\n%  such that every vertex $v \\in V$ satisfies $\\abs{N^r[v] \\cap D} = 1$.\n% It is NP-complete to determine whether a graph has a perfect code\n% \\cite{KRATOCHVIL1994191,marilynn1997}.\n% Since $G$ admits an $r$-perfect code if and only if $\\mac_r(G)=1$, this completes the proof.\n\nA minimum congestion dominating set thus represents the ``distance'' to a perfect code,\nleading to a natural graph editing problem whose optimal solution is bounded by a linear function of $\\mac(G)$.\n\n\\begin{ProblemBox}{\\PrbPCE}\n    \\Input & A graph $G=(V,E)$ and integer $k$.\\\\\n    \\Prob  & Is there an edge set $S \\subseteq E$ of size at most $k$ such that\n            $G'=(V,E\\setminus S)$ admits a perfect code?\n\\end{ProblemBox}\n\n\\begin{theorem}\\label{thm:prop-pce}\n    \\raggedright\n    Let $\\textnormal{PCE}(G)$ be the minimum $k$ so $(G,k)$ is a yes-instance of \\PrbPCE.\n    Then $\\textnormal{PCE}(G) \\leq (\\mac (G) - 1)n \\leq 2\\cdot \\textnormal{PCE}(G)$.\n\\end{theorem}\n\n\\begin{proof}\n    Given a dominating set $D \\subseteq V$ that attains $\\mac(G)$, let $v \\in V$ be\n    a vertex such that $\\cong(D,v) > 1$.\n    Then, removing an edge $uv$ for any $u \\in D \\setminus \\{v\\}$ will decrease\n    $(\\mac(G)\\cdot n)$ by at least $1$, so $\\textnormal{PCE}(G)\\leq (\\mac(G)-1)n$.\n    Given a perfect code $D' \\subseteq V$ of $G'$, any edge addition can increase\n    $(\\mac(G)\\cdot n)$ by at most $2$, so $(\\mac(G)-1)n \\leq 2\\cdot\\textnormal{PCE}(G)$.\n\\end{proof}\n\n",
                "subsection 3.1": {
                    "name": "Properties of Minimum Congestion Dominating Sets",
                    "content": "\n\nIn general, a minimum congestion dominating set will not also be a minimum dominating set,\n and in Figure~\\ref{fig:sparse-domset} we give a construction proving their sizes can diverge\n arbitrarily.\nFurther, by definition, we have $\\mac(G) \\geq 1$ for any graph;\n we give a degree-based upper bound below.\n\n\\begin{theorem}\n    $\\mac(G)\\leq (\\overline{d}+1)/2$ for every graph $G$,\n     where $\\overline{d}$ is the average degree of $G$.\n    \\label{thm:prop-cong-bound}\n\\end{theorem}\n\n\\begin{proof}\n    Let $D \\subseteq V$ be a minimal dominating set\n    (i.e. every proper subset $D' \\subset D$ is not dominating).\n     Then, $\\overline{D}:=V\\setminus D$ is also a dominating set. Now,\n    $\\avgcong(D) + \\avgcong(\\overline{D}) =\n    \\frac{1}{n}\\sum_{v \\in D}\\abs{N[v]} + \\frac{1}{n}\\sum_{v \\in \\overline{D}}\\abs{N[v]}$.\n    Rewriting the right-hand side as $\\frac{1}{n}\\sum_{v \\in V}\\abs{N[v]} = \\overline{d} + 1$,\n    we have $\\mac(G)\\leq \\min\\{\\avgcong(D), \\avgcong(\\overline{D})\\} \\leq (\\overline{d} + 1) / 2$.\n\\end{proof}\n\n% It is natural to ask how close this is to best-possible; we leave this as an open question.\n\n% \\begin{conjecture}\n%     There is no constant $c$ such that $\\mac(G) \\leq c$ for every graph $G$.\n%     \\label{cnj:mac-upperbound}\n% \\end{conjecture}\n\n"
                },
                "subsection 3.2": {
                    "name": "Algorithms",
                    "content": "\n\nWe now describe several greedy algorithms along with an ILP formulation for \\PrbMCDSShort.\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Greedy Algorithms",
                        "content": "\n\nWe first recall the standard greedy algorithm for \\PrbMDS\n which we call \\AlgDegree.\nAt each step, the algorithm chooses a vertex $v \\in V$ such that\n the number of undominated vertices in $N^r[v]$ is maximized.\nTo instead target minimizing congestion, we prioritize based on\n the \\textit{ratio} of the undominated vertices.\nSpecifically, \\AlgRatio chooses a vertex $v \\in V$ such that\n $\\frac{\\abs{N^r[v] \\setminus N^r[D]}}{\\abs{N^r[v]}}$ is maximized\n (equivalently, $\\frac{\\abs{N^r[v] \\cap N^r[D]}}{\\abs{N^r[v]}}$ is minimized)\n at each step given\n a partial dominating set $D \\subseteq V$.\nIn both algorithms, ties are broken arbitrarily.\n%\nWhile \\AlgDegree is an $\\bigo{r\\log\\Delta}$-approximation for \\PrbRDom,\n%TODO: citation?\n it is not for \\PrbMCDSShort (Theorem~\\ref{thm:dom-degree-unbounded}).\n\n%===========================================================\n%    Dom-Degree not an approximation for MCDS\n%===========================================================\n\\begin{theorem}\\label{thm:dom-degree-unbounded}\n    \\AlgDegree does not approximate \\PrbMCDSShort.\n\\end{theorem}\n\n\\begin{proof}\n    Let $r=1$. Consider a graph $G=(V,E)$ that has a biclique $(A,B)$ with\n    $\\abs{A}=\\abs{B}=k$ and one attached leaf $x_v$ for each $v \\in A \\cup B$. Thus,\n    $n=\\abs{V}=4k$. In the first two iterations, \\AlgDegree should choose vertices $u \\in A$\n    and $v \\in B$. But then, all $4(k-1)$ vertices in $V \\setminus \\{u, v, x_u, x_v\\}$\n    equally have one undominated vertex in their neighborhoods. Thus, \\AlgDegree may choose\n    $D := A \\cup B$, which gives $\\avgcong(D)=n/8 + 1$.\n    However, $\\overline{D} := V \\setminus D$ is a perfect code, so $\\mac(G)=1$.\\looseness-1\n\\end{proof}\n\nIn contrast, \\AlgRatio can produce sets which are arbitrarily bigger\n than the minimum dominating set.\n (Figure~\\ref{fig:sparse-domset} (right) is an example of this),\n yet we prove it is an approximation for \\PrbMCDSShort\\footnote{An abridged proof of Theorem~\\ref{thm:approx-ratio-greedy} is in Appendix~\\ref{sec:theory}.}:\n\n\\begin{theorem}\n    \\AlgRatio is an $\\bigo{\\sqrt{\\Delta^r}}$-approxima\\-tion algorithm for \\PrbMCDSShort.\n    \\label{thm:approx-ratio-greedy}\n\\end{theorem}\n\nTo evaluate smarter tie-breaking strategies, we define \\linebreak\n \\AlgDegreePlus to be \\AlgDegree with ties broken using \\AlgRatio's criteria\n (the ratio of undominated vertices),\n and \\AlgRatioPlus analogously using \\AlgDegree's criteria (the number of undominated vertices).\nFurther ties are randomly broken.\n%\nAll of these algorithms run in time $\\bigo{\\Delta^{2r}n \\log n}$ (details in Appendix~\\ref{sec:alg-details}).\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Integer Programming",
                        "content": "\n\nWe observe that one may obtain optimal solutions to \\PrbRDom and \\PrbMCDSShort\nusing an ILP solver, allowing empirical evaluation of approximation ratios\nin Section~\\ref{sec:exp}.\nWe use the following ILP formulation for \\PrbRDom:\n%\n\\begin{LabelBox}{\\AlgMDS}\n    \\begin{talign*}\n        \\text{Let } &x_v \\in \\{0,1\\}\\text{ be variables} &\\text{for all }v \\in V.\\\\\n        \\text{Minimize } &\\sum_{v \\in V} x_v\\\\\n        \\text{Subject to } &\\sum_{w \\in N^r[v]}x_w \\geq 1 &\\text{for all }v \\in V.\n    \\end{talign*}\n\\end{LabelBox}\n\nSimilarly, we formulate \\PrbMCDSShort as follows:\n%\n\\begin{LabelBox}{\\AlgMAC}\n    \\begin{talign*}\n        \\text{Let } &x_v \\in \\{0,1\\}\\text{ be variables} &\\text{for all }v \\in V.\\\\\n        \\text{Minimize } &\\sum_{v \\in V} \\abs{N^r[v]}x_v\\\\\n        \\text{Subject to } &\\sum_{w \\in N^r[v]}x_w \\geq 1 &\\text{for all }v \\in V.\n    \\end{talign*}\n\\end{LabelBox}\n\nThis is based on the fact that \\PrbMCDSShort is a specialization of \\PrbMWDS.\nIn both cases, the solution set is provided by $\\{v \\in V: x_v=1\\}$.\n"
                    }
                }
            },
            "section 4": {
                "name": "Neighborhood Partitioning",
                "content": "\n\\label{sec:nbr-prt}\n\n\nWe now turn to the second problem arising in our metagenomics application:\npartitioning the vertex set into pieces around a set of $r$-dominators\nas evenly as possible.\nWe first formalize the notion of a \\textit{neighborhood partitioning} and\nuse variance to define its \\textit{balance}.\nAfter establishing that the resulting problem is NP-hard\n(and remains so under very restrictive conditions),\nwe show that for radius $1$, a flow-based approach gives a polynomial-time solution.\nFinally, we describe several algorithmic approaches for obtaining both exact and heuristic\nsolutions.\n\nWe begin by considering the more general setting where we are given $L$,\na set of \\textit{landmarks}\\footnote{\n    We define landmarks to be any set of vertices $L$ so that every\n     $v \\in V$ is reachable from at least one $u \\in L$.\n},\nand ask for a partition of $V$ into $|L|$ disjoint sets so that\n(a) each piece contains exactly one landmark,\n(b) every vertex is assigned to a piece with one of its closest landmarks from $L$ and\n(c) for every piece $A$, the induced subgraph on $A$ preserves the distance between\n the landmark and other vertices in $A$.\\looseness-1\n\n%===========================================================\n%    Neighborhood partitioning\n%===========================================================\n\\begin{definition}\n  Given a graph $G=(V,E)$ and landmarks $L = \\{u_1,\\ldots,u_{\\ell}\\} \\subseteq V$, we say\n  $\\mathcal{A}=\\{A_1,\\ldots,A_{\\ell}\\}$ is a \\textbf{neighborhood partitioning} of $G$\n  with respect to $L$ if and only if $\\mathcal{A}$ is a set partition of $V$ and\n%   $u_{i} \\in A_i$ and\n  $d_{G[A_i]}(v,u_i) = d_G(v,L)$ for every $1 \\leq i \\leq \\ell$, $v \\in A_i$.\n\\end{definition}\n\nWe note that if $L$ is an $r$-dominating set of $G$, the resulting pieces will necessarily\nhave radius at most $r$, making $G/\\mathcal{A}$ an $r$-shallow minor\\footnote{\n    A graph minor formed by contracting disjoint subgraphs of radius at most $r$.\n}.\nThis is essential for \\sgc to maintain efficiency guarantees when\ncomputing the dominating sets in their hierarchy, since it ensures graphs remain\nwithin the assumed sparse class (bounded expansion) \\cite{Brown2020}.\n\nWe now define the problem of finding a set of pieces whose size distribution is\nas even as possible.\n\n\\begin{ProblemBox}{\\small \\PrbBNP (\\PrbBNPShort) \\normalsize}\n    \\Input & A graph $G=(V,E)$ and landmarks $L = \\{u_1,\\ldots,u_{\\ell}\\} \\subseteq V$\\\\\n    \\Prob  & Find a neighborhood partitioning $\\mathcal{A}=\\{A_1,\\ldots,A_{\\ell}\\}$ of $G$\n    on $L$ such that the piece-size population variance\n    $\\text{Var} (\\{ \\abs{A}: A \\in \\mathcal{A}\\})$ is minimized.\n\\end{ProblemBox}\n\n%===========================================================\n%    Min piece size <=> Min square sum\n%===========================================================\nA key observation is that since $V$ and $L$ are given to us,\nthe average size of $\\mathcal{A}$ is $\\abs{V}/\\abs{L}$, which is fixed.\nThis results in the following equivalence:\n\n\\begin{theorem}\\label{thm:min-var-square-relation}\n    A neighborhood partitioning $\\mathcal{A}$ gives the minimum piece-size variance\n    if and only if the square sum of the piece-sizes is minimized.\n\\end{theorem}\n\n%===========================================================\n%    Hardness of BNP\n%===========================================================\nWe now establish the hardness of \\PrbBNPShort.\n\n\\begin{theorem}\\label{thm:hardness-bnp}\n    \\PrbBNPShort is NP-hard.\n    It remains hard even if \\linebreak $\\max_{v\\in V}d(v,L) = 2$ or if $|L|=2$.\n\\end{theorem}\n\nWe break the proof of Theorem~\\ref{thm:hardness-bnp} into two lemmas,\n    first showing the case when the landmarks are a 2-dominating-set by\n    reduction from \\PrbXTC.\n\n    \\begin{ProblemBox}{\\PrbXTC (\\PrbXTCShort)}\n        Input: &Set $X$ with $|X|=3q$ and a collection $\\mathcal{C}$ of 3-element subsets of\n        $X$.\\\\\n        Problem: &Is there a collection $\\mathcal{C'} \\subseteq \\mathcal{C}$ such that\n        every element of $X$ occurs in exactly one member of $\\mathcal{C'}$?\n    \\end{ProblemBox}\n\n%===========================================================\n%    Hardness Reduction of BNP (from X3C)\n%===========================================================\n\\begin{lemma}\n    \\PrbBNPShort is NP-hard when the landmarks are distance at most two from all vertices.\n\\end{lemma}\n\n\\begin{proof}\n    Consider an instance of \\PrbXTCShort with $n=|\\mathcal{C}|$.\n    If $n < q$, then output ``no''.\n    Assuming $n \\geq q$, we construct a graph $G=(V,E)$ as follows.\n    The set of vertices $V$ includes $n$ landmarks\n     $L=\\{u_i \\mid 1 \\leq i \\leq q\\}$, $\\mathcal{C}$, $X$, and attached leaves;\n     $3$ leaves are attached to each of the $n-q$ vertices in $L$.\n    The edges are constructed as follows:\n     $L$ and $\\mathcal{C}$ form a biclique;\n     for each $C \\in \\mathcal{C}$, $C$ is connected to all elements of $X$ it contains.\n    This construction can be done in time $\\bigo{n^2}$.\n    See Figure~\\ref{fig:nbr-hardness-1} for an example.\n\n    We return ``yes'' if and only if \\PrbBNPShort returns a partition with pieces of equal size\n    (by construction, this must be $5$). Let $L' \\subseteq L$ be the landmarks that do not\n    have attached leaves. If the given \\PrbXTCShort instance admits an exact cover\n    $\\mathcal{C'}$, then for each $C \\in \\mathcal{C'}$, we assign one landmark in $L'$ to $C$\n    and all elements of $C$. Then, we assign each of $L \\setminus L'$ to $n - q$ unused sets\n    in $\\mathcal{C}$, resulting in pieces of equal sizes.\n\n    Conversely, suppose \\PrbBNPShort returns an equally-sized partition. Because each piece\n    must have $5$ vertices, $n-q$ sets in $\\mathcal{C}$ are assigned to $L \\subseteq L'$.\n    Also, since $\\abs{L'}=q$, each landmark in $L'$ must have exactly one vertex of\n    $\\mathcal{C}$ in its piece. Then, each piece contains exactly $3$ elements in $X$, and this\n    is an exact cover by $3$-sets.\\looseness-1\n\\end{proof}\n\n%===========================================================\n%    Hardness Reduction of BNP (from SAT)\n%===========================================================\n\\begin{lemma}\n    \\PrbBNPShort is NP-hard when there are two landmarks.\n\\end{lemma}\n\n\\begin{proof}\n    We reduce from \\PrbSAT (\\PrbSATShort).\n%\n    Let $x_1,\\ldots,x_n$ be the variables and $\\phi_1,\\ldots,\\phi_m$ be the clauses appearing in\n    a \\PrbSATShort instance. We construct a graph $G=(V,E)$ as follows.\n    The set of vertices $V$ includes two landmarks $L=\\{u_1, u_2\\}$,\n     $2n$ vertices $X=\\{x_1,\\overline{x}_1,\\ldots,x_n,\\overline{x}_n\\}$\n     representing \\PrbSATShort literals,\n     $n$ variables $Y=\\{y_1,\\ldots,y_n\\}$,\n     $m$ clauses $\\Phi=\\{\\phi_1,\\ldots,\\phi_m\\}$,\n     and attached leaves.\n    A total of $n-1$ leaves are attached to each of $Y$ and $\\Phi$,\n     and $n(n+m)$ leaves are attached to $u_2$.\n    The set of edges $E$ is constructed as follows:\n     $L$ and $X$ form a biclique;\n     each $y_i \\in Y$, is connected to $x_i$ and $\\overline{x}_i$;\n     each $\\phi_j \\in \\Phi$ is connected to all literals in clause $\\phi_j$.\n    This construction can be done in time $\\bigo{n(n+m)}$.\n    See Figure~\\ref{fig:nbr-hardness-2} for an example.\n\n    Let $\\{A, B\\}$ be a partition such that $u_1 \\in A$ and $u_2 \\in B$.\n    Note that in any neighborhood partitioning, each piece must be connected,\n    so $B$ must contain all $n(n+m)$ of the leaves attached to it. Likewise, if a piece includes $Y_i$ or $\\Phi_j$, it must also include their attached leaves.\n\n    We return ``yes'' if \\PrbBNPShort returns a partition of equal sizes.\n    If the given \\PrbSATShort instance is satisfiable, then we include $Y$, $\\Phi$, and all true\n    literals of $X$ in $A$. $B$ includes all other vertices, and $\\abs{A}=\\abs{B}=1+n+n(n+m)$.\n%\n    Conversely, if \\PrbBNPShort returns an equally-sized partition, then $B$ cannot include\n    any of $\\Phi$ or $Y$, as this forces $\\abs{B} > 1+n+n(n+m) > \\abs{A}$.\n    Since $Y \\subseteq A$\n    and  $\\abs{X \\cap A}=n$, exactly one of $\\{x_i, \\overline{x}_i\\}$ is in $A$ for every $i$.\n    Thus, in order for $\\Phi$ to be a subset of $A$, $X \\cap A$ must be a satisfying assignment for the \\PrbSATShort instance.\n\\end{proof}\n\n%\nHowever, if $\\max_{v\\in V}d(v,L) \\leq 1$\n (equivalently, $L$ is a ($1$-)dominating set of $G$),\nthen \\PrbBNPShort becomes tractable.\n\n%===========================================================\n%    BNP with r=1 solvable in poly-time\n%===========================================================\n\\begin{theorem}\\label{thm:runtime-bnp}\n    \\PrbBNPShort can be solved in $\\bigo{|L|n^3}$ if\\linebreak $\\max_{v\\in V}d(v,L) \\leq 1$.\n\\end{theorem}\n\nOur proof relies on Theorem~\\ref{thm:min-var-square-relation} and the fact that the\nproblem is equivalent to \\PrbSBA (\\PrbSBAShort) which can be transformed into an instance of the maximum-cost minimum flow problem\n(formal definition of \\PrbSBAShort and proof in Appendix~\\ref{sec:nbr-detail}).\n\n\n",
                "subsection 4.1": {
                    "name": "Algorithms",
                    "content": "\n%\nWe now present several algorithms for computing neighborhood partitions\nalong with a quadratic programming formulation for \\PrbBNPShort.\nIn all cases, we work with a preprocessed instance which we refer to as\na \\textit{(compact) neighborhood kernel} which can be computed in linear time.\nOur first two algorithms apply greedy strategies and are complemented by an exact\nbranch-and-bound algorithm based on ideas from an FPT algorithm on bounded-degree instances.\nWe defer some proofs of correctness and running time analyses to\nAppendix~\\ref{sec:alg-details}.\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Neighborhood Kernels",
                        "content": "\n\nBy the definition of \\PrbBNPShort, each non-landmark $v$ in a piece $A$ has a path of\nlength $d(v,L)$ to the landmark in $A$.\nThus, given a graph $G=(V,E)$ and landmarks $L \\subseteq V$,\nif an edge $e \\in E$ is not in any shortest $v$-$L$ paths for all $v \\in V$,\nthen we can safely remove $e$ from the original graph.\nThe underlying ideas of our preprocessing are orienting edges outward from the landmarks,\ncreating layers of vertices by distance from the closest landmark,\nand removing unnecessary edges.\nWe call the output of this preprocessing a \\textit{neighborhood kernel}.\n\n%===========================================================\n%    Neighborhood Kernel\n%===========================================================\n\\begin{definition}[Neighborhood Kernel]\\label{def:nbr-knl}\n    Given a graph $G=(V,E)$ and landmarks $L \\subseteq V$,\n    a \\textbf{neighborhood kernel} $H$ is a digraph with vertex set $V$ and\n    edge set $E'=\\{(v,w): vw\\in E,\\ \\ d(v,L) + 1 = d(w,L)\\}$.\n\\end{definition}\n\nBy construction, for every non-landmark vertex $v$ in a neighborhood kernel,\n all shortest paths from $v$ to any landmark must include one of $v$'s in-neighbors in $H$,\n and thus $v$ must be in the same piece as one of its in-neighbors.\nIf $v$ has only one in-neighbor $w$, then $v$ and $w$ must be assigned to the same piece.\nWe encapsulate this idea in the following data reduction, noting that\n by definition, all landmarks are kept in the compact neighborhood kernel.\n%\n \\looseness-1\n\n%===========================================================\n%    Compact Neighborhood Kernel\n%===========================================================\n\\begin{definition}[\\small Compact Neighborhood Kernel\\normalsize]\\label{def:compact-nbr-knl}\n    Given a neighborhood kernel $H$ for a graph $G=(V,E)$ and landmarks $L \\subseteq V$,\n     a \\textbf{compact neighborhood kernel} $(H_c, \\phi)$ is a pair consisting of\n      a digraph $H_c=(V_c,E_c):=H/\\mathcal{A}$ %\n    %   \\footnote{By definition, $L \\subseteq V_c \\subseteq V$.}\n      and a map $\\phi: V_c \\to 2^V$\n      defining its ``bags'',\n      where the collection of bags $\\mathcal{A}:=\\{\\phi(v): v \\in V_c\\}$\n      is a partition of $V$.\n     Additionally, we require the following conditions:\n\\begin{packed_item}\n    \\item Each vertex $v \\in V_c$ is the representative for its bag,\n     and must be the closest to $L$ in $G$ among $\\phi(v)$.\n    \\item All bag members must be assigned to the same landmark in\n     any valid neighborhood partitioning.\n    \\item $\\phi(v)$ is maximal subject to these conditions.\n\\end{packed_item}\n\\end{definition}\n\nWe visualize this process in Figure~\\ref{fig:nbr-kernel}.\nIn Appendix~\\ref{sec:alg-details}, we describe\nalgorithms (\\AlgNbrKnl and \\AlgCmpNbrKnl) for\ncreating (compact) neighborhood kernels in $\\bigo{n + m}$ time.\nFinally, we note that the maximum degree cannot increase under these transformations\n(Lemma~\\ref{lem:compact-nbr-knl-in-degree}).\nWe use this fact to bound the running time of our branch-and-bound algorithm (Section~\\ref{sec:branch-algorithm}).\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Heuristic Algorithms",
                        "content": "\n\nWe developed two heuristics for \\PrbBNPShort.\n\\AlgPrtWeight is a linear-time $\\bigo{n + m}$ greedy algorithm that works\non the compact neighborhood kernel.\n%\nFrom landmarks, it traverses all bags in BFS order,\n assigning each bag to the smallest piece among viable candidates.\n\n%===========================================================\n%    Prt-Weight\n%===========================================================\n\\begin{theorem}\n    \\AlgPrtWeight gives a valid neighborhood partitioning in $\\bigo{n+m}$ time.\n\\end{theorem}\n\n\\begin{proof}\n    We proceed by induction on distance from a landmark.\n    As the base case, landmarks assign themselves and their bag members,\n    which is the only valid option. For the inductive step, consider when $v$ is assigned.\n    Because bags are processed in BFS order, $v$'s in-neighbors\n    must have already been assigned to a valid landmark.\n    Thus, extending the current assignment with any choice at $v$\n    meets all requirements, and \\AlgPrtWeight produces a valid partitioning.\n\n    The running time is $\\bigo{n+m}$ because\n    this algorithm performs BFS, and a landmark's piece size\n    can be updated in $\\bigo{1}$.\n\\end{proof}\n\n%===========================================================\n%    Prt-Layer\n%===========================================================\n\n\\AlgPrtLayer is a polynomial-time $\\bigo{\\abs{L} n^3}$ algorithm that is exact\nwhen $L$ is a ($1$-)dominating set and heuristic otherwise.\nIt works on the neighborhood kernel and solves \\PrbSBAShort at each layer starting\n from the one closest to landmarks\n (a layer $V_i$ is the set of vertices such that\n the distance to the closest landmark is $i$).\n\n\\begin{theorem}\\label{thm:prt-layer-analysis}\n    \\AlgPrtLayer gives a neighborhood partitioning in $\\bigo{\\abs{L}n^3}$ time.\n    Further, it gives an exact solution to \\PrbBNPShort when $L$ is a dominating set of $G$.\n\\end{theorem}\n\n\\begin{proof}\n    If each assigned piece is connected in the neighborhood kernel,\n     then it is a valid neighborhood partitioning\n     because vertices $V_i$ at layer $i$ are distance $i$ away from their closest landmarks,\n     both in the original graph and their assigned piece.\n    The algorithm examines vertices in level order emanating from the landmarks,\n     and by the formulation of \\PrbSBAShort, each of the resulting pieces must be connected.\n    %\n    Also, from Theorem~\\ref{thm:runtime-bnp},\n    this algorithm gives an exact solution when $L$ is a dominating set.\n\n    The running time (other than constructing a neighborhood kernel)\n    is the sum of the time taken for \\PrbSBAShort at each layer.\n    At layer $i$, only the vertices in $V_i$ have choices\n     (all vertices in earlier layers already have an assigned landmark).\n    In a flow problem, we can efficiently preprocess already assigned vertices,\n    thus resulting in time $\\bigo{\\abs{L}\\abs{V_i}^3}$. Because of the fact\n    $V=\\bigcup_i{V_i}$, the total running time is\n    $\\sum_{i}{\\bigo{\\abs{L}\\abs{V_i}^3}} = \\bigo{\\abs{L}n^3}$.\n\\end{proof}\n\n\n%===========================================================\n%    Prt-Branch\n%===========================================================\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Branch-and-bound Algorithm",
                        "content": "\n\\label{sec:branch-algorithm}\n\nBefore elaborating on our exact algorithm for \\PrbBNPShort,\nwe show the following motivating result.\n\n\\begin{theorem}\\label{thm:bnp-fpt}\n    \\PrbBNPShort is fixed parameter tractable (FPT) in graphs of bounded degree\n    parameterized by $k$, the number of vertices equidistant from multiple landmarks.\n\\end{theorem}\n\n\\begin{proof}\n    Given a graph $G=(V,E)$, landmarks $L \\subseteq V$,\n    and their compact neighborhood kernel $(H_c, \\phi)$,\n    let $\\Delta$ be the maximum degree of $G$.\n    Let $X$ be $V(H_c) \\setminus L$.\n    Since all vertices in $X$ are equidistant from multiple landmarks,\n    $\\abs{X} \\leq k$.\n    From Lemma~\\ref{lem:compact-nbr-knl-in-degree},\n    each vertex in $H_c$ has at most $\\Delta$ in-neighbors.\n    Every vertex in $X$ must be in the same piece as one of its in-neighbors\n    for a valid neighborhood partitioning, giving at most $\\Delta^k$ possible assignments.\n    Other computations can be done in polynomial time in $n$,\n    so the brute-force approach results in time $\\bigo{\\Delta^k n^{\\bigo{1}}}$.\n\\end{proof}\n\nThe algorithm \\AlgPrtBranch (documented and implemented in \\cite{codebase}) reinforces\nthis idea by combining efficient base-case handling with naive branch-cut functionality.\nFor a base case, we apply \\AlgPrtLayer if possible;\nspecifically, if there is only one layer $X_i$ left and all bags are of size 1,\n\\AlgPrtLayer gives an exact solution in $\\bigo{\\abs{L}n^3}$ time.\nTo obtain a lower-bound for a branch cut, we exploit the fact that our\npartial solutions $\\mathcal{A'}$ satisfy the following property:\nany solution $\\mathcal{A}$ extending $\\mathcal{A'}$ has\n$\\sum_{A \\in \\mathcal{A}} \\abs{A}^2 \\geq\n\\left(\\sum_{A' \\in \\mathcal{A'}} \\abs{A'}^2\\right) +\n(n - \\abs{\\bigcup \\mathcal{A'}})^2/\\abs{L}$.\nThe total running time of this algorithm is $\\bigo{\\Delta^k \\abs{L}n^3}$.\n\n\n%===========================================================\n%    Prt-QP\n%===========================================================\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "Quadratic Programming",
                        "content": "\n\nFinally, we give a quadratic programming formulation.\nGiven a graph $G=(V,E)$ and landmarks $L \\subseteq V$, let $(H_c=(V_c,E_c), \\phi)$\nbe the compact neighborhood kernel of $G$ on $L$.\nLet $x_{u,v} \\in \\{0,1\\}$ be variables for all $u \\in L, v \\in V_c$;\nset $x_{u,v}=1$ if and only if $v$ is assigned to landmark $u$.\n\n\\begin{LabelBox}{\\AlgPrtQP}\n\\begin{talign*}\n    \\text{Minimize } &\\sum_{u \\in L} \\left(\\sum_{v \\in V_c} \\abs{\\phi(v)} x_{u,v} \\right)^2\\\\\n    \\text{Subject to } &\\sum_{u \\in L} x_{u,v} = 1 \\quad\\text{for all }v \\in V_c,\\\\\n    &\\sum_{w \\in N_{H_c}^{-}[v]} x_{u,w} \\geq x_{u,v} \\quad \\text{for all } u \\in L, v \\in V_c \\setminus L.\\nonumber\n\\end{talign*}\n\\end{LabelBox}\n\n%\nBy Theorem~\\ref{thm:min-var-square-relation}, the objective function guarantees\nminimum variance.\nThe first constraint enforces that every vertex must be assigned to exactly one landmark,\nand the second constraint guarantees that every non-landmark must be in the same piece\nas one of its in-neighbors.\nThe balanced neighborhood partitioning is given by\n$\\mathcal{A} = \\{A_{u_1}, \\ldots, A_{u_\\ell}\\}$\nwhere $\\displaystyle A_u = \\bigcup_{v \\in V_c: x_{u,v}=1} \\phi(v)$.\n"
                    }
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\n\\label{sec:exp}\n\n\nTo complement our theoretical results, we implemented and evaluated our algorithms\non a diverse corpus of networks including cDBGs constructed from real metagenomes,\nsmall instances from DIMACS10~\\cite{bader2013graph}, and graphs from~\\cite{nguyen2020}.\nWe categorized each graph as \\textit{small}, \\textit{medium}, or \\textit{large}\n based on the number of edges; all data, along with a summary table of statistics\n are available at~\\cite{codebase}. All algorithms were implemented in C++ %\n%\\footnote{\n%    All source code will be made available on GitHub\n%    under a BSD 3-clause open source license prior to publication.\n%} \nand experiments were conducted on a Linux machine\n(details in Appendix~\\ref{sec:exp-setup}).\n\n",
                "subsection 5.1": {
                    "name": "Sparse Dominating Sets",
                    "content": "\n\nTo evaluate our algorithms for finding a low-congestion dominating set,\n we first tested the effectiveness of tie-breaking strategies by running\n \\AlgDegree (denoted \\AlgDegreeShort), \\AlgRatio (\\AlgRatioShort),\n \\AlgDegreePlus (\\AlgDegreePlusShort) as well as \\AlgRatioPlus(\\AlgRatioPlusShort)\n on small instances.\nThe algorithms with a tie-breaking strategy consistently outperformed those without one,\n finding smaller dominating sets in 78\\% of the experiments and\n less-congested dominating sets in 93\\% of them.\nRunning time was also improved by tie-breaking strategies in 55\\% of instances,\n likely due to its proportionality to solution size.\n\nBased on these findings, we restricted our attention to \\AlgDegreePlusShort and\n\\AlgRatioPlusShort for the remaining experiments.\nWe evaluated them, along with \\AlgDomSGC (\\AlgDomSGCShort),\\looseness-1\n~\\AlgMDS (\\AlgMDSShort), and \\AlgMAC (\\AlgMACShort) on the full corpus with radii $\\{1, 2\\}$\n(plus radii $\\{3, 5, 10\\}$ on small and medium instances). We used a default 3-hour timeout,\nreduced to 1 hour for \\AlgMDSShort and \\AlgMACShort for radius $>1$ and medium/large instances.\n\n\nFigure~\\ref{fig:exp-domset} (left) shows the distribution of running times sorted\nby $\\norm{G^r}$, the number of edges in the $r$-th power of $G$.\nWe observe a linear trend for \\AlgDomSGCShort, \\AlgDegreePlusShort, and \\AlgRatioPlusShort,\nestablishing efficiency.\nWhile the exact algorithms \\AlgMDSShort and \\AlgMACShort are prone to timing out, they did\nfinish on some larger instances. We hypothesize this success is directly related to reduction\nrules in the ILP solver related to vertices with degree $1$ and $2$.\n%, as supported by Table~\\ref{tab:dataset}.\n\nTo evaluate solution quality, we plotted the relationship between the solution size and\naverage congestion, both relative to the best-known (Figure~\\ref{fig:exp-domset}, right).\nAs might be expected, \\AlgDegreePlusShort and \\AlgMDSShort find smaller, more congested\ndominating sets, while \\AlgRatioPlusShort and \\AlgMACShort find larger, sparser ones.\nThe algorithm used in the prior metagenomic analysis (\\AlgDomSGCShort) intermediates between\nthe two, but often at the cost of being much larger or more congested.\\looseness-1\n\n"
                },
                "subsection 5.2": {
                    "name": "Balanced Neighborhood Partitioning",
                    "content": "\n\nTurning to the problem of generating uniformly-sized pieces around a set of dominators,\nwe tested the scalability and solution quality of the algorithms for \\PrbBNPShort\nas well as the impact of choosing smaller versus sparser dominating sets.\nTo this end, we ran \\AlgPrtSGC (\\AlgPrtSGCShort),\n\\AlgPrtWeight (\\AlgPrtWeightShort), \\AlgPrtLayer (\\AlgPrtLayerShort),\n\\AlgPrtBranch \\linebreak(\\AlgPrtBranchShort), and \\AlgPrtQP (\\AlgPrtQPShort)\nusing dominating sets produced by \\AlgDomSGCShort, \\AlgDegreePlusShort, and \\AlgRatioPlusShort.\nAll runs were subject to a 1-hour timeout.\n\nWe first evaluated the running time and timeout rate for all \\PrbBNPShort\nalgorithms (Table~\\ref{tab:exp-nbr-runtime}) on the corpus of small instances.\nThe algorithm \\AlgPrtWeightShort was consistently fastest, followed by \\AlgPrtSGCShort;\nboth had no timeouts.\nThe polynomial-time \\AlgPrtLayerShort was slower than the linear-time approaches but still\ncompleted nearly all small and medium instances. It is particularly notable that when $r=1$,\nit successfully output optimal solutions, which \\AlgPrtQPShort could not find.\nWe also observed that \\AlgPrtQPShort's performance improves when given a cDBG and dominators\nfrom \\AlgDegreePlusShort or \\AlgRatioPlusShort.\nLastly, \\AlgPrtBranchShort was unable to finish with larger radii,\neven on these small instances, eliminating it from use in additional experiments.\n\n\\vspace*{-1.5em}\n\nFor both remaining experiments, we ran on all small and medium instances.\nTo assess the impact of dominator selection on the variance of piece sizes, we restrict our\nattention to \\AlgPrtWeightShort and measure the standard deviations of\nthe piece sizes (Figure~\\ref{fig:exp-nbrprt} (left)).\nIn general, dominating sets from \\AlgRatioPlusShort tend to result in more balanced\nneighborhood partitionings than \\AlgDegreePlusShort, with some outliers at larger radii.\nWe note that while \\AlgPrtSGCShort achieves the least variance (except at $r=10$),\nthis is in part due to having larger dominating sets (thus a smaller mean piece size), exposing a limitation of this metric.\n\n\nFinally, we compare the solution quality of our greedy algorithms for neighborhood\npartitioning using fixed dominating sets found by \\AlgRatioPlusShort.\nIn Figure~\\ref{fig:exp-nbrprt} (right), we see that \\AlgPrtWeightShort achieves more balanced\npieces than \\AlgPrtSGCShort at all radii while keeping the same asymptotic\nrunning time. The more expensive \\AlgPrtLayerShort performs even better, achieving nearly optimal results.\nWith larger radii (e.g. $r \\geq 3$), \\AlgPrtWeightShort performs as well as \\AlgPrtLayerShort\n does.\n\n\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Metagenome Neighborhood Queries",
                    "content": "\n\nGiven the motivation for this work is improving the metagenomics analysis pipeline\nfrom~\\cite{Brown2020}, we also assessed the impact of our techniques in this setting.\nSpecifically, we verified that using sparse dominating sets and balanced neighborhood partitionings\nfrom our algorithms does not significantly degrade the containment and similarity of neighborhood\nqueries in \\textsf{HuSB1}, a large real-world metagenome \\cite{hu2016genome}.\n%\nWe used Brown et al.'s replication pipeline\\footnote{https://github.com/ctb/2020-rerun-hu-s1}\nand compared our algorithms (\\AlgDegreePlusShort and \\AlgRatioPlusShort for \\PrbMCDSShort;\n\\AlgPrtWeightShort for \\PrbBNPShort) to those used in~\\cite{Brown2020}\n(\\AlgDomSGCShort and \\AlgPrtSGCShort, respectively).\nWhile~\\cite{Brown2020} restricted their attention to radius 1, we also include results from\nradius 2, as this is of interest in ongoing related work by Brown et al.\\looseness-1\n\nBefore running neighborhood queries, we evaluated the piece size distributions as shown in\nFigure~\\ref{fig:exp-query} (left, center). We observe that independent of dominator\nselection, \\AlgPrtWeightShort successfully reduced the piece size variance.\nWhen comparing dominating sets, we see a trend consistent with other experiments.\n\\AlgDomSGCShort chooses many dominators, resulting in smaller more uniform piece sizes.\n\\AlgDegreePlusShort has the fewest dominators and higher variance, larger pieces and\n\\AlgRatioPlusShort gives larger, balanced pieces.\n\n\nFinally, we re-ran the neighborhood queries from ~\\cite{Brown2020}.\nWhile the containment was completely preserved,\nthe similarities\\footnote{Jaccard similarity to original query} exhibited mild variation,\nshown in Figure~\\ref{fig:exp-query} (right). While there is a slight overall reduction\n(partly explainable by having fewer pieces), the difference is marginal at $r=2$.\nThese preliminary results indicate that our techniques can significantly improve the balance\nof piece sizes (leading to more efficient downstream analysis when a hierarchy of dominating\nsets is constructed) without significantly degrading fidelity of queries.\nAdditional experiments are needed to better assess the impact of this work in the\nmetagenomic setting.\n%\n% \\looseness-1\n"
                }
            },
            "section 6": {
                "name": "Conclusions \\& Future Work",
                "content": "\n\nThis paper tackles two problems arising in a recent approach to metagenome analysis:\nfinding ($r$-)dominating sets which minimize the number of vertices with multiple closest\ndominators, and partitioning the vertices of a graph into connected pieces around a set of landmarks\nminimizing variance of the piece-sizes while guaranteeing every vertex is assigned to some\nclosest landmark.\n\nWe formalize the first using the notion of \\textit{congestion},\n and show that finding minimum congestion $r$-dominating sets (\\PrbMCDSShort) is NP-hard.\nWe introduce linear-time algorithms for finding low-congestion dominating sets and evaluate\n their effectiveness on a large corpus of real-world data,\n showing trade-offs between solution size and average congestion.\nIt remains open whether there is a constant upper bound on the minimum average congestion\n%  (Conjecture~\\ref{cnj:mac-upperbound};\n (the largest value observed in our experiments was 3.7351);\n further, we believe the approximation bound on \\PrbMCDSShort is not tight.\nWe are intrigued by the connection of this problem to \\PrbPCE; this could be a fruitful direction for future work.\n\nTurning to the partitioning problem (\\PrbBNPShort), we show that at radius $1$,\nflow-based techniques give exact solutions in polynomial time, but the problem becomes\nNP-hard even in very restricted cases as soon as $r \\geq 2$.\nOur heuristics, however, produce nearly optimal results in our experiments.\nFurther, we show that using sparse dominating sets (such as those from \\AlgRatioPlusShort)\nimproves both the running time and solution quality of algorithms for \\PrbBNPShort.\nA natural question is whether there are alternative measures of partition balance in\nreal-world data which avoid the inverse relationship between dominating set size\nand the variance of the piece size distribution.\\looseness-1\n\nFinally, we integrate our algorithms into the metagenomic analysis pipeline\nused in~\\cite{Brown2020} and demonstrate that on a large real metagenome (\\textsf{HuSB1}),\nsparse dominating sets and balanced neighborhood partitionings reduce piece size variability\nrelative to the prior approaches without significantly degrading the fidelity of neighborhood\nqueries.\nIt remains to see how these preliminary results extend to other metagenomic datasets and\nqueries.\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\n  \\input{sections/07_acknowledgements.tex}\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{ms}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n\n\\newpage\n"
            },
            "section 7": {
                "name": "Theoretical Results",
                "content": "\n\\label{sec:theory}\n\n% \\input{figures/dataset_table.tex}\n\n"
            },
            "section 8": {
                "name": "Experimental Setup",
                "content": "\n\\label{sec:exp-setup}\n\n% \\subsection{Dataset}\n\nDatasets, along with a summary of their invariants, are available at~\\cite{codebase}.\nMetagenomic cDBGs were generated by BCALM 2 (v2.2.1)\\cite{10.1093/bioinformatics/btw279}\nwith k-size 31. Some non-metagenome networks originated from the Network Data Repository \\cite{nr}.\nFor disconnected graphs, we used the largest connected component.\n\n% \\subsection{Hardware} %\\subsection{Software}\n\nWe ran all experiments on identical hardware, equipped with 40 CPUs\n(Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz) and 190 GB of memory,\nand running CentOS Linux release 7.9.2009. \\sgc is written in Python 3.\nWe used Gurobi Optimizer 9.1.2 for ILP and QP. We used Plotly for creating charts.\nOur algorithms are written in C++ with OpenMP and compiled with gcc 10.2.0.\n\n% \\subsection{Reproducibility}\n\nExperimental results are fully replicable using the code and data\nat~\\cite{codebase}; detailed instructions in \\texttt{README.md}.\n\n"
            }
        },
        "git_link": "https://github.com/spacegraphcats/spacegraphcats"
    }
}