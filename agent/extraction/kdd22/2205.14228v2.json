{
    "meta_info": {
        "title": "Sparse Conditional Hidden Markov Model for Weakly Supervised Named  Entity Recognition",
        "abstract": "Weakly supervised named entity recognition methods train label models to\naggregate the token annotations of multiple noisy labeling functions (LFs)\nwithout seeing any manually annotated labels. To work well, the label model\nneeds to contextually identify and emphasize well-performed LFs while\ndown-weighting the under-performers. However, evaluating the LFs is challenging\ndue to the lack of ground truths. To address this issue, we propose the sparse\nconditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire\nemission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating\nits diagonal elements, which are considered as the reliability scores of the\nLFs. The sparse scores are then expanded to the full-fledged emission matrix\nwith pre-defined expansion functions. We also augment the emission with\nweighted XOR scores, which track the probabilities of an LF observing incorrect\nentities. Sparse-CHMM is optimized through unsupervised learning with a\nthree-stage training pipeline that reduces the training difficulty and prevents\nthe model from falling into local optima. Compared with the baselines in the\nWrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on\nfive comprehensive datasets. Experiments show that each component of\nSparse-CHMM is effective, and the estimated LF reliabilities strongly correlate\nwith true LF F1 scores.",
        "author": "Yinghao Li, Le Song, Chao Zhang",
        "link": "http://arxiv.org/abs/2205.14228v2",
        "category": [
            "cs.CL"
        ],
        "additionl_info": "11 pages, 8 figures, 11 tables"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:introduction}\n\nNamed entity recognition (NER), a task that identifies pre-defined named entities, is fundamental for extracting information from unstructured text data.\nFor example, to facilitate material synthesis, material scientists often need to extract entities such as materials, properties, and catalysts from research articles \\citep{kim.2017.Materials.Synthesis}.\nNER also powers tasks such as question answering and knowledge graph construction.\nCurrently, prevailing NER methods are fully supervised models trained on a substantial amount of human-annotated data.\nHowever, annotating sentences with named entity labels is difficult, expensive, and time-consuming \\cite{Boecking.2021.iws, zhang-2021-wrench}.\nThe manually annotated dataset is also hard to expand---to insert new entity types or append new sentences, annotators must go over each sample, the effort of which is proportional to the data size.\n\\emph{Weak supervision} is proposed in recent works \\citep{hoffmann-etal-2011-knowledge, Ratner-2017-Snorkel} to address this difficulty.\nInstead of human annotations, weakly supervised methods generate multiple sets of \\emph{weak annotations} using labeling functions (LFs, \\aka weak supervision sources).\nThey can be knowledge bases, domain-specific dictionaries, pattern matching regular expressions or out-of-domain supervised models \\cite{lison-etal-2020-named}.\nConstructing LFs is significantly cheaper than annotating tokens manually.\nAnd the weak annotations are expandable---the LFs can automatically annotate incoming sentences without any extra human labor.\n\n\n\nNonetheless, it is challenging to design \\emph{label models} which aggregate the LF annotations without seeing any ground-truth labels.\nLFs are usually simple, and their annotations often suffer from incompleteness (low coverage/recall) and inaccuracy (low precision).\nMoreover, the LFs can conflict with each other, generating contradictory results \\cite{Ratner-2016-data-programming}.\nWithout true labels, solving the conflicts and reducing the annotation noise become a tricky task.\n\nSome weakly supervised NER methods such as majority voting (MV) and Snorkel \\citep{Ratner-2017-Snorkel} classify each token in the sentence independently.\nSuch approaches neglect the complex inter-word relationship of natural language.\nRecent works \\citep{nguyen-etal-2017-aggregating, Safranchik-etal-2020-weakly, lison-etal-2020-named, li-etal-2021-bertifying, parker-yu-2021-named} parameterize the label sequences with graphical models, regarding the unobserved ``true labels'' as latent variables and inferring them by optimizing the probabilities of the observed weak annotations through unsupervised learning.\n\\citet{nguyen-etal-2017-aggregating}, \\citet{Safranchik-etal-2020-weakly} and \\citet{lison-etal-2020-named} apply the hidden Markov models (HMMs) to capture the token dependency with the transition matrix.\nHowever, the Markov property constrains HMMs' modeling of such dependency to only consecutive tokens meanwhile largely ignoring sentence semantics.\nThere are attempts to integrate neural networks (NNs) with graphical models \\citep{li-etal-2021-bertifying, parker-yu-2021-named}.\n\\citet{li-etal-2021-bertifying} propose the conditional HMM (CHMM) that uses BERT token embeddings \\cite{devlin-etal-2019-bert} to predict HMMs' transition and emission probabilities with NNs.\nAs each token embedding encodes the contextual semantics of the whole sentence, CHMM incorporates more comprehensive context information and alleviates the Markov constraint.\nA similar approach is DWS \\citep{parker-yu-2021-named}, which substitutes HMM by the conditional random field (CRF) and predicts the latent variables instead of their transitions.\nThe problem with both CHMM and DWS is that they directly predict all elements in the emission matrices from the embeddings.\nWhen the number of LFs or labels is large, doing so requires a myriad of network parameters.\nAs the optimization is non-convex, this creates many local optima, making the model hard to train.\nBesides, the training and inference of a large NN are much more computationally demanding, which hurts model efficiency.\n\nIn this work, we propose \\ours.\nMaintaining the transition scheme, \\ours enhances CHMM by restricting its emission process with the empirical structure of an emission matrix (Figure~\\ref{fig:lf.examples}).\nGenerally, the diagonal of the emission matrix is its most essential component, which represents the reliability of the LF annotations.\nConsidering this fact, \\ours first focuses on estimating the LF reliability scores instead of the entire emission matrix.\nThen it expands the sparse scores to the full-fledged emission matrix by placing the scores at the diagonals and filling up other vacancies with our designed expansion functions (\\cref{subsec:basic-emiss}).\nCompared with CHMM, \\ours features sparser emission prediction process with much fewer parameters to learn, making the model easier to optimize and more efficient to train.\n\nTo enhance \\ours's estimation of off-diagonal emission elements, we introduce another component---the weighted XOR (WXOR) scores (\\cref{subsec:addon-emiss}).\nThe off-diagonal elements model LFs' probabilities of generating incorrect entity annotations.\nThese probabilities are typically small but sometimes comparable with or larger than the diagonal elements.\nIn such cases, using the diagonal-oriented reliability expansion functions alone is biased and inflexible.\nTo address this issue, we leverage LF annotations and the predicted LF reliabilities to calculate LFs' mutual disagreement frequencies as WXOR scores to refine off-diagonal emission elements.\n% The scores are then scaled by NN-predicted scaling factors and added to the emission matrix \n\nConsidering the amount of uncertainty in the emission, we treat the function-induced deterministic values as priors, with which a Dirichlet distribution is parameterized.\nWe then sample our final emission probabilities from the distribution (\\cref{subsec:emiss-sampling}).\nPrevious works \\citep{Kingma-2014-VAE, chem-2021-DrNAS} show that incorporating the probability model expands the search space, regularizes the gradient flow, and further prevents the model from being trapped by local optima.\n\nIn addition, to improve learning \\ours's inter-correlated components, we introduce a three-stage training strategy.\nEach stage optimizes only a subset of model parameters (\\cref{subsec:training-procedure}) to reduce mutual interference and promote the training stability.\n\nIn summary, our contributions include:\n\\begin{itemize}[leftmargin=*]\n  \\item proposing \\ours to estimate LF reliability scores from sentence embeddings, and expand the scores to sparsely predicted emission matrices with our designed expansion functions;\n  \\item modeling the probabilities of an LF generating incorrect annotations with the WXOR scores to augment the estimation of emission off-diagonal elements;\n  \\item a three-stage training strategy and the Dirichlet sampling process to improve training stability and model robustness; and\n  \\item thorough experiments on $5$ comprehensive datasets with SOTA baselines that demonstrate the superiority of \\ours, and ablation studies that justify the design of model components.\n\\end{itemize}\n\nThe code and data are available at \\href{https://github.com/Yinghao-Li/Sparse-CHMM}{github.com/Yinghao-Li/Sparse-CHMM} for reproducibility.\n\n\n"
            },
            "section 2": {
                "name": "Problem Definition",
                "content": "\n\\label{sec:definition}\n\nIn this section, we define the weakly supervised NER task.\nSuppose we have $T$ tokens $\\w^{(1:T)}$ in the input sentence and a set of candidate entities $\\ents$ with size $E$, NER models assign one entity label to each token $\\w^{(t)}, t\\in 1:T$.\nUsing the BIO tagging scheme, the label set is $\\lbs = \\{ \\texttt{O} \\} \\cup \\{ \\texttt{B-}\\ent, \\texttt{I-}\\ent\\}_{\\ent\\in\\ents}$ with the size of $L=2E+1$.\nLabel ``\\lbfont{O}'' indicates that the current token belongs to no pre-defined entities, which is always indexed by $1$ in the following discussion.\nAn example of NER labels is shown in Figure~\\ref{fig:lf.examples}.\n\nFor weakly supervised NER, we have $K$ independent LFs, each providing a sequence of weak annotations $\\x^{(1:T)}_k$.\n$\\x^{(t)}_k \\in \\{0,1\\}^{L}$ is one-hot over the label set $\\lbs$.\nLabel models aim to approximate the ground-truth labels $\\y^{(1:T)} \\in \\lbs^T$ with latent states $\\z^{(1:T)} \\in \\lbs^T$ given the input tokens and weak annotations $\\{\\w^{(1:T)}, \\x^{(1:T)}_{1:K} \\}$.\n\nIn the following discussion, we do not distinguish the label string and label index, and uniformly represent them by integers $l$, $i$, or $j\\in 1:L$.\nIn addition, we focus on one sentence and omit the sentence index $m \\in 1:M$ unless specified otherwise.\n\n\n\n\n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\n\\label{sec:method}\n\n\\ours has three main components:\n1) the transition matrix $\\bPsi$ (\\cref{subsec:transition}),\n2) the \\emph{emission base prior} matrix $\\bLambda$ (\\cref{subsec:basic-emiss}), and\n3) the \\emph{emission addon prior} matrix $\\bDelta$ (\\cref{subsec:addon-emiss}).\nThe transition matrix controls the probability of moving from one latent state to another.\n$\\bLambda$ and $\\bDelta$ are the components of the emission matrix $\\bPhi$ (\\cref{subsec:emiss-sampling}), each element in which $\\Phi_{k,i,j} \\triangleq p(x_{k,j}^{(t)}=1|z^{(t)}=i)$ defines the probability of LF $k$ observing label $j$ when the true label is $i$.\n$\\bLambda$ focuses on the emission diagonals, whereas $\\bDelta$ refines the off-diagonal values.\nIn \\cref{subsec:training}, we describe the training strategy and present a three-stage pipeline that increases training stability and maximizes the model performance.\n\n\n",
                "subsection 3.1": {
                    "name": "Transition",
                    "content": "\n\\label{subsec:transition}\n\nSame as \\citet{li-etal-2021-bertifying}, we directly predict the \\emph{token-wise} transition probabilities $\\Psi^{(t)}_{i,j} \\triangleq p(z^{(t)}=j|z^{(t-1)}=i, \\e^{(t)})$ from the BERT \\emph{token} embeddings $\\e^{(t)}$.\nSpecifically, we input the embeddings into one fully connected (FC) layer of NN and reshape the output vectors into $L\\times L$ matrices.\nA SoftMax function is applied along the columns to make each row a point in an $L$-dimensional probability simplex.\n\\begin{gather*}\n  \\bm{S}^{(t)} \\in \\R^{L\\times L} = \\text{reshape}(\\fc(\\e^{(t)})); \\\\\n  \\bPsi_{l,:}^{(t)} \\in (0,1)^L = \\softmax(\\bm{S}_{l,:}^{(t)}).\n\\end{gather*}\nHere $\\bm{S}^{(t)}$ is an intermediate variable, and $l$ is the row label index.\n\n"
                },
                "subsection 3.2": {
                    "name": "Emission",
                    "content": "\n\\label{subsec:emission}\n\nInstead of predicting every element in the emission matrices $\\bPhi^{(t)} \\in [0,1]^{K \\times L \\times L}$, \\ours predicts LF reliabilities $\\tilde{\\A} \\in [0,1]^{K \\times L}$ with NN and expands it to the base prior $\\bLambda \\in [0,1]^{K \\times L \\times L}$ through pre-defined functions (\\cref{subsec:basic-emiss}).\nFrom $\\tilde{\\A}$ and LF observations $\\x$, we also calculate the WXOR scores $\\tilde{\\W} \\in [0,1]^{K \\times L \\times L}$ that track the probabilities of LFs' giving incorrect annotations.\n$\\tilde{\\W}$ is then scaled by another NN predicted matrix $\\C\\in[0,1]^{K\\times L}$ to form the addon prior $\\bDelta$ (\\cref{subsec:addon-emiss}).\nWe sample the emission matrix $\\bPhi$ from a Dirichlet distribution parameterized by $\\bLambda$ and $\\bDelta$ to facilitate model training (\\cref{subsec:emiss-sampling}).\nThe pipeline is illustrated in Figure~\\ref{fig:emiss.constr}.\n\nCompared with the transition, the emission variance is much smaller \\wrt the input tokens.\nSo we predict the \\emph{sentence-level} emission-related variables from the BERT \\emph{sentence} embedding $\\e^{(0)}$ \\citep{devlin-etal-2019-bert}, which deprecates their token indicator ``$\\cdot^{(t)}$''.\n\n\n\n",
                    "subsubsection 3.2.1": {
                        "name": "Labeling Function Reliabilities and Emission Base Prior",
                        "content": "\n\\label{subsec:basic-emiss}\n\nFor LF $k$, the diagonal elements of the emission matrix \n\\begin{equation*}\n  \\Phi_{k, l, l} \\triangleq p(x_{k,l}^{(t)}=1|z^{(t)}=l, \\e^{(0)}), \\quad l \\in 1:L,\n\\end{equation*}\nas shown in Figure~\\ref{fig:lf.examples}, has a distinctive physical meaning: the probabilities of LF $k$ observing the correct labels, \\ie, the reliabilities of LF $k$.\n% Since we regard the latent states $\\z$ as a prediction of the true labels $\\y$, the diagonal values are the probabilities of LF $k$ observing the correct labels.\nIn other words, we can estimate the performance of LF $k$ given its emission matrix $\\bPhi_k$, or vice versa: \\emph{we can construct a plausible emission matrix given the knowledge of how an LF performs.}\n\nWith this concept established, we start by predicting the reliability logits $\\A \\in \\R^{K\\times L}$ from the sentence embedding:\n\\begin{equation*}\n  \\A = \\text{reshape}(\\fc(\\e^{(0)})),\n\\end{equation*}\nwhich is to be placed at the diagonal of $\\bPhi$ after normalization.\\footnote{\n  On datasets with more than one entity types, we adopt entity-level reliability scores, \\ie, $A_{k,\\lbfont{B-}\\ent}=A_{k,\\lbfont{I-}\\ent},$.\n  This reduces the model size and training difficulty.\n}\nHowever, as LFs observe more label \\texttt{O} (indexed by $1$) than other labels, the model tend to emphasize the corresponding weight $\\Phi_{k,l,1} \\triangleq p(x_{k,1}^{(t)}=1|z^{(t)}=l, \\e^{(0)})$ in the emission.\nBecause the vector $\\bPhi_{k,l}$ is from a simplex and sums up to $1$, large $\\Phi_{k,l,1}$ results in unreasonably small diagonal values $\\Phi_{k,l,l}$.\nFortunately, this can be fixed by applying SoftMax along the LFs:\n\\begin{equation}\n  \\label{eq:softmax-A}\n  \\begin{gathered}\n    \\hat{\\A}_{:, l} = \\softmax(\\A_{:, l}), \\quad l\\geq 2.\n  \\end{gathered}\n\\end{equation}\nIt guarantees that at least one LF has a high score, preventing the model from neglecting all weak annotations simultaneously.\nSince ${\\A}_{k, 1}$ is the emission confidence to \\lbfont{O} itself, we only need to constrain its value to $(0,1)$ through the element-wise sigmoid function $\\sigma$:\n\\begin{equation*}\n  \\label{eq:sigmoid-A}\n  \\hat{\\A}_{:, 1} = \\sigma( \\A_{:, 1} ).\n\\end{equation*}\n\n\nHowever, SoftMax in \\eqref{eq:softmax-A} enforces $\\sum_k \\hat{\\A}_{k, l} = 1$.\nThis correlation establishes a \\emph{fixed-size} pool from which reliability scores are drawn, which harms the model flexibility as it fails the cases where multiple LFs are confident, or no LF is confident.\nTo de-correlate the scores, we introduce a piecewise scaling function (illustrated in Figure~\\ref{fig:ha}):\n\\begin{equation}\n  \\label{eq:hnsr}\n  \\begin{gathered}\n    \\tilde{A}_{k, l} = h_{n,s,r}(\\hat{A}_{k, l}); \\\\\n    h_{n,s,r}(a)= \n    \\begin{cases}\n      \\frac{1}{r^{n-1}} a & a^\\frac{1}{s} < r; \\\\\n      -\\frac{1}{(1-r)^{(n-1)}}(1-a^\\frac{1}{s})^n + 1 &  a^\\frac{1}{s} \\geq r.\n    \\end{cases}\n  \\end{gathered}\n\\end{equation}\nIt calibrates the SoftMax output with scales defined by exponents $n$ and $s$.\nTo enable more subtle control of the scores, we utilize the split point $r \\in [0, 1]$ to segment the input domain into upper and lower halves, each scaled differently.\\footnote{\\label{criteria}Please refer to \\cref{appsec:function.criteria} for design principles.}\n\nWe then expand $\\tilde{\\A}$ to the \\emph{emission base prior} matrix $\\bLambda \\in [0,1]^{K \\times L \\times L}$ through expansion functions defined according to the latent state $z^{(t)} = i$ and the observation $x_{k,j}^{(t)} = 1$:\n\\begin{equation*}\n  \\Lambda_{k,i,j} = f_{i,j}(\\tilde{A}_{k,i});\\quad \\forall i,j \\in 1:L,\n\\end{equation*}\n\\begin{equation}\n  \\label{eq:fij}\n  f_{i,j}(a) = \n  \\begin{cases}\n    a & i=j; \\\\\n    \\frac{1}{L-1}(1-a) & i=1,j\\geq 2; \\\\\n    g_{n,r}( a ) & i \\geq 2, j=1; \\\\\n    \\frac{1}{L-2}(1-a-g_{n,r}( a )) & i \\geq 2, j \\geq 2, i\\neq j,\n  \\end{cases}\n\\end{equation}\n\\begin{equation}\n  \\label{eq:gnr}\n  g_{n,r}( a ) =\n  \\begin{cases}\n    \\frac{2-L}{(n-1)r^n-nr^{n-1}}a^n + (1-L)x + 1 & a \\leq r; \\\\\n    \\frac{g_{n,r}( r )}{r-1}a - \\frac{g_{n,r}( r )}{r-1} & a > r,\n  \\end{cases}\n\\end{equation}\nwhere $n$ is the exponential term and $r$ is the split point.\nThe reliability scores $\\tilde{\\A}_k$ of LF $k$ are placed at the diagonal of $\\bLambda_k$ (\\ie, function $f_{i=j}$).\nWhen the latent state is \\lbfont{O} ($z^{(t)}=i=1$), the probabilities of emitting to non-\\lbfont{O} $p(\\x^{(t)}_{k,j} = 1 | z^{(t)}=1, \\e^{(0)}),\\ \\forall j\\geq 2$ are uniform and sum up to $1-\\tilde{A}_{k,1}$.\nWhen the latent state is not \\lbfont{O} ($z^{(t)}=i \\geq 2$), an LF statistically inclines toward observing \\lbfont{O} than other entities,\nmaking emit-to-\\lbfont{O} probabilities $p(\\x^{(t)}_{k,1} = 1 | z^{(t)}=i, \\e^{(0)}),\\ \\forall i\\geq 2$ larger and more important than other off-diagonal values $p(\\x^{(t)}_{k,j} = 1 | z^{(t)}=i, \\e^{(0)}),\\ \\forall i,j\\geq 2, i\\neq j$, as shown in Figure~\\ref{fig:lf.examples}.\nTherefore, we specify $g_{n,r}$ to emphasize emit-to-\\lbfont{O} probabilities.\\footnoteref{criteria}\nWith small reliability scores, an LF is less confident about choosing which label, so the Uniform off-diagonal non-\\lbfont{O} values are set close to the diagonals.\nWhen the scores are above threshold $r$, the LF is considered confident enough to abate the emission probabilities to other entities.\nThe exponent $n$ controls how fast the emit-to-\\lbfont{O} probabilities decrease, \\ie, how close off-diagonal values to the diagonal before $r$.\nThe functions $f$ and $g$ are illustrated in Figure~\\ref{fig:ga}.\n\n\n\n"
                    },
                    "subsubsection 3.2.2": {
                        "name": "Weighted XOR and Emission Addon Prior",
                        "content": "\n\\label{subsec:addon-emiss}\n\nIn contrast to the diagonals $\\Phi_{k,l,l}$, the off-diagonal values $\\Phi_{k,i,j}; i,j\\geq 2; i\\neq j$ are the probabilities of LF $k$ \\emph{misclassifying} label $i$ as $j$.\nSuch probabilities are generally insignificant.\nBut in some cases, they can be prominent (as the example shown in Figure~\\ref{fig:lf.examples}) and affects the model performance.\nThe base prior $\\bLambda$ assigns equal weights to all off-diagonals and fails to adjust to such cases.\n\nTherefore, we develop the WXOR scores $\\tilde{\\W} \\in [0,1]^{K \\times L \\times L}$ to capture the misclassification probabilities.\nGiven the annotations of LF $k$ and other LFs $k'\\in 1:K\\backslash k$, a WXOR logit is specified between the query label $\\lquery \\in 2:L$ and the target label $\\ltgt \\in 2:L$:\n\\begin{equation}\n  \\label{eq:wxor}\n  \\begin{gathered}\n    W^{(t)}_{k, \\lquery, \\ltgt} = (1-\\tilde{A}_{k, \\lquery})x^{(t)}_{k, \\lquery} \\sum_{k'=1}^K \\tilde{A}_{k', \\ltgt}x^{(t)}_{k',\\ltgt}; \\\\\n    \\W^{(t)}_{k, 1, :} = \\W^{(t)}_{k, :, 1} = \\0; \\quad W^{(t)}_{k, l, l} = 0,\\ \\forall l\\in 1:L.\n  \\end{gathered}\n\\end{equation}\nIt depicts the likelihood that LF $k$ makes mistakes in its observation of $\\lquery$ \\wrt the other LFs at time-step $t$.\n$1-\\tilde{A}_{k, \\lquery}$ is the \\emph{uncertainty level} of LF $k$ to its observation of $\\lquery$;\n$\\tilde{A}_{k', \\ltgt}$ is the confidence of LF $k'$ to $\\ltgt$;\nand the binary $x^{(t)}_{k, l}$ decides whether $l$ is observed.\n$W^{(t)}_{k, \\lquery, \\ltgt}$ is large when LF $k$ observes $\\lquery$ but is uncertain, while other LFs are confident about observing $\\ltgt$.\nAs $W^{(t)}_{k, \\lquery, \\ltgt}$ is non-zero \\iff LF $k$ does but other LFs do not observe $\\lquery$, it is similar to the XOR gate and hence gets its name.\nLabel \\lbfont{O} and the diagonal need no WXOR as these elements are well-defined in $\\bLambda$, so the corresponding values in $\\W$ are fixed to $0$.\n\n$\\W^{(t)}$ tensors are then summed up and normalized to form the ubiquitous WXOR:\nfor $M$ sentences with length $T_m,m\\in 1:M$ each,\n\\begin{equation*}\n  \\hat{W}_{k, \\lquery, \\ltgt} = \\frac{\\sum_{m=1}^M \\sum_{t=1}^{T_m} {W}^{(t)}_{m, k, \\lquery, \\ltgt}}{\\sum_{m=1}^M \\sum_{t=1}^{T_m} {x}^{(t)}_{m, k, \\lquery}}.\n\\end{equation*}\n\nCombining $\\hat{\\W}$ with the emission base prior $\\bLambda$ is also nontrivial.\nStraightforward summation is unpractical since the relative scale of $\\hat{\\W}$ and $\\bLambda$ is obscure due to the summation in \\eqref{eq:wxor}.\nIn regard of this, we leverage another scaling factor $\\C \\in (0,1)^{K \\times L}$ predicted from the BERT sentence embedding through one layer of NN:\n\\begin{equation*}\n  \\C = \\text{reshape}(\\sigma(\\fc(\\e^{(0)}))).\n\\end{equation*}\nIts domain $(0,1)$, however, enables $\\C$ to down-scale values but not up-scale them.\nConsequently, we need to properly adjust the elements of $\\hat{\\W}$ so that they have large enough pre-scaling values:\n\\begin{equation*}\n  \\tilde{\\W}_{k, :, \\ltgt} = \\softmax (\\hat{\\W}_{k, :, \\ltgt}).\n\\end{equation*}\n\nWith the established matrices, we construct the \\emph{emission addon prior} $\\bDelta \\in [0,1]^{K\\times L \\times L}$ by re-scaling the rows of $\\tilde{\\W}_{k}$:\n\\begin{equation}\n  \\bDelta_{k,:,l} = C_{k,l} \\times \\tilde{\\W}_{k,l,:}.\n\\end{equation}\nNotice that in $\\bDelta_{k}$, the target labels are at the first axis and the query labels second, different from $\\tilde{\\W}_k$.\nAs the target labels are essentially the latent states we want to predict, this unifies the physical meaning of $\\tilde{\\W}$ and the emission matrix $\\bPhi$.\n\n"
                    },
                    "subsubsection 3.2.3": {
                        "name": "Emission Matrix Sampling",
                        "content": "\n\\label{subsec:emiss-sampling}\n\nInstead of using the deterministic priors $\\bLambda$ and $\\bDelta$ directly, we choose to sample the latent emission variables from the Dirichlet distribution parameterized by the priors.\nThis is a common practice in graphical models considering that proper latent distributions are more representative and robust to noise than the deterministic priors \\citep{Kingma-2014-VAE, chem-2021-DrNAS}.\nThe random sampling helps the model escape the saddle points or local optima.\nIn addition, Dirichlet distribution samples from probability simplex without requiring its concentration parameters to sum up to one, which makes the parameters selection more flexible.\n\nTo construct the concentration parameters $\\bOmega \\in \\R_+^{K\\times L \\times L}$, we add the base prior and the addon prior together and scale the results:\n\\begin{equation*}\n  \\bOmega = \\nu^{\\rm expan} \\times (\\bLambda + \\bDelta) + \\nu^{\\rm base}.\n\\end{equation*}\n$\\nu^{\\rm base}\\in \\R_+$ and $\\nu^{\\rm expan}\\in \\R_+$ controls the minimum concentration value and the concentration range, respectively.\nLarger $\\nu_{\\rm expan}$ results in smaller sampling variance.\nEach row of the emission matrix $\\bPhi_{k}$ of LF $k$ is sampled independently from the distribution:\n\\begin{equation*}\n  \\bPhi_{k,l} \\sim \\Dir (\\bOmega_{k,l} ).\n\\end{equation*}\nWe use pathwise derivative estimators developed by \\citet{Jankowiak-2018-pathwise-derivatives} to push the gradient back through the Dirichlet.\n\nNotice that we only apply Dirichlet sampling when it requires backpropagation.\nOn other occasions, such as validation and test, the samples are substituted by the mean of the Dirichlet distribution.\n\n\n"
                    }
                },
                "subsection 3.3": {
                    "name": "Model Training",
                    "content": "\n\\label{subsec:training}\n\n\\ours is an unsupervised model whose optimization does not leverage any ground-truth labels $\\y$ but only the weak labels $\\x$.\nSection~\\ref{subsec:initialization}--\\ref{subsec:training-procedure} describe \\ours's training strategy, whereas \\cref{subsec:training.complexity} analyzes the inference complexity.\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Model Pre-training",
                        "content": "\n\\label{subsec:initialization}\n\nA good initialization is critical for HMMs to gain good performance.\nHowever, as \\ours contains several NNs, assigning proper initial values to model parameters is unrealistic.\nThus, we adopt the approach used by \\citet{li-etal-2021-bertifying}, which pre-trains the neural networks by minimizing the Euclidean distances between the predicted transitions and emissions and the initial matrices $\\bPsi^*$ and $\\bPhi^*$ that are from the observation statistics.\nThe objective is the mean squared error (MSE) loss:\n\\begin{equation}\n  \\label{eq:model.initialization}\n  \\ell_{\\rm MSE} = \\frac{1}{K} \\sum_{k=1}^K \\|\\bPhi_k - \\bPhi^*\\|_F^2 + \\frac{1}{T}\\sum_{t=1}^T \\|\\bPsi^{(t)} - \\bPsi^*\\|_F^2,\n\\end{equation}\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm.\nDifferent from \\citep{li-etal-2021-bertifying} or \\citep{lison-etal-2020-named}, we require no prior knowledge of LF performance.\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Training Objective",
                        "content": "\n\\label{subsec:objective}\n\nSame as CHMM \\citep{li-etal-2021-bertifying}, \\ours is trained using the generalized EM algorithm.\nSpecifically, we leverage the gradient ascent method in the M step to optimize the expected complete data log likelihood computed in the E step:\n\\begin{equation}\n    \\label{eq:em.q}\n    Q(\\btht, \\btht^{\\rm old}) \\triangleq \\expt_{\\z} [\\ell_c (\\btht) | \\x, \\btht^{\\rm old}].\n\\end{equation}\nThe expectation is defined over the hidden states given LF observations and the current model parameters $p(\\z^{(1:T)} | \\x^{(1:T)}, \\btht^{\\rm old})$.\nThe data log likelihood $\\ell_c({\\btht})$ is defined as:\n\\begin{equation}\n  \\label{eq:lc}\n  \\begin{aligned}\n    & \\ell_c(\\btht) \\triangleq \\log p(\\z^{(0:T)}, \\x^{(1:T)} | \\btht, \\e^{(0:T)}) = \\log p(z^{(0)}) +\\\\\n    &\\quad \\sum_{t=1}^T \\log p(z^{(t)}|z^{(t-1)}, \\e^{(t)}) + \\sum_{t=1}^T \\log p(\\x^{(t)}|z^{(t)}, \\e^{(0)}),\n  \\end{aligned}\n\\end{equation}\nof which the conditional independency comes from the Markov assumption.\nThe computation of \\eqref{eq:em.q} largely aligns with CHMM.\nOne difference is the dependency of the emission evidence:\n\\begin{equation}\n  \\label{eq:varphi}\n  \\varphi_l^{(t)} \\triangleq p(\\x^{(t)}|z^{(t)}=l, \\e^{(0)})=\\prod_{k=1}^K \\sum_{j=1}^L \\Phi_{k,l,j}x_{k,j},\n\\end{equation}\nwhich in our case is sentence-level $\\e^{(0)}$, whereas in CHMM is token-level $\\e^{(t)}$.\nThis essentially is attributed to the different approaches to obtain the emission matrix $\\bPhi$ (\\cref{subsec:emission}).\nThe computation of \\eqref{eq:em.q} and other forward inference details are in \\cref{appsec:objective}.\n\n\n\n"
                    },
                    "subsubsection 3.3.3": {
                        "name": "Training Procedure",
                        "content": "\n\\label{subsec:training-procedure}\n\nAs the computation of the emission is complicated and inter-dependent, we adopt a three-stage training strategy to allow sufficient training of each model component, as shown in Figure~\\ref{fig:training-process}. \nIt decouples the optimization of the model components while keeping the training efficient.\n\nStage 1 is focused on optimizing the transition matrix $\\bPsi$ and the emission base prior $\\bLambda$, excluding the addon prior by setting $\\bDelta = \\0$.\nThe reason is straightforward:\ncalculating the WXOR scores $\\tilde{\\W}$ requires high-quality reliability scores $\\tilde{\\A}$, which are obtained by optimizing $\\bLambda$ (\\cref{subsec:training}).\nStage 2 trains the addon prior $\\bDelta$, leaving $\\bPsi$ and $\\bLambda$ frozen.\nThis stage aims to search for the best scaling factors $\\C$ for $\\tilde{\\W}$, and freezing the irrelevant parameters relieves the training pressure.\n$\\tilde{\\W}$ is calculated right after stage 1 with all training and validation instances and remains constant for stages 2 and 3.\n\nStage 3 is inspired by our empirical discovery about HMMs.\nWe find that if we only optimize the transition matrix $\\bPsi$ with the emission fixed, generally to the true values, the model performance can be improved.\nThe true emission is the statistics of the annotations $\\x_k$ of LF $k$ given ground-truth labels $\\y$:\n\\begin{equation*}\n  \\Phi^{\\rm true}_{k,i,j} = p(\\x_k|\\y) \\triangleq \\frac{\\sum_{m=1}^M \\sum_{t=1}^{T_m} \\I(x^{(t)}_{m,k,j}=1, y^{(t)}_{m}=i) }{\\sum_{m=1}^M \\sum_{t=1}^{T_m} \\I(y^{(t)}_m=i) },\n\\end{equation*}\nwhere $\\I(\\cdot)$ is the indicator function.\nConsequently, we believe that continuing training the transition of \\ours for several more epochs with the emission frozen would lead to a similar performance improvement.\n\nIn addition, stages 1, 2, and 3 use different pre-training strategies.\nThe pre-training of stage 1 is the same as \\cref{subsec:initialization}.\n% In stage 2, the transition part of the MSE loss in \\eqref{eq:model.initialization} is dropped as we do not update the transition probabilities in this stage.\nStage 2's pre-training is to initialize the NN parameters associated with the addon prior $\\bDelta$ only, so we drop the transition part of the MSE loss and substitute the target emission by\n$\\bPhi' = \\lambda \\bPhi^* + \\frac{(1-\\lambda)}{M}\\sum_{m=1}^M \\bPhi_m^{(\\rom{1})}$\nto take advantage of stage 1's results.\nHere $\\lambda \\in [0,1]$ is the weight of observation statistics, which is fixed to $0.2$ in our experiments.\n$\\bPhi_m^{(\\rom{1})}$ is the optimized emission from stage 1.\nStage 3 successes all model parameters from the previous stage and thus has no pre-training.\n\n% Another issue with the training is the LF competition.\n% When several LFs classify different tokens as the same entity, they have to compete for the reliability score due to the normalization brought by the SoftMax function \\eqref{eq:softmax-A}.\n% In this case, the LF with the highest observation frequency tends to dominate the score, making other LFs neglected.\n% To deal with this issue, we append a simple \\emph{majority voting} (MV) to the LF set during the training of the reliability scores.\n% MV aggregates the token-entity mappings from all LFs into a single annotation sequence, which forces the model to pay equal attention to the patterns and resolves the competition.\\footnote{We use the validation performance to decide whether to include this technique.}\n\n"
                    },
                    "subsubsection 3.3.4": {
                        "name": "Complexity Analysis",
                        "content": "\n\\label{subsec:training.complexity}\n\nCompared with CHMM, \\ours significantly reduces the training resource consumption by predicting sparse emission elements.\nThe emission NN parameter number and computation complexity are shown in Table~\\ref{tb:inference.complexity}, where the transition attributes are not presented because they are the same for both models.\nThe factor $2$ for \\ours comes from matrices $\\tilde{\\A}$ and $\\C$.\nWe can see that \\ours reduces the emission NN parameter number to $2/L$ of CHMM and the complexity to $1/(T\\times L)$, which are substantial when the number of entity labels $L$ is large.\nThe complexity of other emission elements, \\ie, \\eqref{eq:hnsr}--\\eqref{eq:wxor}, is negligible because they do not contain the embedding dimension $d^{\\rm emb}$, which is much larger than other terms.\nCalculating the WXOR scores can be as complex as $\\O(M\\times T\\times K^2 \\times L^2)$, but they are calculated only once at stage 2 and stay fixed henceforth.\n\n\n\n"
                    }
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\\label{sec:experiment}\n\n% We benchmark \\ours on \\num{5} representative datasets against strong baseline label models.\n% We also conduct a series of ablation studies to validate the model components.\n% We introduce the experiment setup in \\cref{subsec:exp.setup}.\n% The main results, ablation studies and case studies are presented in \\cref{subsec:main.results}, \\cref{subsec:ablation.studies} and \\cref{subsec:case.studies} respectively along with result discussion.\n\n",
                "subsection 4.1": {
                    "name": "Experiment Setup",
                    "content": "\n\\label{subsec:exp.setup}\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Datasets",
                        "content": "\nWe consider \\num{5} NER datasets from the generic domains to the biomedical and chemical domains:\n1) \\textbf{CoNLL 2003} (English subset) \\citep{tjong-kim-sang-de-meulder-2003-introduction} labels \\num{22137} sentences from the Reuters news stories with \\num{4} entity types, including \\lbfont{PER}, \\lbfont{LOC}, \\lbfont{ORG} and \\lbfont{MISC}.\n2) The \\textbf{LaptopReview} dataset \\citep{pontiki-etal-2014-semeval} consists of \\num{3845} sentences of laptop reviews.\nThe laptop-related \\lbfont{Terms} are regarded as named entities.\n3) The \\textbf{NCBI-Disease} dataset \\citep{Dogan-2014-NCBI} contains \\num{793} PubMed abstracts annotated with \\lbfont{Disease} mentions.\n4) \\textbf{BC5CDR} \\citep{Li-2016-BC5CDR} consists of \\num{1500} PubMed articles with \\lbfont{Chemical} and \\lbfont{Disease} entities.\n5) \\textbf{OntoNotes 5.0} \\citep{Weischedel-2013-ontonotes} is a very large dataset containing \\num{143709} sentences labeled with \\num{18} fine-grained entity types.\n\nWe use the LFs included in the Wrench benchmark platform \\citep{zhang-2021-wrench} for all datasets.\nTable~\\ref{tb:dataset.statistics} shows the dataset statistics, including the number of entities and labeling functions.\nThe performance of the LFs on the test dataset is listed in \\cref{appsec:lf.metrics}.\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Baselines",
                        "content": "\nWe compare our model with the following Wrench baselines:\n1)~\\textbf{Majority Voting} (MV) returns the label that has been observed by most LFs and chooses randomly from the tie if it exists.\n2)~\\textbf{Snorkel} \\citep{Ratner-2017-Snorkel} is a context-free token-based simple graphical model which assumes the tokens are independent.\n3)~\\textbf{HMM}, used in \\citep{nguyen-etal-2017-aggregating, lison-etal-2020-named, Safranchik-etal-2020-weakly}, is a popular weakly-supervised NER label model with certain context representation ability.\n4)~\\textbf{Conditional HMM} (CHMM, \\citealp{li-etal-2021-bertifying}) augments HMM by predicting the token-wise transition and emission probabilities from the BERT token embeddings through NNs.\n5)~\\textbf{ConNet} \\citep{lan-etal-2020-learning} uses the context-aware attention mechanism to aggregate the CRF representations of different LFs.\\footnote{DWS \\citep{parker-yu-2021-named} is similar to CHMM but has no open-source implementation.}\n\nWe also include \\num{3} supervised methods as references:\n1)~a fully supervised \\textbf{BERT-NER} model trained with human annotations,\n2)~the \\textbf{best consensus} of LFs, which is an oracle that always selects the correct token annotations from the LFs; and\n3)~\\textbf{CHMM-FE}, which is CHMM with the fixed ground-truth emissions as described in \\cref{subsec:training-procedure}.\nEvery supervised method accesses the true labels in one way or another during training.\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Evaluation Metrics",
                        "content": "\n\nThe NER label models are evaluated using the \\emph{micro}-averaged \\emph{entity-level} precision, recall, and \\fone scores.\nWe calculate the metrics with the \\textbf{seqeval} Python package.\\footnote{\\href{https://github.com/chakki-works/seqeval}{https://github.com/chakki-works/seqeval}}\nThe results come from the average of \\num{5} random trials.\n\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "Implementation Details",
                        "content": "\n\nFollowing \\citet{li-etal-2021-bertifying}, we apply different BERT variants to different datasets.\nSpecifically, we use bert-base-uncased \\citep{devlin-etal-2019-bert} on CoNLL 2003, LaptopReview and OntoNotes 5.0, bioBERT \\citep{Lee-2019-biobert} on NCBI-Disease and SciBERT \\citep{beltagy-etal-2019-scibert} on BC5CDR.\n% We do not investigate the influence of BERT variants, so there might exist pre-trained language models that are more suitable than our choices.\n\nIn alignment with \\citet{zhang-2021-wrench}, we evaluate \\ours inductively.\nThe model is trained on the training dataset and tested on the test dataset.\nThe validation set is for early stopping and hyper-parameter fine-tuning.\nIn addition, the WXOR scores are calculated on the combination of training and validation datasets.\nPlease refer to \\cref{appsec:parameters} for more implementation details and the model hyper-parameters that lead to the presented results.\n\n\n\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Main Results",
                    "content": "\n\\label{subsec:main.results}\n\n% The experiment results are shown in Table~\\ref{tb:results.domains}.\nTable~\\ref{tb:results.domains} shows the performance comparison of \\ours and the baselines in the Wrench benchmark.\n\\ours outperforms all other label models, achieving \\num{3.01} average \\fone improvement over the strongest baselines.\nIn general, the increment in recall contributes to most of \\ours's performance gain.\nThis is accredited to the well-founded emission structure, which prevents the correct annotations of inferior LFs from being neglected.\nOn \\num{3} out of \\num{5} datasets, \\ours has larger recall than the best consensus.\nThis situation indicates that \\ours is capable of predicting entities observed by \\emph{no} LF, which is attributed to the well-trained token-wise transition probabilities.\n\nSurprisingly, we find that \\ours is generally superior to CHMM-FE, a model that incorporates true labels.\nThe reason is that the ground-truth emission matrix in CHMM-FE is not necessarily the best when used as model parameters.\nMoreover, as the emission varies according to input sentences, using a fixed average value renounces the flexibility to accommodate the sentence pattern, which can be captured by \\ours.\n\nNonetheless, a gap exists between \\ours and the best consensus on CoNLL 2003 and OntoNotes 5.0.\nOn these datasets, even CHMM-FE fails to achieve good performance.\nApart from the difficulty these datasets bring with the increased number of entities and LFs, the reason is primarily the quality of their LFs (\\cref{appsec:lf.metrics}).\nThe majority of LFs have low recall scores, letting the few reasonably-performed LFs dominate the training, which impedes the attention to the labels correctly observed by these LFs.\n% We plan to address this issue in future works.\n\n\n\n\n% \\begin{figure}[htbp]\n%   \\centerline{\\includegraphics[width = 0.45\\textwidth]{figures/s5-NCBI-emiss.pdf}}\n%   \\caption{Emissions of LF ``tag-CoreDictionaryExact'' trained on NCBI-Disease with/without the additional MV LF.}\\label{fig:ablation.ncbi}\n%   \\Description{Case study}\n% \\end{figure}\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Ablation Studies",
                    "content": "\n\\label{subsec:ablation.studies}\n\n",
                    "subsubsection 4.3.1": {
                        "name": "Training Stages",
                        "content": "\n\\label{subsec:training.stages}\nTable~\\ref{tb:ablation.study} shows the test performance from different training stages.\nThe results demonstrate that the three-stage strategy contributes to \\ours's performance in general.\nOn BC5CDR, although stage 2 slightly weakens the model, the addon prior built in this stage is vital for stage 3's training.\nOn OntoNotes 5.0, the minor decrease in stage 3 is because of the discrepancy between the training and test datasets, as we observe the valid \\fone increases from stage 2 to stage 3, as expected.\n\n\n"
                    },
                    "subsubsection 4.3.2": {
                        "name": "Model Components",
                        "content": "\n\\label{subsec:model.components.ablation}\nTo investigate the effectiveness of each model component, we include a series of ablation studies:\n1)~\\ours with \\textbf{na\\\"ive emission} matrices, which substitutes the reliability expansion function \\eqref{eq:gnr} by a Uniform over all non-diagonal values so that the emit-to-\\lbfont{O} probabilities are not emphasized as the original model;\n2)~\\ours \\textbf{w/o} the scaling function $h_{n,s,r}$, which is equal to substituting \\eqref{eq:hnsr} by $\\tilde{\\A} = \\hat{\\A}$ or setting the exponents $n=s=1$;\n3)~\\ours \\textbf{w/o} the \\textbf{SoftMax} function defined in \\eqref{eq:softmax-A};\n4)~\\ours \\textbf{w/o} the \\textbf{Dirichlet} sampling process during training;\n5)~\\ours \\textbf{w/o stage 2} training or the weighted XOR scores;\n6)~\\ours that is trained with \\textbf{merged stage 2 and 3}, \\ie, the transition parameters are not frozen in stage 2.\n\nFrom Table~\\ref{tb:ablation.study}, we conclude that each component introduced in \\cref{subsec:emission} contributes to the model performance.\nThe model does not work with the na\\\"ive emission, which verifies the importance of the appropriately designed emission structure \\eqref{eq:fij}.\nWe also find that both SoftMax and $h$ are vital for \\ours to predict good reliability scores.\nIf Dirichlet sampling is deprecated, \\ours has a higher chance to be trapped by the local optima.\n% In addition, training each model component individually and gradually achieves a better performance than training end-to-end.\nThe 3-stage training strategy also makes the model optimization more stable.\n\n% Figure~\\ref{fig:ablation.ncbi} shows the effect of the additional majority voting LF.\n% Without using MV, the model is biased against the presented and LF and fails to assign proper reliability score to the LF's observation of label \\lbfont{B-Disease}, which in return decreases \\ours's \\fone score to $66.48$.\n% Adding the MV LF not only balances the reliabilities and constructs sound emissions, but also improves the \\fone to $82.24$.\n% This indicates that the designed MV LF is crucial for \\ours to get the best performance on some datasets.\n\n\n\n\n\n\n\n"
                    },
                    "subsubsection 4.3.3": {
                        "name": "Hyper-Parameters",
                        "content": "\nFigure~\\ref{fig:hyper.parameters} presents the impact of different exponential terms of functions $h_{n,s,r}$ and $g_{n,r}$ to the model performance.\nThe exponents help the element values of the base prior $\\bLambda$ to rest within the desired range instead of leaving them arbitrary.\nThis is essential for \\ours to generate good base emission patterns and calculate accurate WXOR scores.\n% When $s=n=1$, $h$ and $g$ become straight lines (\\eg, Figure~\\ref{eq:gnr}) and lose their functionality.\n\n"
                    },
                    "subsubsection 4.3.4": {
                        "name": "Model Efficiency",
                        "content": "\nTable~\\ref{tb:training.time} supports that compared with CHMM, \\ours is advanced in training efficiency.\nThe advantage is more prominent with an increasing number of LFs and labels, aligning with our analysis in \\cref{subsec:training.complexity}.\nSince \\ours is a simpler model than CHMM, it not only is faster to train per epoch, but also requires less epochs to be fully optimized.\nThus, \\ours is more efficient than CHMM from both perspectives.\n\n\n\n\n\n"
                    }
                },
                "subsection 4.4": {
                    "name": "Case Studies",
                    "content": "\n\\label{subsec:case.studies}\nFigure~\\ref{fig:emiss.matrix.case.study} illustrates the averaged emission matrix from \\ours and compares it with the ground-truth emissions.\nWe can see that the model focuses on the diagonal and emit-to-\\lbfont{O} (\\ie, first-column) values in the first stage, and refines the emission matrix by adding the addon prior $\\bDelta$ to include the prominent off-diagonal values into the Dirichlet parameters.\nComparing the diagonal values of the true and predicted emission matrices, we find that \\ours fits the LF reliability scores well without using any clean labeled data.\nThe reliability scores can also facilitate other tasks such as LF design and evaluation.\n\nWe further investigate the correlation between the predicted LF reliabilities and the entity \\fone scores of LFs.\nFigure~\\ref{subfig:reliability.scatter} illustrates correlation, and Figure~\\ref{subfig:correlation.coefficient} measures the correlation quantitatively.\nWe observe that the predicted reliabilities have a strong correlation with \\fone scores, reaching almost $1.0$ correlation coefficients on $3$ datasets.\nCompared with CHMM, \\ours performs better in identifying unreliable LFs, which is critical for ruling out the incorrect observations.\n% An interesting observation is that \\ours tends to underestimate the reliabilities.\n% We can rectify this by increasing $s$ and $n$ in function \\eqref{eq:hnsr}, but this harms the model performance (see Figure~\\ref{subfig:h.parameters}).\n% It suggests that \\fone may be unsuitable for emission matrix construction without proper scaling.\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Related Works",
                "content": "\n\\label{sec:related.works}\n\nWeak supervision is a widely-investigated approach for natural language processing tasks such as sentence classification \\cite{Ratner-2016-data-programming, Ratner-2017-Snorkel, ren-etal-2020-denoising} and NER \\cite{nguyen-etal-2017-aggregating, shang-etal-2018-learning, Safranchik-etal-2020-weakly, lison-etal-2020-named, lan-etal-2020-learning, lison-etal-2021-skweak, parker-yu-2021-named, li-etal-2021-bertifying}.\nIt substitutes the laborious manual annotation process with multiple simple labeling functions, relieving the human effort to annotate large training data.\nHowever, the LFs are generally noisy and controversial, which makes their utilization challenging.\nConsequently, the primary track of weak supervision focuses on designing a robust label model that aggregates the weak annotations by de-noising them and resolving the conflict \\cite{Ratner-2016-data-programming, Ratner-2017-Snorkel, Ratner-2019-training-complex, ren-etal-2020-denoising, nguyen-etal-2017-aggregating, shang-etal-2018-learning, Safranchik-etal-2020-weakly, lison-etal-2020-named, lan-etal-2020-learning, lison-etal-2021-skweak, parker-yu-2021-named, li-etal-2021-bertifying}.\n% In addition, another track intends to improve weak supervision performance by constructing and selecting more reliable LFs \\cite{Nashaat-2018-Hybridization, Boecking.2021.iws, Biegel-2021-Active-WeaSuL}.\n% Our work mainly concerns the improvement of the label model.\nThe hidden Markov model with multiple independent observation sequences is principled and popular for this task \\citep{nguyen-etal-2017-aggregating, Safranchik-etal-2020-weakly, lison-etal-2020-named,lison-etal-2021-skweak, parker-yu-2021-named, li-etal-2021-bertifying}.\nTransferring from sequence classification to sequence labeling (\\aka, token classification), it extends the na\\\"ive Bayes generative model proposed by \\citet{Ratner-2016-data-programming} by incorporating the chronological dependency relationship through the transition matrix.\nThe weak labels from different LFs are considered as multiple sets of observations that are conditionally independent given the latent variables.\nAmong these works, \\citet{nguyen-etal-2017-aggregating} and \\citet{lison-etal-2020-named, lison-etal-2021-skweak} use conventional HMMs.\n\\citet{Safranchik-etal-2020-weakly} design the linked HMM, which uses linking rules to model whether consecutive tokens belong to the same entity.\nOther works \\citep{li-etal-2021-bertifying, parker-yu-2021-named} improve HMM's context representation with BERT embeddings and alleviate the Markov assumption.\n% CHMM \\citep{li-etal-2021-bertifying} predicts token-wise transition and emission matrices to replace their constant counterparts in HMM, whereas DWS \\citep{parker-yu-2021-named} substitutes HMM by CRF and predicts the hidden variables directly from the embeddings.\n\nIn the data programming framework, people also train \\emph{end models}, which are deep supervised neural networks, with the label model outputs in seek of refining the results with the power of deep networks.\nOne exception is the Consensus Network \\citep{lan-etal-2020-learning} that trains the label model and the end model jointly within a two-stage framework \\citep{zhang-2021-wrench}.\nAnother is ALT \\citep{li-etal-2021-bertifying}, which treats the end model as an additional LF and optimizes it alternately with the label model.\n\\citet{zhang-2021-wrench}, however, show that the end model does not necessarily outperform the label model. \nTherefore, we focus on the label models here and leave end models to future works.\n\nOur work is also related to the neuralized graphical models.\nMany efforts seek to inject neural networks into graphical models, especially for variational inference, such as the variational autoencoder (VAE) \\citep{Kingma-2014-VAE}.\nOther works include \\citep{tran-etal-2016-unsupervised}, which neuralizes the HMM with one observation set for the unsupervised part-of-speech tagging.\n\\citet{Dai-2017-recurrent} and \\citet{Liu-2018-structured-inference} incorporate recurrent units into the hidden semi-Markov model to segment and label high-dimensional time series;\n\\citet{wiseman-etal-2018-learning} learn discrete template structures for conditional text generation also with neuralized graphical models.\nCHMM and DWS also fall into this category by predicting graphical models components through NNs.\n\n% \\subsection{Pre-trained Language Models}\n\n% Similar to \\citep{li-etal-2021-bertifying, parker-yu-2021-named}, our method uses large pre-trained language models such as BERT \\cite{devlin-etal-2019-bert}, RoBERTa \\cite{Liu-2019-roberta}, or XLNet \\cite{Yang-2019-XLNet}.\n% Trained on huge corpora in a semi-supervised way, these models gain the ability to capture the semantic and syntactic information of natural language into their numerous parameters.\n% The information is embedded in the output of their last hidden layer, which noticeably improves the performance of the downstream tasks when properly utilized.\n% There are other BERT models trained on domain-specific corpora such as BioBERT \\cite{Lee-2019-biobert}and SciBERT \\cite{beltagy-etal-2019-scibert} which provides context representation more specific to the target domains.\n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\n\nWe presented \\ours, a label model that aggregates the weak annotations from multiple noisy NER labeling functions.\nIn contrast to CHMM, \\ours constructs the sentence-level sparse emission probabilities from the LF reliabilities predicted using the BERT sentence embeddings.\nThe off-diagonal emission elements are further augmented by the weighted XOR scores, which estimate the possibilities of an LF observing incorrect entity labels.\nWrapped in a three-stage training process with Dirichlet sampling, \\ours outperforms all baseline label models on five representative datasets with different statistics and attributes.\nIn addition, the approximated LF reliability scores strongly correlate with the true LF performance, making it eligible to contribute to other tasks such as automated LF generation and evaluation.\nIn the future, we will consider leveraging prior heuristic knowledge to strengthen \\ours's ability to distill clean NER labels from noisy LF annotations.\nIn addition, we also plan to extend this technique to other sequence labeling tasks.\n\n\\begin{acks}\n  This work was supported in part by ONR MURI N00014-17-1-2656, NSF IIS-2008334, IIS-2106961, CAREER IIS-2144338, and Kolon Industries.\n\\end{acks}\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{reference.st}\n\n%%\n%% If your work has an appendix, this is the place to put it.\n\\appendix\n\n"
            },
            "section 7": {
                "name": "Training Details",
                "content": "\n\\label{appsec:objective}\n\n",
                "subsection 7.1": {
                    "name": "Training Objective",
                    "content": "\n\\label{appsubsec:objective}\n\nIn this section, we focus on the computation of the expected complete data log likelihood $Q$ defined in \\eqref{eq:em.q} as well as the training objective.\nThe derivation largely aligns with \\citep{li-etal-2021-bertifying}, so we will skip some trivial steps and explanations.\n\nBy inserting \\eqref{eq:lc} into \\eqref{eq:em.q} and specifying the expectation of $z$, we can write \\eqref{eq:em.q} as:\n\\begin{equation}\n    \\label{eq:q.calculate}\n    \\begin{aligned}\n        Q&(\\btht, \\btht^{\\rm old}) = \\sum_{i=1}^{L} p(z^{(0)} = i| \\x^{(1:T)}, \\e^{(0:T)}) \\log p(z^{(0)}=i) + \\\\\n        & \\sum_{t=1}^{T}\\sum_{i=1}^{L} \\sum_{j=1}^{L} p(z^{(t-1)}=i, z^{(t)}=j | \\x^{(1:T)}, \\e^{(0:T)}) \\log \\Psi^{(t)}_{i,j} +\\\\\n        & \\sum_{t=1}^{T}\\sum_{i=1}^{L} p(z^{(t)} = i| \\x^{(1:T)}, \\e^{(0:T)}) \\log \\varphi^{(t)}_{i},\n    \\end{aligned}\n\\end{equation}\nwhere $\\bPsi$ is the transition matrix and $\\varphi_i^{(t)}$ is defined in \\eqref{eq:varphi}.\n$p(z^{(0)})$ is the probability of the initial hidden state without any corresponding observations.\nAs we can predict the token-wise transition matrix from the embeddings, we can simply set it to Uniform or, as \\citet{li-etal-2021-bertifying} proposed, set $p(z^{(0)}=1)$ to $1$ and $p(z^{(0)}=i), \\forall i\\in 2:L$ to $0$.\n\nTo calculate \\eqref{eq:q.calculate}, we define the smoothed marginal $\\bgamma^{(t)}\\in [0,1]^{L}$ as:\n\\begin{equation*}\n    \\gamma^{(t)}_i \\triangleq p(z^{(t)} = i| \\x^{(1:T)}, \\e^{(0:T)}),\n\\end{equation*}\nand the expected number of transitions $\\bxi^{(t)} \\in [0,1]^{L\\times L}$ as:\n\\begin{equation*}\n    \\xi^{(t)}_{i,j} \\triangleq p(z^{(t-1)}=i, z^{(t)}=j | \\x^{(1:T)}, \\e^{(0:T)}).\n\\end{equation*}\nThese two variables are acquired using the \\emph{forward-backward} algorithm.\n\nFirst, we define the filtered marginal $\\balpha \\in [0,1]^L$ as:\n\\begin{equation*}\n    \\alpha^{(t)}_i \\triangleq p(z^{(t)}=i|\\x^{(1:t)}, \\e^{(0:T)}),\n\\end{equation*}\nand the conditional future evidence $\\bbeta \\in [0,1]^{L}$ as:\n\\begin{equation*}\n    \\beta^{(t)}_i \\triangleq p(\\x^{(t+1:T)}|z^{(t)} = i, \\e^{(0:T)}).\n\\end{equation*}\n\nIn the forward pass, $\\alpha^{(t)}_i$ is computed iteratively:\n\\begin{equation*}\n  \\begin{aligned}\n    \\alpha^{(t)}_i &\\propto p(\\x^{(t)}|z^{(t)}=i, \\e^{(0)}) p(z^{(t)}=i|\\x^{(1:t-1)}, \\e^{(0:t)}) \\\\\n    &= \\sum_{j=1}^L \\varphi^{(t)}_i \\Psi^{(t)}_{j,i} \\alpha^{(t-1)}_j,\n  \\end{aligned}\n\\end{equation*}\nwhich can be written in the matrix form:\n\\begin{equation*}\n    \\balpha^{(t)} \\propto \\bphi^{(t)} \\odot ({\\bPsi^{(t)}}^{\\sf T}\\balpha^{(t-1)}),\n\\end{equation*}\nwhere $\\odot$ is the element-wise product.\nWe initialize $\\balpha$ with $\\alpha^{(0)}_l = p(z^{(0)}=l), \\forall l\\in 1:L$ since we have no observation at time step $0$.\n\nSimilarly, we do the backward pass and compute $\\bbeta$:\n\\begin{equation*}\n    \\begin{aligned}\n      \\beta^{(t-1)}_i &= \\sum_{j=1}^L p(z^{(t)}=j, \\x^{(t)}, \\x^{(t+1:T)}|z^{(t-1)}=i, \\e^{(0,t:T)}) \\\\\n      &= \\sum_{j=1}^L \\beta^{(t)}_j \\varphi^{(t)}_j \\Psi^{(t)}_{i,j}.\n    \\end{aligned}\n\\end{equation*}\nIn the matrix form, it becomes:\n\\begin{equation*}\n    \\bm{\\beta}^{(t-1)} = \\bPsi^{(t)}(\\bphi^{(t)} \\odot \\bm{\\beta}^{(t)}),\n\\end{equation*}\nwith base case:\n\\begin{equation*}\n    \\beta^{(T)}_i = p(\\x^{(T+1:T)}|z^{(T)}=i) = 1, \\forall i \\in 1:L.\n\\end{equation*}\n\nWith $\\balpha$ and $\\bbeta$ calculated, $\\gamma^{(t)}_i$ and $\\xi^{(t)}_{i,j}$ can be written as:\n\\begin{gather*}\n    \\begin{aligned}\n        \\gamma^{(t)}_i &\\propto p(z^{(t)}=i|\\x^{(1:t)}, \\e^{0:t}) p(\\x^{(t+1:T)}|z^{(t)} = i, \\e^{(0, t+1:T)}) \\\\\n        &= \\alpha^{(t)}_i \\beta^{(t)}_i,\n    \\end{aligned} \\\\\n    \\begin{aligned}\n        \\xi^{(t)}_{i,j} & \\propto p(z^{(t-1)}=i|\\x^{(1:t-1)} \\e^{(0:t-1)}) p(\\x^{(t)}|z^{(t)}=j, \\e^{(0)}) \\\\\n        & \\quad \\ p(\\x^{(t+1:T)}|z^{(t)}=j, \\e^{(0, t+1:T)})p(z^{(t)}=j|z^{(t-1)}=i, \\e^{(t)})\\\\\n        &= \\alpha^{(t-1)}_i\\varphi^{(t)}_j\\beta^{(t)}_j\\Psi^{(t)}_{i,j}.\n      \\end{aligned}\n\\end{gather*}\nWritten in the matrix form, they become:\n\\begin{gather*}\n    \\bm{\\gamma}^{(t)} \\propto \\balpha^{(t)} \\odot \\bm{\\beta}^{(t)}, \\\\\n    \\bm{\\xi}^{(t)} \\propto \\bPsi^{(t)} \\odot (\\balpha^{(t-1)}(\\bphi^{(t)} \\odot \\bm{\\beta}^{(t)})^{\\sf T}).\n\\end{gather*}\n\nEventually, we insert $\\bgamma$ and $\\bxi$ into \\eqref{eq:q.calculate} to compute the value of $Q$.\nThe training objective is to maximize $Q$, which can be readily done using the gradient ascend.\nPlease refer to \\citep{li-etal-2021-bertifying} for more details.\n\n"
                },
                "subsection 7.2": {
                    "name": "Inference",
                    "content": "\n\\label{appsubsec:inference}\n\nSame as \\citet{li-etal-2021-bertifying}, we use the Viterbi algorithm to find the sequence of latent variables $\\hat{\\z}^{(1:T)}$ that maximize the posterior:\n\\begin{equation*}\n    \\hat{\\z}^{(1:T)} = \\argmax_{\\z^{(1:T)}}p(\\z^{(1:T)} | \\x^{(1:T)}, \\e^{(0:T)}).\n\\end{equation*}\nThis sequence of latent variables $\\hat{\\z}^{(1:T)}$ is considered as \\ours's approximation of the true labels $\\y$.\n% Except for the way of predicting the transition and emission matrices, we do not change any other parts of the Viterbi algorithm.\n\n"
                }
            },
            "section 8": {
                "name": "Labeling Function Metrics",
                "content": "\n\\label{appsec:lf.metrics}\n\nTable~\\ref{tb:lf.performance.conll.2003}--\\ref{tb:lf.performance.ontonotes} present the performance of each labeling function on the test set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
            },
            "section 9": {
                "name": "Hyper-parameters",
                "content": "\n\\label{appsec:parameters}\n\nThe experiments are conducted on one GeForce RTX 2080 Ti GPU.\nThe selected model hyper-parameters are presented in Table~\\ref{tb:hyperparameters}.\n\nTo increase the training speed on the OntoNotes 5.0 dataset, the WXOR scores are calculated only on the validation set instead of the combination of training and validation sets.\n\n"
            },
            "section 10": {
                "name": "Function design criteria",
                "content": "\n\\label{appsec:function.criteria}\n\nHere we introduce the design criteria for the reliability scaling function \\eqref{eq:hnsr} and the reliability expansion function \\eqref{eq:gnr}.\nThere are infinite solutions that meet the criteria.\nWe select the simplest polynomials for better interpretability and calculation efficiency.\n\n",
                "subsection 10.1": {
                    "name": "Reliability Scaling Function",
                    "content": "\n\nFunction $f_{n,s,r}$, illustrated in Figure~\\ref{fig:ha}, is 1) continuous, smooth and monotonic; 2) passing through coordinates $(0,0)$ and $(1,1)$; and 3) having zero gradient at $(0,0)$ and $(1,1)$.\nIt is designed such that its shape is controllable without making the function complicated.\n\n"
                },
                "subsection 10.2": {
                    "name": "Reliability Expansion Function",
                    "content": "\n\nAs shown in Figure~\\ref{fig:ga}, $g_{n,r}$ is 1) continuous, smooth and monotonic; 2) passing through $(0,1)$ and $(1,0)$; and 3) $\\nabla_{a}g(a)|_{a=0} = \\frac{1}{L-1}$.\nWe want the emission from non-\\lbfont{O} latent state to non-\\lbfont{O} observation $\\bLambda_{k,i,j\\geq 2}$ to be close to Uniform when $\\tilde{A}_{k,i}$ is small, which indicates that LF $k$ may equally observe anything when the latent label is $i$.\nThis builds the constraint of $\\nabla_{a}\\frac{1-a-g(a)}{L-2}|_{a=0}=\\nabla_{a}a|_{a=0}=1$, which leads to the third feature defined above.\nThe hyperparameter $r$ controls the threshold where we trust LF $k$ enough to stop increasing the off-diagonal emissions.\n\n"
                }
            }
        },
        "tables": {
            "tb:dataset.statistics": "\\begin{table}[tbp]\\small\n    \\caption{\n    Dataset statistics.\n    }\n    \\centering\n    \\begin{tabular}{c|c|c|c|c|c}\n    \\toprule\n     & CoNLL & NCBI & BC5CDR & Laptop & OntoNotes \\\\\n    \\midrule\n    \\# Instance & \\num{22137} & \\num{793} & \\num{1500} & \\num{3845} & \\num{143709} \\\\\n    \\# Training & \\num{14041} & \\num{593} & \\num{500} & \\num{2436} & \\num{115812} \\\\\n    \\# Validation & \\num{3250} & \\num{100} & \\num{500} & \\num{609} & \\num{5000} \\\\\n    \\# Test & \\num{3453} & \\num{100} & \\num{500} & \\num{800} & \\num{22897} \\\\\n    \\midrule\n    \\# Entities & \\num{4} & \\num{1} & \\num{2} & \\num{1} & \\num{18} \\\\\n    \\# LFs & \\num{16} & \\num{5} & \\num{9} & \\num{3} & \\num{17} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tb:dataset.statistics}\n\\end{table}",
            "tb:results.domains": "\\begin{table*}[t!]\\small\n    \\caption{\n      Evaluation results on the test datasets, presented as ``\\fone (Precision / Recall)'' in \\%.\n    }\n    \\centering\n    \\begin{threeparttable}\n      \\begin{tabular}{c|c|c|c|c|c|c}\n        \\toprule\n        \\multicolumn{2}{c|}{Models} & CoNLL 2003 & NCBI-Disease & BC5CDR & LaptopReview & OntoNotes 5.0 \\\\\n        \\midrule\n        \\multirow{3}{*}{\\shortstack{Supervised\\\\Methods}}\n        & BERT-NER & 90.74 (90.37 / 91.10) & 88.89 (87.05 / 90.82) & 88.81 (87.12 / 90.57) & 81.34 (82.02 / 80.67) & 84.11 (83.11 / 85.14) \\\\\n        & Best consensus & 86.73 (98.62 / 77.39) & 81.65 (99.85 / 69.06) & 88.42 (99.86 / 79.33) & 77.60 (100.0 / 63.40) & 85.11 (97.35 / 75.61)\\\\\n        & CHMM-FE & 71.43 (72.89 / 70.02) & 81.86 (90.75 / 74.55) & 86.45 (91.73 / 81.75) & 72.38 (88.13 / 61.41) & 67.99 (65.23 / 71.00) \\\\\n        \\midrule\n        \\multirow{6}{*}{\\shortstack{Weakly\\\\Supervised\\\\Models}}\n        & ConNet* & 66.02 (67.98 / 64.19) & 63.04 (74.55 / 55.16) & 72.04 (77.71 / 67.18) & 50.36 (63.04 / 42.73) & 60.58 (59.43 / 61.83) \\\\\n        & MV* & 60.36 (59.06 / 61.72) & 78.44 (93.04 / 67.79) & 80.73 (83.79 / 77.88) & 73.27 (88.86 / 62.33) & 58.85 (54.17 / 64.40)\\\\\n        & Snorkel* & 62.43 (61.62 / 63.26) & 78.44 (93.04 / 67.79) & 83.50 (91.69 / 76.65) & 73.27 (88.86 / 62.33) & 61.85 (57.44 / 66.99) \\\\\n        & HMM* & 62.18 (66.42 / 58.45) & 66.80 (\\textbf{96.79} / 51.00)  & 71.57 (\\textbf{93.48} / 57.98) & 73.63 (89.30 / 62.63) & 55.67 (57.95 / 53.57) \\\\\n        & CHMM* & 63.22 (61.93 / 64.56) & 78.74 (93.21 / 68.15)  & 83.66 (91.76 / 76.87)  & 73.26 (88.79 / 62.36) & 64.06 (59.70 / \\textbf{69.09})\\\\\n        \\cmidrule{2-7}\n        & \\ours & \\textbf{71.53} (\\textbf{73.80} / \\textbf{69.39}) & \\textbf{82.24} (93.18 / \\textbf{73.60})  &  \\textbf{86.63} (89.56 / \\textbf{83.88})  & \\textbf{75.90} (\\textbf{91.94} / \\textbf{64.62}) & \\textbf{64.85} (\\textbf{61.26} / 68.88) \\\\\n        \\bottomrule\n      \\end{tabular}\n      \\begin{tablenotes}\n        \\footnotesize{\n          \\item * Results are from the Wrench benchmark \\citep{zhang-2021-wrench}.\n          All weakly supervised models are evaluated with identical data and weak annotations.\n        }\n      \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tb:results.domains}\n\\end{table*}",
            "tb:ablation.study": "\\begin{table}[t!]\\small\n    \\caption{\n      \\fone scores of ablation studies.\n    }\n    \\centering\n    \\begin{threeparttable}\n    \\begin{tabular}{c|c|c|c|c|c}\n      \\toprule\n      & CoNLL & NCBI & BC5CDR & Laptop & OntoNotes \\\\\n      \\midrule\n      \\ours & \\textbf{71.53} & \\textbf{82.24} & \\textbf{86.63} & \\textbf{75.90} & 64.85 \\\\\n      \\midrule\n      \\multicolumn{6}{c}{Training Stages} \\\\\n      \\midrule\n      SpMM S1 & 69.73 & 77.89 & 86.15 & 73.59 & 64.15 \\\\\n      SpMM S2 & 70.79 & 79.18 & 86.04 & 74.42 & \\textbf{64.92} \\\\\n      \\midrule\n      \\multicolumn{6}{c}{Model Components} \\\\\n      \\midrule\n      Na\\\"ive emiss & 33.02 & 77.32 & -* & 71.77 & -* \\\\\n      w/o $h_{nsr}$ & 58.66 & -* & 86.48 & 75.38 & 64.69 \\\\\n      w/o SoftMax & 62.86 & 80.61 & 82.56 & 73.71 & 53.19 \\\\\n      w/o Dirichlet & 64.70 & 81.34 & 86.05 & 73.51 & 64.64 \\\\\n      w/o S2 & 71.18 & 81.48 & 86.02 & 73.11 & 63.90 \\\\\n      Merge S2 \\& S3 & 71.27 & 79.81 & 85.95 & 71.49 & 64.22 \\\\\n      \\bottomrule\n      \\end{tabular}\n      \\begin{tablenotes}\n        \\footnotesize{\n          \\item * The model fails training; the output labels are all ``\\lbfont{O}''.\n          \\item ``SpMM'' represents \\ours and ``S$i$'' is short for ``stage $i$''.\n        }\n      \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tb:ablation.study}\n\\end{table}",
            "tb:training.time": "\\begin{table}[htbp]\\small\n    \\caption{Training efficiency as ``second/epoch (\\# epoch)$^*$''.}\n    \\centering\n    \\begin{threeparttable}\n    \\begin{tabular}{c|c|c|c|c|c}\n      \\toprule\n      & CoNLL & NCBI & BC5CDR & Laptop & OntoNotes \\\\\n      \\midrule\n      CHMM & 25.46 (50) & 5.96 (55) & 8.54 (72) & 2.60 (56) & 729.38 (28) \\\\\n      \\midrule\n      SpMM S1 & 9.64 (30) & 4.66 (42) & 4.02 (87) & 2.06 (25) & 277.60 (3) \\\\\n      SpMM S2 & 8.36 (5) & 3.92 (3) & 3.36 (5) & 1.82 (6) & 294.23 (2) \\\\\n      SpMM S3 & 8.50 (4) & 4.04 (5) & 3.54 (9) & 1.84 (10) & 250.16 (2) \\\\\n      \\bottomrule\n      \\end{tabular}\n      \\begin{tablenotes}\n        \\footnotesize{\n          \\item[] $^*$The number of epochs required to get similar results as presented in Table~\\ref{tb:results.domains}.\n          \\item The batch size is ajusted such that the models consume similar computation resources during training and inference.\n          \\item ``SpMM'' represents \\ours and ``S$i$'' is short for ``stage $i$''.\n        }\n      \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tb:training.time}\n\\end{table}",
            "tb:lf.performance.conll.2003": "\\begin{table}[htbp]\\small\n    \\caption{LF performance on CoNLL 2003.}\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    LF name & Precision & Recall & \\fone \\\\\n    \\midrule\n    BTC &   67.26 &   44.56 &   53.61 \\\\\n    core_web_md &   70.52 &   58.27 &   63.81 \\\\\n    crunchbase_cased &   37.76 &   6.69 &   11.37 \\\\\n    crunchbase_uncased &   32.75 &   7.31 &   11.96 \\\\\n    full_name_detector &   84.63 &   11.60 &   20.40 \\\\\n    geo_cased &   67.99 &   16.63 &   26.72 \\\\\n    geo_uncased &   64.35 &   20.20 &   30.75 \\\\\n    misc_detector &   85.26 &   20.68 &   33.29 \\\\\n    multitoken_crunchbase_cased &   72.73 &   3.40 &   6.50 \\\\\n    multitoken_crunchbase_uncased &   71.22 &   3.51 &   6.68 \\\\\n    multitoken_geo_cased &   72.06 &   1.74 &   3.39 \\\\\n    multitoken_geo_uncased &   66.83 &   2.35 &   4.55 \\\\\n    multitoken_wiki_cased &   94.36 &   16.01 &   27.37 \\\\\n    multitoken_wiki_uncased &   90.55 &   16.63 &   28.09 \\\\\n    wiki_cased &   75.62 &   35.48 &   48.30 \\\\\n    wiki_uncased &   71.50 &   38.95 &   50.43 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tb:lf.performance.conll.2003}\n\\end{table}",
            "tb:lf.performance.ncbi.disease": "\\begin{table}[htbp]\\small\n    \\caption{LF performance on NCBI-Disease.}\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    LF name & Precision & Recall & \\fone \\\\\n    \\midrule\n    tag-CoreDictionaryUncased &   80.99 &   41.39 &   54.79 \\\\ \n    tag-CoreDictionaryExact &   80.69 &   17.21 &   28.37 \\\\ \n    tag-CancerLike &   34.88 &   1.58 &   3.03 \\\\ \n    tag-BodyTerms &   68.52 &   3.91 &   7.39 \\\\ \n    link-ExtractedPhrase &   96.88 &   36.11 &   52.62 \\\\ \n    \\bottomrule\n    \\end{tabular}\n    \\label{tb:lf.performance.ncbi.disease}\n\\end{table}",
            "tb:lf.performance.bc5cdr": "\\begin{table}[htbp]\\small\n    \\caption{LF performance on BC5CDR.}\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    LF name & Precision & Recall & \\fone \\\\\n    \\midrule\n    tag-DictCore-Chemical &   93.24 &   29.68 &   45.03  \\\\\n    tag-DictCore-Chemical-Exact &   89.55 &   3.26 &   6.29  \\\\\n    tag-DictCore-Disease &   84.19 &   26.91 &   40.78  \\\\\n    tag-DictCore-Disease-Exact &   81.40 &   1.08 &   2.13  \\\\\n    tag-Organic Chemical &   94.06 &   30.17 &   45.68  \\\\\n    tag-Antibiotic &   97.88 &   2.38 &   4.64  \\\\\n    tag-Disease or Syndrome &   79.01 &   11.81 &   20.55  \\\\\n    link-PostHyphen &   86.24 &   7.93 &   14.53  \\\\\n    link-ExtractedPhrase &   87.21 &   17.88 &   29.68  \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tb:lf.performance.bc5cdr}\n\\end{table}",
            "tb:lf.performance.laptop.review": "\\begin{table}[htbp]\\small\n    \\caption{LF performance on LaptopReview.}\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    LF name & Precision & Recall & \\fone \\\\\n    \\midrule\n    tag-CoreDictionary &   72.63 &   51.61 &   60.34  \\\\\n    link-ExtractedPhrase &   97.46 &   29.40 &   45.18  \\\\\n    link-ConsecutiveCapitals &   35.29 &   0.92 &   1.79  \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tb:lf.performance.laptop.review}\n\\end{table}",
            "tb:lf.performance.ontonotes": "\\begin{table}[htbp]\\small\n    \\caption{LF performance on OntoNotes 5.0.}\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    LF name & Precision & Recall & \\fone \\\\\n    \\midrule\n    Core_Keywords &   51.25 &   8.01 &   13.86  \\\\\n    Regex_Patterns &   75.08 &   3.44 &   6.58  \\\\\n    Numeric_Patterns &   60.58 &   10.71 &   18.21  \\\\\n    wiki_fine &   77.24 &   53.15 &   62.97  \\\\\n    money_detector &   47.37 &   1.40 &   2.72  \\\\\n    date_detector &   66.74 &   4.34 &   8.15  \\\\\n    number_detector &   39.71 &   5.70 &   9.97  \\\\\n    company_type_detector &   65.14 &   1.43 &   2.80  \\\\\n    full_name_detector &   53.62 &   4.54 &   8.37  \\\\\n    misc_detector &   61.75 &   10.84 &   18.44  \\\\\n    crunchbase_cased &   23.07 &   4.80 &   7.94  \\\\\n    crunchbase_uncased &   22.63 &   4.84 &   7.97  \\\\\n    geo_cased &   62.87 &   9.03 &   15.79  \\\\\n    geo_uncased &   62.73 &   9.05 &   15.82  \\\\\n    Multitoken_wiki &   87.38 &   13.06 &   22.72  \\\\\n    wiki_cased &   48.61 &   15.62 &   23.64  \\\\\n    wiki_uncased &   48.24 &   15.68 &   23.67  \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tb:lf.performance.ontonotes}\n\\end{table}",
            "tb:hyperparameters": "\\begin{table}[htbp]\\small\n    \\caption{\n    Hyper-parameters.\n    }\n    \\centering\n    \\begin{threeparttable}\n        \\begin{tabular}{cc|c|c|c|c|c}\n        \\toprule\n        & & CoNLL & NCBI & BC5CDR & Laptop & OntoNotes \\\\\n        \\midrule\n        \\multicolumn{7}{c}{Training hyper-parameters} \\\\\n        \\midrule\n        \\multicolumn{2}{c|}{Batch size} & 256 & 128 & 128 & 256 & 32 \\\\\n        \\multicolumn{2}{c|}{PT LR} & 5e-4 & 5e-4 & 5e-4 & 5e-4 & 1e-4 \\\\\n        \\multicolumn{2}{c|}{LR (S1)} & 2e-4 & 1e-3 & 1e-3 & 1e-4 & 1e-4 \\\\\n        \\multicolumn{2}{c|}{LR (S2)} & 4e-5 & 2e-4 & 2e-4 & 2e-5 & 2e-5 \\\\\n        \\multicolumn{2}{c|}{LR (S3)} & 2e-4 & 1e-3 & 1e-3 & 1e-4 & 1e-4 \\\\\n        \\multicolumn{2}{c|}{Use MV$^1$} & false & true & false & true & false \\\\\n        \\midrule\n        \\multicolumn{7}{c}{Model hyper-parameters} \\\\\n        \\midrule\n        \\multicolumn{2}{c|}{Reliab LV} & ENT & LB & ENT & LB & ENT \\\\\n        \\multicolumn{2}{c|}{$\\nu^{\\rm base}$} & 10 & 2 & 2 & 2 & 2 \\\\\n        \\multicolumn{2}{c|}{$\\nu^{\\rm expan}$} & \\num{1000} & \\num{1500} & \\num{1500} & \\num{1000} & \\num{1000} \\\\\n        \\midrule\n        \\multirow{3}{*}{$h$}\n        & $n$ & 1.2 & 1.2 & 0.9 & 0.8 & 1.1 \\\\\n        & $s$ & 1.5 & 3 & 1.1 & 1.5 & 1 \\\\\n        & $r$ & \\multicolumn{5}{c}{$\\frac{1}{K}$}   \\\\\n        \\midrule\n        \\multirow{2}{*}{$g$}\n        & $n$ & \\multicolumn{5}{c}{4} \\\\\n        & $r$ (S1) & $\\frac{1}{2L}$ & $\\frac{1}{20L}$ & $\\frac{1}{10L}$ & $\\frac{1}{20L}$ & $\\frac{1}{5L}$ \\\\\n        & $r$ (S2,3) & $\\frac{1}{20L}$ & $\\frac{1}{20L}$ & $\\frac{1}{10L}$ & $\\frac{1}{20L}$ & $\\frac{1}{5L}$ \\\\\n        \\bottomrule\n        \\end{tabular}\n    \\begin{tablenotes}\n        \\footnotesize{\n        \\item ``PT'' is ``pre-training''; ``Reliab LV'' is short for the ``reliability level''; ``LB'' and ``ENT'' indicate ``label''-level reliability (one score per label) and ``entity''-level reliability (one score per entity).\n        \\item ``S$i$'' represents ``stage $i$''; paramters with no specified stage remain constant for all training stages.\n        \\item $L$ and $K$ are the numbers of labels and LFs, respectively (\\cref{sec:definition}).\n        \\item $h$ is defined in \\eqref{eq:hnsr}; $g$ is defined in \\eqref{eq:gnr}.\n        \\item[1] On some datasets, we add an additional majority voting LF to balance the annotations from existing LFs. The majority voting is only deployed in training but not involved in inference or evaluation.\n        }\n    \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tb:hyperparameters}\n\\end{table}"
        },
        "figures": {
            "fig:example.and.model": "\\begin{figure*}[t!]\n  \\centering{\n    \\subfloat[The annotations and the corresponding true emission matrix of \\emph{one} example LF]{%\n      \\label{fig:lf.examples}%\n      \\includegraphics[height=1.7in]{figures/s4-lf-examples.pdf}%\n    }\\hfil\n    \\subfloat[Model structures of CHMM and \\ours]{%\n      \\label{fig:reg-chmm}%\n      \\includegraphics[height=1.7in]{figures/s4-hmms.pdf}%\n    }\n  }\n  \\caption{\n    \\protect\\subref{fig:lf.examples} shows an example of the weak annotations for weakly supervised NER and HMM's corresponding true emission pattern;\n    \\protect\\subref{fig:reg-chmm} illustrates \\ours's model structure.\n    $\\bLambda_k$ and $\\bDelta_k$ in \\protect\\subref{fig:reg-chmm} respectively parameterize the diagonal and off-diagonal emission elements in \\protect\\subref{fig:lf.examples}, which are LF's probabilities of generating correct and incorrect labels.\n  }\n  \\label{fig:example.and.model}\n  \\Description{Case study}\n\\end{figure*}",
            "fig:emiss.constr": "\\begin{figure*}[t!]\n  \\centerline{\\includegraphics[width = 0.98\\textwidth]{figures/s4-emiss.constr.pdf}}\n  \\caption{\n    Emission construction pipeline for LF $k=1$ with $L=5$ labels.\n    Darker colors indicate larger values in range $[0,1]$.\n  }\n  \\Description{Emission Matrix}\n  \\label{fig:emiss.constr}\n\\end{figure*}",
            "fig:ha": "\\begin{figure}[tbp]\n  \\centerline{\\includegraphics[width = 0.45\\textwidth]{figures/s4-h-a.pdf}}\n  \\caption{\n    Reliability scaling function $h_{n,s,r}(a)$ \\wrt its exponential terms $n$ and $s$.\n    $s>1$ scales up all values, whereas $n>1$ suppresses small values but augments large ones.\n  }\n  \\label{fig:ha}\n  \\Description{Reliability scaling function.}\n\\end{figure}",
            "fig:ga": "\\begin{figure}[tbp]\n  \\centerline{\\includegraphics[width = 0.45\\textwidth]{figures/s4-g-a.pdf}}\n  \\caption{\n    Left: the elements in $\\bLambda_{k,l}, \\forall k, l$ \\wrt the reliability score $A_{k,l}$ with $L=5, r=0.3, n=2$.\n    Right: $g_{n,r}(a)$ \\wrt exponent $n$.\n    Larger $n$ decreases emit-to-\\lbfont{O} probabilities faster.\n  }\n  \\Description{reliability expansion function}\n  \\label{fig:ga}\n\\end{figure}",
            "fig:training-process": "\\begin{figure}[t!]\n  \\centerline{\\includegraphics[width = 0.4\\textwidth]{figures/s4-training-process.pdf}}\n  \\caption{\n    \\ours training procedure.\n    The model parameters of the dotted squares are frozen.\n    MSE and EM are optimization approaches associated with the model pre-training and training steps.\n  }\n  \\label{fig:training-process}\n  \\Description{Training process.}\n\\end{figure}",
            "fig:hyper.parameters": "\\begin{figure}[tbp]\n  \\centering{\n    \\subfloat[\\fone from $h_{n,s,r}$ with different $n$ and $s$]{\n      \\label{subfig:h.parameters}\n      \\includegraphics[height=1.2in]{figures/s5-hnsr.pdf}\n    }\n    \\subfloat[$g_{n,r}$ with different $n$]{\n      \\label{subfig:g.parameters}\n      \\includegraphics[height=1.2in]{figures/s5-gnra.pdf}\n    }\n  }\n  \\caption{\n    Hyper-parameter studies on CoNLL 2003\n  }\n  \\label{fig:hyper.parameters}\n  \\Description{Ablation study}\n\\end{figure}",
            "subfig:emiss.btc": "\\begin{figure*}[!t]\n  \\centering\n  \\subfloat[Emissions of LF BTC\\label{subfig:emiss.btc}]{%\n    \\includegraphics[width=0.5\\textwidth]{figures/s5-emiss-BTC.pdf}}\n  \\subfloat[Emissions of LF crunchbase_uncased\\label{subfig:emiss.cb}]{%\n    \\includegraphics[width=0.5\\textwidth]{figures/s5-emiss-crunchbase-uncased.pdf}}\n  \\caption{Emission probabilities of two LFs on CoNLL 2003.}\\label{fig:emiss.matrix.case.study}\n  \\Description{Case study}\n\\end{figure*}",
            "subfig:emiss.cb": "\\begin{figure*}[!t]\n  \\centering\n  \\subfloat[Emissions of LF BTC\\label{subfig:emiss.btc}]{%\n    \\includegraphics[width=0.5\\textwidth]{figures/s5-emiss-BTC.pdf}}\n  \\subfloat[Emissions of LF crunchbase_uncased\\label{subfig:emiss.cb}]{%\n    \\includegraphics[width=0.5\\textwidth]{figures/s5-emiss-crunchbase-uncased.pdf}}\n  \\caption{Emission probabilities of two LFs on CoNLL 2003.}\\label{fig:emiss.matrix.case.study}\n  \\Description{Case study}\n\\end{figure*}",
            "fig:emiss.matrix.case.study": "\\begin{figure*}[!t]\n  \\centering\n  \\subfloat[Emissions of LF BTC\\label{subfig:emiss.btc}]{%\n    \\includegraphics[width=0.5\\textwidth]{figures/s5-emiss-BTC.pdf}}\n  \\subfloat[Emissions of LF crunchbase_uncased\\label{subfig:emiss.cb}]{%\n    \\includegraphics[width=0.5\\textwidth]{figures/s5-emiss-crunchbase-uncased.pdf}}\n  \\caption{Emission probabilities of two LFs on CoNLL 2003.}\\label{fig:emiss.matrix.case.study}\n  \\Description{Case study}\n\\end{figure*}",
            "fig:lf.performance.case.study": "\\begin{figure}[tbp]\n  \\centering{\n    \\subfloat[Predicted LF reliabilities \\vs LF entity \\fone]{%\n      \\label{subfig:reliability.scatter}%\n      \\includegraphics[height=1.6in]{figures/s5-reliability-scatter.pdf}%\n    }\\hfil\n    \\subfloat[Correlation coefficients]{%\n      \\label{subfig:correlation.coefficient}%\n      \\includegraphics[height=1.6in]{figures/s5-correlation.pdf}%\n    }\n  }\n  \\caption{\n    Illustration of the correlation between the predicted LF reliability scores and the true LF \\fone scores.\n    \\protect\\subref{fig:lf.examples} shows the results from the CoNLL 2003 dataset.\n  }\n  \\label{fig:lf.performance.case.study}\n  \\Description{Case study}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation*}\n  \\Phi_{k, l, l} \\triangleq p(x_{k,l}^{(t)}=1|z^{(t)}=l, \\e^{(0)}), \\quad l \\in 1:L,\n\\end{equation*}",
            "eq:2": "\\begin{equation*}\n  \\A = \\text{reshape}(\\fc(\\e^{(0)})),\n\\end{equation*}",
            "eq:3": "\\begin{equation}\n  \\label{eq:softmax-A}\n  \\begin{gathered}\n    \\hat{\\A}_{:, l} = \\softmax(\\A_{:, l}), \\quad l\\geq 2.\n  \\end{gathered}\n\\end{equation}",
            "eq:4": "\\begin{equation*}\n  \\label{eq:sigmoid-A}\n  \\hat{\\A}_{:, 1} = \\sigma( \\A_{:, 1} ).\n\\end{equation*}",
            "eq:5": "\\begin{equation}\n  \\label{eq:hnsr}\n  \\begin{gathered}\n    \\tilde{A}_{k, l} = h_{n,s,r}(\\hat{A}_{k, l}); \\\\\n    h_{n,s,r}(a)= \n    \\begin{cases}\n      \\frac{1}{r^{n-1}} a & a^\\frac{1}{s} < r; \\\\\n      -\\frac{1}{(1-r)^{(n-1)}}(1-a^\\frac{1}{s})^n + 1 &  a^\\frac{1}{s} \\geq r.\n    \\end{cases}\n  \\end{gathered}\n\\end{equation}",
            "eq:6": "\\begin{equation*}\n  \\Lambda_{k,i,j} = f_{i,j}(\\tilde{A}_{k,i});\\quad \\forall i,j \\in 1:L,\n\\end{equation*}",
            "eq:7": "\\begin{equation}\n  \\label{eq:fij}\n  f_{i,j}(a) = \n  \\begin{cases}\n    a & i=j; \\\\\n    \\frac{1}{L-1}(1-a) & i=1,j\\geq 2; \\\\\n    g_{n,r}( a ) & i \\geq 2, j=1; \\\\\n    \\frac{1}{L-2}(1-a-g_{n,r}( a )) & i \\geq 2, j \\geq 2, i\\neq j,\n  \\end{cases}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n  \\label{eq:gnr}\n  g_{n,r}( a ) =\n  \\begin{cases}\n    \\frac{2-L}{(n-1)r^n-nr^{n-1}}a^n + (1-L)x + 1 & a \\leq r; \\\\\n    \\frac{g_{n,r}( r )}{r-1}a - \\frac{g_{n,r}( r )}{r-1} & a > r,\n  \\end{cases}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n  \\label{eq:wxor}\n  \\begin{gathered}\n    W^{(t)}_{k, \\lquery, \\ltgt} = (1-\\tilde{A}_{k, \\lquery})x^{(t)}_{k, \\lquery} \\sum_{k'=1}^K \\tilde{A}_{k', \\ltgt}x^{(t)}_{k',\\ltgt}; \\\\\n    \\W^{(t)}_{k, 1, :} = \\W^{(t)}_{k, :, 1} = \\0; \\quad W^{(t)}_{k, l, l} = 0,\\ \\forall l\\in 1:L.\n  \\end{gathered}\n\\end{equation}",
            "eq:10": "\\begin{equation*}\n  \\hat{W}_{k, \\lquery, \\ltgt} = \\frac{\\sum_{m=1}^M \\sum_{t=1}^{T_m} {W}^{(t)}_{m, k, \\lquery, \\ltgt}}{\\sum_{m=1}^M \\sum_{t=1}^{T_m} {x}^{(t)}_{m, k, \\lquery}}.\n\\end{equation*}",
            "eq:11": "\\begin{equation*}\n  \\C = \\text{reshape}(\\sigma(\\fc(\\e^{(0)}))).\n\\end{equation*}",
            "eq:12": "\\begin{equation*}\n  \\tilde{\\W}_{k, :, \\ltgt} = \\softmax (\\hat{\\W}_{k, :, \\ltgt}).\n\\end{equation*}",
            "eq:13": "\\begin{equation}\n  \\bDelta_{k,:,l} = C_{k,l} \\times \\tilde{\\W}_{k,l,:}.\n\\end{equation}",
            "eq:14": "\\begin{equation*}\n  \\bOmega = \\nu^{\\rm expan} \\times (\\bLambda + \\bDelta) + \\nu^{\\rm base}.\n\\end{equation*}",
            "eq:15": "\\begin{equation*}\n  \\bPhi_{k,l} \\sim \\Dir (\\bOmega_{k,l} ).\n\\end{equation*}",
            "eq:16": "\\begin{equation}\n  \\label{eq:model.initialization}\n  \\ell_{\\rm MSE} = \\frac{1}{K} \\sum_{k=1}^K \\|\\bPhi_k - \\bPhi^*\\|_F^2 + \\frac{1}{T}\\sum_{t=1}^T \\|\\bPsi^{(t)} - \\bPsi^*\\|_F^2,\n\\end{equation}",
            "eq:17": "\\begin{equation}\n    \\label{eq:em.q}\n    Q(\\btht, \\btht^{\\rm old}) \\triangleq \\expt_{\\z} [\\ell_c (\\btht) | \\x, \\btht^{\\rm old}].\n\\end{equation}",
            "eq:18": "\\begin{equation}\n  \\label{eq:lc}\n  \\begin{aligned}\n    & \\ell_c(\\btht) \\triangleq \\log p(\\z^{(0:T)}, \\x^{(1:T)} | \\btht, \\e^{(0:T)}) = \\log p(z^{(0)}) +\\\\\n    &\\quad \\sum_{t=1}^T \\log p(z^{(t)}|z^{(t-1)}, \\e^{(t)}) + \\sum_{t=1}^T \\log p(\\x^{(t)}|z^{(t)}, \\e^{(0)}),\n  \\end{aligned}\n\\end{equation}",
            "eq:19": "\\begin{equation}\n  \\label{eq:varphi}\n  \\varphi_l^{(t)} \\triangleq p(\\x^{(t)}|z^{(t)}=l, \\e^{(0)})=\\prod_{k=1}^K \\sum_{j=1}^L \\Phi_{k,l,j}x_{k,j},\n\\end{equation}",
            "eq:20": "\\begin{equation*}\n  \\Phi^{\\rm true}_{k,i,j} = p(\\x_k|\\y) \\triangleq \\frac{\\sum_{m=1}^M \\sum_{t=1}^{T_m} \\I(x^{(t)}_{m,k,j}=1, y^{(t)}_{m}=i) }{\\sum_{m=1}^M \\sum_{t=1}^{T_m} \\I(y^{(t)}_m=i) },\n\\end{equation*}",
            "eq:21": "\\begin{equation}\n    \\label{eq:q.calculate}\n    \\begin{aligned}\n        Q&(\\btht, \\btht^{\\rm old}) = \\sum_{i=1}^{L} p(z^{(0)} = i| \\x^{(1:T)}, \\e^{(0:T)}) \\log p(z^{(0)}=i) + \\\\\n        & \\sum_{t=1}^{T}\\sum_{i=1}^{L} \\sum_{j=1}^{L} p(z^{(t-1)}=i, z^{(t)}=j | \\x^{(1:T)}, \\e^{(0:T)}) \\log \\Psi^{(t)}_{i,j} +\\\\\n        & \\sum_{t=1}^{T}\\sum_{i=1}^{L} p(z^{(t)} = i| \\x^{(1:T)}, \\e^{(0:T)}) \\log \\varphi^{(t)}_{i},\n    \\end{aligned}\n\\end{equation}",
            "eq:22": "\\begin{equation*}\n    \\gamma^{(t)}_i \\triangleq p(z^{(t)} = i| \\x^{(1:T)}, \\e^{(0:T)}),\n\\end{equation*}",
            "eq:23": "\\begin{equation*}\n    \\xi^{(t)}_{i,j} \\triangleq p(z^{(t-1)}=i, z^{(t)}=j | \\x^{(1:T)}, \\e^{(0:T)}).\n\\end{equation*}",
            "eq:24": "\\begin{equation*}\n    \\alpha^{(t)}_i \\triangleq p(z^{(t)}=i|\\x^{(1:t)}, \\e^{(0:T)}),\n\\end{equation*}",
            "eq:25": "\\begin{equation*}\n    \\beta^{(t)}_i \\triangleq p(\\x^{(t+1:T)}|z^{(t)} = i, \\e^{(0:T)}).\n\\end{equation*}",
            "eq:26": "\\begin{equation*}\n  \\begin{aligned}\n    \\alpha^{(t)}_i &\\propto p(\\x^{(t)}|z^{(t)}=i, \\e^{(0)}) p(z^{(t)}=i|\\x^{(1:t-1)}, \\e^{(0:t)}) \\\\\n    &= \\sum_{j=1}^L \\varphi^{(t)}_i \\Psi^{(t)}_{j,i} \\alpha^{(t-1)}_j,\n  \\end{aligned}\n\\end{equation*}",
            "eq:27": "\\begin{equation*}\n    \\balpha^{(t)} \\propto \\bphi^{(t)} \\odot ({\\bPsi^{(t)}}^{\\sf T}\\balpha^{(t-1)}),\n\\end{equation*}",
            "eq:28": "\\begin{equation*}\n    \\begin{aligned}\n      \\beta^{(t-1)}_i &= \\sum_{j=1}^L p(z^{(t)}=j, \\x^{(t)}, \\x^{(t+1:T)}|z^{(t-1)}=i, \\e^{(0,t:T)}) \\\\\n      &= \\sum_{j=1}^L \\beta^{(t)}_j \\varphi^{(t)}_j \\Psi^{(t)}_{i,j}.\n    \\end{aligned}\n\\end{equation*}",
            "eq:29": "\\begin{equation*}\n    \\bm{\\beta}^{(t-1)} = \\bPsi^{(t)}(\\bphi^{(t)} \\odot \\bm{\\beta}^{(t)}),\n\\end{equation*}",
            "eq:30": "\\begin{equation*}\n    \\beta^{(T)}_i = p(\\x^{(T+1:T)}|z^{(T)}=i) = 1, \\forall i \\in 1:L.\n\\end{equation*}",
            "eq:31": "\\begin{equation*}\n    \\hat{\\z}^{(1:T)} = \\argmax_{\\z^{(1:T)}}p(\\z^{(1:T)} | \\x^{(1:T)}, \\e^{(0:T)}).\n\\end{equation*}"
        },
        "git_link": "https://github.com/Yinghao-Li/Sparse-CHMM"
    }
}