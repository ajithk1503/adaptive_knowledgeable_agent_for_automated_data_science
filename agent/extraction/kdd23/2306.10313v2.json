{
    "meta_info": {
        "title": "Adversaries with Limited Information in the Friedkin--Johnsen Model",
        "abstract": "In recent years, online social networks have been the target of adversaries\nwho seek to introduce discord into societies, to undermine democracies and to\ndestabilize communities. Often the goal is not to favor a certain side of a\nconflict but to increase disagreement and polarization. To get a mathematical\nunderstanding of such attacks, researchers use opinion-formation models from\nsociology, such as the Friedkin--Johnsen model, and formally study how much\ndiscord the adversary can produce when altering the opinions for only a small\nset of users. In this line of work, it is commonly assumed that the adversary\nhas full knowledge about the network topology and the opinions of all users.\nHowever, the latter assumption is often unrealistic in practice, where user\nopinions are not available or simply difficult to estimate accurately.\n  To address this concern, we raise the following question: Can an attacker sow\ndiscord in a social network, even when only the network topology is known? We\nanswer this question affirmatively. We present approximation algorithms for\ndetecting a small set of users who are highly influential for the disagreement\nand polarization in the network. We show that when the adversary radicalizes\nthese users and if the initial disagreement/polarization in the network is not\nvery high, then our method gives a constant-factor approximation on the setting\nwhen the user opinions are known. To find the set of influential users, we\nprovide a novel approximation algorithm for a variant of MaxCut in graphs with\npositive and negative edge weights. We experimentally evaluate our methods,\nwhich have access only to the network topology, and we find that they have\nsimilar performance as methods that have access to the network topology and all\nuser opinions. We further present an NP-hardness proof, which was an open\nquestion by Chen and Racz [IEEE Trans. Netw. Sci. Eng., 2021].",
        "author": "Sijing Tu, Stefan Neumann, Aristides Gionis",
        "link": "http://arxiv.org/abs/2306.10313v2",
        "category": [
            "cs.SI",
            "cs.DS",
            "cs.LG"
        ],
        "additionl_info": "KDD'23"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n\tOnline social networks have become an integral part of modern societies and\n\tare used by billions of people on a daily basis.  In addition to connecting\n\tpeople with their friends and family, online social networks facilitate\n\tsocietal deliberation and play an important role in forming the political\n\twill in modern democracies.\n\n\tHowever, during recent years we have had ample evidence of\n\tmalicious actors performing attacks on social networks so as to destabilize\n\tcommunities, sow disagreement, and increase polarization.  For instance, a\n\treport issued by the United States Senate finds that Russian ``trolls\n\tmonitored societal divisions and were poised to pounce when new events\n\tprovoked societal discord'' and that this ``campaign [was] designed to sow\n\tdiscord in American politics and society''~\\cite{senatereport}. \n\tAnother report found that both left- and\n\tright-leaning audiences were targeted by these trolls~\\cite{diresta2019tactics}. \n\tSimilarly, a recent analysis regarding the Iranian\n\tdisinformation claimed that ``the main goal is to control public \n\topinion---pitting groups against each other and tarnishing the reputations of\n\tactivists and protesters''~\\cite{hassaniyan2022long}.\n\t\n\tThe study of how such attacks influence societies can be fa\\-cil\\-i\\-tat\\-ed \n\tby models of \\emph{opinion dynamics}, \n\twhich study the mechanisms for individuals to form their opinions in social networks.\n\tRelevant research questions have been investigated in different disciplines, \n\te.g., psychology, social sciences, and\n\teconomics~\\cite{castellano2009statistical,jackson2008social,acemoglu2011opinion,\n\t\txia2011opinion,lorenz2007continuous}. \n\tA popular model for studying such questions in \n\tcomputer science~\\cite{gionis2013opinion,musco2018minimizing,zhu2021minimizing,bindel2015bad,abebe2021opinion,xu2021fast,tu22viral}\n\tis the \\emph{Friedkin--Johnsen model} (FJ)~\\cite{friedkin1990social}, \n\twhich is a generalization of the \\emph{DeGroot~model}~\\cite{degroot1974reaching}.\n\n\tTo understand the power of an adversarial actor over the opinion-formation\n\tprocess in a social network, there are two popular measures of\n\t{discord}: \\emph{disagreement} and \\emph{polarization}; \n\tfor the rest of the paper, we use the word \\emph{discord} \n\tto refer to either disagreement or polarization; \n\tsee Section~\\ref{sec:preliminaries} for the formal definitions.\n\t\n\tPrevious works studied the increase of discord that can be\n\tinflicted by a malicious attacker who can change the opinions of a small\n\tnumber of users. As an example, Chen and Racz~\\cite{chen2020network} showed\n\tthat even simple heuristics, such as changing the opinions of centrists, can\n\tlead to a significant increase of the disagreement in the network. They also\n\tpresented theoretical bounds, which were later extended by Gaitonde,\n\tKleinberg and Tardos~\\cite{gaitonde2020adversarial}.\n\n\tCrucially, the previous methods assume that the attacker has access to\n\tthe network topology \\emph{as well as} the opinions of all users. \n\tHowever, the latter assumption is rather impractical: \n\tuser opinions are either not available or difficult to estimate accurately.\n\tOn the other hand, obtaining the network\n\ttopology is more feasible, as networks often provide access to\n\tthe follower and interaction~graphs.\n\n\tAs knowledge of all user opinions appears unrealistic, \n\twe raise the following question: \n\t\\emph{Can attackers sow a significant amount of discord in a social network, \n\teven when only the network topology is known?}\n\tIn other words, we consider a setting \\emph{with limited information} in\n\twhich the adversary has to pick a small set of users, without knowing the\n\tuser opinions in the network.\n\n\t\\smallskip\n\t\\noindent\n\t\\textbf{Our Contributions.} Our main contributions are as follows.\n\tFirst, we provide a formal connection between the settings of full (all user\n\topinions are known) and limited information (the user opinions are unknown).\n\tInformally, we show that if the variance of user opinions in the network \n\tis not very high (and some other mild technical assumptions),\n\tthen an adversary who radicalizes the users who are highly influential\n\tfor the network obtains a $\\bigO(1)$-approximation for the setting when all\n\tuser opinions are known. Thus, we\n\tanswer the above question affirmatively from a theoretical point of view.\n\n\tSecond, we implement our algorithms and evaluate them on real-world datasets.\n\tOur experiments show that for maximizing disagreement, our algorithms,\n\t\\emph{which use only topology information}, outperform simple baselines and\n\thave similar performance as existing algorithms that have \\emph{full}\n\tinformation. Therefore, we also answer the above question affirmatively in\n\tpractice.\n\n\tThird, we provide constant-factor approximation algorithms for identifying\n\t$\\Omega(n)$ users who are \\emph{highly influential} for the discord in the\n\tnetwork, where $n$ is the number of users in the network. \n\tWe derive analytically the concept of highly-influential users for\n\tnetwork discord and we formalize an associated computational task\t\n\t(Section~\\ref{sec:algorithms-our-problem}).\n\tOur formulation allows us to obtain insights into which users drive the\n\tdisagreement and the polarization in social networks.  We also show that\n\tthis problem is \\NPhard, which solves an open problem by Chen and\n\tRacz~\\cite{chen2020network}.\n\n\tFourth, we show that to find the users who are influential on the discord,\n\twe have to solve a version of cardinality constrained {\\sc Max\\-Cut} in\n\tgraphs with \\emph{both positive and negative edge weights}.  For this\n\tproblem, we present the first constant-factor approximation algorithm when\n\tthe number of users to radicalize is $\\Omega(n)$. Here the main technical challenge arises\n\tfrom the presence of negative edges, which imply that the problem is\n\tnon-submodular and which rule out using \n\t\\emph{averaging arguments} that are often used to\n\tanalyze such algorithms~\\cite[A.3.2]{arora2009computational}. \n\tHence, existing algorithms do not extend to our\n\tmore general case and we prove analogous results for graphs with positive\n\tand negative edge weights.  In addition, our \\NP-hardness proof provides a\n\tfurther connection between maximizing the disagreement and {\\sc Max\\-Cut}.\n\n\tWe discuss some of the ethical aspects of our findings \n\tregarding the power of a malicious adversary who has access to the topology\n\tof a social network in the conclusion (Section~\\ref{sec:conclusion}). \n\n\\sbpara{Related work.}\nA recently emerging and popular topic in the area of graph mining \nis to study optimization problems based on FJ opinion dynamics. \nPapers considered minimizing disagreement and\npolarization indices~\\cite{musco2018minimizing}, maximizing\nopinions~\\cite{gionis2013opinion}, changes of the network\ntopology~\\cite{zhu2021minimizing,bindel2015bad} or changes of the susceptibility\nto persuasion~\\cite{abebe2021opinion}. Xu et al.~\\cite{xu2021fast} show how to\nefficiently estimate quantities such as the polarization and disagreement indices.\nOur paper is also conceptually related to the topic of maximizing influence in\nsocial networks, pioneered by Kempe, Kleinberg and\nTardos~\\cite{kempe2015maximizing}; \nthe influence-maximization model has recently been\ncombined with opinion-formation processes~\\cite{tu22viral}.\nFurthermore, many extensions of the classic FJ model have been\nproposed~\\cite{amelkin17polar,parsegov2016novel}.\n\n\nMost related to our work are the papers by Chen and Racz~\\cite{chen2020network}\nand by Gaitonde, Kleinberg and Tardos~\\cite{gaitonde2020adversarial}, who\nconsider adversaries who plan network attacks.  They provide upper bounds when\nan adversary can take over $k$~nodes in the network, and they present heuristics\nfor maximizing disagreement in the setting with full information.  A practical\nconsideration of this model has motivated us to study settings with limited\ninformation.  While their adversary can change the opinions of $k$~nodes to\neither $0$ or $1$, in this paper we are mainly concerned with adversaries which\ncan change the opinions of $k$~nodes to $1$; we consider the adversary's\nactions as ``radicalizing $k$~nodes.''  Our setting is applicable in scenarios\nwhen opinions near opinion value $0$ ($1$) correspond to non-radicalized \n(radicalized) views.\n\nOur algorithm for {\\sc MaxCut} in graphs with positive and negative edge weights \nis based on the SDP-rounding techniques by Goemans and Williamson~\\cite{goemans1995improved} for {\\sc MaxCut}, \nand by Frieze and Jerrum~\\cite{frieze1997improved} for {\\sc Max\\-Bi\\-section}. \nWhile their results assume\nthat the matrix~$\\m+A$ in Problem~\\eqref{eq:our-problem} is the Laplacian of a\ngraph with positive edge weights, our result in Theorem~\\ref{thm:unbalanced}\napplies to more general matrices, %\nalbeit with worse approximation ratios.\nCurrently, the best approximation algorithm for {\\sc Max\\-Bi\\-section} is by Austrin et al.~\\cite{austrin2016better}.  \nAgeev and Sviridenko~\\cite{ageev1999approximation} \ngives LP-based algorithms for\nversions of {\\sc Max\\-Cut} with given sizes of parts,\nFeige and Langberg~\\cite{feige2001approximation} extended this work to an SDP-based algorithm;\nbut their techniques appear to be\ninherently limited to positive edge-weight graphs and cannot be extended to our more general\nsetting of Problem~\\eqref{eq:our-problem}.\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\\label{sec:preliminaries}\n\nLet $\\graph = (V, E, w)$ be an undirected weighted graph \nrepresenting a social network.\nThe edge-weight function \n$w \\colon E \\rightarrow \\Real_{>0}$\nmodels the strengths of user interactions. \nWe write $\\abs{V} = n$ for the number of nodes, %\nand use $N(u)$ to denote the set of neighbors of node $u\\in V$, \ni.e., $N(u) = \\{v : (u, v) \\in E\\}$. \nWe let $\\m+D$ be the $n\\times n$ diagonal matrix with\n$\\m+D_{v,v} = \\sum_{u\\in N(v)}w(u,v)$ and \ndefine the weighted adjacency matrix $\\m+W$\nby $\\m+W_{u,v} = w(u,v)$. %\nThe Laplacian of the graph $G$ is given by $\\laplacian = \\m+D - \\m+W$.\n\nIn the \\emph{Friedkin--Johnsen opinion-dynamics model} (FJ)~\\cite{friedkin1990social}, \neach node $u\\in V$ corresponds to a person who has\nan \\emph{innate opinion} and an \\emph{expressed opinion}.\nFor each node~$u$, the innate opinion $s_u\\in[0,1]$ is fixed over time and kept\nprivate; the expressed opinion $z_u^{(t)}\\in[0,1]$ is publicly known \nand it changes over time $t\\in\\mathbb{N}$ due to peer pressure. \nInitially, $z_u^{(0)}=s_u$ for all users $u\\in V$. \nAt each time\n$t>0$, all users $u\\in V$ update their expressed opinion $z_u^{(t)}$ as the\nweighted average of their innate opinion and the expressed opinions of their\nneighbors, as \nfollows:\n\\begin{align}\n\\label{eq:update-opinions}\n\tz_u^{(t)}\n\t= \\frac{s_u + \\sum_{v\\in N(u)} w_{uv} z_v^{(t-1)}}{1 + \\sum_{v\\in N(u)} w_{uv}}.\n\\end{align}\n\nWe write $\\v+z^{(t)} = (z_1^{(t)},\\dots,z_n^{(t)})$\nto denote the vector of expressed opinions at time $t$.\nSimilarly, we set $\\v+s = (s_1,\\dots,s_n)$ for the innate opinions.\nIn the limit $t\\to\\infty$, the expressed opinions\nreach the equilibrium $\\finop = \\lim_{t\\to\\infty} \\v+z^{(t)} = (\\ID + \\laplacian)^{-1} \\begop$. \n\nWe study the behavior of the following two \\emph{discord measures} in the FJ\nopinion-dynamics model:\n\\begin{description}\n\\item \\textbf{Disagreement index}~(\\cite{musco2018minimizing})\n\t$\\DisIdx{G, \\begop} = \\sum_{(u,v)\\in E} w_{u,v} (z_u-z_v)^2 \n\t= \\begop^{\\intercal} \\MasIdx{\\DisIdx{}} \\begop$, where\n\t$\\MasIdx{\\DisIdx{}} = (\\laplacian + \\ID)^{-1} \\laplacian (\\laplacian +\n\t\t\t\\ID)^{-1}$, and\n\\item \\textbf{Polarization index}~(\\cite{matakos2017measuring, musco2018minimizing})\n\t$\\PolIdx{G, \\begop} = \\sum_{u\\in V} (z_u - \\bar{\\finop})^2 =\n\t\\begop^{\\intercal} \\MasIdx{\\PolIdx{}} \\begop$, where\n\t$\\bar{\\finop} = \\frac{1}{n} \\sum_{u\\in V} z_u$ is the average user\n\topinion and \n\t$\\MasIdx{\\PolIdx{}} = (\\ID + \\laplacian)^{-1} (\\ID - \\frac{\\ind \\ind^\\intercal}{n}) (\\ID + \\laplacian)^{-1}$.\n\\end{description}\nNote that the disagreement index measures the discord along the edges of the network,\ni.e., it measures how much interacting nodes disagree. The polarization index measures\nthe overall discord in the network by considering the variance of the opinions.\n\nWe note that the matrices $\\MasIdx{\\DisIdx{}}$ and $\\MasIdx{\\PolIdx{}}$ may have\npositive and negative off-diagonal entries and it is not clear whether they are\ndiagonally dominant; this is in contrast to graph Laplacians that have\nexclusively non-positive off-diagonal entries and are diagonally dominant.\nHaving positive and negative entries will be one of the challenges we need to\novercome later.  The following lemma presents some additional properties.\n\\begin{lemma}\n\\label{lem:matrices}\n\tLet $\\m+A\\in\\{\\MasIdx{\\DisIdx{}}, \\MasIdx{\\PolIdx{}}\\}$.\n\tThen $\\m+A$ is positive semi\\-definite and satisfies $\\m+A \\v+1 = \\v+0$, where $\\v+1$ is the all-ones\n\tvector and $\\v+0$ is the all-zeros vector.\n\\end{lemma}\n\nWhile we consider opinions in the interval $[0,1]$, \nfor the \\NPhardness results presented later, \nfor technical reasons,\nit will be useful to consider opinions in the interval~$[-1,1]$.\nIn Appendix~\\ref{sec:scaling}, we show that the solutions \nof optimization problems are maintained \nunder scaling, which implies that our \\NPhardness results and \nour $\\bigO(1)$-approximation algorithm also apply for opinions in the $[0, 1]$~interval.\n\nWe present all omitted proofs in Appendix~\\ref{sec:omitted-proofs}. \n\n"
            },
            "section 3": {
                "name": "Problem definition and algorithms",
                "content": "\n\\label{sec:algorithms}\n\nWe start by defining the problem of maximizing the discord when~$k$ user\nopinions can be \\emph{radicalized}, i.e., when for $k$~users the innate opinions\ncan be changed from their current value $\\begop_0(u)$ to the extreme value $1$.\nThis problem is of practical relevance when opinions close to~$0$ correspond to\nnon-radicalized opinions (``{\\sc covid-19} vaccines are generally safe'') and\nopinions close to $1$ correspond to radicalized opinions (``{\\sc covid-19}\nvaccines are harmful'').  Then an adversary can radicalize $k$~people by setting\ntheir opinion to $1$, for instance, by supplying them with fake news or by\nhacking their social network accounts.  Formally, our problem is stated as\nfollows.\n\n\\begin{problem}\n\tLet $\\m+A\\in\\{\\MasIdx{\\DisIdx{}}, \\MasIdx{\\PolIdx{}}\\}$.\n\tConsider an undirected weighted graph $G = (V, E, w)$, and innate opinions\n\t$\\begop_0\\in[0, 1]^n$.\n\tWe want to maximize the discord\n    where we can radicalize the innate opinions of $k$~users.\n\tIn matrix notation, the problem is as follows:\n\t\\begin{equation}\n    \\label{problem:max-disagreement}\n\t\t\\begin{aligned}\n\t\t\t\\max_{\\begop} \\quad &  \\begop^{\\intercal} \\m+A \\begop,\\\\\n\t\t\t\\st \\quad &\\lVert \\begop - \\begop_0 \\rVert_0 = k, \\text{ and}\\\\\n\t\t\t& \\begop(u) \\in \\{\\begop_0(u), 1\\} \\text{ for all } u\\in V.\n\t\t\\end{aligned}\n\t\\end{equation}   \n\\end{problem}\n\n\nNote that if we set $\\m+A = \\MasIdx{\\DisIdx{}}$ the problem is to\nmaximize the disagreement in the network. If we set $\\m+A = \\MasIdx{\\PolIdx{}}$\nwe seek to max\\-i\\-mize the polarization.\n\nFurther observe that for Problem~\\eqref{problem:max-disagreement}, the algorithm\nobtains as input the graph~$G$ \\emph{and the vector of innate opinions\n$\\begop_0$}.  Therefore, we view this formulation as the setting \\emph{with full\ninformation}.\n\nCentral to our paper is the idea that the algorithm has access to the topology of the graph~$G$,\n\\emph{but it does not have access to the initial innate opinions~$\\begop_0$}.\nAs discussed in the introduction, we believe that this scenario is of higher\npractical relevance, as it seems infeasible for an attacker to gather the\nopinions of millions of users in online social networks.  On the other hand,\nassuming access to the network topology, i.e., the graph~$G$, appears more\nfeasible because networks, such as Twitter, make this information publicly\navailable. \n\nOur approach for maximizing the discord, even when we have limited information,\ni.e., we only have access to the graph topology, has two steps:\n\\begin{enumerate}\n\t\\item[{\\bf 1.}] Detect a small set~$S$ of $k$~users who are highly influential\n\t\tfor the discord in the network.\n\t\\item[{\\bf 2.}] Change the innate opinions for the users in the set~$S$ to $1$ and\n\t\tleave all other opinions unchanged.\n\\end{enumerate}\n\n\\smallskip\nIn the rest of this section, we will describe our overall approach for finding a\nset of $k$~influential users on the discord\n(Section~\\ref{sec:algorithms-our-problem}) and then we will discuss\napproximation algorithms (Section~\\ref{sec:balanced-maxcut}) and heuristics\n(Section~\\ref{sec:greedy}) for this task. \nThen, we prove computational hardness (Section~\\ref{sec:hardness}).\n\n",
                "subsection 3.1": {
                    "name": "Finding influential users on the discord",
                    "content": "\n\\label{sec:algorithms-our-problem}\n\nNext, we describe the implementation of Step~(1) discussed above. \nIn other words, we wish\nto find a set~$S$ of $k$~users who are highly influential for the discord in the\nnetwork. \n\n\nTo form an intuition about highly-influential users for the network discord in\nthe absence of information about user innate opinions, we consider scenarios of\nnon-controversial topics. Since the topics are non-controversial, we expect most\nusers to have opinions near a consensus opinion~$c$. %\nIn such scenarios, an adversary who aims to radicalize $k$~users so as to maximize\nthe network discord, will seek to find a set~$S$ of $k$~users and set\n$\\begop_0(u)=1$, for $u\\in S$, so as to maximize the discord\n$\\begop^{\\intercal} \\m+A \\begop$, where\n$\\m+A\\in\\{\\MasIdx{\\DisIdx{}}, \\MasIdx{\\PolIdx{}}\\}$.\n\nSince we assume most opinions to be near consensus~$c$, it seems natural that\nthe concrete value of~$c\\in[0,1]$ has no big effect on the choice of the users\npicked by the adversary (see also Theorem~\\ref{thm:relationship} which\nformalizes that this intuition is correct).  Hence, we consider $c=0$ and study\nthe idealized version of the problem, where $\\begop_0(u)=0$ for all users $u$ in\nthe network.  In this case, the adversary will need to solve the following\noptimization problem:\n\\begin{align}\n\\begin{aligned}\n\\label{eq:our-problem}\n\t\\max_{\\begop} \\quad & \\begop^{\\intercal} \\m+A \\begop,\\\\\n\t\\st \\quad &\\lVert \\begop \\rVert_0 = k, \\text{ and} \\\\\n\t\t& \\begop \\in [0,1]^n.\n\\end{aligned}\n\\end{align}\nThe result of the above optimization problem is a vector~$\\v+s$ that\nhas $k$ non-zero entries, all of which are equal to $1$. %\nThus, we can view the set $S=\\{u \\mid \\begop_0(u)=1\\}$ as a set of users who are\nhighly influential for the discord in the network.  \n\nWe provide a constant-factor approximation algorithm for this problem in\nTheorem~\\ref{thm:unbalanced}.  We also show that the problem is \\NPhard in\nTheorem~\\ref{thm:disagreement-np-hard} when $\\m+A=\\MasIdx{\\DisIdx{}}$, which\nanswers an open question by Chen and Racz~\\cite{chen2020network}.\n\n\\sbpara{Relationship between the limited and full information settings.}\nAt first glance, it may not be obvious why a solution for\nProblem~\\eqref{eq:our-problem} with limited information implies a good solution\nfor Problem~\\eqref{problem:max-disagreement} with full information. However, we\nwill show that this is indeed the case when there is little initial discord in\nthe network; we believe this is the most interesting setting for attackers\nwho wish to increase the discord.\n\nSlightly more formally (see Theorem~\\ref{thm:relationship} for\ndetails), we show the following. If initially all innate opinions are close to\nthe average opinion $c$ and some mild assumptions hold, then an\n$\\bigO(1)$-approximate solution for Problem~\\ref{eq:our-problem} (when only the\nnetwork topology is known) implies an $\\bigO(1)$-approximate solution for\nProblem~\\ref{problem:max-disagreement} (when full information including user\nopinions are~known).\n\nBefore stating the theorem, we define some additional notation.  For a set of\nusers $X\\subseteq V$, we write $\\begop_X$ to denote the vector of innate\nopinions when we radicalize the users in $X$, i.e., $\\begop_X\\in[0,1]^n$\nsatisfies $\\begop_X(u) = 1$ if $u\\in X$ and $\\begop_X(u) = \\begop_0(u)$  if\n$u\\not\\in X$.  Furthermore, given $X$ and a vector $\\v+v$, we write\n$\\v+v_{\\vert X}$ to denote the restriction of $v$ to the entries in $X$, i.e.,\n$\\v+v_{\\vert X}(u) = \\v+v(u)$ if $u\\in X$ and \n$\\v+v_{\\vert X}(u) = 0$ if $u\\not\\in X$.  We discuss our technical assumptions\nafter the theorem.\n\n\\begin{theorem}\n\\label{thm:relationship}\n\tLet $\\m+A\\in\\{\\MasIdx{\\DisIdx{}}, \\MasIdx{\\PolIdx{}}\\}$.\n\tLet $c \\in [0,1)$ and $\\epsvec \\in [-c,1-c]^n$ be such that\n\t$\\begop_0 = c\\v+1 + \\epsvec$.\n\tLet $\\gamma_1,\\gamma_2, \\gamma_3 \\in (0,1)$ be parameters.  Furthermore, assume\n\tthat for all sets $X\\subseteq V$ with $\\abs{X}=k$ it holds that:\n\t\\begin{enumerate}\n\t\t\\item[{\\bf 1.}] $(\\begop_X - \\begop_0)^\\intercal \\m+A \\begop_0 \n\t\t\t\t\\geq - \\gamma_1 \\begop_0^\\intercal \\m+A \\begop_0$,\n\t\t\\item[{\\bf 2.}] $\\epsvec_{\\vert X}^\\intercal \\m+A \\epsvec_{\\vert X} \n\t\t\t\t\\leq \\gamma_2 \\begop_0^\\intercal \\m+A \\begop_0$, and \n\t\t\\item[{\\bf 3.}] $\\abs{\\epsvec_{\\vert X}^\\intercal \\m+A \\v+1_{\\vert X}}\n\t\t\t\t\\leq \\gamma_3 \\begop_0^\\intercal \\m+A \\begop_0$.\n\t\\end{enumerate}\n\tSuppose we have access to a $\\beta$-approximation algorithm for\n\tProblem~\\eqref{eq:our-problem} with limited information.\n\tThen we can compute a solution for Problem~\\eqref{problem:max-disagreement}\n\twith full information with approximation ratio\n\t$\\frac{1}{4} \\min\\{\\beta,\n\t\t   \\frac{ 1-2\\gamma_1-2(1-c)\\gamma_3 }{ 1+2(1-c)\\gamma_3+\\gamma_2 } \\}$,\n\teven if we only have access to the graph topology (but \\emph{not} the user\n\topinions).\n\\end{theorem}\n\nOne may think of $c$ as the average\ninnate opinion and $\\epsvec$ as the vector that indicates how much each innate\nopinion deviates from $c$.  Indeed, for topics that initially have little\ndiscord, one may assume that most entries in $\\epsvec$ are small.\n\nThe intuitive interpretation of the technical conditions from the theorem is as\nfollows.\nCondition~(1) corresponds to the assumption that no matter which\n$k$~users the adversary radicalizes, the discord will not \\emph{drop} by\nmore than a $\\gamma_1$-fraction. This rules out some unrealistic scenarios in\nwhich, for example, all but $k$~users have initial innate opinion~$1$ and one\ncould subsequently remove the entire discord by radicalizing the remaining \n$k$ users. \nConditions~(2) and~(3) are\nof similar nature and essentially state that if only $k$~users have the opinions\ngiven by~$\\epsvec$ and all other users have opinion~$0$, then the discord\nin the network is significantly smaller than the initial discord when all\n$n$~users have the opinions in $\\begop_0$.\n\nNote that when $k\\ll n$, it is reasonable to assume that\n$\\gamma_1$, $\\gamma_2$, $\\gamma_3$ are upper-bounded by a small constant, \nsay $\\frac{1}{5}$. In this case the theorem states\nthat if we have a $\\beta$-approximation algorithm for the setting with\nlimited information then we obtain an $\\bigO(\\beta)$-approx\\-i\\-ma\\-tion algorithm\nfor the setting with full information, even though we only use the network\ntopology but not the innate opinions. \n\n\n"
                },
                "subsection 3.2": {
                    "name": "$\\alpha$-Balanced MaxCut",
                    "content": "\n\\label{sec:balanced-maxcut}\n\nIn this section, we study the {$\\alpha$-{\\sc Balanced-Max\\-Cut}} problem for\nwhich we present a constant-factor approximation algorithm. \nThis algorithm allows us to solve Problem~\\eqref{eq:our-problem} (maximizing\n\t\tdiscord with limited information) approximately. Combined with\nTheorem~\\ref{thm:relationship} above, this implies that (under some assumptions)\nadversaries with limited information only perform a constant factor worse than\nthose with full information (see Corollary~\\ref{cor:relationship} below).\n\nIn the $\\alpha$-{\\sc Balanced-Max\\-Cut} problem, the goal is to\npartition a set of nodes into two sides such that one side contains an\n$\\alpha$-fraction of the nodes and the cut is maximized. Formally, we are given\na positive semi\\-definite matrix $\\m+A\\in\\mathbb{R}^{n\\times n}$ and a parameter\n$\\alpha\\in[0,1]$.\nThe goal is to solve the following problem:\n\\begin{equation}\n\\label{problem:maxcut-unbalanced}\n    \\begin{aligned}\n        \\max_{\\v+x} \\quad \\frac{1}{4} & \\v+x^{\\intercal} \\m+A \\v+x ,\\\\\n        \\st \\quad & \\lVert \\v+x + \\v+1 \\rVert_0 = \\alpha n, \\text{ and} \\\\\n\t\t\t\t  & \\v+x \\in [-1,1]^n.\n    \\end{aligned}\n\\end{equation}\n\nNote that the optimal solution vector $\\v+x$ takes values in $\\{-1,1\\}$ (since\nthe objective function is convex, as $\\m+A$ is positive semi\\-definite) and thus\nit partitions the set~$V$ into two sets $S = \\{ u \\colon \\v+x_u = 1 \\}$ and\n$\\bar{S} = \\{ u \\colon \\v+x_u = -1 \\}$.  The first constraint ensures $\\abs{S}=\n\\alpha n$ and $\\abs{\\bar{S}}= (1-\\alpha)n$, i.e., one side contains an\n$\\alpha$-fraction of the nodes and the other side contains a\n$(1-\\alpha)$-fraction.  If $\\m+A$ is the Laplacian of a graph and\n$\\alpha=\\frac{1}{2}$, this is the classic {\\sc Max\\-Bi\\-section}\nproblem~\\cite{frieze1997improved}.  Hence, we will sometimes refer to $\\v+x$ and\nthe corresponding partition $(S,\\bar{S})$ as a \\emph{cut} and to\n$\\v+x^{\\intercal} \\m+A \\v+x$ as the \\emph{cut value}.  \n\nOur main result for Problem~\\eqref{problem:maxcut-unbalanced} is as follows.\n\\begin{theorem}\n\\label{thm:unbalanced}\n\tSuppose $\\alpha\\in[0,1]$ is a constant and $\\m+A$ is a symmetric,\n\tpositive semi\\-definite matrix with $\\ind^{\\intercal} \\m+A = \\m+0^{\\intercal}$.\n\tThen for any $0< \\epsilon <1$, there exists a randomized polynomial time algorithm \n\tthat with probability at least $1-\\epsilon$, outputs a solution for the\n\t$\\alpha$-{\\sc Balanced-Max\\-Cut} (Problem~\\eqref{problem:maxcut-unbalanced}) with \n\tapproximation ratio presented in Figure~\\ref{fig:approx-all}.\n\tThe algorithm runs in time $O(1/\\epsilon \\log(1/\\epsilon)poly(n))$.\n\\end{theorem}\n\n\n\nIn Figure~\\ref{fig:approx-all} we visualize the approximation ratios for\ndifferent values of~$\\alpha$. In particular, observe that for any constant\n$\\alpha\\in(0,1]$, our approximation ratio is $\\Omega(1)$ and that for most values\nof $\\alpha$ it performs within at a least a factor of~$10$ of the optimal\nsolution. Furthermore, for $\\alpha$ close to $0.5$, our approximation is better\nthan $\\frac{1}{3}$.\n\nBefore we discuss Theorem~\\ref{thm:unbalanced} in more detail, we first present\ntwo corollaries.  First, we observe that the theorem implies that we obtain a\nconstant-factor approximation algorithm for maximizing the discord with limited\ninformation (Problem~\\eqref{eq:our-problem}).\n\\begin{corollary}\n\\label{cor:limited-information}\n\tLet $k = \\alpha n$. If $\\alpha\\in[0,1]$ is a constant, there exists an $\\bigO(1)$-approximation\n\talgorithm for Problem~\\eqref{eq:our-problem} with limited information that\n\truns in polynomial time.\n\\end{corollary}\n\\begin{proof}\n\tObserve that a solution for Problem~\\eqref{problem:maxcut-unbalanced}\n\timplies a solution for Problem~\\eqref{eq:our-problem} as follows:\n\tSuppose in Problem~\\eqref{problem:maxcut-unbalanced} we set\n\t$\\m+A=4\\MasIdx{\\DisIdx{}}$ or $\\m+A=4\\MasIdx{\\PolIdx{}}$ and $\\alpha =\n\t\\frac{k}{n}$ to obtain a solution $\\v+x$. Now we define a solution $\\begop$\n\tfor Problem~\\eqref{problem:maxcut-unbalanced} by setting $\\begop(u)=1$ if\n\t$\\v+x(u)=1$ and $\\begop(u)=0$ if $\\v+x(u)=-1$.  Observe that\n\t$\\lVert\\begop\\rVert_0=k$ as desired. Since the last step can be viewed as\n\trescaling opinions from $[-1,1]$ to $[0,1]$, the objective function values\n\tof $\\v+x^\\intercal \\m+A \\v+x$ and $\\begop^\\intercal\\m+A\\begop$ only\n\tdiffer by a factor of~$4$ (see Appendix~\\ref{sec:scaling}).\n\\end{proof}\n\nSecond, observe that by combining Theorems~\\ref{thm:relationship} and\nCorollary~\\ref{cor:limited-information}, we immediately obtain the following\nresult for solving the setting with full information, even when we only have\naccess to the network topology.\n\\begin{corollary}\n\\label{cor:relationship}\n\tSuppose $\\alpha\\in[0,1]$ is a constant and the conditions of\n\tTheorem~\\ref{thm:relationship} hold with $\\gamma_1,\\gamma_2,\\gamma_3\\leq\n\t\\frac{1}{5}$.  Then there exists a polynomial time algorithm for\n\tProblem~\\eqref{problem:max-disagreement} with full information that has\n\tapproximation ratio~$\\bigO(1)$ and \\emph{only uses the graph topology}\n\t(but \\emph{not} the user opinions).\n\\end{corollary}\n\nNext, let us discuss Theorem~\\ref{thm:unbalanced} in more detail.\n\n\\smallskip\nThe theorem \\emph{generalizes} previous results and its\napproximation ratios are only a small constant factor worse than classic\nresults~\\cite{goemans1995improved,frieze1997improved,han02improved}. In\nparticular, the previous results assumed that $\\m+A$ is the Laplacian of a graph\nwith positive edge weights, and thus $\\m+A$ has the structure that all\noff-diagonal entries are non\\-positive. In contrast, in our result we do require\nthe latter assumption and allow for positive off-diagonal\nentries, which appear, for instance, in graphs with negative edge weights.\nIndeed, this is the case for the matrices~$\\MasIdx{\\DisIdx{}}$ and\n$\\MasIdx{\\PolIdx{}}$ from Section~\\ref{sec:preliminaries}, which may have\npositive off-diagonal entries.  Therefore, our generalized theorem is necessary\nto maximize the discord in Problem~\\eqref{eq:our-problem}.\n\nFurthermore, we note that to apply Theorem~\\ref{thm:unbalanced} on graphs with\nboth positive \\emph{and negative} edge weights, we have to assume that their\nLaplacian is positive semi\\-definite. This assumption cannot be dropped, as\npointed out by Williamson and Shmoys~\\cite[Section~6.3]{williamson2011design}.\nThis is crucial since, while for graphs with positive edge weights the Laplacian\nis always positive semi\\-definite, this is not generally true for graphs with\nnegative edge weights.\\footnote{\n\tWe note that this property of graphs with positive and negative edges also\n\trules out simple algorithms of the type: ``Randomly color the graph, pick\n\tthe color for which the induced subgraph has the highest edge weights and\n\tthen solve unconstrained MaxCut in this subgraph.'' The issue here is that\n\tthe Laplacian of such a randomly picked subgraph is not necessarily positive\n\tsemidefinite (even if the Laplacian of the original graph is positive\n\tsemidefinite). Thus such simple tricks cannot be applied here and we need\n\tother solutions.\n} However, this assumption holds in our use cases due to\nLemma~\\ref{lem:matrices}.\n\nIn the theorem we require $\\alpha\\in[0,1]$ to be a constant and thus\n$k=\\Omega(n)$. While this is somewhat undesirable, there are underlying\ntechnical reasons for it: the SDP-based approach by Frieze and\nJerrum~\\cite{frieze1997improved} also has this requirement; LP-based algorithms\nwhich work for $k=o(n)$ (as shown, for instance, by\n\\citet{ageev1999approximation}) do not generalize to the setting in which the\nmatrices are not graph Laplacians; the same is the case for the SDP-based\napproach by \\citet{feige2001approximation}.\n\n\\sbpara{Algorithm.} Our algorithm is based on solving the SDP relaxation of\nProblem~\\eqref{problem:maxcut-unbalanced} and applying random hyperplane\nrounding~\\cite{goemans1995improved}, followed by a greedy step in which we adjust\nthe sizes of the sets $S$ and $\\bar{S}$. Later, we will see that our main\ntechnical challenge will be to prove that the greedy adjustment step still works\nin our more general setting.\n\nTo obtain our SDP relaxation of Problem~\\eqref{problem:maxcut-unbalanced},\nwe observe that by the convexity of the objective function we can assume that\n$\\begop\\in\\{-1,1\\}^n$ (see Appendix~\\ref{sec:convexity}) \nand thus, we can rewrite the constraint\n$\\lVert \\v+x + \\v+1 \\rVert_0 = \\alpha n$ \n~as~ $2\\sum_{i<j}^n \\v+x_i \\v+x_j = n^2(1 - 2\\alpha)^2 - n$.\n\n\nNow the semi\\-definite relaxation of Problem~\\eqref{problem:maxcut-unbalanced}\nbecomes:\n\\begin{align}\n\\begin{aligned}\n\\label{problem:maxcut-unbalaced-relax}\n\t\\max_{\\v+v_1,\\dots,\\v+v_n} \\quad & \\frac{1}{4} \\sum_{ij} \\m+A_{ij}\\v+v_i^{\\intercal} \\v+v_j,\\\\\n\t\\st \\quad & \\sum_{i<j} \\v+v_i^{\\intercal} \\v+v_j = \\frac{1}{2}n^2(1 - 2 \\alpha)^2-\\frac{n}{2}, \\text{ and} \\\\\n\t& \\v+v_i \\in \\Real^n, \\quad\\quad \\|\\v+v_i\\|_2 = 1.\n\\end{aligned}\n\\end{align}  \n\n\n{\\SetAlgoNoLine\n \\LinesNumbered\n \\DontPrintSemicolon\n \\SetAlgoNoEnd\n\\begin{algorithm2e}[t]\nSolve the SDP in Equation~\\eqref{problem:maxcut-unbalaced-relax}\n\t\twith solution $\\v+v_1,\\dots,\\v+v_n$\\;\n\\For{$\\kappa = \\bigO(1/\\varepsilon \\lg(1/\\varepsilon))$ times}{\n\t Sample vector $\\v+r$ with each entry $\\sim {\\mathcal N}(0,1)$\\;\n\t Set $S = \\{ i : \\langle \\v+v_i, \\v+r \\rangle \\geq 0 \\}$ and $\\bar{S}=V\\setminus S$\\;\n\tSet %\n\t\t\t$\\bar{\\v+x}_i=1$ if $i\\in S$ and \n\t\t\t$\\bar{\\v+x}_i=-1$ if $i\\in \\bar{S}$\\;\n\t\\If{$\\abs{S}>\\alpha n$}\n\t\t{greedily move elements from $S$ to $\\bar{S}$ until $\\abs{S} = \\alpha n$}\n\t\\If{$\\abs{S}<\\alpha n$}\n\t\t{greedily move elements from $\\bar{S}$ to $S$ until $\\abs{S} = \\alpha n$}\n\t\\tcp*{\\small In each step, move the element that\n\t\t\tdecreases the value of the objective function the least}\n}\n\\Return{best solution over all trials}\\;\n\\caption{SDP-based relaxation followed by iterative local improvement}\n\\label{alg:SDP-based}\n\\end{algorithm2e}\n}\n\n\nOur approach for solving Problem~\\eqref{problem:maxcut-unbalanced}\nis shown as Algorithm~\\ref{alg:SDP-based}.\nFor simplicity, we assume that $\\alpha\\leq \\frac{1}{2}$; \nfor $\\alpha>\\frac{1}{2}$ we can run the algorithm with $\\alpha'=1-\\alpha\\leq \\frac{1}{2}$\nand obtain the desirable result.\n\n\n\n\\sbpara{Analysis.}\nOur analysis has two parts. The first part is the\nhyperplane rounding of the SDP solution; it follows the techniques of\nGoemans and Williamson~\\cite{goemans1995improved} and \nFrieze and Jerrum~\\cite{frieze1997improved}. \nThe next lemma summarizes the first part of the analysis.\n\\begin{lemma}[\\cite{goemans1995improved,williamson2011design,frieze1997improved}]%\n\\label{lem:standard}\n\tThe expected cut of $(S, \\bar{S})$ is at least\n\t$\\frac{2}{\\pi}\\, \\OPT$ and \n\t$\\Exp[\\abs{S}\\abs{\\bar{S}}] \\geq 0.878 \\cdot \\alpha(1-\\alpha)n^2$, \n\twhere $\\OPT$ is the optimal solution for \n\t$\\alpha$-{\\sc Balanced-Max\\-Cut}.\n\\end{lemma}\n\nThe second part of the analysis is novel and considers the greedy procedure that\nensures that $S$ contains~$k$ elements. \nWhen $\\m+A$ is the Laplacian of a graph with \\emph{non\\-negative} edge weights, \nan averaging argument (see, e.g., ~\\cite[A.3.2]{arora2009computational}) implies\nthat there exists $u\\in S$ such that we can move $u$ from $S$ to $\\bar{S}$ and\nthe cut value drops by a factor of at most $1/\\abs{S}$.  \nHowever, for more general matrices $\\m+A$ this may not hold, \ne.g., when $\\m+A$ is the Laplacian of a \\emph{signed graph} with negative edge\nweights or when $\\m+A$ is the matrix that corresponds to the disagreement index,\n\t\tas in Problem~\\eqref{eq:our-problem}.\nWe also illustrate this in Appendix~\\ref{sec:illustration-worst-case}. \nHowever, we show that in our setting there always\nexists a node in~$S$ such that if we move $u$ from $S$ to $\\bar{S}$ \nthen the cut value drops by a factor of at most $2/\\abs{S}$.\n\n\\begin{lemma}\n    \\label{lemma:boundSUmodify}\n\tSuppose that $\\m+A$ is a symmetric, positive semi\\-definite matrix with\n\t$\\v+1^{\\intercal} \\m+A = \\v+0^{\\intercal}$ and let $\\vx\\in\\{-1,1\\}^n$.\n    Set $M = \\frac{1}{4}\\vx^{\\intercal} \\m+A \\vx$ and\n\t$S = \\{i \\in \\{1, \\ldots, n\\} \\mid \\vx_i = 1\\}$.\n    Then there exists $i \\in S$ such that, by modifying $\\x_i$ to be $-1$, \n    $M$ decreases at most $\\frac{2M}{\\abs{S}}$.\n\\end{lemma}\n\\begin{proof}\n    We prove the lemma by contradiction. \n    Suppose there does not exist such $i \\in S$ and\n    let $\\v+e_i \\in \\Real^n$ denote the vector whose $i$-th entry is $1$ and all other entries are $0$s.   \n    Then for any $i$, it holds \n    \\begin{align*}\n\t\tM - \\frac{1}{4}(\\vx - 2\\v+e_i)^{\\intercal} \\m+A (\\vx - 2\\v+e_i) > \\frac{2M}{\\abs{S}}.\n\t\\end{align*}\n    Expanding and simplifying the formula, we get\n\t$$\\v+e_i^{\\intercal} \\m+A \\vx > \\frac{2M}{\\abs{S}} + \\v+e_i^\\intercal \\m+A \\v+e_i,$$ \n\tfor all $i\\in S$.\n\tSumming this inequality over all $i\\in S$, we obtain\n    \\begin{align}\n\t\\label{eq:boundSUmodify-1}\n\t\t %\n\t\t \\sum_{i\\in S} \\v+e_i^{\\intercal} \\m+A \\vx > 2M + \\sum_{i\\in S} \\v+e_i^\\intercal \\m+A \\v+e_i.\n\t\\end{align}\n\n    By $\\ind^{\\intercal} \\m+A = \\m+0^{\\intercal}$, and $\\sum_{i\\in S} \\v+e_i + \\sum_{i \\in U} \\v+e_i = \\ind$, \n    we get\n    \\begin{equation*}\n        \\sum_{i\\in S} \\v+e_i^{\\intercal} \\m+A \\vx + \\sum_{i\\in \\bar{S}} \\v+e_i^{\\intercal} \\m+A \\vx\n\t\t= \\ind^{\\intercal} \\m+A \\vx \n\t\t= 0. \n    \\end{equation*}\n\n\tThus,\n\t\\begin{align}\n        \\label{eq:boundSUmodify-2}\n\t\t\\sum_{i\\in S} \\v+e_i^{\\intercal} \\m+A \\vx = - \\sum_{i\\in \\bar{S}} \\v+e_i^{\\intercal} \\m+A \\vx.\n\t\\end{align}\n\n    Using $\\sum_{i \\in S} \\v+e_i - \\sum_{i \\in \\bar{S}} \\v+e_i = \\vx$ and Equation~\\eqref{eq:boundSUmodify-2},\n    we get \n    \\begin{align}\n\t\\label{eq:boundSUmodify-3}\n        \\vx^{\\intercal} \\m+A \\vx \n\t\t= \\left(\\sum_{i \\in S} \\v+e_i - \\sum_{i \\in \\bar{S}} \\v+e_i\\right)^{\\intercal} \\m+A \\vx\n\t\t= 2\\sum_{i\\in S} \\v+e_i^{\\intercal} \\m+A \\vx.\n    \\end{align}\n\n\tRecalling that $M = \\frac{1}{4} \\vx^{\\intercal} \\m+A \\vx$ and\n\tusing Equations~\\eqref{eq:boundSUmodify-1} and~\\eqref{eq:boundSUmodify-3},\n    \\begin{align*}\n\t\t \\sum_{i\\in S} \\v+e_i^{\\intercal} \\m+A \\vx \n\t\t > \\sum_{i\\in S} \\v+e_i^{\\intercal} \\m+A \\vx + \\sum_{i\\in S} \\v+e_i^\\intercal \\m+A \\v+e_i.\n\t\\end{align*}\n\n\tThus,\n\t\\begin{align*}\n\t\t 0 > \\sum_{i \\in S} \\v+e_i^{\\intercal} \\m+A \\v+e_i.  \n    \\end{align*}\n\n    However, since $\\m+A$ is positive semi\\-definite we must have that\n\t$\\v+e_i^{\\intercal} \\m+A \\v+e_i \\geq 0$ for all $i$ and thus \n\t$\\sum_{i \\in S} \\v+e_i^{\\intercal} \\m+A \\v+e_i \\geq 0$. This yields our\n\tdesired contradiction.\n\\end{proof}\n\nNext, let us consider how our approximation behaves when we apply \nLemma~\\ref{lemma:boundSUmodify} multiple times in a row.\nHere, the issue is that we may need to apply the lemma more than\n$\\frac{\\abs{S}}{2}$ times in a row and then a na\\\"{i}ve analysis would yield a cut\nvalue of less than $M - \\frac{\\abs{S}}{2} \\cdot \\frac{2M}{\\abs{S}} = 0$, i.e.,\nwe would not be able to obtain our desired approximation result. However, this\nanalysis is too pessimistic because it assumes that after each application of\nthe lemma, the cut decreases by a $\\frac{2}{\\abs{S}}$-frac\\-tion with respect to the\n\\emph{initial} cut. Therefore, the following lemma presents a more refined\nanalysis, which takes into account that during each application of\nLemma~\\ref{lemma:boundSUmodify}, the cut only decreases by a\n$\\frac{2}{\\abs{S}}$-fraction with respect to the \\emph{previous} cut.\nA similar idea was used by Srivastav and Wolf~\\cite[Lemma 1]{srivastav1998finding}\nto solve the \\emph{densest $k$-sub\\-graph} problem. \n\nIntuitively, in the lemma $(S,\\bar{S})$ corresponds to the cut we obtain from\nthe hyperplane rounding and $(T,\\bar{T})$ corresponds to the $\\alpha$-balanced\nsolution that we wish to return.\n\\begin{lemma}\n    \\label{lemma:cut-value-after-moving}\n\tSuppose that $\\m+A$ is a symmetric, positive semi\\-definite matrix with\n\t$\\v+1^{\\intercal} \\m+A = \\v+0^{\\intercal}$ and let $\\vx\\in\\{-1,1\\}^n$.\n    Let $M_0 = \\frac{1}{4}\\vx^{\\intercal} \\m+A \\vx$,\n\tlet $(S_0,\\bar{S}_0)$ denote the cut induced by $\\vx$ and assume that\n\t$\\abs{S_0}\\leq n/2$.\n\tFurthermore, let $s,t\\in (0,\\frac{1}{2}]$ be such that $\\abs{S_0} = sn$\n\tand $tn$ is an integer. \n\tThen there exists a set of nodes $T$ of size $\\abs{T}=tn$ such that the\n\tcut $(T,\\bar{T})$ has value at least \n\t$\\frac{(1-t)^2 -7(1-t)/n + 12/n^2}{(1-s)^2  + (1-s)/n} M_0$,\n\tif $t>s$, and value at least \n\t$\\frac{t^2 - t/n}{s^2 - s/n} M_0$,\n\tif $t<s$.\n\tFurthermore, $T$ can be found by repeatedly applying\n\tLemma~\\ref{lemma:boundSUmodify}.\n\\end{lemma}\n\nThe proof of Theorem~\\ref{thm:unbalanced} follows from applying\nLemma~\\ref{lemma:cut-value-after-moving}, where we set $t=\\alpha$ and\nadditionally we set $S_0$ to the set $S$ from Lemma~\\ref{lem:standard} which\ninitially has cut value at least $\\frac{2}{\\pi}\\OPT$. Then a case distinction\nfor $\\abs{S}>tn$ and $\\abs{S}<tn$ yields the theorem.\n\n\n\n"
                },
                "subsection 3.3": {
                    "name": "Greedy heuristics",
                    "content": "\n\\label{sec:greedy}\n\nNext, we discuss two greedy heuristics, \nwhich can be applied in two different ways. \nFirst they can be used to solve \nProblem~\\eqref{problem:max-disagreement}\nin the model with full information, \ni.e., when the graph topology and the innate opinions of all users are available.\n\nSecond, by setting $\\begop_0=\\v+0$,\nthese greedy heuristics can be used to solve Problem~\\eqref{eq:our-problem}, \nand thus, be used as subroutines for the first step \nof our approach in the model with limited information. \nIn other words, they can be used to substitute the SDP-based algorithm\nthat we presented in the previous section. \nThis is particularly useful, since solving an SDP is not scalable for large graphs, \nwhile the greedy methods are significantly more efficient.\n\n\\sbpara{Adaptive greedy}~(\\cite{chen2020network}) initializes\n$\\begop = \\begop_0$ and performs $k$ iterations. In each iteration, for all\nindices $u$ it computes how the objective function changes when setting\n$\\begop_u = 1$. Then it picks the index $u$ that increases\nthe objective function the most.\n\n\\sbpara{Non-adaptive greedy} works similarly. In a first step, it initializes\n$\\begop = \\begop_0$ and computes for all indices $u$ the score that indicates \nhow the objective function changes when setting $\\begop_u = 1$. Then it orders the indices\n$u_1,\\dots,u_n$ such that the score is non-increasing. Now it iterates over\n$i=1,\\dots,n$ and for each $i$, it sets $\\begop_{u_i} = 1$ if this increases the\nobjective function; otherwise it proceeds with $i+1$. \nThe non-adaptive greedy algorithm stops after it has changed $k$~entries.\n\n"
                },
                "subsection 3.4": {
                    "name": "Computational hardness",
                    "content": "\n\\label{sec:hardness}\n\nChen and Racz~\\cite{chen2020network} left it as an open problem to prove that\nmaximizing the disagreement of the expressed opinions is \\NPhard; they studied a\nversion of Problem~\\ref{problem:max-disagreement} in which they had an\ninequality constraint $\\lVert \\begop - \\begop_0 \\rVert \\leq k$ rather than the\nequality constraint we study and in which the adversary could pick a solution\nvector~$\\begop\\in[0,1]^n$. We show that this problem, as well as\nProblems~\\eqref{problem:max-disagreement} and~\\eqref{eq:our-problem} are \\NPhard.\nIn addition, in Corollary~\\ref{cor:our-problem-np-hard}, we show that these two \nproblems are \\NPhard even when $k = \\Omega(n)$, \nwhich implies that Problem~\\eqref{problem:maxcut-unbalanced} is also \\NPhard when $\\alpha$ \nis constant. \n\\begin{theorem}\n\\label{thm:disagreement-np-hard}\n\tProblem~\\eqref{problem:max-disagreement} is \\NPhard for\n\t$\\m+A=\\MasIdx{\\DisIdx{}}$, even for $\\begop_0 = \\v+0$. The problem by Chen\n\tand Racz~\\cite{chen2020network} is \\NPhard, even when $\\begop_0=\\v+0$ and\n\t$k=n$.  \n\tProblem~\\eqref{eq:our-problem} with $\\m+A=\\MasIdx{\\DisIdx{}}$ is also \\NPhard.\n\\end{theorem}\n\n\\begin{corollary}\n\t\\label{cor:our-problem-np-hard}\n\tProblem~\\eqref{eq:our-problem} with $\\m+A = \\MasIdx{\\DisIdx{}}$ and \n\t$k \\in \\Omega(n)$ is \\NPhard. \n\\end{corollary}\n\n\n\n\n"
                }
            },
            "section 4": {
                "name": "Experimental evaluation",
                "content": "\n\\label{sec:experiments}\n\nWe empirically evaluate the methods we propose.\nDue to lack of space, we only present here our results for maximizing the\ndisagreement. We defer our results for maximizing the polarization to \nAppendix~\\ref{sec:experiments-polarization}.\n\nOur objective is to answer the following research questions:\n\n\\begin{description}\n\t\\item[RQ1:] Does the SDP-based method outperform the greedy methods?\n\t\\item[RQ2:] Is there a big gap between the settings with full information and with\n\t\tlimited information?\n\t\\item[RQ3:] Which dataset parameters determine the gap between full and limited\n\t\tinformation?\n\t\\item[RQ4:] How does our approach scale with respect to~$k$? \n\\end{description}\n\nOur implementations are available in on GitHub.\\footnote{\\, \\url{https://github.com/SijingTu/KDD-23-Adversaries-With-Limited-Information}}\n\n\n\\sbpara{Algorithms.}\nIn our experiments, we consider several algorithms that work with full\ninformation and limited information. \n\nFirst, our algorithms with \\emph{full} information are as follows. \nWe use the two greedy algorithms described in Section~\\ref{sec:greedy};\nwe refer to the adaptive greedy as \\AGFull\\ and \nthe non-adaptive greedy as \\NAGFull. \nWe use the suffix~\\FullInfo\nto indicate that they use full information.\nFor \\AGFull we adapt the implementation of Chen and Racz~\\cite{chen2020network}.\\footnote{\n\t\\label{foot:chen-code}\n\t\\url{https://github.com/mayeechen/network-disruption}\n}\t\n\nSecond, we use the suffix \\LimitedInfo to refer to our methods with\n\\emph{limited} information, which only know the network structure\n(see Section~\\ref{sec:algorithms}).\nFor picking the seed nodes, we consider the following algorithms:\n\\AG\\ is the adaptive greedy algorithm with $\\begop_0=\\v+0$, \n\\NAG\\ is the non-adaptive greedy algorithm with $\\begop_0=\\v+0$, and\n\\SDPalgo\\ is the SDP-based algorithm from Theorem~\\ref{thm:unbalanced}.\n\\IM\\ finds the seed nodes by solving the influence-maximization\nproblem~\\cite{kempe2015maximizing} and our implementation is based on the\nMartingale approach, i.e., IMM, proposed by Tang et\nal.~\\cite{tang2015influence}; we set the graph edge weights as in\nthe weighted cascade model~\\cite{kempe2015maximizing}.\n\\Random\\ randomly picks~$k$ nodes,  and \n\\Deg\\ picks the $k$~nodes of the highest degree.  \n\n\n\\sbpara{Datasets.}\nWe present statistics for our smaller datasets in\nTable~\\ref{tab:disagreement-small} and for our larger datasets in\nTable~\\ref{tab:disagreement-large}.  \nFor each of the datasets, we provide the number of vertices and edges. We also\nreport the \\emph{normalized disagreement index}\n$\\DisIdx{G, \\begop}' = \\frac{\\DisIdx{G, \\begop} \\cdot 10^5}{\\abs{E}}$, where we\nnormalize by the number of edges for better comparison across different datasets\nand we multiply with $10^5$ because $\\frac{\\DisIdx{G, \\begop}}{\\abs{E}}$ is\ntypically very small. We also report average innate opinions~$\\bar{\\begop}_0$\nand the standard deviation of the innate opinions~$\\sigma(\\begop_0)$.\n\nWe note that the datasets \\karate, \\books, \\blogs, \\SBM and \\GplusLTWO do not \ncontain ground-truth opinions. \nHowever, the these datasets contain ground-truth communities; thus,\nwe set the nodes' innate opinions by sampling from Gaussian distributions with different\nparameters, based on the community membership.\nMore details for all\ndatasets are presented in Appendix~\\ref{sec:experiments-datasets}.\n\n\n\n\n\\sbpara{Evaluation.} To evaluate our methods, we compare the initial\ndisagreement with the disagreement after the algorithms changed the innate\nopinions. More concretely, let $\\begop_0$ denote the initial innate opinions and\nlet $\\begop$ denote the output of an algorithm. We report the score\n$\\frac{\\begop^{\\intercal} \\m+A \\begop - \\begop_0^{\\intercal} \\m+A \\begop_0}{\\begop_0^{\\intercal} \\m+A \\begop_0}$,\nwhere $\\m+A$ is one of the matrices~$\\MasIdx{\\DisIdx{}}$ or $\\MasIdx{\\PolIdx{}}$\nfrom Section~\\ref{sec:preliminaries}.\nFor example, if $\\m+A=\\MasIdx{\\DisIdx{}}$ then we measure the\nrelative increase in disagreement compared to the initial setting.\n\n\n\\sbpara{Maximizing disagreement on small datasets.}\nWe start by studying the performance of our methods for maximizing\ndisagreement. We present the results on small datasets in\nTable~\\ref{tab:disagreement-small}, where $k=10\\%\\,n$. \nWe consider these small datasets\nas they allow us to evaluate our \\SDPalgo-based algorithm, \nwhich does not scale to larger~graphs. \n\n\nOur results in Table~\\ref{tab:disagreement-small} show that for all datasets, our\nlimited-information algorithms, i.e., \\SDPalgo\\LimitedInfo, \\NAG\\LimitedInfo,\nand \\AG\\LimitedInfo, perform surprisingly well.  Indeed, on all datasets these\nalgorithms have performance similar to the algorithms using full information.\nSurprisingly, on \\karate, \\books, and \\Twitter, \\SDPalgo\\LimitedInfo outperforms\nthe best algorithms with full information, even though only by very small\nmargins.  \nSince \\SDPalgo\\LimitedInfo is the best method only on the three smallest\ndatasets, we believe that this exceptionally good performance \nis an artifact of the datasets being small.\n\n\nAmong the three algorithms with limited information, \\SDPalgo\\LimitedInfo\nperforms best on all datasets, but the gap to the two greedy algorithms\nis relatively small. This answers {\\bf RQ1}. \n\nWe also observe that the SDP and greedy algorithms with limited information\nachieve better results than the baselines.  \n\n\nNext, we note that for the \\Reddit dataset, the disagreement increases by a factor of more\nthan~48.  A close look at the ground-truth opinions on \\Reddit reveals that the\nstandard deviation of the innate opinions is just~0.042, and the normalized initial \ndisagreement is also among the second smallest. \nThese two factors make the dataset susceptible to increasing\nthe disagreement by a large amount.\n\n\\sbpara{Maximizing disagreement on larger datasets.}\nNext, we consider the larger datasets in Table~\\ref{tab:disagreement-large} with\n$k=1\\%\\,n$. Here, we drop \\SDPalgo\\LimitedInfo due to scalability issues.\n\nFirst, we observe that for the larger datasets, the dataset properties, such as, the normalized \ninitial disagreement, the mean of the innate opinions, and the standard deviations of the \ninnate opinions are similar to those of the smaller datasets.\nDue to these similarities, we expect a similar gap between the full-information algorithms and \nthe limited-information algorithms as in the smaller datasets. \n\n\nSecond, we observe that the methods with full information indeed are just slightly \nbetter than \\NAG\\LimitedInfo and \\AG\\LimitedInfo over all the datasets.  \nThe biggest gap in performance is on~\\TweetSFOUR where the full-information\nmethods are about 40\\% better. Note that both $\\bar{\\begop}_0=0.568$ and\n$\\sigma(\\begop_0)=0.302$ are large for \\TweetSFOUR; this is somewhat\nuncharacteristic for our other datasets, which have either smaller\n$\\bar{\\begop}_0$ or smaller $\\sigma(\\begop_0)$.\nWe also observe that the there is no clear winner between $\\NAG$ and $\\AG$ in\nthe limited information setting, which have very similar performance. In\naddition, the greedy algorithms clearly outperform the baseline algorithms.\nWe present the running time analysis in Appendix~\\ref{sec:experiments-running-time}. \n\n\n\n\n\nSummarizing our results, we can answer {\\bf RQ2}:\nwe find\nthat the setting with limited information is at most a factor of~$1.4$ worse than\nthe setting with full information. \n\n\\sbpara{Relationship of dataset parameters and the gap between full and limited\n\tinformation.}\nTo understand how the dataset parameters influence the performance of our\nalgorithms with limited information, we perform a regression analysis and report\nthe results in Figure~\\ref{fig:regression}. On the $y$-axis, we consider the\nratio between the best of $\\NAG\\LimitedInfo$ and $\\AG\\LimitedInfo$, which only use limited information,\nand the best method with full information.\nObserve that this ratio can be viewed as the gap between having full and having\nlimited information. On the $x$-axis, we plot the dataset parameters\n$\\DisIdx{G, \\begop}'$, $\\bar{\\begop}_0$ and $\\sigma(\\begop_0)$.\n\nFirst, we find that there is a low correlation between the ratio of\nlimited/full-information algorithms and the average innate\nopinions~$\\bar{\\begop}_0$ ($R^2 = 0.17$) and the initial disagreement\n$\\DisIdx{G, \\begop}'$ ($R^2=0.08$) in the datasets.\nSecond, we find that the correlation between the standard deviation of the\ninnate opinions~$\\sigma(\\begop_0)$ is moderately high ($R^2=0.62$). \n\nThese finding align well with the intuition that if~$\\sigma(\\begop_0)$\nis high, an adversary that only knows the graph lacks more information than\nwhen~$\\sigma(\\begop_0)$ is small;\nadditionally, note that if $\\sigma(\\begop_0)$ is small, then the vector\n$\\epsvec$ from Theorem~\\ref{thm:relationship} will have small norm and the second and the third\ncondition of the theorem should be satisfied on our datasets.\nSimilarly, it is intuitive that $\\bar{\\begop}_0$ should not have a large impact\non the adversary's decisions if it is not too high (here we consider datasets\n\t\twith$\\bar{\\begop}_0\\leq 0.61$). However, in preliminary experiments\n(not reported here) we also observed that if$\\bar{\\begop}_0$ is very large\n($\\bar{\\begop}_0 \\geq 0.8$) then the performance of the algorithms becomes much\nworse.\nFurthermore, it might be considered somewhat surprising that the correlation with~$\\DisIdx{G, \\begop}'$ is low, \nsince one might intuitively expect that $\\DisIdx{G, \\begop}'$ and~$\\sigma(\\begop_0)$\nshould be closely related.\nFor this discrepancy, we note that $\\DisIdx{G, \\begop}'$ also involves the network structure. \n\n\nHence, we can answer {\\bf RQ3}:\nwe find that the standard deviation of the initial opinions is the most important for\ndetermining the gap between full and limited information, while the average\ninnate opinions and initial disagreement play no major role.\n\n\n\n\\sbpara{Dependency on~$k$.}\nFor $k=0.5\\%\\,n,1\\%\\,n,\\dots,2.5\\%\\,n$, we present our results on \\TweetLTWO and\n\\GplusLTWO in Figure~\\ref{fig:scale-k}.  The figure indicates that the\ndisagreement grows linearly in~$k$; this behavior was also suggested by the\nupper bounds of Chen and Racz~\\cite{chen2020network} and Gaitonde et\nal.~\\cite{gaitonde2020adversarial} who considered a slightly stronger adversary.\nSimilar to the results in Table~\\ref{tab:disagreement-large}, \\AG\\FullInfo is\nthe best method, followed by \\NAG\\LimitedInfo and \\AG\\LimitedInfo.  The ranking\nof the algorithms is consistent across the different values of~$k$.\nThis answers {\\bf RQ4}.\n\n\n\n\\sbpara{Additional experiments.}\nIn the appendix we present additional experiments.  \nFirst, in Appendix~\\ref{sec:experiments-influential}\nwe evaluate the algorithms for solving Problem~\\eqref{eq:our-problem}.  \nSecond, in Appendix~\\ref{sec:experiments-polarization}\nwe also use our algorithms to maximize the polarization in the network; we remark\nthat all of our results extend to this setting, including the guarantees from\nTheorem~\\ref{thm:unbalanced}.\n\n\n"
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\n\\label{sec:conclusion}\n\nWe have studied how adversaries can sow discord in social networks, even when\nthey only have access to the network topology and cannot assess the opinions of\nthe users. We proposed a framework in which we first detect a small set of users who\nare highly influential on the discord and then we change the opinions of\nthese users. We showed experimentally that our approach can increase the\ndiscord significantly in practice and that it performs within a constant\nfactor to the greedy algorithms that have access to the full information about the\nuser opinions.\n\nOur practical results demonstrate that attackers of social networks are quite\npowerful, even when they can only access the network topology. From an ethical\npoint of view, these findings showcase the power of malicious attackers to \nsow discord and increase polarization in social networks.\nHowever, to draw a final conclusion further study is needed, for\nexample because the assumption that the adversary can radicalize $k$~opinions\n\\emph{arbitrarily much} may be too strong. Nonetheless, the upshot is that \nby understanding attackers with limited information, one may be able to make\nrecommendations to policy makers regarding the data that social-network\nproviders can share with the public.\n\nFurthermore, in this paper we only studied one possible definition for\ndisagreement that is common in the computer science\nliterature~\\cite{chen2020network,gaitonde2020adversarial}. Klofstad et\nal.~\\cite{klofstad2013disagreeing} point out that in the political science\nliterature there are different viewpoints on how disagreement should be defined,\nand that these different definitions will lead to different conclusions, with\ndifferent empirical and democratic consequences.  Understanding the connection\nof our definition and the ones in political science is an interesting question.\nAlso, Edenberg~\\cite{edenberg2021problem} argues that to solve current societal\nproblems like polarization, purely technical solutions, such as social media\nliteracy campaigns and fact checking, are not enough; instead \\emph{``we must find\nways to cultivate mutual respect for our fellow citizens in order to reestablish\ncommon moral ground for political debate.} \nWhile certainly true, such considerations and course of actions \nare out of the scope of our paper.\n\nAs we already mentioned above, in future work it will be interesting to validate\nwhich adversary models are realistic in practice. Theoretically, it is\ninteresting to obtain approximation algorithms for\nProblem~\\eqref{problem:max-disagreement} and the problem by Chen and\nRacz~\\cite{chen2020network}; note that such algorithms must generalize our\nresult from Theorem~\\ref{thm:unbalanced}, as Problem~\\eqref{eq:our-problem} is a\nspecial case of Problem~\\eqref{problem:max-disagreement}.\n\n"
            },
            "section 6": {
                "name": "Acknowledgements",
                "content": "\nWe are grateful to Tianyi Zhou for providing the Twitter datasets with innate\nopinions. We thank Sebastian Lderssen for pointing out a mistake in an earlier\nversion of this paper. This research is supported by the Academy of Finland project MLDB\n(325117), the ERC Advanced Grant REBOUND (834862),\nthe EC H2020 RIA project SoBigData++ (871042),\nand the Wallenberg AI, Autonomous Systems and Software Program (WASP)\nfunded by the Knut and Alice Wallenberg Foundation.\nThe computations were enabled by resources in project \nSNIC 2022/22-631 provided by Uppsala University at UPPMAX.\n\n\\clearpage\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{bibclean}\n\n\\appendix\n\\clearpage \n"
            },
            "section 7": {
                "name": "Omitted Experiments",
                "content": "\n\\label{sec:omit-experiments}\nWe present further details of our experiments. \nIn Section~\\ref{sec:experiments-datasets}, we present details about our\ndatasets.  In Section~\\ref{sec:experiments-polarization}, we present how our\nalgorithms perform when the goal is to maximize the polarization.\nIn Section~\\ref{sec:experiments-influential} we compare different algorithms for\nfinding users that are influential on the disagreement in the network\n(Problem~\\eqref{eq:our-problem}).\nIn Section~\\ref{sec:experiments-stability} we discuss the stability \nof $\\SDPalgo\\LimitedInfo$ and $\\Random\\LimitedInfo$, which use randomization.\nIn Section~\\ref{sec:experiments-running-time} we present the running time of\nour algorithms.\n\nOur algorithms are implemented in Python, except IMM~\\cite{tang2015influence}\n(related to our \\IM\\LimitedInfo algorithm) which is implemented in Julia.  We\nuse Mosek to solve semidefinite programs.  \n\n",
                "subsection 7.1": {
                    "name": "Datasets",
                    "content": "\n\\label{sec:experiments-datasets}\nThe datasets \\TweetSTWO, \\TweetSFOUR, \\TweetMFIVE, \\TweetLTWO are sampled from\na Twitter dataset with innate opinions, which we obtained from Tianyi Zhou.\nThe original Twitter dataset is collected in the following way.\nWe start from a list of Twitter accounts who actively engage in political discussions in the US, \nwhich was compiled by Garimella and Weber~\\cite{garimella2017long}.\nThen we randomly sample a smaller subset of 50\\,000 accounts. For these active\naccounts, we obtained the entire list of followers, except for users with more\nthan 100\\,000 followers for whom we got only the 100\\,000 most recent\nfollowers (users with more than 100\\,000 followers account for less than 2\\% of\nour dataset). Based on this obtained information, we construct a graph in which the nodes\ncorrespond to Twitter accounts and the edges correspond to the accounts'\nfollowing relationships. Then we consider only the largest connected component\nin the network. To obtain the innate opinions of the nodes in the graphs, we proceed as follows.\nFirst, we compute the political polarity score for each account using the method\nproposed by Barber\\'a~\\citep{barbera2015birds}, which has been used widely in\nthe literature~\\cite{brady2017emotion,boutyline2017social}.  The polarity scores\nrange from -2 to 2 and are computed based on following known political accounts. \nThen we re-scale them into the interval $[0,1]$.  \nTo create our smaller datasets, we select a seed node uniformly at random, and\nrun breadth-first search (BFS) from this seed node, until a given number of nodes\nhave been explored.\n\nWe note that for \\TweetMFIVE and \\TweetLTWO, the innate opinions were very large\n($\\bar{\\begop}_0 \\geq 0.85$) and thus for these datasets we flipped the innate\nopinions around~0.5 (i.e., we set $\\begop_0 = \\v+1 - \\begop_0$).  In other\nwords, we assume that initially most people are not on the extreme side of the\nopinion spectrum.  By flipping the innate opinions, we guarantee that the\nattacker can still radicalize the opinions.  We note that this has no influence\non the the initial indices for polarization and disagreement (since for \n$\\m+A\\in\\{\\MasIdx{\\DisIdx{}}, \\MasIdx{\\PolIdx{}}\\}$ it holds that\n$\\begop_0^\\intercal \\m+A \\begop_0 = (\\v+1 - \\begop_0)^\\intercal \\m+A (\\v+1 - \\begop_0)$\nwhich is implied by Lemma~\\ref{lem:matrices}).\n\nThe dataset \\GplusLTWO is sampled from the ego-Gplus dataset obtained from\nSNAP~\\cite{snapnets} using the same BFS-approach as above.  The innate opinions\nfor \\GplusLTWO are drawn independently from $N(0.3, 0.1)$.  Here,\n$N(\\mu,\\sigma)$ denotes the Gaussian distribution with mean~$\\mu$ and standard\ndeviation~$\\sigma$.\n\n\nWe also use the public\\cref{foot:chen-code} datasets \\Twitter and \\Reddit from De et\nal.~\\cite{de2019learning}, which have previously been used by Musco et\nal.~\\cite{musco2018minimizing} and Chen and Racz~\\cite{chen2020network}.  The\n\\Twitter dataset was obtained from tweets about the Delhi legislative assembly\nelections of 2013 and contains ground-truth opinions. The opinions for the\n\\Reddit dataset were generated by Musco et al.~\\cite{musco2018minimizing} using a power law\ndistribution.\n\nFurthermore, we consider the datasets \\karate, \\books, \\blogs, which we obtained\nfrom KONECT~\\cite{konect} and which do not contain ground-truth opinions.\nHowever, these datasets contain two ground-truth communities. For the first\ncommunity, we sample the innate opinions of the users from the Gaussian\ndistribution $N(0.1,0.1)$, and for the second community we use $N(0.3,0.1)$.\n\nLast, we consider graphs generated from the Stochastic Block Model.  We generate\na Stochastic Block Model graph that consists of $1000$~nodes divided equally\ninto $4$ communities.  The intra-community edge probability is $0.4$ and an the\ninter-community edge probability is $0.1$.  The innate opinions for each\ncommunity are drawn from $N(0.2,0.1)$, $N(0.3,0.1)$, $N(0.4, 0.1)$ and\n$N(0.5, 0.1)$, respectively. \n\n"
                },
                "subsection 7.2": {
                    "name": "Maximizing the polarization",
                    "content": "\n\\label{sec:experiments-polarization}\nNext, we use our algorithms to maximize the polarization. We remark that all of\nour results extend to this setting, including the guarantees from\nTheorem~\\ref{thm:unbalanced}.\n\nWe report our results on larger datasets with\n$k=1\\%n$ in Table~\\ref{tab:polarization-large}.\nFor each of the datasets, we provide the number of vertices and edges. We also\nreport the \\emph{normalized polarization index} $\\PolIdx{G, \\begop}' =\n\\frac{\\PolIdx{G, \\begop} \\cdot 10^5}{\\abs{V}}$, where we normalize\nby the number of vertices for better comparison across different\ndatasets. Further, we report average innate opinions~$\\bar{\\begop}_0$ and the\nstandard deviation of the innate opinions~$\\sigma(\\begop_0)$.\nFinally, as before, for each algorithm we report the score\n$\\frac{\\begop^{\\intercal} \\MasIdx{\\PolIdx{}} \\begop - \\begop_0^{\\intercal} \\MasIdx{\\PolIdx{}} \\begop_0}{\\begop_0^{\\intercal} \\MasIdx{\\PolIdx{}} \\begop_0}$.\n\n\n\nWe see that the results for polarization are somewhat similar to those for\ndisagreement: algorithms with full information are the best, but the best\nalgorithm that only knows the topology still achieves similar performance.\n\nFurthermore, the best algorithms with limited information, i.e., \\AG\\LimitedInfo\nand \\NAG\\LimitedInfo, consistently outperform the baselines \\Deg\\LimitedInfo,\n\\IM\\LimitedInfo, and \\Random\\LimitedInfo; this shows that our strategy leads to\nnon-trivial results.  Furthermore, we observe that across all settings, simply\npicking high-degree vertices, or picking nodes with large influence in the\nindependent cascade model are poor strategies. \n\nIn addition, we show results for the increase of polarization in the small\ndatasets in Table~\\ref{tab:polarization-small-addition}.  Again, the methods\nwith full information perform best, and again our methods generally perform\nquite well and clearly outperform the baseline methods. \n\n\n\n\n\n"
                },
                "subsection 7.3": {
                    "name": "Finding influential users",
                    "content": "\n\\label{sec:experiments-influential}\nIn this section, we evaluate different methods for finding the influential users\nwhich can maximize the disagreement.\nMore specifically, we evaluate different methods for solving \nProblem~\\eqref{eq:our-problem}. \n\nWe report our results in Figure~\\ref{fig:evaluate-disagreement}.\nNotice that when $\\begop = \\ind$ and $k=0$, the disagreement is 0. \nThus, instead of evaluating the relative gain of the \\disidx, we \nreport absolute values of the \\disidx. \n\nWe observe that the baselines which pick random seed nodes, high degree nodes,\nand nodes with high influence in the independent cascade model are clearly the\nworst methods. Among the other methods, the \\SDPalgo-based methods are typically\nthe best.  We observe that when $k$ is below $0.25n$, the greedy methods \\AG{}\nand \\NAG{} often perform as well as \\SDPalgo; however, when $k$ is larger than\n$0.3n$, the \\SDPalgo-based algorithm performs better. These observations\nare in line with the analysis of Theorem~\\ref{thm:unbalanced} which achieves the\nbest approximation ratios when $k$ is close to $0.5n$ (see also\nFigure~\\ref{fig:approx-all}).\n\n\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Stability of randomized algorithms",
                    "content": "\n\\label{sec:experiments-stability}\nIn this section, we study how the randomization involved in some of the\nalgorithms affects their results. In particular,  $\\Random\\LimitedInfo$ and\n$\\SDPalgo\\LimitedInfo$ randomly select nodes and we wish to study how this\nimpacts their performance.  We report the \\disidx and output the mean over\n5~runs of the algorithms, together with error bars that indicate standard deviations.\n\nIn Figure~\\ref{fig:evaluate-disagreement-std} we present the \\disidx and the \nstandard deviation with randomized algorithms on small datasets. \nWe observe that as the number of nodes increases, the standard deviations of\ndifferent algorithms becomes relatively small (note that the largest dataset\nbelow is \\blogs).  Besides, we also observe that the outputs of\n\\SDPalgo\\LimitedInfo are stable, with standard deviations close to 0. \n\n\n\nIn Figure~\\ref{fig:evaluate-disagreement-large-std} we present results on larger graphs, \n\\TweetLTWO and \\GplusLTWO; here, we omit \\SDPalgo\\LimitedInfo due to scalability\nissues.\n\n\n\n\n"
                },
                "subsection 7.5": {
                    "name": "Running time of algorithms",
                    "content": "\n\\label{sec:experiments-running-time}\nNext, we present the running time of the algorithms to maximize the disagreement\non different datasets.  Note that for full information algorithms we directly\npresent the running time, while for limited information algorithms we report the\nrunning time for solving Problem~\\eqref{eq:our-problem}.  This is because the\nrunning time for setting the innate opinions to $1$ is negligible.  In addition,\nsince the running times of \\AGFull and \\AG\\LimitedInfo are almost the same, we\nonly report the running time of \\AGFull.  The same holds for \\NAGFull and\n\\NAG\\LimitedInfo.\n\nIn Figure~\\ref{fig:large-dis-time}, we notice that \\AGFull and \\IM\\LimitedInfo are \nthe two most costly algorithms, but even for those  algorithm the running time\nincreases moderately in terms of $k$.  However, on the less dense graph\n\\TweetLTWO, \\IM\\LimitedInfo runs faster than on the denser graph \\GplusLTWO,\neven though they have almost the same number of vertices.  This is consistent\nwith the time complexity of IMM~\\cite{tang2015influence}.  The graph density\ndoes not influence the running time of the adaptive greedy algorithm \\AGFull.\nInterestingly, we observe that that \\NAGFull is orders of magnitude faster than\n\\AGFull.\n\n\n\n\nIn Table~\\ref{tab:small_running_time} we report the absolute running times (in\nseconds) of our algorithms on the small datasets. We notice that\n\\SDPalgo\\LimitedInfo is the slowest algorithm and on \\blogs it is almost\n30~times slower than any other algorithm. This is within our expectation, since\nsolving semidefinite programs is costly.  We again observe that that \\NAGFull is\norders of magnitude faster than \\AGFull.\n\n\n\n%\\clearpage\n\n"
                }
            },
            "section 8": {
                "name": "Omitted Proofs and Discussions",
                "content": "\n\\label{sec:omitted-proofs}\nWe present proofs and discussions which are not contained in the main content below.\n\n",
                "subsection 8.1": {
                    "name": "Rescaling Opinions",
                    "content": "\n\\label{sec:scaling}\n\nIn Section~\\ref{sec:preliminaries}, we mention that we consider\nthe opinions in the interval $[0, 1]$. \nIn this appendix we prove that scaling the opinions from\nan interval~$[a,b]$ to an interval~$[x,y]$ only influences the disagreement in\nthe network by a fixed factor of $\\left(\\frac{y-x}{b-a}\\right)^2$. In particular,\nwe show that the optimizers of optimization problems are maintained under\nscaling. This implies that all\n\\NPhardness results we derive in this paper carry over to the setting with\n$[-1,1]$-opinions and our $O(1)$-approximation algorithms for $[-1,1]$-opinions\nalso yield $O(1)$-approximation algorithms for $[0,1]$-opinions.\n\n\n\nConsider real numbers $a < b$ and $x < y$.\nSuppose that we have innate opinions $\\begop_u\\in[a,b]$ and we wish to rescale\nthem into the interval $[x,y]$. Then we set\n\\begin{align*}\n\t\\begop_u' = \\begop_u \\cdot \\frac{y-x}{b-a}\n\t\t\t\t+ \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right).\n\\end{align*}\nFor convenience we set $\\alpha = \\frac{y-x}{b-a}$ and \n$\\beta = \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right)$.\nObserve that $\\begop_u' = \\alpha \\begop_u + \\beta$.\nWe also set $\\finop_u'^{(0)} = \\alpha \\finop_u'^{(0)} + \\beta$.\n\nIndeed, let $f(\\xi) = \\alpha \\xi + \\beta$. Then we note that under this\ntransformation we have that\n\\begin{align*}\n\tf(a) &= a \\frac{y-x}{b-a} + \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right) \\\\\n\t\t&= \\frac{1}{2} \\left( 2a \\frac{y-x}{b-a} + x + y - \\frac{a+b}{b-a}(y-x)\\right) \\\\\n\t\t&= \\frac{1}{2} \\left( x + y - \\frac{b-a}{b-a} (y-x) \\right) \\\\\n\t\t&= x,\n\\end{align*}\nand\n\\begin{align*}\n\tf(b) &= b \\frac{y-x}{b-a} + \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right) \\\\\n\t\t&= \\frac{1}{2} \\left( 2b \\frac{y-x}{b-a} + x + y - \\frac{a+b}{b-a}(y-x)\\right) \\\\\n\t\t&= \\frac{1}{2} \\left( x + y - \\frac{a-b}{b-a} (y-x) \\right) \\\\\n\t\t&= y.\n\\end{align*}\nAdditionally, note that $f(\\xi)$ is an affine linear function. Hence, $f$ maps\n$[a,b]$ bijectively into $[x,y]$.\n\nNext, consider the expressed opinion $\\finop_u'^{(t+1)}$ then by the update\nrule of the FJ model and by induction we have that\n\\begin{align*}\n\t\\finop_u'^{(t+1)}\n\t&= \\frac{\\begop_u' + \\sum_{v \\in N(u)} w_{uv} \\finop_u'^{(t)}}{1 + \\sum_{v\\in N(u)} w_{uv}} \\\\\n\t&= \\frac{\\alpha \\begop_u + \\beta + \\sum_{v \\in N(u)} w_{uv} (\\alpha \\finop_u^{(t)} + \\beta)}{1 + \\sum_{v\\in N(u)} w_{uv}} \\\\\n\t&= \\alpha \\frac{\\begop_u' + \\sum_{v \\in N(u)} w_{uv} \\finop_u'^{(t)}}{1 + \\sum_{v\\in N(u)} w_{uv}} + \\beta \\\\\n\t&= \\alpha \\finop_u^{(t+1)} + \\beta.\n\\end{align*}\nIn particular, in the limit we have that\n$\\finop' = \\lim_{t\\to\\infty} \\finop'^{(t+1)} \n\t= \\lim_{t\\to\\infty} (\\alpha \\finop^{(t+1)} + \\beta)\n\t= \\alpha \\finop + \\beta$.\n\nNext, for the disagreement in the network we have that:\n\\begin{align*}\n\t\\DisIdx{G, \\begop'}\n\t&= \\sum_{(u,v)\\in E} w_{u,v} (\\efinop{u}'-\\efinop{v}')^2 \\\\\n\t&= \\sum_{(u,v)\\in E} w_{u,v} (\\alpha \\efinop{u} + \\beta - \\alpha \\efinop{v} - \\beta)^2 \\\\\n\t&= \\alpha^2 \\sum_{(u,v)\\in E} w_{u,v} (\\efinop{u} - \\efinop{v})^2 \\\\\n\t&= \\alpha^2 \\DisIdx{G, \\begop'}.\n\\end{align*}\n\nNow we consider the mean opinion $\\bar{\\finop}'$:\n\\begin{align*}\n\t\\bar{\\finop}'\n\t&= \\frac{1}{n} \\sum_u \\finop_u' \\\\\n\t&= \\frac{1}{n} \\sum_u (\\alpha \\finop_u + \\beta) \\\\\n\t&= \\alpha \\left(\\frac{1}{n} \\sum_u \\finop_u\\right) + \\beta \\\\\n\t&= \\alpha \\bar{\\finop} + \\beta.\n\\end{align*}\nHence, for the network polarization we obtain:\n\\begin{align*}\n\t\\PolIdx{G, \\begop'}\n\t&= \\sum_{u\\in V} (\\finop_u' - \\bar{\\finop}')^2 \\\\\n\t&= \\sum_{u\\in V} (\\alpha \\finop_u + \\beta - \\alpha \\bar{\\finop} - \\beta)^2 \\\\\n\t&= \\alpha^2 \\sum_{u\\in V} (\\finop_u - \\bar{\\finop})^2 \\\\\n\t&= \\alpha^2 \\PolIdx{G, \\begop'}.\n\\end{align*}\n\n\nWe note that the results from above hold for all vectors $\\begop\\in[a,b]^n$. In\nparticular, this implies that if $\\begop^*$ is the optimizer for an optimization\nproblem of the form $\\max_{\\begop\\in[a,b]^n} \\DisIdx{G, \\begop'}$ then the vector\n$f(\\begop)\\in[x,y]^n$ is the maximizer for the optimization problem \n$\\max_{\\begop\\in[x,y]^n} \\DisIdx{G, \\begop'}$.\n\n"
                },
                "subsection 8.2": {
                    "name": "lem:matrices",
                    "content": "\nWe start by recalling two facts about positive semi-definite matrices. First, a\nmatrix $\\m+A$ is positive semi-definite if $\\m+A = \\m+B^{\\intercal} \\m+C \\m+B$,\nwhere $\\m+C$ is a positive semi-definite matrix. Second, $\\m+A$ is positive\nsemi-definite if we can write it as $\\m+A = \\m+B^{\\intercal} \\m+B$.\n\nLet us consider the matrix \n$\\MasIdx{\\PolIdx{}} = (\\ID + \\laplacian)^{-1} \\left(\\ID - \\frac{\\ind \\ind^\\intercal}{n}\\right) (\\ID + \\laplacian)^{-1}$\nfor polarization. Observe that $\\ID - \\frac{\\ind \\ind^\\intercal}{n}$ is the\nLaplacian of the full graph with edge weights $1/n$ and, hence, this matrix\npositive semi-definite. By our first property from above and the fact that $(\\ID + \\laplacian)^{-1}$ \nis symmetric, this implies that $\\MasIdx{\\PolIdx{}}$ is positive semidefinite.\nProving that $\\MasIdx{\\DisIdx{}} = (\\laplacian + \\ID)^{-1} \\laplacian (\\laplacian + \\ID)^{-1}$ \nis positive semi-definite works in the same way.\n\n\nNext, we observe that $(\\ID+\\laplacian)^{-1}$ satisfies \n$(\\ID+\\laplacian)^{-1} \\v+1 = \\v+1$ since\n$(\\ID+\\laplacian)\\v+1 = \\v+1 + \\laplacian\\v+1 = \\v+1$ and by multiplying with\n$(\\ID+\\laplacian)^{-1}$ from both sides we obtain the claim.\n\nNow we apply the previous observation for our matrices from the table and obtain\n\\begin{align*}\n\t\\MasIdx{\\PolIdx{}}\\v+1\n\t= (\\ID + \\laplacian)^{-1} \\left(\\ID - \\frac{\\ind \\ind^\\intercal}{n}\\right) (\\ID + \\laplacian)^{-1}\\v+1\n\t= (\\ID + \\laplacian)^{-1} \\left(\\ID - \\frac{\\ind \\ind^\\intercal}{n}\\right) \\v+1\n\t= \\v+0.\n\\end{align*}\nAnd, \n\\begin{align*}\n\t\\MasIdx{\\DisIdx{}} \\v+1\n\t= (\\laplacian + \\ID)^{-1} \\laplacian (\\laplacian + \\ID)^{-1} \\v+1\n\t= (\\laplacian + \\ID)^{-1} \\laplacian \\v+1\n\t= \\v+0.\n\\end{align*}\n\n\n"
                },
                "subsection 8.3": {
                    "name": "thm:relationship",
                    "content": "\n\tConsider the optimal solution $\\begopopt$ for\n\tProblem~\\ref{problem:max-disagreement} and let $\\begopalg$ denote the\n\t$\\beta$-approximate solution for Problem~\\ref{eq:our-problem}. \n\tFurthermore, set $\\Deltaopt = \\begopopt - \\begop_0$ and\n\t$\\Deltaalg=\\begopalg-\\begop_0$.\n\n\tThen we get that\n\t\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&=\n\t\t\t\\frac{(c\\v+1+\\epsvec+\\Deltaalg)^\\intercal \\m+A (c\\v+1+\\epsvec+\\Deltaalg)}\n\t\t\t\t{(c\\v+1+\\epsvec+\\Deltaopt)^\\intercal \\m+A (c\\v+1+\\epsvec+\\Deltaopt)} \\\\\n\t\t&=\n\t\t\t\\frac{(\\epsvec+\\Deltaalg)^\\intercal \\m+A (\\epsvec+\\Deltaalg)}\n\t\t\t\t{(\\epsvec+\\Deltaopt)^\\intercal \\m+A (\\epsvec+\\Deltaopt)} \\\\\n\t\t&=\n\t\t\t\\frac{\\Deltaalg^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ 2 \\epsvec^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ \\epsvec^\\intercal \\m+A \\epsvec}\n\t\t\t\t{\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+ 2 \\epsvec^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\\Deltaalg^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ (1-2\\gamma_1)\\epsvec^\\intercal \\m+A \\epsvec}\n\t\t\t\t{\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+ 2 \\epsvec^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\\Deltaalg^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ (1-2\\gamma_1)\\epsvec^\\intercal \\m+A \\epsvec}\n\t\t\t\t{2 (\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec)},\n\t\\end{align*}\n\twhere in the first step we used the assumption\n\t$\\begop_0=c\\v+1+\\epsvec$ and the definitions of $\\Deltaalg$ and $\\Deltaopt$.\n\tIn the second step we used that $\\m+A \\v+1 = \\v+0$ by\n\tLemma~\\ref{lem:matrices} and that $\\m+A$ is symmetric.\n\tIn the fourth step we used Assumption~(1) and the observations that\n\t$\\begopalg - \\begop_0 = \\Deltaalg$\n\tand\n\t$\\begop_0^{\\intercal} \\m+A\n\t\t= (c\\v+1 + \\epsvec)^{\\intercal} \\m+A\n\t\t= \\epsvec^{\\intercal} \\m+A$\n\tusing Lemma~\\ref{lem:matrices}.\n\tIn the fifth step we used that\n\t$2 \\epsvec^\\intercal \\m+A \\Deltaopt \\leq \n\t\t\t\t\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec$.\n\tThe last fact can be seen by letting $\\m+A = D^\\intercal D$ for a suitable\n\tmatrix~$D$ (which exists since $\\m+A$ is positive semi-definite by\n\tLemma~\\ref{lem:matrices}) and observing that\n\t$0 \\leq || D(\\Deltaopt - \\epsvec) ||_2^2\n\t\t= (\\Deltaopt - \\epsvec)^\\intercal D^\\intercal D (\\Deltaopt - \\epsvec)\n\t\t= \\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t- 2 \\epsvec^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec$;\n\tby rearranging terms we obtain the claimed inequality.\n\n\tNext, we let $\\OPT\\subseteq V$ denote the set of nodes such that\n\t$\\begop_0(u) \\neq \\begopopt(u)$ and similarly we set $\\ALG\\subseteq V$\n\tto the set of nodes with $\\begop_0(u) \\neq \\begopalg(u)$.\n\tObserve that since\n\t$\\begopopt = c\\v+1 + \\epsvec + \\Deltaopt$ and $\\begopopt(u)=1$ for all\n\t$u\\in\\OPT$, we have that\n\t$\\Deltaopt = (1-c)\\v+1_{\\vert \\OPT} - \\epsvec_{\\vert \\OPT}$.\n\tSimilarly, \n\t$\\Deltaalg = (1-c)\\v+1_{\\vert \\ALG} - \\epsvec_{\\vert \\ALG}$.\n\n\tThen we get that\n\t\\begin{align*}\n\t\t&\\Deltaalg^\\intercal \\m+A \\Deltaalg \\\\\n\t\t&=\n\t\t\t(1-c)^2 \\v+1_{\\vert \\ALG}^\\intercal \\m+A \\v+1_{\\vert \\ALG}\n\t\t\t- 2(1-c) \\epsvec_{\\vert \\ALG}^\\intercal \\m+A \\v+1_{\\vert \\ALG} \\\\\n\t\t\t&\\quad + \\epsvec_{\\vert \\ALG}^\\intercal \\m+A \\epsvec_{\\vert \\ALG} \\\\\n\t\t&\\geq \n\t\t\t(1-c)^2 \\v+1_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG}\n\t\t\t- 2(1-c) \\epsvec_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG} \\\\\n\t\t&\\geq \n\t\t\t(1-c)^2 \\v+1_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG}\n\t\t\t- 2 (1-c) \\gamma_3 \\epsvec^\\intercal \\m+A \\epsvec,\n\t\\end{align*}\n\twhere in the second step we used that \n\t$\\epsvec_{\\vert\\ALG} \\m+A \\epsvec_{\\vert\\ALG} \\geq 0$\n\tand in the third step we used Assumption~(3).\n\n\tFurthermore, we obtain that\n\t\\begin{align*}\n\t\t& \\Deltaopt^\\intercal \\m+A \\Deltaopt \\\\\n\t\t&=\n\t\t\t(1-c)^2 \\v+1_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT}\n\t\t\t- 2(1-c) \\epsvec_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT} \\\\\n\t\t\t&\\quad + \\epsvec_{\\vert\\OPT}^\\intercal \\m+A \\epsvec_{\\vert\\OPT} \\\\\n\t\t&\\leq\n\t\t\t(1-c)^2 \\v+1_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT}\n\t\t\t+ (2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec,\n\t\\end{align*}\n\twhere we used Assumptions~(3) and~(2).\n\n\tBy combining our derivations from above, we obtain that\n\t\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\v+1_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG}\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec\n\t\t\t}{\n\t\t\t\t2( (1-c)^2 \\v+1_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT}\n\t\t\t\t\t+ (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)\n\t\t\t}.\n\t\\end{align*}\n\t\n\tNext, let $\\begopoptp\\in\\{0,1\\}^n$ denote the optimal solution for\n\tProblem~\\ref{eq:our-problem}. \n\tFurthermore, observe that $\\v+1_{\\vert\\OPT}$ and $\\v+1_{\\vert\\ALG}$ are\n\tfeasible solutions for Problem~\\ref{eq:our-problem}.\n\tHence, we obtain\n\t$$ \\v+1_{\\vert\\OPT}^\\intercal\\m+A\\v+1_{\\vert\\OPT}\n\t\t\\leq \\begopoptp^\\intercal\\m+A\\begopoptp.$$\n\tFurthermore, since in our algorithm we use an $\\beta$-approximation algorithm for\n\tProblem~\\ref{eq:our-problem} to pick the set of nodes $\\ALG$, it also holds that\n\t$$ \\v+1_{\\vert\\ALG}^\\intercal\\m+A\\v+1_{\\vert\\ALG}\n\t\t\\geq \\beta \\cdot \\begopoptp^\\intercal\\m+A\\begopoptp.$$\n\tThis implies that\n\t\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\beta \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec,\n\t\t\t}{\n\t\t\t\t2( (1-c)^2 \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t\t+ (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)\n\t\t\t}.\n\t\\end{align*}\n\t\n\tTo obtain our approximation, \n\tobserve that if\n\t$$(1-c)^2 \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t\\geq (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)$$\n\tthen \n\t\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\beta \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec,\n\t\t\t}{\n\t\t\t\t4(1-c)^2 \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t} \\\\\n\t\t&\\geq \\frac{\\beta}{4}.\n\t\\end{align*}\n\tSimilarly, if \n\t$$(1-c)^2 \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t< (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)$$\n\tthen\n\t\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&>\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\beta \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec\n\t\t\t}{\n\t\t\t\t4 (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)\n\t\t\t} \\\\\n\t\t&\\geq \n\t\t\t\\frac{\n\t\t\t\t(1 - 2\\gamma_1 - 2 (1-c) \\gamma_3)\n\t\t\t}{\n\t\t\t\t4 (1 + 2(1-c)\\gamma_3+\\gamma_2)\n\t\t\t}.\n\t\\end{align*}\n\tWe conclude that the approximation ratio of our algorithm is given by\n\t$\\frac{1}{4} \\min\\{\\beta,\n\t\t   \\frac{ 1-2\\gamma_1-2(1-c)\\gamma_3 }{ 1+2(1-c)\\gamma_3+\\gamma_2 } \\}$.\n\n\n"
                },
                "subsection 8.4": {
                    "name": "Convexity Implies Extreme Values",
                    "content": "\n\\label{sec:convexity}\n\tLet $\\m+A\\in\\mathbb{R}^{n\\times n}$ be a symmetric matrix, let\n\t$\\begop_0\\in[-1,1]^n$ and let $k>0$ be an integer.\n\tConsider the following optimization problem, which generalizes\n\tProblems~\\eqref{problem:max-disagreement},~\\eqref{eq:our-problem},\n\tand~\\eqref{problem:maxcut-unbalanced}:\n\t\\begin{equation}\n    \\label{eq:convex}\n\t\t\\begin{aligned}\n\t\t\t\\max_{\\begop} \\quad &  \\begop^{\\intercal} \\m+A \\begop,\\\\\n\t\t\t\\st \\quad & \\begop \\in [-1,1]^n, \\text{ and}\\\\\n\t\t\t& \\lVert \\begop - \\begop_0 \\rVert_0 \\leq k.\n\t\t\\end{aligned}\n\t\\end{equation}   \n\n\tNow we show prove a lemma about optimal solutions of\n\tProblem~\\eqref{eq:convex}, where we write $\\v+x(i)$ to denote the $i$'th\n\tentry of a vector~$\\v+x$.\n\t\\begin{lemma}\n\t\\label{lemma:max-disagreement-extreme}\n\t\\label{lemma:convex}\n\t\tSuppose that $\\m+A$ is a positive semi-definite matrix. Then there\n\t\texists an optimal solution~$\\begop$ for Problem~\\ref{eq:convex} such\n\t\tthat $\\begop(i)\\in\\{-1,1\\}$ for all entries~$i$ with $\\begop(i)\\neq\\begop_0(i)$.\n\t\tIn particular, if $\\begop_0\\in\\{-1,1\\}^n$ or $k=n$ then there exists an\n\t\toptimal solution~$\\begop\\in\\{-1,1\\}^n$.\n\t\\end{lemma}\n\t\\begin{proof}\n\t\tFirst, note that since $\\m+A$ is positive semi-definite, the quadratic\n\t\tform $f(\\begop) = \\begop^{\\intercal} \\m+A \\begop$ is convex.\n\n\t\tSecond, consider an optimal solution $\\begop$. If $\\begop$ satisfies\n\t\tthe property from the lemma, we are done. Otherwise, there exists at least\n\t\tone entry~$i$ such that $\\begop(i)\\neq\\begop_0(i)$ and\n\t\t$\\begop(i)\\in(0,1)$. Now let $\\v+t_{-1}$ denote the vector which has\n\t\tits $i$'th entry set to $-1$ and in which all other entries are the\n\t\tsame as in $\\begop$, i.e., \n\t\t$\\v+t_{-1}(j)=\\begop(j)$ for all $j\\neq i$ and $\\v+t_{-1}(i)=-1$.\n\t\tSimilarly, we set $\\v+t_1$ to the vector with\n\t\t$\\v+t_{1}(j)=\\begop(j)$ for all $j\\neq i$ and $\\v+t_{1}(i)=1$.\n\t\tNote that $\\v+t_{-1}$ and $\\v+t_{1}$ are feasible solutions to\n\t\tProblem~\\eqref{eq:convex}.\n\n\t\tThird, observe that there exists an $\\alpha\\in(0,1)$ such that\n\t\t$\\begop = \\alpha \\v+t_{-1} + (1-\\alpha)\\v+t_1$. Now by the convexity of\n\t\t$f(\\begop)$ we get that\n\t\t\\begin{align*}\n\t\t\tf(\\begop)\n\t\t\t&= f(\\alpha \\v+t_{-1} + (1-\\alpha)\\v+t_1) \\\\\n\t\t\t&\\leq \\alpha f(\\v+t_{-1}) + (1-\\alpha) f(\\v+t_1) \\\\\n\t\t\t&\\leq \\max\\{ f(\\v+t_{-1}), f(\\v+t_1)\\}.\n\t\t\\end{align*}\n\t\tThus, at least one of $\\v+t_{-1}$ and $\\v+t_1$ achieves an objective\n\t\tfunction value that is at least as large as that of $\\begop$. Hence, we\n\t\tcan assume that the $i$'th entry of $\\begop$ is from the set $\\{-1,1\\}$.\n\t\tRepeating the above procedure for all entries with $\\begop(i)\\neq\\begop_0(i)$ and\n\t\t$\\begop(i)\\in(0,1)$ proves the first part of the lemma.\n\n\t\tThe second part of the lemma (if $\\begop_0\\in\\{-1,1\\}^n$ or $k=n$)\n\t\tfollows immediately from the first part.\n\t\\end{proof}\n\n"
                },
                "subsection 8.5": {
                    "name": "An illustration of graphs with mixed weights",
                    "content": "\n\\label{sec:illustration-worst-case}\nFigure~\\ref{fig:cut-example} shows how the cut of a graph can be influenced by\npositive \\emph{and negative} weights. We use this example to show that\n``badly-behaved\" graphs with negative weights can make the cut value drop\nsignificantly, whereas in graphs with only positive edges this is not the case. \nThe graphs we discuss in the paper are in the class of ``well-behaved'' graphs.\n\n"
                },
                "subsection 8.6": {
                    "name": "thm:unbalanced",
                    "content": "\n\tWe start by defining notation. \n\tLet $\\OPT$ denote the optimal solution for $\\alpha$-{\\sc Balanced-Max\\-Cut},\n\tlet $M_0$ be the objective function value obtained through randomized rounding after solving\n\tProblem~\\eqref{problem:maxcut-unbalaced-relax}, \n\tand let $M$ be the cut value for $(T, \\bar{T})$ we obtain in the end. \n\tLet $M^*$ be the optimal solution for Problem~\\eqref{problem:maxcut-unbalaced-relax}. \n\n\tWe start with an overview of our analysis which is similar to the one by\n\tFrieze and Jerrum~\\cite{frieze1997improved}.\n\tBy solving the SDP and applying the hyperplane rounding enough times,\n\twe show that Lemma~\\ref{lem:standard} implies that \n\t$M_0$ is close $(1-\\varepsilon) \\frac{2}{\\pi} \\OPT$\n\tand simultaneously\n\t$\\abs{S_0} \\abs{\\bar{S_0}} = \\abs{S_0} (n - \\abs{S_0})$ does not differ too\n\tmuch from $\\alpha (1-\\alpha) n^2$. This then implies that\n\tthat the loss from the greedy procedure for ensuring the\n\t$\\alpha$-balancedness constraint is not too large.\n\tTo bound the loss from our greedy procedure for the size adjustments,\n\twe apply Lemma~\\ref{lemma:cut-value-after-moving} with $(S_0,\\bar{S}_0)$\n\tcorresponding to the (unbalanced) solution $(S,\\bar{S})$ from the hyperplane\n\trounding and $(T,\\bar{T})$ corresponding to the $\\alpha$-balanced solution\n\tthat we return.\n\n\tNext, we proceed with the concrete details of the proof.\n\tFirst, observe that since the objective function of the optimization problem\n\tis convex (see\n\tSection~\\ref{sec:convexity}) there exists an\n\toptimal solution with $\\begop\\in\\{-1,1\\}^n$.  Hence, we can focus on\n\tsolutions with $\\begop\\in\\{-1,1\\}^n$.\n\t\n\tConsider the $p$-th iteration of Algorithm~\\ref{alg:SDP-based}.\n\tLet $X_p$ denote the cut value of $(S, \\bar{S})$, and let\n\t$Y_p=\\abs{S}\\abs{\\bar{S}}$. \n\tLet $Z_p = \\frac{X_p}{M^*} + \\frac{Y_p}{N}$, where $N = \\frac{n^2}{\\beta}$\n\tand $\\beta\\in(0,7]$ is a parameter that depends on $\\alpha$ and that we will\n\tpick below.  We use a similar approach as~\\cite{frieze1997improved} to do the analysis.\n\n\tBy Lemma~\\ref{lem:standard}, $\\Exp[Z_p] \\geq \\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta$.\n\tAs $X_{p} \\leq M^*$ and $Y_{p} \\leq \\frac{n^2}{4}$, we obtain $Z_p \\leq 1 + \\frac{\\beta}{4}$.\n\tWe will prove that in the $\\kappa$~iterations of Algorithm~\\ref{alg:SDP-based}, there\n\texists a $\\tau$ where $Z_{\\tau} = \\max_{p} Z_p$ such that $Z_{\\tau} \\geq\n\t(1-\\epsilon)(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)$ with\n\tprobability at least $1-\\epsilon$.\n\n\tWe first bound the probability of\n\t$Z_p \\leq (1-\\epsilon)\\left(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)\\right)$\n\tfor a single iteration~$p$ as follows:\n\t\\begin{equation*}\n\t\t\\begin{aligned}\n\t\t&\\Prob{ Z_p \\leq (1-\\epsilon)\\left(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)\\right) } \\\\\n\t\t&= \\Prob{\\frac{\\beta}{4} + 1 - Z_p \\geq \\frac{\\beta}{4} + 1 -\n\t\t\t\\left((1-\\epsilon)\\left(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta\n\t\t\t\t\t\t\\right)\\right)} \\\\\n\t\t&\\leq \\frac{\\frac{\\beta}{4} + 1 - \\Exp[Z_p]}{\\frac{\\beta}{4} + 1 - ((1-\\epsilon)(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta))} \\\\\n\t\t&\\leq \\frac{\\frac{\\beta}{4} + 1 - (\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)}{\\frac{\\beta}{4} + 1 - ((1-\\epsilon)( \\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta))} \\\\\n\t\t&= \\frac{1 - c}{1 - (1-\\epsilon)c},\n\t\t\\end{aligned}\n\t\\end{equation*}\n\twhere $c = \\frac{\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta}{1 + \\frac{\\beta}{4}}$.\n\tThe first equality holds as we multiply with $-1$ and add $\\frac{\\beta}{4} +1$ to both sides;\n\tthe first inequality holds by Markov inequality;\n\tthe second inequality holds by $\\Exp[Z_p] \\geq 0.878 \\alpha (1-\\alpha) \\cdot \\beta + \\frac{2}{\\pi}$.\n\tIn the end we simplify the formula by introducing $c$. \n\tNotice that since we assume $\\beta > 0$, $c$ is in $(0,1)$. \n\tMoreover, by Lemma~\\ref{lem:helper}, we can bound $c$ between $\\frac{\\frac{2}{\\pi}}{1}$ and $\\frac{0.878 \\alpha (1-\\alpha) \\cdot \\beta}{\\frac{\\beta}{4}}$.\n\tNamely, either $c \\in [{0.878 \\alpha (1-\\alpha) \\cdot 4}, \\frac{2}{\\pi}]$ or $c \\in [\\frac{2}{\\pi}, {0.878 \\alpha (1-\\alpha) \\cdot 4}]$.\n\n\t\\begin{lemma}\n\t\t\\label{lem:helper}\n\t\tLet $a_1, a_2, a_3, a_4$ be positive real numbers, \n\t\tthen either $\\frac{a_2}{a_4} \\leq \\frac{a_1 + a_2}{a_3 + a_4} \\leq \\frac{a_1}{a_3}$ \n\t\tor $\\frac{a_2}{a_4} \\geq \\frac{a_1 + a_2}{a_3 + a_4} \\geq \\frac{a_1}{a_3}$.\n\t\\end{lemma}\n\t\\begin{proof}\n\t\tWe prove the lemma with two case distinctions. \n\n\t\tCase 1: Assume $a_1 a_4 \\geq a_2 a_3$.\n\t\tIf we add $a_2 a_4$ on both sides, the formula becomes $(a_1 + a_2) a_4 \\geq a_2 (a_3 + a_4)$, which implies $\\frac{a_1 + a_2}{a_3 + a_4} \\geq \\frac{a_2}{a_4}$;\n\t\tif we add $a_1 a_3$ on both sides, the formula becomes $a_1 (a_3 + a_4) \\geq (a_1 + a_2) a_3$, which implies $\\frac{a_1}{a_3} \\geq \\frac{a_1 + a_2}{a_3 + a_4}$;\n\t\tthus $\\frac{a_1}{a_3} \\geq \\frac{a_1 + a_2}{a_3 + a_4} \\geq \\frac{a_2}{a_4}$.\n\n\t\tCase 2: Assume $a_1 a_4 \\leq a_2 a_3$.\n\t\tIf we add $a_2 a_4$ on both sides, the formula becomes $(a_1 + a_2) a_4 \\leq a_2 (a_3 + a_4)$, which implies $\\frac{a_1 + a_2}{a_3 + a_4} \\leq \\frac{a_2}{a_4}$;\n\t\tif we add $a_1 a_3$ on both sides, the formula becomes $a_1 (a_3 + a_4) \\leq (a_1 + a_2) a_3$, which implies $\\frac{a_1}{a_3} \\leq \\frac{a_1 + a_2}{a_3 + a_4}$;\n\t\tthus $\\frac{a_1}{a_3} \\leq \\frac{a_1 + a_2}{a_3 + a_4} \\leq \\frac{a_2}{a_4}$.\n\t\\end{proof}\n\t\n\tNotice that as the algorithm repeats the procedure $\\kappa$ times, and the\n\truns of the procedure are independent from each other, the\n\tprobability that $Z_{p} \\leq (1-\\epsilon)(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)$\n\tfor all $p$ is then bounded from above by\n\t\\begin{equation*}\n\t\t\\begin{aligned}\n\t\t\t\\left(\\frac{1 - c}{1 - (1-\\epsilon)c}\\right)^\\kappa\n\t\t\t&= \\left(1 - \\frac{1}{1+ \\frac{1-c}{\\epsilon c}}\\right)^\\kappa \\\\\n\t\t\t&= \\left(1 - \\frac{1}{1+ \\frac{1-c}{\\epsilon c}}\\right)^{(1+ \\frac{1-c}{\\epsilon c}) \\frac{\\epsilon c}{1 - c + \\epsilon c}\\kappa} \\\\\n\t\t\t&\\leq \\exp\\left(-\\frac{\\epsilon c}{1 - c + \\epsilon c}\\kappa \\right),\n\t\t\\end{aligned}\n\t\\end{equation*}\n\twhere the inequality holds through $(1-\\frac{1}{x})^x \\leq \\frac{1}{e}$ for any $x > 1$.\n\n\tAs a result, if we choose $\\kappa  \\geq \\frac{1 - c + \\epsilon c}{\\epsilon c} \\log (\\frac{1}{\\epsilon}) \\in \\bigO(\\frac{1}{\\epsilon} \\log (\\frac{1}{\\epsilon}))$, \n\twith probability at least $1 - \\epsilon$, $Z_{\\tau} \\geq (1-\\epsilon)(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)$.\n\t\n\tNow consider the (non-$\\alpha$-balanced) solution $(S,\\bar{S})$ from the\n\t$\\tau$-th iteration with cut-value $M_0 := X_\\tau$. We set\n\t$s=\\frac{\\abs{S}}{n}$ and $t=\\alpha$ and apply\n\tLemma~\\ref{lemma:cut-value-after-moving} to obtain a solution that satisfies\n\tthe $\\alpha$-balancedness constraint.\n\tSuppose that in the $\\tau$-th run, $X_{\\tau} = \\lambda M^*$ for suitable\n\t$\\lambda$.  Then it follows that $Y_{\\tau} \\geq ((1-\\epsilon)(0.878 \\alpha (1-\\alpha) \\cdot \\beta + \\frac{2}{\\pi}) - \\lambda) N$.\n\tBy replacing $Y_{\\tau}$ with $Y_{\\tau} = n^2(1-s)s$, we obtain\n\t$\\lambda \\geq (1 - \\epsilon)(\\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta) - \\beta (1-s)s$. \n\n\tWe distinguish the two cases $0 < s \\leq t \\leq 0.5$ and $0< t < s \\leq 0.5$.\n\t\n\t\\textbf{Case 1:} $0 < s < t \\leq 0.5$. By\n\tLemma~\\ref{lemma:cut-value-after-moving} we have that\n\t$M \\geq \\frac{(1-t)^2 -7(1-t)/n + 12/n^2}{(1-s)^2  + (1-s)/n} M_0$.\n\tNow notice that $\\lim_{n \\rightarrow \\infty} \\frac{(1-t)^2 -7(1-t)/n + 12/n^2}{(1-s)^2  + (1-s)/n} = \\frac{(1-t)^2}{(1-s)^2}$. \n\tWhich indicates that for any $\\epsilon' > 0$, \n\tthere exists a constant $C$, such that for any $n \\geq C$,\n\t$M \\geq \\left(\\frac{(1-t)^2}{(1-s)^2} - \\epsilon'\\right) M_{0}$. \n\tUsing the above analysis, and setting $t = \\alpha$, \n\twe obtain that $M \\geq \\lambda \\frac{(1-\\alpha)^2}{(1-s)^2} \\OPT\n\t\\geq [\\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta - \\beta (1-s)s - \\varepsilon'']\\frac{(1-\\alpha)^2}{(1-s)^2} \\OPT$. \n\n\t\\textbf{Case 2:} $0 < t < s  \\leq 0.5$. By Lemma~\\ref{lemma:cut-value-after-moving}, \n\twe obtain $M \\geq \\frac{t^2 - t/n}{s^2 - s/n} M_0$.\n\tNotice that $\\lim_{n \\rightarrow \\infty} \\frac{t^2 - t/n}{s^2 - s/n} = \\frac{t^2}{s^2}$,\n\twhich indicates that for any $\\epsilon' > 0$, there exists a constant $C$,\n\tsuch that for any $n \\geq C$, $M \\geq \\left(\\frac{t^2}{s^2} - \\epsilon'\\right) M_{0}$. \n\tUsing the above analysis, and setting $t = \\alpha$,\n\twe obtain that $M \\geq \\lambda \\frac{\\alpha^2}{s^2} \\OPT\n\t= [\\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta - \\beta (1-s)s - \\varepsilon''']\\frac{\\alpha^2}{s^2} \\OPT$. \n\t\n\tNow we combine the two cases together, i.e., we take the minimum of the two\n\tsolutions given any $\\alpha \\leq 0.5$.  To do so, we numerically solve the\n\tfollowing problem for any $\\alpha\\leq 0.5$, \n\t\\begin{align*}\n\t\\max_{0<\\beta <7}\n\t\\quad\n\t\\min_{0< s_1 < \\alpha \\leq 0.5, \\,\\, 0<\\alpha \\leq s_2 \\leq 0.5}\n\t&\\left\\{\n\t\t\\left( \\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta - \\beta\n\t\t\t\t(1-s_1)s_1\\right)\\frac{(1-\\alpha)^2}{(1-s_1)^2}, \\right. \\\\\n\t\t&\\quad \\left. \\left(\\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta - \\beta (1-s_2)s_2\\right)\\frac{\\alpha^2}{s_2^2}\n\t\\right\\}.\n\t\\end{align*}\n\tNotice that we only consider values for $\\beta$ in a small domain $(0, 7)$\n\tsince the running time of our algorithm depends on $\\beta$.  We present the\n\tapproximation ratio for given $\\alpha$ and the choice of $\\beta$ in\n\tTable~\\ref{tab:selected_ratio}. \n\n\t\n\n\tNote that similar analysis holds when $\\alpha > 0.5$, since essentially \n\tin this case, our greedy procedure starts from a $S_0$ where $\\abs{S_0} > \\frac{1}{2}n$ (otherwise, \n\twe take $\\bar{S}_0$ as $S_0$). \n\tThe approximation ratio holds a symmetric property. \n\n\tWe plot the approximation ratio of our algorithm for different values of\n\t$\\alpha$ in Figure~\\ref{fig:approx-all}. \n\t\n\n\t\n",
                    "subsubsection 8.6.1": {
                        "name": "lem:standard",
                        "content": "\n\\begin{proof} \n\tThe analysis by Williamson and \n\tShmoys~{\\cite[Theorem 6.16]{williamson2011design}} shows that the expected\n\tcut of $(S, \\bar{S})$ is not less than $\\frac{2}{\\pi} M^*$, \n\twhere $M^*$ denotes the optimal solution for Problem~\\ref{problem:maxcut-unbalaced-relax}. \n\tTheir analysis assumes that there is no $\\ell_0$ constraint; however, \n\ttheir proof still holds in our setting, since the $\\ell_0$ constraint and its relaxation \n\tdo not influence the analysis and since\n\tProblem~\\eqref{problem:maxcut-unbalaced-relax} is a relaxation of\n\tProblem~\\eqref{eq:our-problem}.\n\tTo prove that $\\Exp[\\abs{S}\\abs{\\bar{S}}] \\geq 0.878 \\cdot \\alpha (1- \\alpha) n^2$, \n\twe use the same method as {\\cite[Section 3]{frieze1997improved}}.\n\n\tFor the sake of completeness, we now present the details of the analysis.\n\t\n\t\\begin{lemma}[Lemma 6.12~\\cite{williamson2011design}]\n\t\t\\label{lemma:maxbisectionrelax}\n\t\t$\\Exp[\\x_i\\x_j] = \\frac{2}{\\pi} \\arcsin(\\v+v_i \\cdot \\v+v_j)$. \n\t\\end{lemma}\n\t\\begin{corollary}\n\t\t\\label{corollary:maxbisectionrelcut}\n\t\t$\\frac{1}{2}\\Exp[1 - \\x_i\\x_j] = \\frac{1}{\\pi}\\arccos(\\v+v_i \\cdot \\v+v_j)$. \n\t\\end{corollary}\n\t\\begin{proof}\n\t\tThis is because $\\arccos(x) = \\frac{\\pi}{2} - \\arcsin(x)$ for any $x$. \n\t\\end{proof}\n\n\t\\begin{lemma}[Corollary 6.15~\\cite{williamson2011design}]\n\t\t\\label{lemma:maxbisectionrelaxcorollary}\n\t\tIf $\\m+X \\succcurlyeq \\m+0$, $X_{ij} \\leq 1$ for all $i, j$ and \n\t\t$\\m+Z = (Z_{ij})$ such that $Z_{ij} = \\arcsin(X_{ij}) - X_{ij}$, \n\t\tthen $\\m+Z \\succcurlyeq \\m+0$. \n\t\\end{lemma}\n\n\tWe are ready to prove the first part of Lemma~\\ref{lem:standard}.\n\tNote the expected cut of $(S, \\bar{S})$ can be formulated as $\\Exp[\\sum_{i, j}\\frac{1}{4}A_{ij} \\x_i\\x_j]$. \n\tThen we have:\n\t\\begin{displaymath}\n\t\t\\begin{aligned}\n\t\t\t\\Exp[\\sum_{i, j}\\frac{1}{4}A_{ij} \\x_i\\x_j]\n\t\t\t&= \\sum_{i, j}\\frac{1}{4}A_{ij} \\Exp[\\x_i\\x_j] \\\\\n\t\t\t&= \\frac{2}{\\pi} \\frac{1}{4} \\sum_{i,j}A_{ij}\\arcsin(\\v+v_i \\cdot \\v+v_j) \\\\\n\t\t\t&\\geq \\frac{2}{\\pi} \\frac{1}{4} \\sum_{i,j}A_{ij}\\v+v_i \\cdot \\v+v_j \\\\\n\t\t\t&= \\frac{2}{\\pi} M^* \\geq \\frac{2}{\\pi} \\OPT,\n\t\t\\end{aligned}\n\t\\end{displaymath}\n\twhere the second step is based on Lemma~\\ref{lemma:maxbisectionrelax}, \n\tand the third step is based on Lemma~\\ref{lemma:maxbisectionrelaxcorollary}.\n\n\tTo prove the second part of Lemma~\\ref{lem:standard}, we bound the imbalance\n\tof the partition that we get from the random rounding procedure.\n\tWe will show that $\\abs{S}\\abs{\\bar{S}}$ does not deviate from $\\alpha (1-\\alpha) n^2$ too much.\n\n\t\\begin{lemma}[Lemma 6.8~\\cite{williamson2011design}]\n\t\t\\label{lemma:maxbisectionrelcutbounc}\n\t\tFor $x \\in [-1, 1]$, $\\frac{1}{\\pi} \\arccos(x) \\geq 0.878 \\cdot \\frac{1}{2} (1 - x)$.  \n\t\\end{lemma}\n\n\tNow we can calculate $\\Exp[\\abs{S}\\abs{\\bar{S}}]$:\n\t\\begin{displaymath}\n\t\t\\begin{aligned}\n\t\t\\Exp[\\abs{S}\\abs{\\bar{S}}]\n\t\t&= \\sum_{i<j}\\frac{1}{2} \\Exp[1 - \\x_i\\x_j] \\\\\n\t\t&= \\sum_{i<j} \\frac{1}{\\pi} \\arccos(\\v+v_i \\cdot \\v+v_j) \\\\\n\t   \t&\\geq 0.878 \\sum_{i<j}\\frac{1}{2} (1 - \\v+v_i \\cdot \\v+v_j)  \\\\\n\t\t&= 0.878 \\left(\\frac{n^2 - n}{4} - \\frac{1}{2}\\sum_{i<j}\\v+v_i^{\\intercal}\\v+v_j\\right) \\\\\n\t\t&= 0.878 \\left(\\frac{n^2 - n}{4} - \\frac{1}{4}(n^2(1-2\\alpha)^2 - n) \\right) \\\\\n\t\t&= 0.878\\cdot \\alpha (1-\\alpha) n^2,\n\t\t\\end{aligned}\n\t\\end{displaymath}\n\twhere the second step is based on\n\tCorollary~\\ref{corollary:maxbisectionrelcut}, the third step is based on\n\tLemma~\\ref{lemma:maxbisectionrelcutbounc}, and the fourth step is based on\n\tthe fact that Problem~\\ref{problem:maxcut-unbalaced-relax} is a semidefinite\n\trelaxation of Problem~\\ref{problem:maxcut-unbalanced} and the fifth step\n\tuses the constraint from the SDP relaxation.\n\n\n\\end{proof}\n\n"
                    },
                    "subsubsection 8.6.2": {
                        "name": "lemma:cut-value-after-moving",
                        "content": "\n\\begin{proof}\n\tWe repeatedly apply Lemma~\\ref{lemma:boundSUmodify} until our set has size~$tn$.\n\tMore concretely, if $\\abs{S_0}>tn$, we use Lemma~\\ref{lemma:boundSUmodify} to remove vertices from\n\t$S_0$ one by one, \n\tand if $\\abs{S_0}< tn$, we use Lemma~\\ref{lemma:boundSUmodify} to remove vertices from $\\bar{S}_0$ one by one. \n\tWe let $S_i$ be the set of vertices after removing\n\t$i$~vertices. When this process terminates we denote the resulting set by~$T$.\n\tLet $M_i$ denote the value of the cut given by the partition\n\t$(S_i,\\bar{S_i})$. \n\tWe will distinguish the two cases $s < t$ and $s > t$. \n\t\n\tFirst, suppose $s > t$. Observe that by Lemma~\\ref{lemma:boundSUmodify},\n\t\\begin{align*}\n\t\tM_i \\geq M_{i-1} - \\frac{2 M_{i-1}}{\\abs{S_{i-1}}}\n\t\t\t= \\left(1- \\frac{2}{\\abs{S_{i-1}}}\\right) M_{i-1},\n\t\\end{align*} \n\tfor all $i\\geq 1$. Note that for $k=\\abs{s-t}n$, it is $T=S_k$, and thus $M_k$\n\tis the value of the cut $(T,\\bar{T})$.  \n\tBy recursively applying the above inequality, we obtain that\n\t\\begin{align}\n\t\\label{eq:Mk}\n\t\tM_k \\geq M_0 \\prod_{i=1}^k \\left(1- \\frac{2}{\\abs{S_{i-1}}}\\right).\n\t\\end{align}\n\n\tNow let us consider the term $\\prod_{i=1}^k \\left(1- \\frac{2}{\\abs{S_{i-1}}}\\right)$.\n\tWe are removing vertices from the $S_i$, and thus $\\abs{S_i}=\\abs{S_{i-2}}-2$. \n\tHence,\n\t\\begin{align*}\n\t\t\\prod_{i=1}^k \\left(1- \\frac{2}{\\abs{S_{i-1}}}\\right)\n\t\t    &=\\prod_{i=0}^{k-1} \\frac{\\abs{S_{i}}-2}{\\abs{S_{i}}} \\\\\n\t\t\t&= \\frac{\\abs{S_{0}}-2}{\\abs{S_{0}}} \\cdot\n\t\t\t\t\t\\frac{\\abs{S_{1}}-2}{\\abs{S_{1}}} \\cdot\n\t\t\t\t\t\\frac{\\abs{S_{2}}-2}{\\abs{S_{2}}}\n\t\t\t\t\t\\cdots\n\t\t\t\t\t\\frac{\\abs{S_{k-1}}-2}{\\abs{S_{k-1}}} \\\\\n\t\t\t&= \\frac{\\abs{S_{k-2}}-2}{\\abs{S_{0}}} \\cdot\n\t\t\t\t\t\\frac{\\abs{S_{k-1}}-2}{\\abs{S_{1}}} \\\\\n\t\t\t&= \\frac{tn(tn-1)}{sn(sn-1)}\n\t\t\t= \\frac{t^2n^2 - tn}{s^2n^2 - sn} = \\frac{t^2 - t/n}{s^2 - s/n}.\n\t\\end{align*}\n\n\tCombining the result for $\\prod_{i=1}^k \\left(1- \\frac{2}{\\abs{S_{i-1}}}\\right)$\n\twith Equation~\\eqref{eq:Mk} we obtain the claim of the lemma for $s>t$. \n\n\tSecond, consider the case $s<t$. \n\tWe apply Lemma~\\ref{lemma:boundSUmodify} to we remove vertices from \n\t$\\bar{S}_i$. Since $\\abs{\\bar{S}_i} = n - \\abs{S_i}$, we have that \n\t\\begin{align*}\n\t\tM_i \\geq M_{i-1} - \\frac{2 M_{i-1}}{n - \\abs{S_{i-1}}}\n\t\t\t= \\left(1- \\frac{2}{n - \\abs{S_{i-1}}}\\right) M_{i-1},\n\t\\end{align*} \n\tfor all $i\\geq 1$. Let $k=\\abs{s-t}n$, so  $T=S_k$, and $M_k$\n\tis the value of the cut $(T,\\bar{T})$.  \n\tBy recursively applying this inequality,\n\t\\begin{align}\n\t\\label{eq:Mk-2}\n\t\tM_k \\geq M_0 \\prod_{i=1}^k \\left(1- \\frac{2}{n - \\abs{S_{i-1}}}\\right).\n\t\\end{align}\n\n\tRemoving vertices from $\\bar{S}_i$ is equivalent with the procedure of \n\tadding vertices to the $S_i$ and thus $\\abs{S_{i}} = \\abs{S_{i-2}}+2$. \n\tHence,\n\t\\begin{align*}\n\t\t\\prod_{i=1}^k \\left(1\\,- \\frac{2}{n - \\abs{S_{i-1}}}\\right)\n\t\t\t&= \\prod_{i=0}^{k-1} \\frac{n - \\abs{S_{i}}-2}{n - \\abs{S_{i}}} \\\\\n\t\t\t&= \\frac{n - \\abs{S_{0}}-2}{n - \\abs{S_{0}}} \\cdot\n\t\t\t\t\t\\frac{n - \\abs{S_{1}}-2}{n - \\abs{S_{1}}}\n\t\t\t\t\t\\cdots\n\t\t\t\t\t\\frac{n - \\abs{S_{k-1}}-2}{n - \\abs{S_{k-1}}} \\\\\n\t\t\t&= \\frac{n - \\abs{S_{k-2}}-2}{n - \\abs{S_{0}}} \\cdot\n\t\t\t\t\t\\frac{n-\\abs{S_{k-1}}-2}{n -\\abs{S_{1}}} \\\\\n\t\t\t&= \\frac{(n - tn-4)(n-tn-3)}{(n-sn)(n-sn+1)} \\\\\n\t\t\t&= \\frac{(1-t)^2n^2 -7(1-t)n + 12}{(1-s)^2 n^2 + (1-s)n} \\\\\n\t\t\t&= \\frac{(1-t)^2 -7(1-t)/n + 12/n^2}{(1-s)^2  + (1-s)/n}.\n\t\\end{align*}\n\n\n\tCombining the result for $\\prod_{i=1}^k \\left(1- \\frac{2}{\\abs{S_{i-1}}}\\right)$\n\twith Equation~\\eqref{eq:Mk}, \n\tand the result for $\\prod_{i=1}^k \\left(1- \\frac{2}{n - \\abs{S_{i-1}}}\\right)$\n\twith Equation~\\eqref{eq:Mk-2},\n\twe obtain the claim of the lemma.\n\\end{proof}\n\n"
                    }
                },
                "subsection 8.7": {
                    "name": "thm:disagreement-np-hard",
                    "content": "\nWe dedicate the rest of this section to the proof of this theorem.\nFor technical reasons, it will be\nconvenient for us to consider opinion vectors $\\begop,\\begop_0\\in[-1,1]^n$. Our\nhardness results still hold for opinions vectors $\\begop,\\begop_0\\in[0,1]^n$ by\nthe results from Section~\\ref{sec:scaling} which shows that we only lose a fixed\nconstant factor and that maximizers of optimization problems are the same under\na simple bijective transformation.\n\nWe also note that in the following, we prove that Problem~\\eqref{problem:max-disagreement-without-constraint} is \\NPhard. \nNotice that Problem~\\eqref{problem:max-disagreement-without-constraint} is a variant of \nProblem~\\eqref{problem:max-disagreement} by removing cardinality constraint, setting $\\m+A = \\MasIdx{\\DisIdx{}}$, \nand setting $\\begop_0 = -\\v+1$:\n\\begin{equation}\n\\label{problem:max-disagreement-without-constraint}\n\t\\begin{aligned}\n\t\t\\max_{\\begop} \\quad &  \\begop^{\\intercal} \\MasIdx{\\DisIdx{}} \\begop,\\\\\n\t\t\\st \\quad & \\begop(u) \\in \\{-1, 1\\} \\text{ for all } u\\in V.\n\t\\end{aligned}\n\\end{equation}  \n\nWe thus answer the question by \nChen and Racz~\\cite{chen2020network}, by setting the $k$ in their problem to be $n$.  \nWe remark the hardness of Problem~\\eqref{problem:max-disagreement-without-constraint} \nimplies hardness for Problem~\\eqref{problem:max-disagreement} as follows: \nIf we can solve the Problem~\\eqref{problem:max-disagreement}, i.e. the problem\nwith an equality constraint $\\lVert\\begop-\\begop_0\\rVert=k'$, then we can solve\nthe problem with $\\begop_0 = \\mathbf{0}$ and for all values $k'=1,\\dots,n$, and \ntake the maximum over all answers.\nThis gives us an optimal solution for Problem~\\eqref{problem:max-disagreement-without-constraint}. \nSince there are only $n$~choices\nfor~$k'$ and since we show hardness for Problem~\\eqref{problem:max-disagreement-without-constraint},\nwe obtain hardness for Problem~\\eqref{problem:max-disagreement}.\n\n\nWe first prove the hardness of two\nauxiliary problems and then give the proof of the theorem. We start by introducing\nProblem~\\eqref{problem:max-cut-variant}, which is a variant of {\\sc Max\\-Cut}; \ncompared to classic {\\sc Max\\-Cut}, we scale the objective function by factor~4 and consider the\nconstraints $\\begop\\in[-1,1]^n$ rather than $\\begop\\in\\{-1,1\\}^n$. We show that\nthe problem is \\NPhard.\n\n\\begin{problem}\n    \\label{problem:max-cut-variant}\n\tLet $G=(V,E,w)$ be an undirected weighted graph with integer edge\n\tweights and let $\\laplacian$ be the Laplacian of $G$. We want to solve the\n\tfollowing problem:\n\\begin{equation*}\n    \\begin{aligned}\n        \\max_{\\begop} \\quad & \\begop^{\\intercal} \\laplacian \\begop\\\\\n        \\st \\quad & \\begop \\in [-1,1]^n.\n    \\end{aligned}\n\\end{equation*}   \n\\end{problem}\n\n\\begin{lemma}[{\\cite{GAREY1976237}}]\n\\label{problem:two-hard}\n\tProblem~\\eqref{problem:max-cut-variant} is \\NPhard, even in unweighted graphs.\n\\end{lemma}\n\nNext, in Problem~\\eqref{problem:cut-middle-problem} we consider a version\nof Problem~\\eqref{problem:max-disagreement-without-constraint}.\n\\begin{problem}\n    \\label{problem:cut-middle-problem}\n    Let $Q$ be an integer. \n\tLet $G=(V,E,w)$ be an undirected weighted graph with integer edge\n\tweights and let $\\laplacian$ be the Laplacian of $G$.\n    We want to solve the following problem:\n\\begin{equation*}\n    \\begin{aligned}\n        \\max_{\\begop} \\quad & \\finop^{\\intercal} \\laplacian \\finop\\\\\n        \\st \\quad & \\begop \\in [-1,1]^n, \\text{ and}\\\\\n        & \\finop = (\\ID + \\frac{1}{Q}\\laplacian)^{-1} \\begop.\n    \\end{aligned}\n\\end{equation*}   \n\\end{problem}\nNote that by substituting the constraint\n$\\finop=(\\ID + \\frac{1}{Q}\\laplacian)^{-1} \\begop$ in the objective function, we\nobtain our original objective function from\nProblem~\\eqref{problem:max-disagreement} and Problem~\\eqref{problem:max-disagreement-without-constraint} \nfor $Q=1$.\n\n\nNext, we show that Problem~\\eqref{problem:cut-middle-problem} is \\NPhard.\nHere, our proof strategy is as follows.\nConsider an instance of Problem~\\eqref{problem:max-cut-variant}. Then,\nintuitively, for large $Q$ it should hold that \n$\\finop = (\\ID + \\frac{1}{Q}\\laplacian)^{-1} \\begop\n\t\\approx (\\ID + 0)^{-1} \\begop = \\begop$\nand in this case the optimal solutions of\nProblems~\\eqref{problem:cut-middle-problem} and~\\eqref{problem:max-cut-variant} \nshould be almost identical.\nIndeed, we will be able to show that both problems have the same maximizer and \nthat their objective function values are almost identical (up to rounding).\nThis implies the hardness of Problem~\\eqref{problem:cut-middle-problem} via\nLemma~\\ref{problem:two-hard}. \nWe summarize our result in the following lemma, where we use the following\nnotation: \n$\\OPTTWO(V, E, w)$ and\n$\\OPTTHREE(V, E, w)$ denote the optimal objective values of\nProblems~\\eqref{problem:max-cut-variant} and~\\eqref{problem:cut-middle-problem}, respectively,\nand $[\\cdot]$ denotes rounding to the nearest integer.\n\n\n\\begin{lemma}\n    \\label{lemma:cut-middle-cut-variat}\n\tThere exists an integer $Q$ such that $\\OPTTWO = [\\OPTTHREE]$.\n\tMoreover, the optimal solution~$\\begop^*$ for Problem~\\eqref{problem:cut-middle-problem}\n\tis also the optimal solution for Problem~\\eqref{problem:max-cut-variant}. \n    This implies that Problem~\\eqref{problem:cut-middle-problem} is \\NPhard. \n\\end{lemma} \n\n",
                    "subsubsection 8.7.1": {
                        "name": "problem:two-hard",
                        "content": "\n    Since $\\frac{1}{4} \\begop^{\\intercal} \\laplacian \\begop = \\frac{1}{2} \\sum_{i < j}w_{ij}(1 - \\ebegop{i}\\ebegop{j})$ for $\\begop \\in \\{-1, 1\\}^n$, we \n    can formulate the MaxCut problem~\\cite{goemans1995improved} as\n\t\\begin{equation*}\n\t\t\\begin{aligned}\n\t\t\t\\max_{\\begop} \\quad & \\frac{1}{4}\\begop^\\intercal \\laplacian \\begop\\\\\n\t\t\t\\st \\quad & \\begop \\in \\{-1,1\\}^n\n\t\t\\end{aligned}\n\t\\end{equation*}   \n\tThus, $\\OPT_{MaxCut}(V, E, w) = \\frac{1}{4} \\OPTTWO(V, E, w)$. \n\tGiven that MaxCut is \\NPhard in unweighted graphs, we obtain that\n\tProblem~\\ref{problem:max-cut-variant} is \\NPhard in unweighted graphs.\n\n\n"
                    },
                    "subsubsection 8.7.2": {
                        "name": "lemma:cut-middle-cut-variat",
                        "content": "\n    The proof has four steps. \n\t(1)~We argue that the optimal solution $\\begop$ for\n\tProblem~\\ref{problem:cut-middle-problem} must be from the set~$\\{-1,1\\}^n$.\n\t(2)~We derive a useful characterization of the entries $\\finop_i$ of\n\t$\\finop$.\n\t(3)~We exploit the previous characterization by showing that there exists a\n\tlarge enough $Q$ such that $[\\finop^{\\intercal} \\laplacian \\finop] = \\begop \\laplacian \\begop$. \n\t(4)~We prove that $\\begop^*$ is also an optimal solution for\n\tProblem~\\ref{problem:max-cut-variant}.\n\n\tStep~(1):~We observe that Problem~\\ref{problem:cut-middle-problem} is a\n\tconvex maximization problem over the hypercube, where the objective function\n\tis a quadratic form with a positive semidefinite matrix.\n\tThus the optimal solution must be from the set $\\{-1,1\\}^n$ (see\n\tSection~\\ref{sec:convexity}).\n\tThus, for the remainder of the proof we consider the optimization with\n\t$\\begop \\in \\{-1, 1\\}^n$. \n\n    Step~(2): We derive a useful characterization of $\\efinop{i}$, which we\n\tobtain through the updating rule of the FJ model. \n    Given an undirected graph $G' = (V, E, \\frac{1}{Q}w)$, and innate opinion $\\begop$,  \n\tif we apply the FJ model on this graph $G'$, \n    the updates of the expressed opinions follow the rule\n    \\begin{equation*}\n        \\begin{aligned}\n            z_i^{(t+1)} &= \\frac{s_i + \\frac{1}{Q}\\sum_j w_{ij}z_j^{(t)}}{1 + \\frac{1}{Q}\\sum_j w_{ij}} \\\\\n            &= \\frac{Q}{Q + \\sum_j w_{ij}} s_i + \\frac{\\sum_j w_{ij}}{Q + \\sum_j w_{ij}}\\frac{\\sum_j w_{ij}z_j^{(t)}}{\\sum_j w_{ij}}.\n        \\end{aligned}\n    \\end{equation*}\n\t\n\tNext, since\n\t$\\finop = \\lim_{t \\rightarrow \\infty} \\finop^{(t)}\n\t\t= (\\ID + \\frac{1}{Q}\\laplacian)^{-1} \\begop$\n\tis the vector of expressed opinions in the equilibrium, we obtain that for\n\tall $i$,\n\t\\begin{align*}\n\t\tz_i = \\delta_i s_i + (1 - \\delta_i) \\Delta_i,\n\t\\end{align*}\n\twhere we set $\\delta_i = \\frac{Q}{Q + \\sum_j w_{ij}}$, and $\\Delta_i =\n\t\\frac{\\sum_{j}w_{ij}z_j}{\\sum_j w_{ij}}$. \n\n\tStep~(3): We present a technical claim which bounds\n\t$\\finop^{\\intercal} \\laplacian \\finop$ by\n\t$\\begop^{\\intercal} \\laplacian \\begop$ and some small additive terms. We\n\tprove the claim at the end of the section.\n\t\\begin{claim}\n\t\\label{claim:technical}\n\t\tSuppose $\\begop\\in\\{-1,1\\}^n$ and let\n\t\t$\\finop=(\\ID+\\frac{1}{Q}\\laplacian)^{-1} \\begop$. Let $M = \\sum_{i,j=1} w_{ij}$.\n\t\tThen\n\t\t$$\\begop^{\\intercal} \\laplacian \\begop - \\frac{8}{M} - \\frac{2}{M^3}\n\t\t\t\\leq \\finop^{\\intercal} \\laplacian \\finop\n\t\t\t\\leq \\begop^{\\intercal} \\laplacian \\begop + \\frac{4}{M} + \\frac{5}{2M^3}.$$\n\t\\end{claim}\n\n    The claim implies that for any graph with sum of weights larger than 8.5,\n\ti.e., $M \\geq 17$, it holds that\n\t$\\begop^{\\intercal} \\laplacian \\begop - 0.5\n\t\t< \\begop^{\\intercal} \\laplacian \\begop - \\frac{8}{M} - \\frac{2}{M^3}$ \n    and that\n\t$\\finop^{\\intercal} \\laplacian \\finop\n\t\t\\leq \\begop^{\\intercal} \\laplacian \\begop + \\frac{4}{M} + \\frac{5}{2M^3}\n\t\t< \\begop^{\\intercal} \\laplacian \\begop + 0.5$. \n    Thus $[\\finop^{\\intercal} \\laplacian \\finop] = \\begop^{\\intercal} \\laplacian \\begop$. \n\tThis proves that the optimal solutions for Problem~\\ref{problem:cut-middle-problem}\n\tand~\\ref{problem:max-cut-variant} are identical up to rounding. This also\n\timplies that Problem~\\ref{problem:cut-middle-problem} is \\NPhard.\n    \n\tStep~(4): We prove that the optimal solution $\\begop^*$ for\n\tProblem~\\ref{problem:cut-middle-problem} is also the optimal \n\tsolution for Problem~\\ref{problem:max-cut-variant}. We prove this by\n\tcontradiction. Assume $\\begop^o$ is the optimal solution for\n\tProblem~\\ref{problem:max-cut-variant}, and $\\begop^{o\\intercal} \\laplacian\n\t\\begop^{o} > \\begop^{*\\intercal} \\laplacian \\begop^{*}$. \n\tSince the feasible areas for Problem~\\ref{problem:max-cut-variant} and\n\tProblem~\\ref{problem:cut-middle-problem} are the same, $\\begop^{o}$ is also\n\ta feasible solution for the latter. \n\tLet $\\finop^o = (\\ID + \\frac{1}{Q} \\laplacian)^{-1} \\begop^o$ and observe that since\n\t$\\begop^{o} \\in \\{-1, 1\\}^n$, it holds that  \n    $\\begop^{o\\intercal} \\laplacian \\begop^o - \\frac{8}{M} - \\frac{2}{M^2}\n\t\t\\leq \\finop^{o\\intercal} \\laplacian \\finop^o$\n\tby Claim~\\ref{claim:technical}.\n    Thus, if $M\\geq17$,\n   \t\\begin{align*}\n\t\t\\finop^{o \\intercal} \\laplacian \\finop^{o}\n\t\t&\\geq \\begop^{o\\intercal} \\laplacian \\begop^o - \\frac{8}{M} - \\frac{2}{M^3} \\\\\n\t\t&> \\begop^{*\\intercal} \\laplacian \\begop^* - \\frac{8}{M} - \\frac{2}{M^3} \\\\\n\t\t&\\geq \\begop^{*\\intercal} \\laplacian \\begop^* + 1 - \\frac{8}{M} - \\frac{2}{M^3} \\\\\n\t\t&\\geq \\finop^{*\\intercal} \\laplacian \\finop^{*},\n\t\\end{align*}\n   where we used that\n   $\\begop^{o\\intercal} \\laplacian \\begop^o > \\begop^{*\\intercal} \\laplacian \\begop^{*}$\n   by assumption, that the entries in $\\laplacian$ are integers and\n   $\\begop^{o\\intercal}\\in\\{-1,1\\}^n$. In the last step, we applied\n   Claim~\\ref{claim:technical} and the fact that\n   $1 - \\frac{8}{M} - \\frac{2}{M^3} \\geq \\frac{4}{M} + \\frac{5}{2M^3}$ for $M \\geq 13$.\n   Note that we can assume that $M\\geq 13$ since\n   Problem~\\ref{problem:max-cut-variant} is \\NPhard even for unweighted graphs\n   and then $M=\\Omega(\\abs{E})\\gg 13$.\n   Since the above inequality contradicts the fact that $\\begop^*$ is\n   optimal for Problem~\\ref{problem:cut-middle-problem}, this finishes the\n   proof.\n\n\t\\begin{proof}[Proof of Claim~\\ref{claim:technical}]\n\tFirst, let us start by rewriting $\\finop^{\\intercal} \\laplacian \\finop$,\n\twhere we use that $\\efinop{i} = \\delta_i \\ebegop{i} + (1 - \\delta_i) \\Delta_i$.\n\tThen we have\n    \\begin{equation}\n\t\\begin{aligned}\n\t\\label{equation:cut-middle-cut-variat-1}\n\t\t&\\finop^{\\intercal} \\laplacian \\finop\n\t\t= \\sum_{i,j=1}^n \\efinop{i} \\efinop{j} \\laplacian_{ij} \\\\\n\t\t&= \\sum_{i,j=1}^n (\\delta_i \\ebegop{i} + (1 - \\delta_i) \\Delta_i) (\\delta_j \\ebegop{j} + (1 - \\delta_j) \\Delta_j) \\laplacian_{ij} \\\\\n\t\t&= \\sum_{i,j=1}^n \\big[\\delta_i \\delta_j \\ebegop{i} \\ebegop{j} + (1 - \\delta_i)\\Delta_i \\delta_j \\ebegop{j} + \\delta_i \\ebegop{i} (1 - \\delta_j) \\Delta_j \\\\\n\t\t&\\quad\\quad\\quad\\quad + (1-\\delta_i)(1 - \\delta_j)\\Delta_i\\Delta_j \\big] \\laplacian_{ij}.\n    \\end{aligned}\n\t\\end{equation}\n\n\tIn the following, we split\n\tEquation~\\eqref{equation:cut-middle-cut-variat-1} into\n\t$\\sum_{i,j=1}^n \\delta_i \\delta_j \\ebegop{i} \\ebegop{j} \\laplacian_{ij}$\n\tand $\\sum_{i,j=1}^n ((1 - \\delta_i)\\Delta_i \\delta_j \\ebegop{j} + \\delta_i\n\t\t\t\\ebegop{i} (1 - \\delta_j) \\Delta_j + (1-\\delta_i)(1 -\n\t\t\t\t\\delta_j)\\Delta_i\\Delta_j) \\laplacian_{ij}$. \n\tFor both terms, we will derive upper and lower bounds.\n\tHowever, before we do that we first prove bounds for $\\delta_i$, $\\Delta_i$\n\tand for $\\begop^{\\intercal} \\laplacian \\begop$.\n      \n\tNow we derive bounds for $\\delta_i$:\n\tLet $D_{ii} = \\sum_{j=1}w_{ij}$. If we pick $Q \\geq (M^2 - 1) \\max_i D_{ii}$\n\tand using the definition $\\delta_i = \\frac{Q}{Q+\\sum_j w_{ij}}$, it follows that \n\tfor any $i$, $\\delta_i \\leq 1$ and\n    \\begin{equation}\n        \\label{equation:cut-middle-cut-variat-1-2}\n        \\begin{aligned}\n            \\delta_i \n\t\t\t\\geq \\frac{(M^2 - 1) \\max_j D_{jj}}{(M^2 - 1) \\max_j D_{jj} + D_{ii}}\n\t\t\t\\geq \\frac{(M^2 - 1) D_{ii}}{(M^2 - 1)D_{ii} + D_{ii}} = \\frac{M^2 - 1}{M^2}.\n        \\end{aligned}\n    \\end{equation}\n\t\n    Next, we argue that $ -1 \\leq \\Delta_i \\leq 1$:\n\tBy definition of $\\Delta_i$ the claim follows if we show that\n\t$-1\\leq z_j^{(t)}\\leq 1$ for all $t$ and $j$.\n\tFor $t=0$ we have that $\\efinop{j}^{(0)} = \\ebegop{j}$ and the claim\n\tis true.\n\tNow observe that the update rule for $\\efinop{i}^{(t+1)}$ implies that at\n\teach iteration it holds that,\n\t$\\min \\{s_i, z_j^{(t)} \\mid j \\neq i\\}\n\t\t\\leq \\efinop{i}^{(t+1)}\n\t\t\\leq \\max \\{s_i, z_j^{(t)} \\mid j \\neq i\\}$.\n\tNow by induction we obtain that each $\\efinop{i}$ is always upper bounded by\n\t$\\max\\{s_i \\mid i = 1, 2, \\ldots, n\\}$ \n    and lower bounded by $\\min \\{s_i \\mid i = 1, 2, \\ldots, n\\}$.\n\tSince we have that $-1\\leq\\begop_i\\leq1$ for all $i$, we obtain our result\n\tfor $\\Delta_i$.\n    \n\tNext, we obtain the following bounds on\n\t$\\begop^{\\intercal} \\laplacian \\begop$ for $\\begop \\in \\{-1, 1\\}^n$:\n\t\\begin{equation}\n        \\label{equation:cut-middle-cut-variat-1-3}\n        \\begin{aligned}\n            0 \\leq \\begop^{\\intercal} \\laplacian \\begop\n\t\t\t\\leq \\sum_{i,j} \\abs{\\laplacian_{i, j}}\n\t\t\t= 2M,\n        \\end{aligned}\n    \\end{equation}\n\twhere we used that $\\laplacian$ is positive semidefinite and that, since\n\t$G$ is an undirected graph, $M$ is twice the sum of the edge weights.\n   \n\tGiven the fact that $\\ebegop{i}$ is equal to either $1$ or $-1$, and\n\tEquation~\\eqref{equation:cut-middle-cut-variat-1-2}~\\eqref{equation:cut-middle-cut-variat-1-3},\n\tit follows that \n    \\begin{equation}\n        \\label{equation:cut-middle-cut-variat-2}\n        \\begin{aligned}\n            \\sum_{i,j=1}^n \\delta_i \\delta_j \\ebegop{i} \\ebegop{j} \\laplacian_{ij}\n\t\t\t&= \\frac{1}{2} \\sum_{i,j=1}^n w_{ij} (\\delta_i \\ebegop{i} - \\delta_j \\ebegop{j})^2  \\\\\n            &\\geq \\frac{1}{2} \\sum_{i,j=1, \\ebegop{i} \\neq \\ebegop{j}}^n w_{ij} (\\delta_i + \\delta_j)^2  \\\\\n            &\\geq \\frac{1}{2} \\sum_{i,j=1, \\ebegop{i} \\neq \\ebegop{j}}^n w_{ij} \\left(\\frac{M^2 - 1}{M^2}\\right)^2 2^2  \\\\\n            &= \\frac{1}{2} \\sum_{i,j=1}^n w_{ij} \\left(\\frac{M^2 - 1}{M^2}\\right)^2(\\ebegop{i} - \\ebegop{j})^2  \\\\\n            &= \\left(\\frac{M^2 - 1}{M^2}\\right)^2 \\begop^{\\intercal} \\laplacian \\begop \\\\\n\t\t\t&\\geq \\begop^{\\intercal} \\laplacian \\begop - \\frac{4}{M},\n        \\end{aligned}\n    \\end{equation}\n\twhere the last inequality comes from\n\tEquation~\\eqref{equation:cut-middle-cut-variat-1-3} which implies that \n    $\\left(\\frac{M^2 - 1}{M^2}\\right)^2 \\begop^{\\intercal} \\laplacian \\begop \n\t\t\\geq \\begop^{\\intercal} \\laplacian \\begop - \\frac{2}{M^2}\\begop^{\\intercal} \\laplacian \\begop\n\t\t\\geq \\begop^{\\intercal} \\laplacian \\begop - \\frac{4}{M}$.  \n\n\tSimilarly, in Equation~\\eqref{equation:cut-middle-cut-variat-3} we prove an\n\tupper bound on\n\t$\\sum_{i,j=1}^n \\delta_i \\delta_j \\ebegop{i} \\ebegop{j} \\laplacian_{ij}$,\n    \\begin{equation}\n        \\label{equation:cut-middle-cut-variat-3}\n        \\begin{aligned}\n            &\\sum_{i,j=1}^n \\delta_i \\delta_j \\ebegop{i} \\ebegop{j} \\laplacian_{ij}\n\t\t\t= \\frac{1}{2} \\sum_{i,j=1}^n w_{ij} (\\delta_i \\ebegop{i} - \\delta_j \\ebegop{j})^2  \\\\\n\t\t\t&= \\frac{1}{2}\\sum_{i,j=1, \\ebegop{i} \\neq \\ebegop{j}}^n w_{ij}(\\delta_i + \\delta_j)^2 + \\frac{1}{2} \\sum_{i,j=1, \\ebegop{i} = \\ebegop{j}}^n w_{ij}(\\delta_i - \\delta_j)^2 \\\\\n            &\\leq  \\frac{1}{2}\\sum_{i,j=1, \\ebegop{i} \\neq \\ebegop{j}}^n w_{ij} 2^2\n\t\t\t\t+ \\frac{1}{2}\\sum_{i,j=1}^n w_{ij} \\left(\\frac{1}{M^2}\\right)^2 \\\\\n            &\\leq \\begop^{\\intercal} \\laplacian \\begop + \\frac{1}{2}\\sum_{i,j=1}^n w_{ij} \\frac{1}{M^4} \\\\\n            &= \\begop^{\\intercal} \\laplacian \\begop + \\frac{1}{2M^3},\n        \\end{aligned}\n    \\end{equation}\n    where in the second line we used that\n\t$\\delta_i + \\delta_j \\leq 2$ and\n\t$\\abs{\\delta_i - \\delta_j} \\leq 1 - \\frac{M^2 - 1}{M^2} \\leq \\frac{1}{M^2}$,\n\tand in the final equality we used that $M$ is twice the sum over all edge weights.\n\n\tWe continue to bound the remaining part of $\\finop^\\intercal \\laplacian\n\t\\finop$, first we get an upper bound in\n\tEquation~\\eqref{equation:cut-middle-cut-variat-4}, \n     \\begin{equation}\n        \\label{equation:cut-middle-cut-variat-4}\n        \\begin{aligned}\n            &\\sum_{i,j=1}^n ((1 - \\delta_i)\\Delta_i \\delta_j \\ebegop{j} + \\delta_i \\ebegop{i} (1 - \\delta_j) \\Delta_j + (1-\\delta_i)(1 - \\delta_j)\\Delta_i\\Delta_j) \\laplacian_{ij} \\\\\n            &\\leq \\sum_{i,j=1}^n ((1-\\delta_i) + (1 - \\delta_j) + (1 - \\delta_i)(1 - \\delta_j))\\abs{\\laplacian_{ij}} \\\\\n            &\\leq \\sum_{i,j=1}^n \\left(\\frac{1}{M^2} +\\frac{1}{M^2} + \\frac{1}{M^4}\\right)\\abs{\\laplacian_{ij}} = \\frac{4}{M} + \\frac{2}{M^3},\n        \\end{aligned}\n    \\end{equation}\n\twhere the first inequality holds since $\\abs{\\Delta_i} \\leq 1$,\n\t$\\abs{\\delta_i} \\leq 1$ and $\\abs{s_i} \\leq 1$. \n    The second inequality holds since $\\delta_i \\geq \\frac{M^2 - 1}{M^2}$, and \n    $\\sum_{i,j}\\abs{\\laplacian_{ij}} = 2M$.\n\t\n\tSimilarly, we give a lower bound in\n\tEquation~\\eqref{equation:cut-middle-cut-variat-5}:\n     \\begin{equation}\n        \\label{equation:cut-middle-cut-variat-5}\n        \\begin{aligned}\n            &\\sum_{i,j=1}^n ((1 - \\delta_i)\\Delta_i \\delta_j \\ebegop{j} + \\delta_i \\ebegop{i} (1 - \\delta_j) \\Delta_j + (1-\\delta_i)(1 - \\delta_j)\\Delta_i\\Delta_j) \\laplacian_{ij} \\\\\n            &\\geq -\\sum_{i,j=1}^n ((1-\\delta_i) + (1 - \\delta_j) + (1 - \\delta_i)(1 - \\delta_j))\\abs{\\laplacian_{ij}} \\\\\n            &\\geq -\\sum_{i,j=1}^n \\left(\\frac{1}{M^2} +\\frac{1}{M^2} + \\frac{1}{M^4}\\right)\\abs{\\laplacian_{ij}} = -\\frac{4}{M} - \\frac{2}{M^3}.  \n        \\end{aligned}\n    \\end{equation}\n\n    Combining all together, we conclude\n\t$\\begop^{\\intercal} \\laplacian \\begop - \\frac{8}{M} - \\frac{2}{M^3} \n\t\t\\leq \\finop^{\\intercal} \\laplacian \\finop\n\t\t\\leq \\begop^{\\intercal} \\laplacian \\begop + \\frac{4}{M} + \\frac{5}{2M^3}$.\n\t\\end{proof}\n\n\\begin{proof}[Proof of Theorem~\\ref{thm:disagreement-np-hard}]\n\tWe prove that Problem~\\eqref{problem:max-disagreement-without-constraint} is \\NPhard.\n\tNotice that since the objective function of the problem is convex (see\n\tSection~\\ref{sec:convexity}) the optimal\n\tsolution~$\\begop^*$ for Problem~\\eqref{problem:max-disagreement-without-constraint} will be a\n\tvector with all entries in the set $\\{-1,1\\}$.\n\n\tConsider an input $(G=(V,E,w),Q)$ for\n\tProblem~\\eqref{problem:cut-middle-problem}. We use an algorithm for\n\tProblem~\\eqref{problem:max-disagreement-without-constraint} and apply it on the graph with edge\n\tweights $\\frac{w}{Q}$, i.e., each edge~$e$ has weight~$\\frac{w_e}{Q}$.\n\tWe note that for Problem~\\eqref{problem:max-disagreement-without-constraint} we do not assume\n\tthat the edge weights are integers.\n\n\tWe let $\\OPTONE(V, E, \\frac{w}{Q})$ denote the optimal objective value\n\tfor Problem~\\eqref{problem:max-disagreement-without-constraint} on input $G = (V, E, \\frac{w}{Q})$.  \n\tWe have:\n\t\\begin{align*}\n\t\tQ\\cdot \\OPTONE(V, E, \\frac{1}{Q}w) \n\t\t&= Q \\! \\max_{\\begop\\in\\{-1,1\\}^n}\n\t\t\t\t\\begop^{\\intercal}\n\t\t\t\t(\\ID + \\frac{1}{Q} \\laplacian)^{-1}\n\t\t\t\t\\frac{1}{Q} \\laplacian\n\t\t\t\t(\\ID + \\frac{1}{Q} \\laplacian)^{-1}\n\t\t\t\t\\begop \\\\\n\t\t&= \\max_{\\begop\\in\\{-1,1\\}^n,\\finop=(\\ID + \\frac{1}{Q} \\laplacian)^{-1}\\begop}\n\t\t\t\t\\finop^{\\intercal} \\laplacian \\finop \\\\\n\t\t&= \\OPTTHREE(V, E, w).\n\t\\end{align*}\n\n\tWe can now apply\n\tLemma~\\ref{lemma:cut-middle-cut-variat} and it follows that\n\tProblem~\\eqref{problem:max-disagreement-without-constraint} is~\\NPhard.  \n\n\tThe fact that Problem~\\eqref{problem:max-disagreement} and Problem~\\eqref{eq:our-problem} \n\tare \\NPhard follow immediately\n\tfrom the result, and the analysis from the beginning of the section.\n\\end{proof}\n\n"
                    }
                },
                "subsection 8.8": {
                    "name": "cor:our-problem-np-hard",
                    "content": "\n\nGiven that Problem~\\eqref{problem:max-disagreement-without-constraint} is \\NPhard,\nwe proceed to prove that Problem~\\eqref{eq:our-problem} is \\NPhard when $k = \\frac{n}{2}$.\nWe denote the latter problem with cardinality constraint $k = \\frac{n}{2}$ as Problem~$C$. \nWe apply the same technique that shows that Bisection is \\NPhard\ngiven that {\\sc Max\\-Cut} is \\NPhard~\\cite{garey1974some}. \n\nConsider an instance of\nProblem~\\eqref{problem:max-disagreement-without-constraint} with $n'=k$ vertices, \nwhere the disagreement index matrix is represented as \n$\\DisIdx{\\laplacian} = (\\ID + \\laplacian)^{-1} \\laplacian (\\ID + \\laplacian)^{-1}$. \nWe construct an instance of Problem $C$ by adding $n'$ isolated nodes. Note\nthat in the new instance we have $2n' = n$ vertices and $k=\\frac{n}{2} = n'$.\n\nThe Laplacian of the graph after adding the isolated nodes becomes\n$\\laplacian' =\n\\begin{bmatrix}\n\\m+0 & \\m+0 \\\\\n\\m+0 & \\laplacian\n\\end{bmatrix}\n$. Thus, we can express the disagreement index matrix \nof our new instance for Problem~$C$ as:\n\\begin{displaymath}\n\\begin{aligned}\n\\DisIdx{\\laplacian'}\n\t&= \\begin{bmatrix}\n\t\t\t\\m+I & \\m+0 \\\\\n\t\t\t\\m+0 & \\ID + \\laplacian\n\t\t\\end{bmatrix}^{-1}\n\t\t\\begin{bmatrix}\n\t\t\t\\m+0 & \\m+0 \\\\\n\t\t\t\\m+0 & \\laplacian\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\m+I & \\m+0 \\\\\n\t\t\t\\m+0 & \\ID + \\laplacian\n\t\t\\end{bmatrix}^{-1} \\\\\n\t&= \\begin{bmatrix}\n\t\t\t\\m+I & \\m+0 \\\\\n\t\t\t\\m+0 & (\\ID + \\laplacian)^{-1}\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\m+0 & \\m+0 \\\\\n\t\t\t\\m+0 & \\laplacian\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t\\m+I & \\m+0 \\\\\n\t\t\t\\m+0 & (\\ID + \\laplacian)^{-1}\n\t\t\\end{bmatrix} \\\\\n\t&= \\begin{bmatrix}\n\t\t\t\\m+0 & \\m+0 \\\\\n\t\t\t\\m+0 & (\\ID + \\laplacian)^{-1}\\laplacian(\\ID + \\laplacian)^{-1}\n\t\t\\end{bmatrix} \\\\\n\t&= \\begin{bmatrix}\n\t\t\t\\m+0 & \\m+0 \\\\\n\t\t\t\\m+0 & \\DisIdx{\\laplacian}\n\t\t\\end{bmatrix},\n\\end{aligned}\n\\end{displaymath}\ni.e., $\\DisIdx{\\laplacian'}$ can be partitioned into four blocks, where\neach block is an $n' \\times n'$ matrix, with three of them being $\\mathbf{0}$ matrices. \nThe remaining block is the $n' \\times n'$ matrix $\\DisIdx{\\laplacian}$ from our\ninstance of Problem~\\eqref{problem:max-disagreement-without-constraint}.\n\nNow consider the solution~$\\begop$ of our instance $\\DisIdx{\\laplacian'}$ for\nProblem~$C$. Observe that the solution's objective function value is given by\n$\\begop^\\intercal \\DisIdx{\\laplacian'} \\begop$. Note that, due to the\nblock structure of $\\DisIdx{\\laplacian'}$, only the final $n'$ entries of\n$\\begop$ create a non-negative disagreement. Furthermore, the first $n'$\nentries of $\\begop$ do not contribute to the final disagreement and can be set\narbitrarily.\n\nHence, since our cardinality constraint is $k=n'$, the algorithm can pick the\nsolution for the final $n'$~vertices such as to maximize the disagreement\n$\\DisIdx{\\laplacian}$ on our instance of\nProblem~\\eqref{problem:max-disagreement-without-constraint}; the first\n$n'$~entries of $\\begop$ can be picked such that the cardinality constraint is\nsatisfied.  Therefore, the optimal objective function values of our new instance\nand the original instance for\nProblem~\\eqref{problem:max-disagreement-without-constraint} are identical.\n\nConsequently, we conclude that\nProblem~\\eqref{problem:max-disagreement-without-constraint} can be solved by\nutilizing a solver for \nProblem~$C$, and the reduction is in polynomial time.\n\\balance\n\n"
                }
            }
        },
        "tables": {
            "tab:disagreement-small": "\\begin{table*}[t]\n\t\\centering\n\t\\caption{Results on the small datasets for the relative increase of the\n\t\tdisagreement, where we set $k = 10\\%\\,n$. For each dataset, we have marked the\n\t\thighest value in bold and we have made the highest value for each\n\t\tsetting italic.}\n\t\\label{tab:disagreement-small}\n\t\\resizebox{\\textwidth}{!}{%\n\t\t\\input{tables/disagreement_small_extended.tex}\n\t}\n\\end{table*}",
            "tab:disagreement-large": "\\begin{table*}\n\t\\centering\n\t\\caption{Results on the large datasets for the relative increase of the\n\t\tdisagreement, where we set $k = 1\\%\\,n$. For each dataset, we have marked the\n\t\thighest value in bold and we have made the highest value for each setting italic.}\n\t\\label{tab:disagreement-large}\n\t\\resizebox{\\textwidth}{!}{%\n\t\t\\input{tables/disagreement_large.tex}\n\t}\n\\end{table*}",
            "tab:polarization-large": "\\begin{table*}[t]\n\\centering\n\\caption{Results on the larger datasets for the relative increase of the\n\tpolarization, where we set $k = 1\\% n$. For each dataset, we have marked the\n\thighest value in bold and we have made the highest value for each setting italic.}\n\\label{tab:polarization-large}\n\\resizebox{0.9\\textwidth}{!}{%\n\t\\input{tables/polarization_large.tex}\n}\n\\end{table*}",
            "tab:polarization-small-addition": "\\begin{table*}[t]\n\t\\centering\n\t\\caption{Results on the small datasets for the relative increase of the\n\t\tpolarization, where we set $k = 10\\%\\,n$. For each dataset, we have marked the\n\t\thighest value in bold and we have made the highest value for each\n\t\tsetting italic.}\n\t\\label{tab:polarization-small-addition}\n\t\\resizebox{0.9\\textwidth}{!}{%\n\t\t\\input{tables/polarization_small_all.tex}\n\t}\n\\end{table*}",
            "tab:small_running_time": "\\begin{table}[t]\n\\centering\n\\caption{Running time (in seconds) of our algorithms for maximizing the disagreement, where we set $k = 10\\% n$.}\n\\label{tab:small_running_time}\n\\resizebox{0.6\\textwidth}{!}{%\n\t\\input{tables/small_running_time}\n}\n\\end{table}",
            "tab:selected_ratio": "\\begin{table*}[t]\n\t\\centering\n\t\\caption{The approximation ratio of Algorithm~\\ref{alg:SDP-based} for some\n\t\tvalues of $\\alpha$, and the corresponding choice of $\\beta$.}\n\t\\label{tab:selected_ratio}\n\t\\resizebox{0.3\\textwidth}{!}{%\n\t\t\\input{tables/selected_ratio.tex}\n\t}\n\t\\end{table*}"
        },
        "figures": {
            "fig:approx-all": "\\begin{figure}[t]\n\t\\centering \n        \\resizebox{0.40\\textwidth}{!}{%\n\t\t\t\\inputrealtikz{tikz/approx_ratio}\n\t\t}\\\\\n\t\\caption{\n\tThe approximation ratio from Theorem~\\ref{thm:unbalanced} as function\n\tof~$\\alpha$. We present numerical approximation ratio results compared\n\twith a piece-wise quadratic function defined by the formulas: \n\t$2.059\\alpha^2$ for $0 < \\alpha < 0.448$, \n\t$1.36(1-\\alpha)^2$ for $0.448\\leq \\alpha < 0.5$, \n\t$1.36\\alpha^2$ for $0.5 \\leq \\alpha < 0.552$, \n\tand $2.059(1-\\alpha)^2$ for $0.552 \\leq \\alpha < 1$. }\n\t\\label{fig:approx-all}\n\\end{figure}",
            "fig:regression": "\\begin{figure*}[t]\n\t\\centering \n\t\\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{ccc}\n        \\resizebox{0.3\\textwidth}{!}{%\n\t\t\t\\inputtikz{tikz/regression_disagreement}\n\t\t}&\n        \\resizebox{0.3\\textwidth}{!}{%\n\t\t\t\\inputtikz{tikz/regression_avg}\n\t\t}&\n        \\resizebox{0.3\\textwidth}{!}{%\n\t\t\t\\inputtikz{tikz/regression_std}\n\t\t}\\\\\n\t\t(a)~initial disagreement, $R^2=0.08$ &\n\t\t(b)~average innate opinion, $R^2=0.17$ &\n\t\t(c) standard deviation of opinions, $R^2=0.62$ \\\\\n\t\\end{tabular}\n\t}\n\t\\caption{Regression analysis of dataset parameters and the ratio between the\n\t\tbest methods with full and limited information.\n\t\t}\n\t\\label{fig:regression}\n\\end{figure*}",
            "fig:scale-k": "\\begin{figure}[t]\n\t\\centering \n    \\begin{tabular}{cc}\n        \\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/twitterl2dis}\n\t\t}&\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/gplusl2dis}\n\t\t}\\\\\n\t\t(a)~\\TweetLTWO & (b)~\\GplusLTWO \\\\\n\t\\end{tabular}\n\t\\caption{Results for the relative increase of the\n\t\tdisagreement on \\TweetLTWO and \\GplusLTWO. Here, we vary\n\t\t$k=0.5\\%\\,n, 1\\%\\,n, \\dots, 2.5\\%\\,n$.}\n\t\\label{fig:scale-k}\n\\end{figure}",
            "fig:evaluate-disagreement": "\\begin{figure}[t]\n\t\\centering \n    \\begin{tabular}{cc}\n        \\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/reddit}\n\t\t}&\n\t\t\\resizebox{0.37\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/real_twitter}\n\t\t}\\\\\n\t\t(a)~\\Reddit & (b)~\\Twitter \\\\\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/polblogs}\n\t\t}&\n\t\t\\resizebox{0.37\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/polbooks}\n\t\t}\\\\\n\t\t(c)~\\blogs & (d)~\\books \\\\\n\t\\end{tabular}\n\t\\caption{Results for Problem~\\eqref{eq:our-problem} on different datasets.\n\t\tWe varied $k = 10\\%n,15\\%n,...,50\\%n$.}\n\t\\label{fig:evaluate-disagreement}\n\\end{figure}",
            "fig:evaluate-disagreement-std": "\\begin{figure}[t]\n\t\\centering \n    \\begin{tabular}{cc}\n        \\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/reddit_std}\n\t\t}&\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/twitter_std}\n\t\t}\\\\\n\t\t(a)~\\Reddit & (b)~\\Twitter \\\\\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/polblogs_std}\n\t\t}&\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/polbooks_std}\n\t\t}\\\\\n\t\t(c)~\\blogs & (d)~\\books \\\\\n\t\\end{tabular}\n\t\\caption{Results of our randomized algorithms for\n\t\tProblem~\\eqref{eq:our-problem} on different datasets.  We varied\n\t\t$k = 10\\%n,15\\%n,...,30\\%n$ and report averages and standard deviations \n\t\tover 5~runs of the algorithms.}\n\t\\label{fig:evaluate-disagreement-std}\n\\end{figure}",
            "fig:evaluate-disagreement-large-std": "\\begin{figure}[t]\n\t\\centering \n    \\begin{tabular}{cc}\n        \\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/twitterl2_std}\n\t\t}&\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/gplusl2_std}\n\t\t}\\\\\n\t\t(a)~\\TweetLTWO & (b)~\\GplusLTWO \\\\\n\t\\end{tabular}\n\t\\caption{Results of $\\Random\\LimitedInfo$ for Problem~\\eqref{eq:our-problem}\n\t\ton different datasets.  We varied $k = 0.5\\%n, 1\\%n,...,2.5\\%n$ and\n\t\treport averages and standard deviations over 5~runs of the algorithms.}\n\t\\label{fig:evaluate-disagreement-large-std}\n\\end{figure}",
            "fig:large-dis-time": "\\begin{figure}[t]\n\t\\centering \n    \\begin{tabular}{cc}\n        \\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/twitterl2_dis_time}\n\t\t}&\n\t\t\\resizebox{0.3\\columnwidth}{!}{%\n\t\t\t\\inputtikz{tikz/gplusl2_dis_time}\n\t\t}\\\\\n\t\t(a)~\\TweetLTWO & (b)~\\GplusLTWO \\\\\n\t\\end{tabular}\n\t\\caption{Running time (in seconds) of our algorithms for increasing the\n\t\t\tdisagreement on \\TweetLTWO and \\GplusLTWO. Here, we vary\n\t\t\t$k=0.5\\%n$, $1\\%n$, $1.5\\%n$, $2\\%n$, $2.5\\%n$.}\n\t\\label{fig:large-dis-time}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\label{eq:update-opinions}\n\tz_u^{(t)}\n\t= \\frac{s_u + \\sum_{v\\in N(u)} w_{uv} z_v^{(t-1)}}{1 + \\sum_{v\\in N(u)} w_{uv}}.\n\\end{align}",
            "eq:2": "\\begin{align}\n\\begin{aligned}\n\\label{eq:our-problem}\n\t\\max_{\\begop} \\quad & \\begop^{\\intercal} \\m+A \\begop,\\\\\n\t\\st \\quad &\\lVert \\begop \\rVert_0 = k, \\text{ and} \\\\\n\t\t& \\begop \\in [0,1]^n.\n\\end{aligned}\n\\end{align}",
            "eq:3": "\\begin{equation}\n\\label{problem:maxcut-unbalanced}\n    \\begin{aligned}\n        \\max_{\\v+x} \\quad \\frac{1}{4} & \\v+x^{\\intercal} \\m+A \\v+x ,\\\\\n        \\st \\quad & \\lVert \\v+x + \\v+1 \\rVert_0 = \\alpha n, \\text{ and} \\\\\n\t\t\t\t  & \\v+x \\in [-1,1]^n.\n    \\end{aligned}\n\\end{equation}",
            "eq:4": "\\begin{align}\n\\begin{aligned}\n\\label{problem:maxcut-unbalaced-relax}\n\t\\max_{\\v+v_1,\\dots,\\v+v_n} \\quad & \\frac{1}{4} \\sum_{ij} \\m+A_{ij}\\v+v_i^{\\intercal} \\v+v_j,\\\\\n\t\\st \\quad & \\sum_{i<j} \\v+v_i^{\\intercal} \\v+v_j = \\frac{1}{2}n^2(1 - 2 \\alpha)^2-\\frac{n}{2}, \\text{ and} \\\\\n\t& \\v+v_i \\in \\Real^n, \\quad\\quad \\|\\v+v_i\\|_2 = 1.\n\\end{aligned}\n\\end{align}",
            "eq:5": "\\begin{align*}\n\t\\begop_u' = \\begop_u \\cdot \\frac{y-x}{b-a}\n\t\t\t\t+ \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right).\n\\end{align*}",
            "eq:6": "\\begin{align*}\n\tf(a) &= a \\frac{y-x}{b-a} + \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right) \\\\\n\t\t&= \\frac{1}{2} \\left( 2a \\frac{y-x}{b-a} + x + y - \\frac{a+b}{b-a}(y-x)\\right) \\\\\n\t\t&= \\frac{1}{2} \\left( x + y - \\frac{b-a}{b-a} (y-x) \\right) \\\\\n\t\t&= x,\n\\end{align*}",
            "eq:7": "\\begin{align*}\n\tf(b) &= b \\frac{y-x}{b-a} + \\frac{1}{2}\\left( x+y - \\frac{a+b}{b-a}(y-x) \\right) \\\\\n\t\t&= \\frac{1}{2} \\left( 2b \\frac{y-x}{b-a} + x + y - \\frac{a+b}{b-a}(y-x)\\right) \\\\\n\t\t&= \\frac{1}{2} \\left( x + y - \\frac{a-b}{b-a} (y-x) \\right) \\\\\n\t\t&= y.\n\\end{align*}",
            "eq:8": "\\begin{align*}\n\t\\finop_u'^{(t+1)}\n\t&= \\frac{\\begop_u' + \\sum_{v \\in N(u)} w_{uv} \\finop_u'^{(t)}}{1 + \\sum_{v\\in N(u)} w_{uv}} \\\\\n\t&= \\frac{\\alpha \\begop_u + \\beta + \\sum_{v \\in N(u)} w_{uv} (\\alpha \\finop_u^{(t)} + \\beta)}{1 + \\sum_{v\\in N(u)} w_{uv}} \\\\\n\t&= \\alpha \\frac{\\begop_u' + \\sum_{v \\in N(u)} w_{uv} \\finop_u'^{(t)}}{1 + \\sum_{v\\in N(u)} w_{uv}} + \\beta \\\\\n\t&= \\alpha \\finop_u^{(t+1)} + \\beta.\n\\end{align*}",
            "eq:9": "\\begin{align*}\n\t\\DisIdx{G, \\begop'}\n\t&= \\sum_{(u,v)\\in E} w_{u,v} (\\efinop{u}'-\\efinop{v}')^2 \\\\\n\t&= \\sum_{(u,v)\\in E} w_{u,v} (\\alpha \\efinop{u} + \\beta - \\alpha \\efinop{v} - \\beta)^2 \\\\\n\t&= \\alpha^2 \\sum_{(u,v)\\in E} w_{u,v} (\\efinop{u} - \\efinop{v})^2 \\\\\n\t&= \\alpha^2 \\DisIdx{G, \\begop'}.\n\\end{align*}",
            "eq:10": "\\begin{align*}\n\t\\bar{\\finop}'\n\t&= \\frac{1}{n} \\sum_u \\finop_u' \\\\\n\t&= \\frac{1}{n} \\sum_u (\\alpha \\finop_u + \\beta) \\\\\n\t&= \\alpha \\left(\\frac{1}{n} \\sum_u \\finop_u\\right) + \\beta \\\\\n\t&= \\alpha \\bar{\\finop} + \\beta.\n\\end{align*}",
            "eq:11": "\\begin{align*}\n\t\\PolIdx{G, \\begop'}\n\t&= \\sum_{u\\in V} (\\finop_u' - \\bar{\\finop}')^2 \\\\\n\t&= \\sum_{u\\in V} (\\alpha \\finop_u + \\beta - \\alpha \\bar{\\finop} - \\beta)^2 \\\\\n\t&= \\alpha^2 \\sum_{u\\in V} (\\finop_u - \\bar{\\finop})^2 \\\\\n\t&= \\alpha^2 \\PolIdx{G, \\begop'}.\n\\end{align*}",
            "eq:12": "\\begin{align*}\n\t\\MasIdx{\\PolIdx{}}\\v+1\n\t= (\\ID + \\laplacian)^{-1} \\left(\\ID - \\frac{\\ind \\ind^\\intercal}{n}\\right) (\\ID + \\laplacian)^{-1}\\v+1\n\t= (\\ID + \\laplacian)^{-1} \\left(\\ID - \\frac{\\ind \\ind^\\intercal}{n}\\right) \\v+1\n\t= \\v+0.\n\\end{align*}",
            "eq:13": "\\begin{align*}\n\t\\MasIdx{\\DisIdx{}} \\v+1\n\t= (\\laplacian + \\ID)^{-1} \\laplacian (\\laplacian + \\ID)^{-1} \\v+1\n\t= (\\laplacian + \\ID)^{-1} \\laplacian \\v+1\n\t= \\v+0.\n\\end{align*}",
            "eq:14": "\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&=\n\t\t\t\\frac{(c\\v+1+\\epsvec+\\Deltaalg)^\\intercal \\m+A (c\\v+1+\\epsvec+\\Deltaalg)}\n\t\t\t\t{(c\\v+1+\\epsvec+\\Deltaopt)^\\intercal \\m+A (c\\v+1+\\epsvec+\\Deltaopt)} \\\\\n\t\t&=\n\t\t\t\\frac{(\\epsvec+\\Deltaalg)^\\intercal \\m+A (\\epsvec+\\Deltaalg)}\n\t\t\t\t{(\\epsvec+\\Deltaopt)^\\intercal \\m+A (\\epsvec+\\Deltaopt)} \\\\\n\t\t&=\n\t\t\t\\frac{\\Deltaalg^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ 2 \\epsvec^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ \\epsvec^\\intercal \\m+A \\epsvec}\n\t\t\t\t{\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+ 2 \\epsvec^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\\Deltaalg^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ (1-2\\gamma_1)\\epsvec^\\intercal \\m+A \\epsvec}\n\t\t\t\t{\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+ 2 \\epsvec^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\\Deltaalg^\\intercal \\m+A \\Deltaalg\n\t\t\t\t+ (1-2\\gamma_1)\\epsvec^\\intercal \\m+A \\epsvec}\n\t\t\t\t{2 (\\Deltaopt^\\intercal \\m+A \\Deltaopt\n\t\t\t\t+\\epsvec^\\intercal \\m+A \\epsvec)},\n\t\\end{align*}",
            "eq:15": "\\begin{align*}\n\t\t&\\Deltaalg^\\intercal \\m+A \\Deltaalg \\\\\n\t\t&=\n\t\t\t(1-c)^2 \\v+1_{\\vert \\ALG}^\\intercal \\m+A \\v+1_{\\vert \\ALG}\n\t\t\t- 2(1-c) \\epsvec_{\\vert \\ALG}^\\intercal \\m+A \\v+1_{\\vert \\ALG} \\\\\n\t\t\t&\\quad + \\epsvec_{\\vert \\ALG}^\\intercal \\m+A \\epsvec_{\\vert \\ALG} \\\\\n\t\t&\\geq \n\t\t\t(1-c)^2 \\v+1_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG}\n\t\t\t- 2(1-c) \\epsvec_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG} \\\\\n\t\t&\\geq \n\t\t\t(1-c)^2 \\v+1_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG}\n\t\t\t- 2 (1-c) \\gamma_3 \\epsvec^\\intercal \\m+A \\epsvec,\n\t\\end{align*}",
            "eq:16": "\\begin{align*}\n\t\t& \\Deltaopt^\\intercal \\m+A \\Deltaopt \\\\\n\t\t&=\n\t\t\t(1-c)^2 \\v+1_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT}\n\t\t\t- 2(1-c) \\epsvec_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT} \\\\\n\t\t\t&\\quad + \\epsvec_{\\vert\\OPT}^\\intercal \\m+A \\epsvec_{\\vert\\OPT} \\\\\n\t\t&\\leq\n\t\t\t(1-c)^2 \\v+1_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT}\n\t\t\t+ (2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec,\n\t\\end{align*}",
            "eq:17": "\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\v+1_{\\vert\\ALG}^\\intercal \\m+A \\v+1_{\\vert\\ALG}\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec\n\t\t\t}{\n\t\t\t\t2( (1-c)^2 \\v+1_{\\vert\\OPT}^\\intercal \\m+A \\v+1_{\\vert\\OPT}\n\t\t\t\t\t+ (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)\n\t\t\t}.\n\t\\end{align*}",
            "eq:18": "\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\beta \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec,\n\t\t\t}{\n\t\t\t\t2( (1-c)^2 \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t\t+ (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)\n\t\t\t}.\n\t\\end{align*}",
            "eq:19": "\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&\\geq\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\beta \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec,\n\t\t\t}{\n\t\t\t\t4(1-c)^2 \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t} \\\\\n\t\t&\\geq \\frac{\\beta}{4}.\n\t\\end{align*}",
            "eq:20": "\\begin{align*}\n\t\t&\\frac{\\begopalg^\\intercal \\m+A \\begopalg}{\\begopopt^\\intercal \\m+A \\begopopt} \\\\\n\t\t&>\n\t\t\t\\frac{\n\t\t\t\t(1-c)^2 \\beta \\begopoptp^\\intercal \\m+A \\begopoptp\n\t\t\t\t+ (1 - 2\\gamma_1 - 2 (1-c) \\gamma_3) \\epsvec^\\intercal \\m+A \\epsvec\n\t\t\t}{\n\t\t\t\t4 (1 + 2(1-c)\\gamma_3+\\gamma_2) \\epsvec^\\intercal \\m+A \\epsvec)\n\t\t\t} \\\\\n\t\t&\\geq \n\t\t\t\\frac{\n\t\t\t\t(1 - 2\\gamma_1 - 2 (1-c) \\gamma_3)\n\t\t\t}{\n\t\t\t\t4 (1 + 2(1-c)\\gamma_3+\\gamma_2)\n\t\t\t}.\n\t\\end{align*}",
            "eq:21": "\\begin{equation}\n    \\label{eq:convex}\n\t\t\\begin{aligned}\n\t\t\t\\max_{\\begop} \\quad &  \\begop^{\\intercal} \\m+A \\begop,\\\\\n\t\t\t\\st \\quad & \\begop \\in [-1,1]^n, \\text{ and}\\\\\n\t\t\t& \\lVert \\begop - \\begop_0 \\rVert_0 \\leq k.\n\t\t\\end{aligned}\n\t\\end{equation}",
            "eq:22": "\\begin{equation*}\n\t\t\\begin{aligned}\n\t\t&\\Prob{ Z_p \\leq (1-\\epsilon)\\left(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)\\right) } \\\\\n\t\t&= \\Prob{\\frac{\\beta}{4} + 1 - Z_p \\geq \\frac{\\beta}{4} + 1 -\n\t\t\t\\left((1-\\epsilon)\\left(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta\n\t\t\t\t\t\t\\right)\\right)} \\\\\n\t\t&\\leq \\frac{\\frac{\\beta}{4} + 1 - \\Exp[Z_p]}{\\frac{\\beta}{4} + 1 - ((1-\\epsilon)(\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta))} \\\\\n\t\t&\\leq \\frac{\\frac{\\beta}{4} + 1 - (\\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta)}{\\frac{\\beta}{4} + 1 - ((1-\\epsilon)( \\frac{2}{\\pi} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta))} \\\\\n\t\t&= \\frac{1 - c}{1 - (1-\\epsilon)c},\n\t\t\\end{aligned}\n\t\\end{equation*}",
            "eq:23": "\\begin{equation*}\n\t\t\\begin{aligned}\n\t\t\t\\left(\\frac{1 - c}{1 - (1-\\epsilon)c}\\right)^\\kappa\n\t\t\t&= \\left(1 - \\frac{1}{1+ \\frac{1-c}{\\epsilon c}}\\right)^\\kappa \\\\\n\t\t\t&= \\left(1 - \\frac{1}{1+ \\frac{1-c}{\\epsilon c}}\\right)^{(1+ \\frac{1-c}{\\epsilon c}) \\frac{\\epsilon c}{1 - c + \\epsilon c}\\kappa} \\\\\n\t\t\t&\\leq \\exp\\left(-\\frac{\\epsilon c}{1 - c + \\epsilon c}\\kappa \\right),\n\t\t\\end{aligned}\n\t\\end{equation*}",
            "eq:24": "\\begin{align*}\n\t\\max_{0<\\beta <7}\n\t\\quad\n\t\\min_{0< s_1 < \\alpha \\leq 0.5, \\,\\, 0<\\alpha \\leq s_2 \\leq 0.5}\n\t&\\left\\{\n\t\t\\left( \\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta - \\beta\n\t\t\t\t(1-s_1)s_1\\right)\\frac{(1-\\alpha)^2}{(1-s_1)^2}, \\right. \\\\\n\t\t&\\quad \\left. \\left(\\frac{\\pi}{2} + 0.878 \\alpha (1-\\alpha) \\cdot \\beta - \\beta (1-s_2)s_2\\right)\\frac{\\alpha^2}{s_2^2}\n\t\\right\\}.\n\t\\end{align*}",
            "eq:25": "\\begin{equation}\n\\label{problem:max-disagreement-without-constraint}\n\t\\begin{aligned}\n\t\t\\max_{\\begop} \\quad &  \\begop^{\\intercal} \\MasIdx{\\DisIdx{}} \\begop,\\\\\n\t\t\\st \\quad & \\begop(u) \\in \\{-1, 1\\} \\text{ for all } u\\in V.\n\t\\end{aligned}\n\\end{equation}",
            "eq:26": "\\begin{equation*}\n\t\t\\begin{aligned}\n\t\t\t\\max_{\\begop} \\quad & \\frac{1}{4}\\begop^\\intercal \\laplacian \\begop\\\\\n\t\t\t\\st \\quad & \\begop \\in \\{-1,1\\}^n\n\t\t\\end{aligned}\n\t\\end{equation*}",
            "eq:27": "\\begin{equation*}\n        \\begin{aligned}\n            z_i^{(t+1)} &= \\frac{s_i + \\frac{1}{Q}\\sum_j w_{ij}z_j^{(t)}}{1 + \\frac{1}{Q}\\sum_j w_{ij}} \\\\\n            &= \\frac{Q}{Q + \\sum_j w_{ij}} s_i + \\frac{\\sum_j w_{ij}}{Q + \\sum_j w_{ij}}\\frac{\\sum_j w_{ij}z_j^{(t)}}{\\sum_j w_{ij}}.\n        \\end{aligned}\n    \\end{equation*}",
            "eq:28": "\\begin{align*}\n\t\tz_i = \\delta_i s_i + (1 - \\delta_i) \\Delta_i,\n\t\\end{align*}",
            "eq:29": "\\begin{align*}\n\t\t\\finop^{o \\intercal} \\laplacian \\finop^{o}\n\t\t&\\geq \\begop^{o\\intercal} \\laplacian \\begop^o - \\frac{8}{M} - \\frac{2}{M^3} \\\\\n\t\t&> \\begop^{*\\intercal} \\laplacian \\begop^* - \\frac{8}{M} - \\frac{2}{M^3} \\\\\n\t\t&\\geq \\begop^{*\\intercal} \\laplacian \\begop^* + 1 - \\frac{8}{M} - \\frac{2}{M^3} \\\\\n\t\t&\\geq \\finop^{*\\intercal} \\laplacian \\finop^{*},\n\t\\end{align*}"
        },
        "git_link": "https://github.com/SijingTu/KDD-23-Adversaries-With-Limited-Information"
    }
}