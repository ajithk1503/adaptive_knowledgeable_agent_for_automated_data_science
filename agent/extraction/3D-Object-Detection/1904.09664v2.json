{
    "meta_info": {
        "title": "Deep Hough Voting for 3D Object Detection in Point Clouds",
        "abstract": "Current 3D object detection methods are heavily influenced by 2D detectors.\nIn order to leverage architectures in 2D detectors, they often convert 3D point\nclouds to regular grids (i.e., to voxel grids or to bird's eye view images), or\nrely on detection in 2D images to propose 3D boxes. Few works have attempted to\ndirectly detect objects in point clouds. In this work, we return to first\nprinciples to construct a 3D detection pipeline for point cloud data and as\ngeneric as possible. However, due to the sparse nature of the data -- samples\nfrom 2D manifolds in 3D space -- we face a major challenge when directly\npredicting bounding box parameters from scene points: a 3D object centroid can\nbe far from any surface point thus hard to regress accurately in one step. To\naddress the challenge, we propose VoteNet, an end-to-end 3D object detection\nnetwork based on a synergy of deep point set networks and Hough voting. Our\nmodel achieves state-of-the-art 3D detection on two large datasets of real 3D\nscans, ScanNet and SUN RGB-D with a simple design, compact model size and high\nefficiency. Remarkably, VoteNet outperforms previous methods by using purely\ngeometric information without relying on color images.",
        "author": "Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas",
        "link": "http://arxiv.org/abs/1904.09664v2",
        "category": [
            "cs.CV"
        ],
        "additionl_info": "ICCV 2019"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n% \\begin{itemize}\n%     \\item 3D object detection is important\n%     \\item Limitations in current methods (3D CNNs, birdâ€™s eye view detector, frustum-based detector)\n%     \\item What we want: generic, 3D-based, efficient (leverages sparsity) 3D detector\n%     \\item Naive way: propose objects on every (subsampled) existing points. Challenges: existing points can be far away from object centers.\n%     \\item Our approach: deep Hough voting.\n%     \\item We achieved SOTA results. Key lessons/conclusions.\n% \\end{itemize}\n\n\n\nThe goal of 3D object detection is to localize and recognize objects in a 3D scene. More specifically, in this work, we aim to estimate oriented 3D bounding boxes as well as semantic classes of objects from point clouds.\n% This is a fundamental task for applications ranging from augmented- and mixed-reality, all the way to home assistant robots and self-driving cars. %In contrast to 2D object detection, the 3D detection problem involves both 3D input and 3D output.\n% The input is often a point cloud from either depth cameras or range sensors such as LIDAR.\n%\n\nCompared to images, 3D point clouds provide accurate geometry and robustness to illumination changes. On the other hand, point clouds are irregular.\n% and inherently sparse, as depth sensors only capture 2D surfaces of objects.\nthus typical CNNs are not well suited to process them directly.\n% In addition, the output are oriented 3D bounding boxes that have more degrees of freedom compared to the 2D case, which results in a much larger search space. % for 3D object proposals.\n\n\n\nTo avoid processing irregular point clouds, current 3D detection methods heavily rely on 2D-based detectors in various aspects. For example, \\cite{song2016deep,hou20183d} extend 2D detection frameworks such as the Faster/Mask R-CNN~\\cite{ren2015faster,he2017mask} to 3D. They voxelize the irregular point clouds to regular 3D grids and apply 3D CNN detectors, which fails to leverage sparsity in the data and suffer from high computation cost due to expensive 3D convolutions.\n% Note that this search is unavoidable in those detectors, because the centroid of 3D objects can be in those empty spaces.\nAlternatively,~\\cite{cvpr17chen,zhou2018voxelnet} project points to regular 2D bird's eye view images and then apply 2D detectors to localize objects. This, however, sacrifices geometric details which may be critical in cluttered indoor environments. % and robotics use cases.\n%which suits well for simple scenes in the driving scenario.\n% but loses 3D geometry due to the projection.\n% A bird's eye view is suitable in outdoor driving scenes where objects are well-separated and there is rarely an object on top of another--- assumptions that cease to hold in cluttered indoor environments and robotics use cases.\n% To avoid a lossy quantization process, a third family of works leverages a 2D view to localize and restrict the 3D search space to a frustum shaped region. These, however, are too dependant on the 2D detector stage and may entirely miss an object if it is not seen by the 2D detector due to occlusion or illumination challenges.\nMore recently,~\\cite{lahoud20172d,qi2018frustum} proposed a cascaded two-step pipeline by firstly detecting objects in front-view images and then localizing objects in frustum point clouds extruded from the 2D boxes, which however is strictly dependent on the 2D detector and will miss an object entirely if it is not detected in 2D.% due to occlusion or hard illumination.\n%Both the above two families of work have to convert/rasterize the point clouds before inputting them to CNNs, resulting in information loss from the quantization process. The third family of works is 2D-driven, in the sense that the object is firstly localized in a 2D front-view image and then the 3D detection problem is turned to a 3D localization problem in the frustum point cloud extruded from the 2D object bounding box. This method, however, is too dependent on the 2D detector stage and may entirely miss an object if it is not seen by the 2D detector due to occlusion or illumination challenges.\n\n% So what are the fundamental requirements of a 3D object detector? We desire the method to be (1) \\emph{general}, meaning that it is not bound by any assumption on canonical viewpoints (as in the bird's eye view detector); (2) \\emph{3D-based}, in the sense that it does not rely on 2D detectors as in 2D-driven methods (while 2D information can still be used by projecting them to 3D); and (3) \\emph{efficient}, exploiting sparsity in point clouds and not wasting computation in scanning through large swaths of empty space.\n\nIn this work we introduce a \\emph{point cloud focused} 3D detection framework that directly processes raw data and does not depend on any 2D detectors neither in architecture nor in object proposal. Our detection network, \\emph{\\votenet{}}, is based on recent advances in 3D deep learning models for point clouds, and is inspired by the generalized Hough voting process for object detection~\\cite{leibe2004combined}.\n\nWe leverage PointNet++~\\cite{qi2017pointnetplusplus}, a hierarchical deep network for point cloud learning, to mitigates the need to convert point clouds to regular structures. By directly processing point clouds not only do we avoid information loss by a quantization process, but we also take advantage of the sparsity in point clouds by only computing on sensed points.\n\nWhile PointNet++ has shown success in object classification and semantic segmentation ~\\cite{qi2017pointnetplusplus}, few research study how to detect 3D objects in point clouds with such architectures.\n%While PointNet++ etc. has proven to perform well for tasks ranging from classification, semantic segmentation etc., little work has investigated how to extend the network for more complicated tasks such as region proposal generation or object detection.\nA na\\\"ive solution would be to follow common practice in 2D detectors and perform dense object proposal~\\cite{liu2016ssd,ren2015faster}, i.e. to %directly to the point cloud case, so that we have a backbone point cloud feature learning network and that in turn \n%\npropose 3D bounding boxes directly from the sensed points (with their learned features). %If it is a two-stage scheme, we can also refine the bounding box after RoI (region of interest) pooling of the point features.\n%\nHowever, the inherent sparsity of point clouds makes this approach unfavorable. In images there often exists a pixel near the object center, but it is often not the case in point clouds. As depth sensors only capture surfaces of objects, 3D object centers are likely to be in empty space, far away from any point. As a result, point based networks have difficulty aggregating scene context in the vicinity of object centers. Simply increasing the receptive field does not solve the problem because as the network captures larger context, it also causes more inclusion of nearby objects and clutter.\n\n\n%much larger offsets need to be regressed. %So is there a better way than directly proposing such object centers?\nTo this end, we propose to endow point cloud deep networks with a voting mechanism similar to the classical \\emph{Hough voting}.  \n%which we call \\emph{deep Hough voting}.\n%\nBy \\emph{voting} we essentially generate new points that lie close to objects centers, which can be \\textit{grouped and aggregated} to generate box \\textit{proposals}. %See Fig.~\\ref{fig:teaser}.\n\n% the proposal problem is decomposed to two stages. This decomposition allows us to focus \\emph{attention} on object centers and to more effectively combine information from different parts of the object through a learned \\emph{vote pooling}.\n\nIn contrast to traditional Hough voting with multiple separate modules that are difficult to optimize jointly, \\emph{\\votenet{}} is end-to-end optimizable. %All steps, including point cloud feature learning, voting, vote clustering and vote aggregation are implemented through network layers that are fully differentiable. \n%\nSpecifically, after passing the input point cloud through a backbone network, we sample a set of seed points and generate votes from their features. Votes are targeted to reach object centers. As a result, vote clusters emerge near object centers and in turn can be aggregated through a learned module to generate box proposals. The result is a powerful 3D object detector that is purely geometric and can be applied directly to point clouds.\n\n%a 3D offset plus an updated feature from the seed. The 3D offset of the vote is directly supervised to point towards the object center, if the seed is on an object. Then the network generates 3D object proposals by finding high density regions of votes and aggregating local votes through a learned vote pooling layer. As local votes may come from different object parts and carry different semantic/geometric information the learned vote pooling plays a key role in combining relevant cues and generating high quality proposals. With the proposals we can either have an extra head to RoI pool the points to refine each proposal and classify the object class, or simply adopt a one-stage detection scheme to directly classify the class along with the proposal step.\n\nWe evaluate our approach on two challenging 3D object detection datasets: SUN RGB-D~\\cite{song2015sun} and ScanNet~\\cite{dai2017scannet}. On both datasets \\votenet{}, \\emph{using geometry only}, significantly outperforms prior arts that use both RGB and geometry or even multi-view RGB images.\n% To deepen the understanding of the importance of voting, we  provide a detailed analysis through comparisons to a network that directly proposes objects from surface points.\n% Our study shows that voting effectively broadens the coverage of points from which good proposals are generated, and\nOur study shows that the voting scheme supports more effective context aggregation, and verifies that \\votenet{}~offers the largest improvements when object centers are far from the object surface (e.g. tables, bathtubs, etc.).\n% We also provide analysis experiment to show that voting effectively broadens the coverage of points from which good proposals are generated, and\n% confirms that \\votenet{}~offers the largest improvement where object centers are far from the object surface (e.g. tables, bathtubs, etc.)\n\n% To get further insight into our proposed model, we have also done sophisticated analysis experiments to understand key modules in the deep Hough voting architecture, including the significance of voting (compared to direct proposals from existing points), the impact of vote clustering, and a validation of the importance of learned vote pooling. We further provide qualitative analysis to understand the strengths and limitations of the method.\n\nIn summary, the contributions of our work are:\n\\begin{itemize}\n    \\setlength\\itemsep{0.1em}\n    \\item A reformulation of Hough voting in the context of deep learning through an end-to-end differentiable architecture, which we dub \\votenet{}.\n    \\item State-of-the-art 3D object detection performance on SUN RGB-D and ScanNet.\n    \\item An in-depth analysis of the importance of voting for 3D object detection in point clouds.\n\\end{itemize}\n\n\n\n\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\paragraph{3D object detection.}\n% is a long-standing problem in computer vision.\n% Traditionally, the term detection has been overloaded and used to describe many related tasks such as registration of an existing set of instances, and object recognition to name a few.\n% In this work we refer to 3D object detection as the task of locating and labeling objects belonging to a pre-defined set of semantic classes in an input 3D scene.\nMany previous methods were proposed to detect 3D bounding boxes of objects.\nExamples include: \\cite{lin2013holistic} where a pair-wise semantic context potential helps guide the proposals objectness score;  template-based methods~\\cite{li2015database, Indoor2012, litany2015asist}; Sliding-shapes \\cite{song2014sliding} and its deep learning-based successor \\cite{song2016deep}; Clouds of Oriented Gradients (COG)~\\cite{ren2016three}; and the recent 3D-SIS \\cite{hou20183d}.\n% As some successful methods used 3D CNNs they are computationally heavy. One way to circumvent the complexity is to exploit sparsity of point cloud data, as was recently proposed by GSPN~\\cite{yi2018gspn} and PointRCNN~\\cite{shi2018pointrcnn} in concurrent to our work.\n\nDue to the complexity of directly working in 3D, especially in large scenes, many methods resort to some type of projection.   %(especially for scenes in driving scenarios). \nFor example in MV3D~\\cite{cvpr17chen} and VoxelNet \\cite{zhou2018voxelnet}, the 3D data is first reduced to a bird's-eye view before proceeding to the rest of the pipeline. A reduction in search space by first processing a 2D input was demonstrated in both Frustum PointNets \\cite{qi2018frustum} and \\cite{lahoud20172d}. Similarly, in \\cite{kim2013accurate} a segmentation hypothesis is verified using the 3D map. \nMore recently, deep networks on point clouds are used to exploit sparsity of the data by GSPN~\\cite{yi2018gspn} and PointRCNN~\\cite{shi2018pointrcnn}.% which are in concurrent to our work.\n\n% \\orl{Discuss complex YOLO: \\cite{simon2018complex} ?}\n\n\\paragraph{Hough voting for object detection.}\nOriginally introduced in the late 1950s, the Hough transform \\cite{hough1959machine} translates the problem of detecting simple patterns in point samples to detecting peaks in a parametric space. The Generalized Hough Transform~\\cite{ballard1981generalizing} further extends this technique to image patches as indicators for the existence of a complex object. Examples of using Hough voting include the seminal work of \\cite{leibe2008robust} which introduced the implicit shape model, planes extraction from 3D point clouds~\\cite{borrmann20113d}, and 6D pose estimation~\\cite{sun2010depth} to name a few. \n% This technique was extremely popular in object detection literature until a decade ago, including the\n% \\cite{leibe2008robust} is a seminal work along this line that introduced the implicit shape model (ISM). detection of planes in 3D point clouds~\\cite{borrmann20113d}, Multiple object detection~\\cite{barinova2012detection}, and 6D pose estimation~\\cite{sun2010depth} to name a few.  \n\nHough voting has also been previously combined with advanced learning techniques. In ~\\cite{maji2009object} the votes were assigned with weights indicating their importance, which were learned using a max-margin framework.  \n% As we will describe in Section, in our method we also use supervision to better separate foreground and background votes.\nHough forests for object detection were introduced in~\\cite{gall2011hough,gall2013class}. More recently, \\cite{kehl2016deep} demonstrated improved voting-based 6D pose estimation by using deep features extracted to build a codebook. Similarly \\cite{milletari2017hough} learned deep features to build codebooks for segmentation in MRI and ultrasiounds images. In~\\cite{huan2017vehicle} the classical Hough algorithm was used to extract circular patterns in car logos, which were then input to a deep classification network.\n% A 2D voting-based part-detection network was recently introduced, utilizing votes from observable parts to predict occluded ones. This relates to our method in that we predict object centers which are often not part of the object itself.\n\\cite{novotny2018semi} proposed the semi-convolutional operator for 2D instance segmentation in images, which is also related to Hough voting.\n\nThere have also been works using Hough voting for 3D object detection~\\cite{woodford2014demisting,knopp2010orientation,velizhev2012implicit,knopp2011scene}, which adopted a similar pipeline as in 2D detectors.\n%So they retain the same limitations as those for the 2D versions.\n\n% \\todo{Start another paragraph to specifically talk about methods that try to use deep networks for hough voting and discuss the differences between our method and theirs.}\n\n% DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion. CVPR 2018 from Alan Y.~\\cite{zhang2018deepvoting}\n\n% Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation ECCV 2016~\\cite{kehl2016deep}\n\n% Depth-Encoded Hough Voting for Joint Object\n% Detection and Shape Recovery ECCV 2010~\\cite{sun2010depth}\n\n% Deep residual Hough voting for mitotic cell detection in histopathology images~\\cite{wollmann2017deep}\n\n% Hough-CNN~\\cite{milletari2017hough}\n\n% ICCV 2017 Hough voting~\\cite{huan2017vehicle} for Vehicle Logo Retrieval\n\n% Semi-convolutional operator ECCV 2018~\\cite{novotny2018semi}\n\n% Hough forest (trained random forest to map from image patch to votes) ~\\cite{gall2011hough,gall2013class}\n\n\n\n% Explain that vote in the work of \\cite{engelcke2017vote3deep} means something entirely different. \n\n\n\n\\paragraph{Deep learning on point clouds.}\n% Out of the many 3D representations available, point clouds seem to offer a powerful functionality while being memory efficient and simple to store. Moreover, they are the raw output of many depth sensors, and thus can be thought of the 3D equivalent of image pixels as they represent \\textit{what is}, as opposed to voxel which also represent \\textit{what is not}.\nRecently we see a surge of interest in designing deep network architectures suited for point clouds~\\cite{qi2017pointnet, qi2017pointnetplusplus, su2018splatnet, atzmon2018point, li2018pointcnn, graham20183d, wang2017cnn, tatarchenko2017octree,tatarchenko2018tangent,le2018pointgrid,klokov2017escape,yang2018foldingnet,xu2018spidercnn,wang2018dynamic,xie2018attentional}, which showed remarkable performance in 3D object classification, object part segmentation, as well as scene segmentation. In the context of 3D object detection, VoxelNet~\\cite{zhou2018voxelnet} learn voxel feature embeddings from points in voxels, while in~\\cite{qi2018frustum} PointNets are used to localize objects in a frustum point cloud extruded from a 2D bounding box. However, few methods studied how to directly propose and detect 3D objects in raw point cloud representation.\n% Recently an efficient sparse convolution network~\\cite{graham20183d} was shown to achieve impressive performance on 3D semantic segmentation. In this work we use PointNet++ as our backbone architecture. \n\n\n\n% Starts from 3d shape classification, goes to semantic segmentation, recently used as a module in 3D object detection (voxelnet, frustum pointnets) but not yet fully utilized.\n\n%pointnet~\\cite{qi2017pointnet}, \n\n% pointnet++~\\cite{qi2017pointnetplusplus}, SPLATNet~\\cite{su2018splatnet}\n% SubmanifoldSparseConv~\\cite{graham20183d}\n% PointCNN~\\cite{li2018pointcnn}\n\n\n\n"
            },
            "section 3": {
                "name": "Deep Hough Voting",
                "content": "\n\\label{sec:deep_hough_voting}\n% As our pipeline is based on the traditional Hough voting procedure, for completeness we provide a brief description of this well known technique.\n%For more detailed explanation, we refer the reader to \\orl{TBD}.\n\nA traditional Hough voting 2D detector~\\cite{leibe2008robust} comprises an offline and an online step. First, given a collection of images with annotated object bounding boxes, a codebook is constructed with stored mappings between image patches (or their features) and their offsets to the corresponding object centers.  \n%\n%involves multiple separate modules and steps in both offline processing and online inference. At offline time, one needs to first construct a codebook that stores mappings from image patches (or their features) to votes (as 3D offset vectors from the pixels to object centers). \n%\nAt inference time, interest points are selected from the image to extract patches around them. These patches are then compared against patches in the codebook to retrieve offsets and compute votes. As \\textit{object} patches will tend to vote in agreement, clusters will form near object centers. Finally, the object boundaries are retrieved by tracing cluster votes back to their generating patches. \n\n%and then match patches around interest points to the patches in the codebook. The votes in the codebook are then copied to the query image to create votes. It happens that a clutter image patch can also create votes, but it is fine as long as they do not agree with each other on where to vote (i.e. they do not create vote clusters). Next, we need to cluster the generated votes to find peaks, which are potential locations of object centers. To recover the object shape, the votes are back-projected to their original image patches, where a bounding box can be computed to cover those patches.\n\nWe identify two ways in which this technique is well suited to our problem of interest.\n% First, voting is designed for sparse sets, and hence is a natural fit to point clouds.\nFirst, voting-based detection is more compatible with sparse sets than region-proposal networks (RPN)~\\cite{ren2015faster} is. For the latter, the RPN has to generate a proposal near an object center which is likely to be in an empty space, causing extra computation.\nSecond, it is based on a bottom-up principle where small bits of partial information are accumulated to form a confident detection. Even though neural networks can potentially aggregate context from a large receptive field, it may be still beneficial to aggregate in the vote space. %object scans are often very partial, which may lead to ambiguous decisions if not aggregated properly.\n% Even more crucial is the lack of object points near the centroid which prohibits direct context aggregation. \n\n%One strength of the Hough voting detection pipeline is it only computes votes on interest points without dense computation on all pixels. So the method is well suited to sparse data such as a point cloud, where we hope not to compute at every spatial location (including the empty ones) but rather just want to compute on existing points on the object surfaces. Other strengths of hough voting including its flexibility to make use of object templates and robustness to occlusions [add refs].\n\nHowever, as traditional Hough voting comprises multiple separated modules, integrating it into state-of-the-art point cloud networks is an open research topic. To this end, we propose the following adaptations to the different pipeline ingredients.\n\\smallskip\n\n%\n% for detection is that it contains multiple separated modules each with a handful of parameters thus the whole system is not end-to-end optimizable.\n%\n\n\n% a new scheme of Hough voting in the era of deep learning. Based on a point cloud deep network backbone, our proposed \\votenet{} end-to-end optimizable. \n\n\n% Below we show how a deep hough voting system aligns/compares with a traditional hough voting pileine.\n\n% \\begin{itemize}\n\n\\noindent \\textbf{Interest points}\n% can be learned to be identified using object point supervision instead of hand-crafted features.\nare described and selected by deep neural networks instead of depending on hand-crafted features.\n\n\\smallskip\n    %\n    % Instead of using heuristics or hand-crafted features to select interest points, a deep network can learn to select and sample interest points.\n    %\n    \n\\noindent\\textbf{Vote} generation is learned by a network instead of using a codebook. Levaraging larger receptive fields, voting can be made less ambiguous and thus more effective. In addition, a vote \\emph{location} can be augmented with a \\emph{feature vector} allowing for better aggregation. \n\n%    Instead of relying on a codebook, we can learn to map from point features to votes with neural networks. In addition, rather than relying on local patches, we can learn deep point features with large receptive field such that lots of ambiguities can be resolved. Compared with traditional voting, now we can vote both much more efficiently (no need for k-NN search in a large codebook) but also more effectively (much higher vote quality due to increased context through learning).\n%\n\n\\smallskip\n\\noindent\\textbf{Vote aggregation} is realized through point cloud processing layers with trainable parameters. Utilizing the vote features, the network can potentially filter out low quality votes and generate improved proposals.\n\n% A network can evaluate and generate cluster scores for all local neighborhoods of votes in parallel, and even learn how to aggregate votes to generate better proposals. A clustering step (which often does not gradient descent) can be totally eliminated in the pipeline. The aggregation step can further filter out low quality votes from clutters and combine complementary cues of votes from different parts of a object.\n\n\\smallskip\n\\noindent\\textbf{Object proposals} in the form of: location, dimensions, orientation and even semantic classes can be directly generated from the aggregated features, mitigating the need to trace back votes' origins.\n\n\n% From the aggregated votes, we can estimate the object scale, pose, class and refine the object location from the vote features. There is no need to back-project from votes to original patches.\n\n% \\end{itemize}\nIn what follows, we describe how to combine all the aforementioned ingredients into a single end-to-end trainable network named as \\votenet{}.\n\n\n% The described end-to-end trainable pipeline \n\n% With an end-to-end trained point cloud network, every step in the pipeline can be optimized through gradient descent for the final detection goal. In the next section, we will describe an instantiation of the deep hough voting scheme, a 3D object detection network called the VotingNet.\n"
            },
            "section 4": {
                "name": "VoteNet Architecture",
                "content": "\n% In this section, we describe our proposed deep hough voting network (named \\emph{VotingNet}) for 3D object detection in point clouds.\n\nFig.~\\ref{fig:votingnet} illustrates our end-to-end detection network (\\emph{\\votenet{}}).\n% learns to vote to object centers from the input point cloud and to propose and classify 3D objects from the votes.\nThe entire network can be split into two parts: one that processes \\emph{existing} points to generate votes; and the other part that operates on \\emph{virtual} points -- the votes -- to propose and classify objects.\n\n\n",
                "subsection 4.1": {
                    "name": "Learning to Vote in Point Clouds",
                    "content": "\n\\label{sec:votingnet:vote}\n% \\rqi{Should we mention somewhere the entire network is a large PointNet++ network except that we have an intermediate voting supervision and switched from learning on existing points to votes after the voting layer.}\n\n% Input to our network is a point cloud of a scene in size $N \\times 3$ with $N$ points and each point has its 3D coordinates. In the voting step we aim to generate a set of \\emph{votes}  in size $M \\times F \\times (3+C_2)$ from a subsampled set of points (seeds) in size $M \\times (3+C_1)$, where $F$ is the voting factor (how many votes we generate from each seed point).\nFrom an input point cloud of size $N \\times 3$, with a 3D coordinate for each of the $N$ points, we aim to generate $M$ votes, where each vote has both a 3D coordinate and a high dimensional feature vector. There are two major steps: point cloud feature learning through a backbone network and learned Hough voting from seed points.\n\n% A voting process involves three key definitions:\n% \\begin{itemize}\n%     \\item Seeds: These are the ``voters'' and are where the votes come from. They are also referred to as interest points.\n%     \\item Votes: a vote is a signal generated from a seed that indicates the geometry and semantics of the object of interest.\n% \\end{itemize}\n\n% Compared to traditional generalized hough transform, voting in our method is different in two aspects. First, the VotingNet does not rely on a codebook that records the mappings from local patch features to votes. Instead, the network learns to generate votes from point cloud features with a trained neural network. Second, the representation of votes in our network is no longer just a 3D offset but rather a 3D offset plus a high-dimensional vote feature vector.\n\n\\smallskip\\noindent\\textbf{Point cloud feature learning.}\n% \\paragraph{Point cloud feature learning.}\nGenerating an accurate vote requires geometric reasoning and contexts. Instead of relying on hand-crafted features, we leverage recently proposed deep networks~\\cite{qi2017pointnetplusplus,graham20183d,su2018splatnet,li2018pointcnn} on point clouds for point feature learning. While our method is not restricted to any point cloud network, we adopt PointNet++~\\cite{qi2017pointnetplusplus} as our backbone due to its simplicity and demonstrated success on tasks ranging from normal estimation~\\cite{guerrero2018pcpnet}, semantic segmentation~\\cite{landrieu2018large} to 3D object localization~\\cite{qi2018frustum}.\n\nThe backbone network has several set-abstraction layers and feature propagation (upsampling) layers with skip connections, which outputs a subset of the input points with XYZ and an enriched $C$-dimensional feature vector. The results are $M$ \\emph{seed points} of dimension $(3+C)$. Each seed point generates one vote\\footnote{The case of more than one vote is discussed in the appendix.}.\n% \\todo{shall we mention there are other choices like hard masking/soft masking for foreground and background seeds? -- we don't have too interesting results for them though.}\n\n% Other networks such as 3D Sparse Convnets~\\cite{graham20183d} or SPLATNet~\\cite{su2018splatnet} can also be used.\n% The backbone PointNet++ network is in a U-Net like structure composed of several set abstraction layers that subsamples the point clouds while learning geometric and semantic features with larger and larger contexts, and several feature upsampling layers (with skip links) that propagate the features back to more dense sets of points.\n% We note that the feature upsampling is important and necessary because it increases the context of point features such that ambiguous cases caused by symmetry can be resolved.\n\n% It is interesting to note how feature context could affect voting. With little context, e.g. just looking at a small neighborhood, voting can be very noisy due to ambiguous cases caused by symmetry (e.g. front and rear wheels of a car; left handle of a chair and a right handle of a near by chair to its left). As the feature context gets larger, those ambiguous cases can be resolved. By learning point cloud features with a encoder-decoder/U-net network, we are able to increase feature context to several meters while still keeping the entire detector translation invariant.\n\n% \\paragraph{Seed points.} Several options exist in terms of choosing the voters. A simplest choice is to uniformly subsample a set of points from the input point cloud and vote from there. This simple strategy puts more burden to the later modules in figuring out proposals from votes. It actually can work reasonably well in dense scenes with lots of objects of interest while signal to noise ratio is lower as we get more sparse scenes with few objects. Another more advanced strategy is to leverage learned point cloud features to segment foreground and background points and only vote from classified foreground points. One can either use a hard masking to completely eliminate classified background points in the voting step, or take a soft masking path to weight the features of votes by the binary segmentation scores.\n\n\n\\smallskip\\noindent\\textbf{Hough voting with deep networks.} Compared to traditional Hough voting where the votes (offsets from local keypoints) are determined by look ups in a pre-computed codebook, we generate votes with a deep network based voting module, which is both more efficient (without kNN look ups) and more accurate as it is trained jointly with the rest of the pipeline.\n\nGiven a set of seed points $\\{s_i\\}_{i=1}^{M}$ where $s_i = [x_i; f_i]$ with $x_i \\in \\mathbb{R}^3$ and $f_i \\in \\mathbb{R}^{C}$, a shared \\emph{voting module} generates votes from each seed independently. Specifically, the voting module is realized with a multi-layer perceptron (MLP) network with fully connected layers, ReLU and batch normalization. The MLP takes seed feature $f_i$ and outputs the Euclidean space offset $\\Delta x_i \\in \\mathbb{R}^3$ and a feature offset $\\Delta f_i \\in \\mathbb{R}^{C}$ such that the vote $v_i = [y_i; g_i]$ generated from the seed $s_i$ has $y_i = x_i + \\Delta x_i$ and $g_i = f_i + \\Delta f_i$.\n\nThe predicted 3D offset $\\Delta x_i$ is explicitly supervised by a regression loss\n%\n\\begin{equation}\n    L_{\\text{vote-reg}} = \\frac{1}{M_{\\text{pos}}} \\sum_i  \\|\\Delta x_i - \\Delta x_i^*\\| \\mathds{1}[s_i \\text{ on object}], \n\\end{equation}\n\n\\noindent\nwhere $\\mathds{1}[s_i \\text{ on object}]$ indicates whether a seed point $s_i$ is on an object surface and $M_\\text{pos}$ is the count of total number of seeds on object surface. $\\Delta x_i^*$ is the ground truth displacement from the seed position $x_i$ to the bounding box center of the object it belongs to. \n\n% From each seed point, the voting layer generates $F$ votes from the seed point's feature, where $F$ is the voting factor (in default $F=1$). Each vote consists of a 3D offset relative to the seed point's 3D location and a residual feature prediction relative to the seed feature. The vote features can carry information of where the vote is from and also carry any useful semantic or geometric information useful for the later object proposal step.\n\nVotes are the same as seeds in tensor representation but are no longer grounded on object surfaces. A more essential difference though is their position -- votes generated from seeds on the same object are now closer to each other than the seeds are, which makes it easier to combine cues from different parts of the object. Next we will take advantage of this semantic-aware locality to aggregate vote features for object proposal.\n\n% We adopt a direct way to supervise the voting process, we require that if the seed point is on object surface, the 3D offset of the votes need to lead to the centers of the objects; otherwise, the offset is not considered in the loss. We do not supervise the feature regression of the votes and leave that to be end-to-end optimized by the 3D object proposal losses.\n\n% There are some alternatives in both what to vote and how to supervise the votes. In~\\cite{novotny2018semi}, they predict 2D votes in images without any feature vectors and do not explicitly supervise the votes but just design a metric learning loss to push votes from the same objects point to the same location and votes from different objects to points that are apart. However, since there are no direct supervision on where the votes go, the votes can settle in arbitrary places thus make the next proposal step hard, or limit the proposal method to be merging/clustering based.\n% Potentially, in the voting, we can also vote for the scale and pose of the objects but we leave that for future research.\n\n% In default we just generate one vote per seed since we find that with large enough context there is little need to generate more than one vote to resolve ambiguous cases. However, it is still possible to generate more than one vote with our network architecture. Yet to break the symmetry in multiple vote generation, we need to introduce some bias to different votes (otherwise three votes will point to a similar place) such as different scale multiplier or direction anchors. In experiments, we find that one vote per seed achieves the best results as more votes can introduce extra noise to the proposal step.\n\n"
                },
                "subsection 4.2": {
                    "name": "Object Proposal and Classification from Votes",
                    "content": "\n\\label{sec:votingnet:pooling}\n\n% Traditionally in generalized hough voting, after the voting step, peaks of votes are found through some clustering algorithm and then the votes in high density clusters are back-projected to their original interest points/patches to recover objects. The clustering and peak search process is necessary because they can reduce the number of proposals and only keep those with high confidence i.e. large density. However this process involves many parameters such as choosing grid size, density thresholds and criterion for convergence on clustering, and most importantly not friendly to gradient descent.\n\nThe votes create canonical ``meeting points'' for context aggregation from different parts of the objects. After clustering these votes we aggregate their features to generate object proposals and classify them. \n\n\\smallskip\\noindent\\textbf{Vote clustering through sampling and grouping.}\nWhile there can be many ways to cluster the votes, we opt for a simple strategy of uniform sampling and grouping according to spatial proximity. Specifically, from a set of votes $\\{v_i\\ = [y_i; g_i] \\in \\mathbb{R}^{3+C}\\}_{i=1}^{M}$, %where $v_i = [y_i; g_i]$ with $y_i \\in \\mathbb{R}^3$ and $g_i \\in \\mathbb{R}^{C}$\nwe sample a subset of $K$ votes using farthest point sampling based on $\\{y_i\\}$ in 3D Euclidean space, to get $\\{v_{i_k}\\}$ with $k=1,...,K$. Then we form $K$ clusters by finding neighboring votes to each of the $v_{i_k}$'s 3D location: $\\mathcal{C}_k = \\{v_i^{(k)} | \\| v_i - v_{i_k}\\| \\leq r\\}$ for $k=1,...,K$. Though simple, this clustering technique is easy to integrate into an end-to-end pipeline and works well in practice.\n\n\\smallskip\\noindent\\textbf{Proposal and classification from vote clusters.}\nAs a vote cluster is in essence a set of high-dim points, we can leverage a generic point set learning network to aggregate the votes in order to generate object proposals. Compared to the back-tracing step of traditional Hough voting for identifying the object boundary, this procedure allows to propose \\textit{amodal} boundaries even from partial observations, as well as predicting other parameters like orientation, class, etc.  \n\nIn our implementation, we use a \\emph{shared} PointNet~\\cite{qi2017pointnet} for vote aggregation and proposal in clusters. Given a vote cluster $\\mathcal{C} = \\{w_i\\}$ with $i=1,...,n$ and its cluster center $w_j$, where $w_i = [z_i; h_i]$ with $z_i \\in \\mathbb{R}^3$ as the vote location and $h_i \\in \\mathbb{R}^{C}$ as the vote feature. To enable usage of local vote geometry, we transform vote locations to a local normalized coordinate system by $z'_i = (z_i - z_j)/r$. Then an object proposal for this cluster $p(\\mathcal{C})$ is generated by passing the set input through a PointNet-like module:\n\n\\begin{equation}\n    p(\\mathcal{C}) = \\text{MLP}_2 \\left\\{ \\underset{i=1,...,n}{\\mbox{max}}\\left\\{\\text{MLP}_1 ([z'_i; h_i])\\right\\} \\right\\}\n    \\label{eq:vote_aggregation}\n\\end{equation}\n\n\\noindent\nwhere votes from each cluster are independently processed by a $\\text{MLP}_1$ before being max-pooled (channel-wise) to a single feature vector and passed to $\\text{MLP}_2$ where information from different votes are further combined. We represent the proposal $p$ as a multidimensional vector with an objectness score, bounding box parameters (center, heading and scale parameterized as in~\\cite{qi2018frustum}) and semantic classification scores.\n\n% The detection approach we take is similar to the single-stage detector in 2D detection in images i.e. each proposal (with its semantic class prediction) from the vote cluster is considered a final detected box.\n% without a further RoI pooling and refinement step.\n\n% Instead of clustering, with features associated with votes we can learn to predict objectness scores for all the votes without the necessity to find peaks among them. That said, we can still sample a subset of votes based on their local density if we have a budget limit on number of proposals. On the other hand, more interestingly, with a learning scheme we can also aggregate information from individual votes through a learned \\emph{vote pooling} process. Given a local neighborhood of votes, a mini-PointNet takes a set of votes with their 3D local coordinates and feature vectors and outputs a pooled vote with new 3D location and a new feature vector. A following FC layer can then take the new feature vector and predict a 3D object proposal relative to the vote position.\n\n% In Section~\\ref{sec:aggregation_variants} we compare different aggregation alternatives. \n\n% Besides vote aggregation through a neural function as in Eq.~\\ref{eq:vote_aggregation}, there exist other simple and non-parametric alternatives such as directly pooling the vote features within each vote cluster (e.g. $p = \\text{MLP}_2 \\left\\{ \\mbox{AVG}\\{h_i\\} \\right\\}$). We will show in the experiment section that learned vote aggregation is much more effective compared to those alternatives.\n\n% \\paragraph{3D object proposal}\n% A 3D object proposal is an oriented 3D bounding box prediction with objectness scores. The bounding box is parameterized in a way similar to~\\cite{qi2018frustum} with decoupled center, size and heading angle regression. The objectness prediction is a two-class binary classification task. The ground truth labels for the objectness is computed on-the-fly by measuring the distances from ground truth box centers to the votes.\n\n% In a one-stage detection system, we directly regress the semantic class scores from the vote's features. In a two-stage system we extract points within the proposal box and have another head to refine the 3D bounding box as well as the semantic classification. We show in experiments that even a simple on-stage detector on top of our deep Hough voting structure works remarkably well.\n\n\\smallskip\\noindent\\textbf{Loss function.}\n% \\todo{Make everything consistent with the notations.}\n% The network is supervised by a multi-task loss (Eq.~\\ref{eq:loss}) including a voting regression loss ($L1$), an object proposal loss (cross entropy) and a object detection loss for bounding box parameter estimation and semantic classification.\nThe loss functions in the proposal and classification stage consist of objectness, bounding box estimation, and semantic classification losses.\n\n% The voting is supervised for its $XYZ$ offset. In SUN RGB-D, since there is no annotations for 3D instance segmentation, we consider all points in an object's bounding box should vote for the object center. To deal with cases a point lives in multiple object's bounding boxes (e.g. a chair point under the table), we assign multiple (three in our implementation) ground truth vote labels to a point and evaluate a minimum of $K$ loss when comparing a predicted vote offset with the set of ground truth votes. In ScanNetV2, points have instance labels so we can acquire a one-to-one mapping of object point to vote label. However, bounding boxes in ScanNetV2 is not amodal, so we just calculate the bounding box of the visible points and compute the ground truth vote as the one points to the visible box center.\n\nWe supervise the objectness scores for votes that are located either close to a ground truth object center (within $0.3$ meters) or far from any center (by more than $0.6$ meters). We consider proposals generated from those votes as \\textit{positive} and \\textit{negative} proposals, respectively. Objectness predictions for other proposals are not penalized. Objectness is supervised via a cross entropy loss normalized by the number of non-ignored proposals in the batch. For positive proposals we further supervise the bounding box estimation and class prediction according to the closest ground truth bounding box. Specifically, we follow ~\\cite{qi2018frustum} which decouples the box loss to center regression, heading angle estimation and box size estimation. For semantic classification we use the standard cross entropy loss. In all regression in the detection loss we use the Huber (smooth-$L_1$~\\cite{ren2015faster}) loss. Further details are provided in the appendix. \n\n% To determine which predictions to supervise, and semantic classification for a proposal from a vote cluster, we have to compute a ground truth objectness label on the fly. In our design, the objectness label is decided by measuring the distance from the vote cluster center to ground truth box centers. If the cluster center is within $0.3$ meter of any ground truth box center, the proposal generated from the cluster is considered positive. If the distance is larger than $0.6$ meter, then the proposal is negative. For other distance in between the proposal is ignored in supervision. The positive predicted box is assigned to a closest ground truth bonding box. The objectness loss is then a cross entropy loss normalized by the number of non-ignored proposals in the batch.\n\n% The detection loss $L_{\\text{det}}$ is similar to that in~\\cite{qi2018frustum}, where we decouple 3D bounding box estimation to center regression, heading angle estimation and box size estimation. Both heading and size estimation losses are further decomposed into a multi-class classification (based on heading angle bin or mean size anchor) and a residual regression loss. \\todo{Refine description here.} In the center loss, we take use a Chamfer loss to require that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it. The latter part also influences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object. In all regression in the detection loss we use the robust $L1$-smooth loss.\n\n% The semantic classification loss is a cross entropy loss. Both of them are only evaluated on positive vote clusters and normalized by the number of positive clusters.\n% The final proposal loss is a weighted average of the three losses as $L_{\\text{proposal}} = \\lambda_1 L_{\\text{obj}} + \\lambda_2 L_{\\text{box}} + \\lambda_3 L_{\\text{sem}}$.\n\n% \\begin{equation}\n% \\begin{split}\n%     L(\\{v_i\\}, \\{o_i\\}) & = \\frac{1}{M_{\\text{pos}}} \\sum_i L_{\\text{vote-reg}} (v_i, v_i^*) \\\\\n%     & + \\frac{1}{K_{\\text{cls}}} \\sum_i L_{\\text{obj-cls}} (o_i, o_i^*) \\\\\n%     & + \\frac{1}{K_{\\text{pos}}} \\sum_i L_{\\text{det}} (o_i, o_i^*)\n% \\end{split}\n% \\label{eq:loss}\n% \\end{equation}\n\n% The detection loss $L_{\\text{det}}$ is similar to that in~\\cite{qi2018frustum}, where we decouple 3D bounding box estimation to center regression, heading angle estimation and box size estimation. Both heading and size estimation losses are further decomposed into a multi-class classification (based on heading angle bin or mean size anchor) and a residual regression loss. In the center loss, we take use a Chamfer loss: $L_{\\text{center-reg}} = Chamfer(\\{c_i\\}, \\{c_i^*\\})$ to require that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it. The latter part also influences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object. In all regression in the detection loss we use the robust $L1$-smooth loss.\n\n% \\begin{equation}\n% \\begin{split}\n%     L_{\\text{det}} (o_i, o_i^*) & = L_{\\text{center-reg}} + L_{\\text{angle-cls}} + L_{\\text{angle-reg}} \\\\\n%     & + L_{\\text{size-cls}} + L_{\\text{size-reg}} + L_{\\text{sem-cls}}\n% \\end{split}\n% \\end{equation}\n\n"
                },
                "subsection 4.3": {
                    "name": "Implementation Details",
                    "content": "\n\\label{sec:votingnet:implmentation}\n\n\\smallskip\\noindent\\textbf{Input and data augmentation.} Input to our detection network is a point cloud of $N$ points randomly sub-sampled from either a popped-up depth image ($N=20k$) or a 3D scan (mesh vertices, $N=40k$).\nIn addition to $XYZ$ coordinates, we also include a height feature for each point indicating its distance to the floor. The floor height is estimated as the $1\\%$ percentile of all points' heights.\n%Note that since our network is translation invariant, it does not use any absolute coordinate of the points so the height channel is necessary. %So the final input is a point cloud of size $20,000 \\times 4$.\nTo augment the training data, we randomly sub-sample the points from the scene points on-the-fly. We also randomly flip the point cloud in both horizontal direction, randomly rotate the scene points by $\\text{Uniform}[-5^{\\circ},5^{\\circ}]$ around the upright-axis, and randomly scale the points by $\\text{Uniform}[0.9,1.1]$.\n\n\\smallskip\\noindent\\textbf{Network architecture details.} The backbone feature learning network is based on PointNet++~\\cite{qi2017pointnetplusplus}, which has four set abstraction (SA) layers and two feature propagation/upsamplng (FP) layers, where the SA layers have increasing receptive radius of $0.2$, $0.4$, $0.8$ and $1.2$ in meters while they sub-sample the input to $2048$, $1024$, $512$ and $256$ points respectively. The two FP layers up-sample the 4th SA layer's output back to $1024$ points with $256$-dim features and 3D coordinates (more details in the appendix).\n% Each set abstraction layer has a receptive field specified by the local ball-region radius $r$, a MLP network for point feature transform $MLP[c_1,...,c_k]$ where $c_i$ is output channel number of the $i$-th layer in the MLP. The SA layer also subsamples the input point cloud with farthest point sampling. Compared to~\\cite{qi2017pointnetplusplus}, we also normalize the XYZ scale of points in each local region by the region radius. Each set feature propagation layer upsamples the point features by interpolating the features on input layer points to output points, and also combining the skip-linked features through a MLP. The layers are\n\n% $\\text{SA1}(2048, 0.2, [64,64,128])$, $\\text{SA2}(1024, 0.4, [128,128,256])$, $\\text{SA3}(512, 0.8, [128,128,256])$, $\\text{SA4}(256, 1.2, [128,128,256])$ and $\\text{FP1}([256,256])$, $\\text{FP2}([256,256])$. All layers have ReLu followed by BatchNorm.\n\n% \\begin{table}[]\n%     \\centering\n%     \\begin{tabular}{|l|l|l|}\n%     \\toprule\n%           name &  type & parameters\\\\ \\hline\n%          sa1 & SA & n=2048, r=0.2, [64,64,128] \\\\\n%          sa2 & SA & n=1024, r=0.4, [128,128,256] \\\\\n%          sa3 & SA & n=512, r=0.8, [128,128,256] \\\\\n%          sa4 & SA & n=256, r=1.2, [128,128,256] \\\\\n%          fp1 & FP & n=512, [256,256] \\\\\n%          fp2 & FP & n=1024, [256,256] \\\\\n%          voting & MLP & [256,256,259] \\\\ \n%          proposal & SA & n=256, r=0.3, [128,128,128], \\\\ \n%          proposal & MLP & [128,128,128,$C_\\text{out}$] \\\\ \\bottomrule\n%     \\end{tabular}\n%     \\caption{Caption}\n%     \\label{tab:votingnet_detail}\n% \\end{table}\n\n% \\begin{table}[]\n%     \\centering\n%     \\begin{tabular}{|c|c|c|c|}\n%     \\hline\n%         type & parameters &  in & out\\\\ \\hline\n%         SA & (2048,0.2,[64,64,128]) & (20k,3) & (2048,3+128) \\\\ \n%         SA & (1024,0.4,[128,128,256]) & (2048,3+128) & (1024,3+256) \\\\\n%         SA & (512,0.8,[128,128,256]) & (1024,3+256) & (512,3+256) \\\\\n%         SA & (256,1.2,[128,128,256]) & (512,3+256) & (256,3+256) \\\\\n%         FP & (512,[256,256]) & (512,3+256) \\\\\n%         FP & (1024,[256,256]) & (1024,3+256) \\\\\n%     \\end{tabular}\n%     \\caption{Caption}\n%     \\label{tab:votingnet_detail}\n% \\end{table}\n\nThe voting layer is realized through a multi-layer perceptron with FC output sizes of $256,256,259$, where the last FC layer outputs XYZ offset and feature residuals.\n% The voting layer involves a vote regression FC layer and a feature update FC layer shared across all seed points. For a seed point $s_i$ with 3D coordinate $X(s_i)$ and feature $F(s_i)$. The output vote $v_i$'s coordinate is $X(v_i) = FC_1(F(s_i)) + X(s_i)$ and its feature is $F(v_i) = FC_2(F(s_i) + F(s_i)$, where $FC_1$ maps from 256-dim feature to 3-dim offset and $FC_2$ maps 256-dim feature to 256-dim feature residual. Both layers do not have non-linear or BatchNorm.\n\n% The vote cluster is sampled with farthest point sampling on the votes during training. At inference time, different sampling strategies can be taken as we will discuss more in the experiment section. $256$ proposals are generated for each scene.\nThe proposal module is implemented as a set abstraction layer with a post processing $\\text{MLP}_2$ to generate proposals after the max-pooling. The SA uses radius $0.3$ and $\\text{MLP}_1$ with output sizes of $128,128,128$. The max-pooled features are further processed by $\\text{MLP}_2$ with output sizes of $128,128,5+2NH+4NS+NC$ where the output consists of $2$ objectness scores, $3$ center regression values, $2NH$ numbers for heading regression ($NH$ heading bins) and $4NS$ numbers for box size regression ($NS$ box anchors) and $NC$ numbers for semantic classification.\n\n\\smallskip\\noindent\\textbf{Training the network.} We train the entire network end-to-end and from scratch with an Adam optimizer, batch size 8 and an initial learning rate of $0.001$. The learning rate is decreased by $10\\times$ after 80 epochs and then decreased by another $10\\times$ after 120 epochs. Training the model to convergence on one Volta Quadro GP100 GPU takes around 10 hours on SUN RGB-D and less than 4 hours on ScanNetV2.\n\n\\smallskip\\noindent\\textbf{Inference.} Our \\votenet{} is able to take point clouds of the entire scenes and generate proposals in one forward pass. The proposals are post-processed by a 3D NMS module with an IoU threshold of $0.25$. The evaluation follows the same protocol as in~\\cite{song2016deep} using mean average precision.\n\n"
                }
            },
            "section 5": {
                "name": "Experiments",
                "content": "\nIn this section, we firstly compare our Hough voting based detector with previous state-of-the-art methods on two large 3D indoor object detection benchmarks (Sec.~\\ref{sec:exp:sota}). We then provide analysis experiments to understand the importance of voting, the effects of different vote aggregation approaches and show our method's advantages in its compactness and efficiency (Sec.~\\ref{sec:exp:analysis}). Finally we show qualitative results of our detector (Sec.~\\ref{sec:qualitative}). More analysis and visualizations are provided in the appendix.\n\n% To further establish the importance of voting, we provide a thorough ablation study of the different ingredient of our pipeline. To this end, we construct a direct-proposal network that generates detection proposals directly from scene points, i.e. without voting. A conclusion from this study is that voting, when aggregated properly, adds a significant boost to overall performance. In particular, we show that \\votenet{} excels in classes where the amodal bounding-box center tends to be far from the object surface (e.g. tables, bathtubs, etc.).\n\n% \\begin{itemize}\n%     \\item Comparison with previous methods on 3D object detection and 3D instance segmentation.\n%     \\item Key ablation studies to validate our design: voting or not (different objectness losses). to learn vote pooling or not. vote context. vote weighting/masking.\n%     \\item Visualizations. Intuitive examples to show why voting could help.\n%     \\item Other experiments: effects of data augmentation. one-stage v.s. two-stage detector (with RoI and box refinement). one v.s. multiple votes. adding rgb to input point channels. \n%     \\item Discussion on more applications of \\votenet/deep hough voting.\n% \\end{itemize}\n\n% \\rqi{Add a section on evaluation of 3d object proposal: recall of the system.}\n\n\n\n\n\n",
                "subsection 5.1": {
                    "name": "Comparing with State-of-the-art Methods",
                    "content": "\n\\label{sec:exp:sota}\n% Evaluated on SUN RGB-D \\cite{song2015sun} and ScanNetV2 \\cite{dai2017scannet}, our method significantly outperforms all previous methods while using purely geometric information. After a short description of the datasets and competing methods, we discuss these results in detail.\n\n% Importantly, we use purely geometric information, suggesting that our algorithm has learned to extract strong semantic and geometric cues directly from 3D. \n% \\rqi{Mention that SUN RGB-D are partial scans (single-frame) while ScanNet has scans from multiple RGB-D views so more complete. Our algorithms work well for both cases with the same architecture and the same set of hyperparameters.}\n\n\\noindent\\textbf{Dataset.}\nSUN RGB-D \\cite{song2015sun} is a single-view RGB-D dataset for 3D scene understanding. It consists of ${\\sim}$5K RGB-D training images %($5,285$ in train set and $5,050$ in val set) \n%annotated with semantic segmentation on images and \nannotated with amodal oriented 3D bounding boxes for $37$ object categories. To feed the data to our network, we firstly convert the depth images to point clouds using the provided camera parameters. We follow a standard evaluation protocol and report performance on the $10$ most common categories.\n% As prescribed by previous works, our evaluation is done on the $10$ most common categories with the metric of mean average precision (mAP) using a 3D IoU threshold of $0.25$.\n\nScanNetV2 \\cite{dai2017scannet} is a richly annotated dataset of 3D reconstructed meshes of indoor scenes. It contains ${\\sim}$1.2K training examples collected from hundreds of different rooms, and annotated with semantic and instance segmentation for $18$ object categories. Compared to partial scans in SUN RGB-D, scenes in ScanNetV2 are more complete and cover larger areas with more objects on average.\n% $1,513$ reconstructed meshes processed from RGB-D videos collected from about seven hundred independent environments. \n% The dataset is annotated with semantic and instance segmentation for $18$ object categories. The public data are split into train and validation sets of size $1.2$k and $312$, respectively. \nWe sample vertices from the reconstructed meshes as our input point clouds.  Since ScanNetV2 does not provide amodal or oriented bounding box annotation, we aim to predict axis-aligned bounding boxes instead, as in~\\cite{hou20183d}.\n\n\\smallskip\n\\noindent\\textbf{Methods in comparison.} We compare with a wide range of prior art methods. Deep sliding shapes (DSS)~\\cite{song2016deep} and 3D-SIS~\\cite{hou20183d} are both 3D CNN based detectors that combine geometry and RGB cues in object proposal and classification, based on the Faster R-CNN~\\cite{ren2015faster} pipeline. Compared with DSS, 3D-SIS introduces a more sophisticated sensor fusion scheme (back-projecting RGB features to 3D voxels) and therefore is able to use multiple RGB views to improve performance. 2D-driven~\\cite{lahoud20172d} and F-PointNet~\\cite{qi2018frustum} are 2D-based 3D detectors that rely on object detection in 2D images to reduce the 3D detection search space. Cloud of gradients~\\cite{ren2016three} is a sliding window based detector using a newly designed 3D HoG-like feature. MRCNN 2D-3D is a na\\\"ive baseline that directly projects Mask-RCNN~\\cite{he2017mask} instance segmentation results into 3D to get a bounding box estimation. GSPN~\\cite{yi2018gspn} is a recent instance segmentation method using a generative model to propose object instances, which is also based on a PointNet++ backbone.\n\n\\smallskip\n\\noindent\\textbf{Results}\nare summarized in Table \\ref{tab:sunrgbd} and \\ref{tab:scannet}. \\votenet{} outperforms all previous methods by at least \\textbf{3.7} and \\textbf{18.4} mAP increase in SUN RGB-D and ScanNet respectively. Notably, we achieve such improvements when we \\emph{use geometric input (point clouds) only} while they used both geometry and RGB images. Table~\\ref{tab:sunrgbd} shows that in the category ``chair'' with the most training samples, our method improves upon previous state of the art by more than \\textbf{11 AP}.\n% , while only falling behind by more than $5\\%$ in the desk category, where most other methods struggle.\nTable~\\ref{tab:scannet} shows that when taking geometric input only, our method outperforms 3D CNN based method 3D-SIS by more than \\textbf{33 AP}.\nA per-category evaluation for ScanNet is provided in the appendix. Importantly, the same set of network hyper-parameters was used in both datasets. \n\n% \\rqi{Add run time comparison, which we are at advantage.}\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Analysis Experiments",
                    "content": "\n\\label{sec:exp:analysis}\n% We now turn to investigate voting-specific design choices in our network. %: whether or not to vote, vote pooling, and vote sampling for generating proposals. \n% First, we validate the necessity and effectiveness of voting and then compare different aggregation techniques.\n\n\n% \\subsubsection{Voting or Not?}\n% \\smallskip\\noindent\\textbf{To Vote or Not To Vote?}\n\\paragraph{To Vote or Not To Vote?}\n\n% \\begin{figure}\n%     \\centering\n%     \\includegraphics[width=\\linewidth]{./fig/vote_or_not}\n%     \\caption{From left to right: propose object locations from rasterized pixels or voxels, the strategy taken by 3D CNN detectors; propose objects from boundary points, the approach of \\boxnet; propose objects from aggregated votes, which is our \\votenet's appraoch.}\n%     \\label{fig:my_label}\n% \\end{figure}\n\n\n% \\begin{table}[t!]\n%     \\small\n%     \\begin{center}\n%     \\begin{tabular}{c|c|cc}\n%     \\toprule\n%         Method & 3D  & \\multicolumn{2}{c}{mAP@0.25} \\\\\n%         & representation & SUN RGB-D & ScanNet \\\\ \\midrule\n%         DSS~\\cite{song2016deep} & Volumetric & 42.1 & 15.2 \\\\ \n%         3D-SIS~\\cite{hou20183d} & Volumetric &  - & 25.4 \\\\ \\midrule\n%         \\boxnet~(ours) & Point clouds & 53.0 & 39.6 \\\\ \\midrule\n%         \\votenet~(ours) & Point clouds & \\textbf{57.7} & \\textbf{46.8} \\\\\n%         \\bottomrule\n%     \\end{tabular}\n%     \\end{center}\n%     \\caption{\\textbf{Comparing methods with different proposal strategies.} 3D CNNs propose from dense voxels; \\votenet~proposes from vote clusters; \\boxnet~(our baseline model without voting) proposes boxes from seed points on object surfaces.}\n%     \\label{tab:vote_or_not}\n% \\end{table}\n\n\n\nA straightforward baseline to \\votenet{} is a network that directly proposes boxes from sampled scene points. Such a baseline -- which we refer to as \\emph{\\boxnet{}} -- is essential to distill the improvement due to voting. The \\boxnet~has the same backbone as the \\votenet{} but instead of voting, it directly generates boxes from the seed points (more details in appendix). Table~\\ref{tab:vote_or_not} shows voting boosts the performance by a significant margin of ${\\sim}$5 mAP on SUN RGB-D and $>$13 mAP on ScanNet.\n\n%As seen in Table~\\ref{tab:vote_or_not}, the \\boxnet{} already outperforms all previous geometry-based methods by a large margin, and is on-par with the best performing RGB-D based ones. Yet, we\n\nIn what ways, then, does voting help? We argue that since in sparse 3D point clouds, existing scene points are often far from object centroids, a direct proposal may have lower confidence and inaccurate amodal boxes. Voting, instead, brings closer together these lower confidence points and allows to reinforce their hypothesis though aggregation. We demonstrate this phenomenon in Fig.~\\ref{fig:voting_vs_novote} on a typical ScanNetV2 scene. We overlay the scene with only those seed points which, if sampled, would generate an accurate proposal. As can be seen, \\votenet{} (right) offers a much broader coverage of ``good'' seed points compared to \\boxnet{} (left), showing its robustness brought by voting.\n\nWe proceed with a second analysis in Fig.~\\ref{fig:perclass} showing on the same plot (in separate scales), for each SUN RGB-D category: (in blue dots) gains in mAP between \\votenet{} and \\boxnet{}, and (in red squares) closest distances between object points (on their surfaces) and their amodal box centers, averaged per category and normalized by the mean class size (a large distance means the object center is usually far from its surface). Sorting the categories according to the former, we see a strong correlation. Namely, when object points tend to be further from the amodal box center, voting helps much more.\n\n% [TBD] Message 1: Voting is better. Point cloud based methods are more effective (even our baseline is on par or better compared with previous state of the arts).\n% Message 2: Voting is better in two ways: one is that it makes a fuller use of information. See~\\ref{fig:voting_vs_novote}; two is that for certain classes like night stand, dresser and table, existing points are far from object centers, so voting based method has a better chance in localizing them more accurately. See~\\ref{fig:perclass}.\n\n% \\begin{table}[]\n%     \\small\n%     \\centering\n%     \\begin{tabular}{c|c|cc}\n%     \\toprule\n%         Method & Objectness & \\multicolumn{2}{c}{mAP@0.25} \\\\\n%         & & SUN RGB-D & ScanNet \\\\ \\midrule\n%         DSS~\\cite{song2016deep} & IoU & $42.1$ & $15.2$ \\\\ \n%         3D-SIS~\\cite{hou20183d} & IoU & - & $25.4$ \\\\ \\midrule\n%         \\multirow{3}{*}{BoxNet (ours)} & Box XYZ & & \\\\\n%         & Seed XYZ & & \\\\\n%         & Seed class & & \\\\ \\midrule\n%         \\multirow{3}{*}{\\votenet (ours)} & Box XYZ & & \\\\\n%         & Vote XYZ & & \\\\\n%         & Vote class & & \\\\\n%         \\bottomrule\n%     \\end{tabular}\n%     \\caption{Comparing \\votenet with BoxNet (our baseline model without voting) and 3D CNN detectors.}\n%     \\label{tab:vote_or_not}\n% \\end{table}\n\n\n\n\n\n\n% \\smallskip\\noindent\\textbf{Effect of Vote Aggregation}\n\\paragraph{Effect of Vote Aggregation}\n\\label{sec:aggregation_variants}\nAggregation of votes is an important component in \\votenet{} as it allows communication between votes. Hence, it is useful to analyze how different aggregation schemes influence performance. \n\n% Although both \\boxnet{} and our voting-based method regress an offset from boundary points to object centers, the major difference between them is that regressed votes are aggregated in the \\votenet while regressed boxes are not combined in the \\boxnet. \n% In this section, we show how vote aggregation matters for the 3D object detection performance and compare our aggregation with a few alternatives.\n\n%With radius equal to $0$, we do not aggregate votes for proposals -- each vote independently proposes an object bounding box, which is equivalent to the~\\boxnet. As we increase the attention region by increasing the radius in vote aggregation, we see the~\\votenet performance increases and peaks at around $0.2$ radius. \n\nIn Fig.~\\ref{fig:vote_pooling} (right), we show that vote aggregation with a learned Pointnet and max pooling achieves far better results than manually aggregating the vote features in the local regions due to the existence of clutter votes (i.e. votes from non-object seeds). We test 3 types of those aggregations (first three rows): max, average, and RBF weighting (based on vote distance to the cluster center). In contrast to aggregation with Pointnet (Eq.~\\ref{eq:vote_aggregation}), the vote features are directly pooled, e.g. for avg. pooling: $p = \\text{MLP}_2 \\left\\{ \\mbox{AVG}\\{h_i\\} \\right\\}$).\n\nIn Fig.~\\ref{fig:vote_pooling} (left), we show how vote aggregation radius affects detection (tested with Pointent using max pooling). As the aggregation radius increases, \\votenet{} improves until it peaks at around $0.2$ radius. Attending to a larger region though introduces more clutter votes thus contaminating the good votes and results in decreased performance.\n\n\n% A slight improvement is achieved when weighting the votes by their distance to the region center via RBF, however results are still far worse than learning to aggregate by Pointnets. Interestingly, naively pooling the features performs even worse than no aggregation at all, which means that a proper weighting of vote features based on their origins (from objects or clutter) can be very important.\n\n% [TBD] Message: Learned vote pooling $>$  no vote pooling $>$ Fixed way of vote pooling.\n\n% \\rqi{An orthogonal step to vote poo ling is the choice of vote cluster centers (shall we just use FPS or sample according to vote density, vote/seed class scores or other criteria?) We need an experiment section for that.}\n\n% \\begin{table}[]\n%     \\small\n%     \\centering\n%     \\begin{tabular}{c|c|cc}\n%     \\toprule\n%          Pooling &  Pooling func. & \\multicolumn{2}{c}{mAP@0.25} \\\\\n%         & & SUN RGB-D & ScanNet \\\\ \\midrule\n%          None & N.A. & $53.0$ & 1\\\\ \\midrule\n%          \\multirow{3}{*}{Fixed} & avg. & $47.2$ & 1\\\\ \n%           & weighted avg. & $48.9$ & 1\\\\\n%          & max & $47.8$ & 1\\\\ \\midrule\n%          \\multirow{3}{*}{Learned} & avg. & $56.5$ & 1\\\\ \n%          & weighted avg. & 1 & 1\\\\\n%          & max & $57.2$ & 1\\\\ \\bottomrule\n%     \\end{tabular}\n%     \\caption{Effects of vote pooling in the \\votenet. Metric is mAP@0.25 on SUN RGB-D V2 val set and ScanNet V2 val set. Weighted average pooling is based on radial basis function (weighted by distance).\n%     % \\rqi{We are using vote FPS in train and seed FPS in test.}\n%     }\n%     \\label{tab:vote_pooling}\n% \\end{table}\n% \\begin{figure}\n%     \\begin{center}\n%     \\includegraphics[width=\\linewidth]{./fig/vote_pooling2.pdf}\n%     \\end{center}\n%     \\caption{\\textbf{Vote aggregation analysis.} \\emph{Left:} mAP@0.25 on SUN RGB-D for varying aggregation radii when aggregating via Pointnet (max). \\emph{Right:} Comparisons of different aggregation methods (radius = $0.3$ for all methods). Using a learned vote aggregation is far more effective than manually pooling the features in a local neighborhood.}\n%     \\label{fig:vote_pooling}\n% \\end{figure}\n\n\n\n\n\\paragraph{Model Size and Speed}\nOur proposed model is very efficient since it leverages sparsity in point clouds and avoids search in empty space. Compared to previous best methods (Table~\\ref{tab:size_speed}), our model is more than $4 \\times$ smaller than F-PointNet (the prior art on SUN RGB-D) in size and more than $20 \\times$ times faster than 3D-SIS (the prior art on ScanNetV2) in speed. Note that the ScanNetV2 processing time by 3D-SIS is computed as averaged time in offline batch mode while ours is measured with sequential processing which can be realized in online applications.\n\n\n\n\n\n\n\n\n\n% % \\subsubsection{Effects of Vote Cluster Sampling}\n% % \\begin{table}[]\n% %     \\small\n% %     \\centering\n% %     \\begin{tabular}{c|c|c}\n% %     \\toprule\n% %         Vote cluster sampling & Vote weighting & mAP \\\\ \\midrule\n% %         Farthest point sampling & None & $56.7$\\\\\n% %         Random sampling & None & $57.2$\\\\\n% %         FPS on seeds & None & $57.2$\\\\\n% %         FPS on seeds & Yes & $57.7$\\\\\n% %         \\bottomrule\n% %     \\end{tabular}\n% %     \\caption{Effects of vote cluster sampling and vote weighting. \\rqi{Currently the ``score weighted sampling'' is just seed FPS sampling. We need a better sampling strategy or a more convincing analysis here.}}\n% %     \\label{tab:seed_masking}\n% % \\end{table}\n\n"
                },
                "subsection 5.3": {
                    "name": "Qualitative Results and Discussion",
                    "content": "\n\\label{sec:qualitative}\nFig.~\\ref{fig:qualitative_results} and Fig.~\\ref{fig:qualitative_results2} show several representative examples of \\votenet{} detection results on ScanNet and SUN RGB-D scenes, respectively. As can be seen, the scenes are quite diverse and pose multiple challenges including clutter, partiality, scanning artifacts, etc. Despite these challenges, our network demonstrates quite robust results. See for example in Fig.~\\ref{fig:qualitative_results}, how the vast majority of chairs were correctly detected in the top scene. Our method was able to nicely distinguish between the attached sofa-chairs and the sofa in the bottom left scene; and predicted the complete bounding box of the much fragmented and cluttered desk at the bottom right scene.\n\nThere are still limitations in our method though. Common failure cases include misses on very thin objects like doors, windows and pictures denoted in black bounding boxes in the top scene (Fig.~\\ref{fig:qualitative_results}). As we do not make use of RGB information, detecting these categories is almost impossible. Fig.~\\ref{fig:qualitative_results2} on SUN RGB-D also reveals the strengths of our method in partial scans with single-view depth images. For example, it detected more chairs in the top-left scene than were provided by the ground-truth. In the top-right scene we can see how \\votenet{} can nicely hallucinate the amodal bounding box despite seeing only part of the sofa. A less successful amodal prediction is shown in the bottom right scene where an extremely partial observation of a very large table is given. \n\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n% the purpose: we aimed to squeeze as much as possible from geometry. \n% When lifting the problem of object detection from 2D to 3D, an inherent challenge emerges: object centroids can be distant from its surface points and are often vacant. To address this challenge while preserving computational efficiency\nIn this work we have introduced \\votenet{}: a simple, yet powerful 3D object detection model inspired by Hough voting. The network learns to vote to object centroids directly from point clouds and learns to aggregate votes through their features and local geometry to generate high-quality object proposals. Using only 3D point clouds, the model showed significant improvements over previous methods that utilize both depth and colored images.\n% Importantly, the voting layer is general and can be assumed with other backbone architectures.\n% Finally, voting can be interpreted as a form of connecting scene points to be processed together, by embedding learned features into $3$ dimensions.\n% This calls to further investigate this technique from a Graph-CNN viewpoint.\n% In future work we intend to explore these directions, as well as means to incorporate RGB information. We are also interested in utilizing our detector in downstream application such as semantic and instance segmentation.  We also believe that the synergy of Hough voting and deep learning can be applicable to more applications such as 6D pose estimation, template based detection etc. and expect to see more future research along this line.\n\nIn future work we intend to explore how to incorporate RGB images into our detection framework and to utilize our detector in downstream application such as 3D instance segmentation. We believe that the synergy of Hough voting and deep learning can be generalized to more applications such as 6D pose estimation, template based detection etc. and expect to see more future research along this line.\n\\paragraph{Ackownledgements.}\nThis work was supported in part by ONR MURI grant N00014-13-1-0341, NSF grant IIS-1763268 and a Vannevar Bush Faculty Fellowship. We thank Daniel Huber, Justin Johnson, Georgia Gkioxari and Jitendra Malik for valuable discussions and feedback.\n\n%%%%%%%%% Acknowledgements\n% Daniel Huber, Justin Johnson, Jitendra Malik, ...\n\n%%%%%%%%% REFERENCES\n% \\clearpage\n\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{pcl}\n}\n\n\\newpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Appendix",
                "content": "\n\\label{sec:overview}\nThis appendix provides additional details on the network architectures and loss functions (Sec.~\\ref{sec:arch_details}), more analysis experiment results (Sec.~\\ref{sec:more_analysis}), per-category results on ScanNet (Sec.~\\ref{sec:percat_scannet}), and finally more visualizations (Sec.~\\ref{sec:more_vis}).\n% \\begin{itemize}\n%     \\item (must have) More than one vote.\n%     \\item (must have) Loss functions.\n%     \\item (must have) Network architectures on \\votenet{} and BoxNet.\n%     \\item (must have) Per-category results on ScanNetV2\n%     \\item Number of proposals v.s. recall/mAP\n%     \\item Vote context: SA4,FP1-4.\n%     \\item Two-stage detectors.\n%     \\item sampling strategies on vote clustering.\n%     \\item (optional) instance segmentation.\n%     \\item (optional) more than one proposal per vote cluster.\n% \\end{itemize}\n\n",
                "subsection 7.1": {
                    "name": "Details on Architectures and Loss Functions",
                    "content": "\n\\label{sec:arch_details}\n\n\\paragraph{\\votenet{} architecture details.} As mentioned in the main paper, the \\votenet{} architecture composes of a backbone point feature learning network, a voting module and a proposal module.\n\nThe backbone network, based on the PointNet++ architecture~\\cite{qi2017pointnetplusplus}, has four set abstraction layers and two feature up-sampling layers. The detailed layer parameters are shown in Table~\\ref{tab:votingnet_detail}. Each set abstraction (SA) layer has a receptive field specified by a ball-region radius $r$, a MLP network for point feature transform $MLP[c_1,...,c_k]$ where $c_i$ is output channel number of the $i$-th layer in the MLP. The SA layer also subsamples the input point cloud with farthest point sampling to $n$ points. Each SA layer is specified by $(n, r, [c_1,...,c_k])$ as shown in the Table~\\ref{tab:votingnet_detail}. Compared to~\\cite{qi2017pointnetplusplus}, we also normalize the XYZ scale of points in each local region by the region radius.\n\nEach set feature propagation (FP) layer upsamples the point features by interpolating the features on input points to output points (each output point's feature is weighted average of its three nearest input points' features). It also combines the skip-linked features through a MLP (interpolated features and skip-linked features are concatenated before fed into the MLP). Each FP layer is specified by $[c_1,...,c_k]$ where $c_i$ is the output of the $i$-th layer in the MLP.\n\nThe voting module as mentioned in the main paper is a MLP that transforms seeds' features to votes including a XYZ offset and a feature offset. The seed points are outputs of the fp2 layer. The voting module MLP has output sizes of $256,256,259$ for its fully connected layers. The last fully connected layer does not have ReLU or BatchNorm.\n\nThe proposal module as mentioned in the main paper is a SA layer followed by another MLP after the max-pooling in each local region. We follow~\\cite{qi2018frustum} on how to parameterize the oriented 3D bounding boxes. The layer's output has $5+2NH+4NS+NC$ channels where $NH$ is the number of heading bins (we predict a classification score for each heading bin and a regression offset for each bin -- relative to the bin center and normalized by the bin size), $NS$ is the number of size templates (we predict a classification score for each size template and 3 scale regression offsets for height, width and length) and $NC$ is the number of semantic classes. In SUN RGB-D: $NH = 12, NS = NC = 10$, in ScanNet: $NH = 12, NS = NC = 18$. In the first $5$ channels, the first two are for objectness classification and the rest three are for center regression (relative to the vote cluster center). \n\n\n\n\\paragraph{\\votenet{} loss function details.}\nThe network is trained end-to-end with a multi-task loss including a voting loss, an objectness loss, a 3D bounding box estimation loss and a semantic classification loss. We weight the losses such that they are in similar scales with $\\lambda_1 = 0.5$, $\\lambda_2 = 1$ and $\\lambda_3 = 0.1$.\n\n\\begin{equation}\n    L_{\\text{\\votenet{}}} = L_{\\text{vote-reg}} + \\lambda_1 L_{\\text{obj-cls}} + \\lambda_2 L_{\\text{box}} + \\lambda_3 L_{\\text{sem-cls}}\n\\end{equation}\n\nAmong the losses, the vote regression loss is as defined in the main paper (with L1 distance). For ScanNet we compute the ground truth votes as offset from the mesh vertices of an instances to the centers of the axis-aligned tight bounding boxes of the instances. Note that since the bounding box is not amodal, they can vary in sizes due to scan completeness (e.g. a chair may have a floating bounding box if its leg is not recovered from the reconstruction). For SUN RGB-D since the dataset does not provide instance segmentation annotations but only amodal bounding boxes, we cannot compute a ground truth vote directly (as we don't know which points are on objects). Instead, we consider any point inside an annotated bounding box as an object point (required to vote) and compute its offset to the box center as the ground truth. In cases that a point is in multiple ground truth boxes, we keep a set of up to three ground truth votes, and consider the minimum distance between the predicted vote and any ground truth vote in the set during vote regression on this point.\n\nThe objectness loss is just a cross-entropy loss for two classes and the semantic classification loss is also a cross-entropy loss of $NC$ classes.\n\nThe box loss follows~\\cite{qi2018frustum} (but without the corner loss regularization for simplicity) and is composed of center regression, heading estimation and size estimation sub-losses. In all regression in the box loss we use the robust $L1$-smooth loss. Both the box and semantic losses are only computed on positive vote clusters and normalized by the number of positive clusters. We refer readers to~\\cite{qi2018frustum} for more details.\n\n\n\\begin{equation}\n\\begin{split}\n    L_{\\text{box}} & = L_{\\text{center-reg}} + 0.1 L_{\\text{angle-cls}} + L_{\\text{angle-reg}} \\\\\n    & + 0.1 L_{\\text{size-cls}} + L_{\\text{size-reg}}\n\\end{split}\n\\end{equation}\n\nOne difference though is that, instead of a naive regression loss, we use a \\emph{Chamfer loss}~\\cite{fan2017point} for $L_{\\text{center-reg}}$ (between regressed centers and ground truth box centers). It requires that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it. The latter part also influences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object, which helps further increase contexts in detection.\n\n\n\\paragraph{BoxNet architecture details.}\nOur baseline network without voting, BoxNet, shares most parts with the \\votenet{}. They share the same backbone architecture. But instead of voting from seeds, the BoxNet directly proposes bounding boxes and classifies object classes from seed points' features. To make the BoxNet and \\votenet{} have similar capacity we also include a SA layer for the proposal in BoxNet. However this SA layer takes \\emph{seed clusters} instead of \\emph{vote clusters} i.e. it samples seed points and then combines neighboring seeds with MLP and max-pooling. This SA layer has exactly the same layer parameters with that in the \\votenet{}, followed by the same $MLP_2$.\n\n\\paragraph{BoxNet loss function details.}\nBoxNet has the same loss function as \\votenet{}, except it is not supervised by vote regression. There is also a slight difference in how objectness labels (used to supervise objectness classification) are computed. As seed points (on object surfaces) are often far from object centroids, it no longer works well to use the distances between seed points and object centroids to compute the objectness labels. In BoxNet, we assign positive objectness labels to seed points that are on objects (those belonging to the semantic categories we consider) and negative labels to all the other seed points on clutter (e.g. walls, floors).\n\n\\begin{equation}\n    L_{\\text{BoxNet}} = \\lambda_1 L_{\\text{obj-cls}} + \\lambda_2 L_{\\text{box}} + \\lambda_3 L_{\\text{sem-cls}}\n\\end{equation}\n\n\n\n\n\n"
                },
                "subsection 7.2": {
                    "name": "More Analysis Experiments",
                    "content": "\n\\label{sec:more_analysis}\n\n\\paragraph{Average precision and recall plots}\nFig.~\\ref{fig:ap_ar} shows how average precision (AP) and average recall (AR) change as we increase the number of proposals. The AP and AR are both averaged across 10 categories on SUN RGB-D. We report two ways of using the proposals: joint and per-class. For the joint proposal we propose $K$ objects' bounding boxes for all the 10 categories, where we consider each proposal as the semantic class it has the largest confidence in, and use their objectness scores to rank them. For the per-class proposal we duplicate the $K$ proposal 10 times thus have $K$ proposals per class where we use the multiplication of semantic probability for that class and the objectness probability to rank them. The latter way of using proposals gives us  a slight improvement on AP and a big boost on AR.\n\nWe see that with as few as 10 proposals our \\votenet{} can achieve a decent AP of around $45\\%$ while having 100 proposals already pushes the AP to above $57\\%$. With a thousand proposals, our network can achieve around $78.7\\%$ recall with joint proposal and around $87.7\\%$ recall with per-class proposal.\n\n\n\n\\paragraph{Context of voting}\nOne difference of a deep Hough voting scheme with the traditional Hough voting is that we can take advantage of deep features, which can provide more context knowledge for voting. In Table~\\ref{tab:vote_context} we show how features from different levels of the PointNet++ affects detection performance (from SA2 to FP3, the network has increasing contexts for voting). FP3 layer is extended from the FP2 with a MLP of output sizes 256 and 256 with 2048 output points (the same set of XYZ as that output by SA1).\n\nIt is surprising to find that voting from even SA2 can achieve reasonable detection results (mAP $51.2\\%$) while voting from FP2 achieves the best performance. Having larger context (e.g. FP3) than FP2 does not show further improvements on the performance.\n\n\n\n% \\paragraph{Two-stage detection}\n% Is the \\boxnet{} on par or better than \\votenet{} with a two-stage detection scheme?\n\n\\paragraph{Multiple votes per seed}\n% How to support multiple votes per seed and does it help?\nIn default we just generate one vote per seed since we find that with large enough context there is little need to generate more than one vote to resolve ambiguous cases. However, it is still possible to generate more than one vote with our network architecture. Yet to break the symmetry in multiple vote generation, one has to introduce some bias to different votes to prevent then from pointing to the same place.\n\nIn experiments, we find that one vote per seed achieves the best results, as shown in Table~\\ref{tab:vote_num}. We ablate by using a vote factor of $3$, where the voting module generates $3$ votes per seed with a MLP layer spec: $[256,256,259*3]$). In computing the vote regression loss on a seed point, we consider the minimum distance between any predicted votes to the ground truth vote (in case of SUN RGB-D where we may have a set of ground truth votes for a seed, we compute the minimum distance among any pair of predicted vote and ground truth vote).\n\n\n\n\n\nTo break symmetry, we generate $3$ random numbers and inject them to the second last features from the MLP layer. We show results both with and without this procedure which shows no observable difference.  %In both cases, the network with $3$ votes per seed perform worse than the one with 1 vote per seed.\n\n\n\n\\paragraph{On proposal sampling}\nIn the proposal step, to generate $K$ proposals from the votes, we need to select $K$ vote clusters. How to select those clusters is a design choice we study here (each cluster is simply a group of votes near a center vote). In Table~\\ref{tab:sampling}, we report mAP results on SUN RGB-D with 256 proposals (joint proposal) using cluster sampling strategies of vote FPS, seed FPS and random sampling, where FPS means farthest point sampling. From $1024$ vote clusters, vote FPS samples $K$ clusters based on votes' XYZ. Seed FPS firstly samples on seed XYZ and then finds the votes corresponding to the sampled seeds -- it enables a direct comparison with BoxNet as it uses the same sampling scheme, making the two techniques similar up to the space in which the points are grouped: \\votenet{} groups votes according to vote XYZ, while BoxNet groups seeds according to seed XYZ. Random sampling simply selects a random set of $K$ votes and take their neighborhoods for proposal generation. Note that the results from Table~\\ref{tab:sampling} are from the same model trained with vote FPS to select proposals.\n\nWe can see that while seed FPS gets the best number in mAP, the difference caused by different sampling strategies is small, showing the robustness of our method.\n\n\n\n% FPS on seeds means that we choose the indices of votes based on the 3D Euclidean distances of the seeds corresponding to the votes.\n% FPS on seeds is the same way as how proposal locations are chosen in the BoxNet, but differently we group local information in the vote $XYZ$ space thus getting a different neighborhood and more effective context.\n\\paragraph{Effects of the height feature}\nIn point clouds from indoor scans, point height is a useful feature in recognition. As mentioned in the main paper, we can use $1\\%$ of the $Z$ values ($Z$-axis is up-right) of all points from a scan as an approximate as the floor height $z_{\\text{floor}}$, and then compute the a point $(x,y,z)$'s height as $z - z_{\\text{floor}}$. In Table~\\ref{tab:height} we show how this extra height feature affect detection performance. We see that adding the height feature consistently improves performance in both SUN RGB-D and ScanNet.\n\n\n\n"
                },
                "subsection 7.3": {
                    "name": "ScanNet Per-class Evaluation",
                    "content": "\n\\label{sec:percat_scannet}\n\nTable~\\ref{tab:perclassscannet025} and Table~\\ref{tab:perclassscannet050} report per-class average precision on 18 classes of ScanNetV2 with $0.25$ and $0.5$ box IoU thresholds respectively. Relying on purely geonetric data, our method excels (esp. with mAP@0.25) in detecting objects like bed, chair, table, desk etc. where geometry is a strong cue for recognition; and struggles with objects best recognized by texture and color like pictures.\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Visualization of Votes",
                    "content": "\n\\label{sec:more_vis}\n% \\begin{figure*}\n%     \\centering\n%     \\includegraphics[width=\\linewidth,height=6cm]{./fig/placeholder.pdf}\n%     \\caption{\\todo{Visualizing votes from clutter seeds and object seeds.}}\n%     \\label{fig:my_label}\n% \\end{figure*}\nFig.~\\ref{fig:showvotes} shows (a subset of) votes predicted from our \\votenet{} in a typical ScanNet scene. We clearly see that seed points on objects (bed, sofa etc.) vote to object centers while clutter points vote either to object center as well (if the clutter point is close to the object) or to nowhere due to lack of structure in the clutter area (e.g. a wall).\n\n"
                }
            }
        },
        "tables": {
            "tab:sunrgbd": "\\begin{table*}[t!]\n\\small\n\\setlength{\\tabcolsep}{4.8pt}\n\\begin{center}\n% The model we used is in eval_0310_uptodate_l1_fps_on_seeds2_clsnms_v1data_run4/log_eval.txt evaluated with use_v1, batch_size 1, 3d NMS iou 0.25 and 256 targets.\n\\begin{tabular}{l|c|x{25}x{25}x{25}x{25}x{25}x{25}x{25}x{25}x{25}x{25}|c}\n\\toprule\n          & Input & bathtub & bed & bookshelf & chair & desk & dresser & nightstand & sofa & table & toilet & mAP \\\\ \\midrule\nDSS~\\cite{song2016deep} & Geo + RGB & 44.2 & 78.8 & 11.9 & 61.2 & 20.5 & 6.4 & 15.4 & 53.5 & 50.3 & 78.9 & 42.1    \\\\\nCOG~\\cite{ren2016three} & Geo + RGB & 58.3 & 63.7 & 31.8 & 62.2 & \\textbf{45.2} & 15.5 & 27.4 & 51.0 & \\textbf{51.3} & 70.1 & 47.6 \\\\\n2D-driven~\\cite{lahoud20172d} & Geo + RGB & 43.5 & 64.5 & 31.4 & 48.3 & 27.9 & 25.9 & 41.9 & 50.4 & 37.0 & 80.4 & 45.1  \\\\\nF-PointNet~\\cite{qi2018frustum} & Geo + RGB & 43.3 & 81.1 & \\textbf{33.3} & 64.2 & 24.7 & \\textbf{32.0} & 58.1 & 61.1 & 51.1 & \\textbf{90.9} & 54.0 \\\\ \\midrule\n\\votenet~(ours) & \\textbf{Geo only} & \\textbf{74.4} & \\textbf{83.0} & 28.8 & \\textbf{75.3} & 22.0 & 29.8 & \\textbf{62.2} & \\textbf{64.0} & 47.3 & 90.1 & \\textbf{57.7} \\\\\n%Ours (two-stage) (v2) & $\\textbf{75.2}$ & $\\textbf{83.0}$ & $29.5$ & $\\textbf{75.8}$ & $18.1$ & $29.5$ & $\\textbf{59.2}$ & $\\textbf{66.1}$ & $49.0$ & $89.2$ & N.A. & \\textbf{57.5} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{\\textbf{3D object detection results on SUN RGB-D val set.} Evaluation metric is average precision with 3D IoU threshold 0.25 as proposed by~\\cite{song2015sun}. Note that both COG~\\cite{ren2016three} and 2D-driven~\\cite{lahoud20172d} use room layout context to boost performance. To have fair comparison with previous methods, the evaluation is on the SUN RGB-D V1 data.\n% \\rqi{Explain why we are not doing well on bookshelf, desk, dresser and table.}\n% Compared with previous state-of-the-arts our method is 6.4\\% to 11.9\\% better in mAP as well as one to three orders of magnitude faster.\n}\n\\label{tab:sunrgbd}\n\\end{table*}",
            "tab:scannet": "\\begin{table}[t!]\n\\small\n\\setlength{\\tabcolsep}{3.2pt}\n\\begin{center}\n\\begin{tabular}{l |c| c c}\n%{ l@{\\hskip 0.01\\textwidth}c@{\\hskip 0.01\\textwidth}c@{\\hskip 0.01\\textwidth}c@{\\hskip 0.01\\textwidth}c@{\\hskip 0.01\\textwidth}c@{\\hskip 0.01\\textwidth}c@{\\hskip 0.01\\textwidth}c  }\n    \\toprule\n    & Input & mAP@0.25 & mAP@0.5 \\\\ \\hline    \n    DSS~\\cite{song2016deep,hou20183d} & Geo + RGB & 15.2 & 6.8  \\\\\n    MRCNN 2D-3D~\\cite{he2017mask,hou20183d} & Geo + RGB & 17.3 & 10.5 \\\\\n    F-PointNet~\\cite{qi2018frustum,hou20183d} & Geo + RGB & 19.8 & 10.8 \\\\\n    GSPN~\\cite{yi2018gspn} & Geo + RGB & 30.6 & 17.7 \\\\ \\midrule\n    3D-SIS \\cite{hou20183d} & Geo + 1 view & 35.1 & 18.7 \\\\ \n    3D-SIS \\cite{hou20183d} & Geo + 3 views & 36.6 & 19.0 \\\\\n    3D-SIS \\cite{hou20183d} & Geo + 5 views & 40.2 & 22.5 \\\\ \\midrule\n    3D-SIS \\cite{hou20183d} & Geo only & 25.4 & 14.6 \\\\\n    % {\\bf Ours} & Geo only &$\\textbf{46.75} \\pm \\textbf{0.32}$ & $\\textbf{24.65} \\pm \\textbf{0.58}$ \\\\ \\bottomrule \n    \\votenet~(ours) & Geo only & \\textbf{58.6} & \\textbf{33.5} \\\\ \\bottomrule \n\\end{tabular}\n\\end{center}\n%   \\vspace{1cm}\n\\caption{\\small \\textbf{3D object detection results on ScanNetV2 val set.} DSS and F-PointNet results are from~\\cite{hou20183d}. Mask R-CNN 2D-3D results are from~\\cite{yi2018gspn}. GSPN and 3D-SIS results are up-to-date numbers provided by the original authors.\n}\n\\label{tab:scannet}\n\\end{table}",
            "tab:vote_or_not": "\\begin{table}[t!]\n    \\small\n    \\begin{center}\n    \\begin{tabular}{c|cc}\n    \\toprule\n        Method & \\multicolumn{2}{c}{mAP@0.25} \\\\\n        & SUN RGB-D & ScanNet \\\\ \\midrule\n        % \\boxnet~(ours) & 53.0 & 39.6 \\\\ \\midrule\n         \\boxnet~(ours) & 53.0 & 45.4 \\\\ \\midrule\n        \\votenet~(ours) & \\textbf{57.7} & \\textbf{58.6} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\end{center}\n    \\caption{\\textbf{Comparing \\votenet{} with a no-vote baseline.} Metric is 3D object detection mAP. \\votenet~estimate object bounding boxes from vote clusters. \\boxnet~ proposes boxes directly from seed points on object surfaces without voting.}\n    \\label{tab:vote_or_not}\n\\end{table}",
            "tab:size_speed": "\\begin{table}[b!]\n\\small\n    \\begin{center}\n    \\begin{tabular}{l|c|c|c}\n    \\toprule\n        Method & Model size & SUN RGB-D & ScanNetV2 \\\\ \\midrule\n        F-PointNet~\\cite{qi2018frustum} & $47.0$MB & $0.09s$ & -\\\\\n        3D-SIS~\\cite{hou20183d} & $19.7$MB & - & $2.85$s \\\\ \\midrule\n        \\votenet~(ours) & 11.2MB & $0.10s$ & $0.14$s\\\\ \\bottomrule\n    \\end{tabular}\n    \\end{center}\n    \\caption{\\textbf{Model size and processing time (per frame or scan).} Our method is more than $4 \\times$ more compact in model size than~\\cite{qi2018frustum} and more than $20 \\times$ faster than~\\cite{hou20183d}.}\n    \\label{tab:size_speed}\n\\end{table}",
            "tab:votingnet_detail": "\\begin{table*}[]\n    \\begin{center}\n    \\begin{tabular}{|c|c|c|c|c|}\n    \\hline\n        layer name & input layer & type & output size &  layer params\\\\ \\hline\n        sa1 & raw point cloud & SA & (2048,3+128) & (2048,0.2,[64,64,128]) \\\\ \n        sa2 & sa1 & SA & (1024,3+256) & (1024,0.4,[128,128,256])\\\\\n        sa3 & sa2 & SA & (512,3+256) & (512,0.8,[128,128,256]) \\\\\n        sa4 & sa3 & SA & (256,3+256) & (256,1.2,[128,128,256]) \\\\\n        fp1 & sa3, sa4 & FP & (512,3+256) & [256,256] \\\\\n        fp2 & sa2, sa3 & FP & (1024,3+256) & [256,256] \\\\ \\hline\n    \\end{tabular}\n    \\end{center}\n    \\caption{Backbone network architecture: layer specifications.}\n    \\label{tab:votingnet_detail}\n\\end{table*}",
            "tab:perclassscannet025": "\\begin{table*}[t!]\n\\begin{center}\n\\footnotesize\n\\setlength{\\tabcolsep}{3pt}\n\\begin{tabular}{l|cccccccccccccccccc|c}\n\\toprule\n& cab & bed & chair & sofa & tabl & door & wind & bkshf & pic & cntr & desk & curt & fridg & showr & toil & sink & bath & ofurn & mAP \\\\ \\midrule\n3DSIS 5views~\\cite{hou20183d} & 19.76 & 69.71 & 66.15 & 71.81 & 36.06 & 30.64 & 10.88 & 27.34 & 0.00 & 10.00 & 46.93 & 14.06 & 53.76 & 35.96 & 87.60 & 42.98 & 84.30 & 16.20 & 40.23 \\\\\n3DSIS Geo~\\cite{hou20183d} & 12.75 & 63.14 & 65.98 & 46.33 & 26.91 & 7.95 & 2.79 & 2.30 & 0.00 & 6.92 & 33.34 & 2.47 & 10.42 & 12.17 & 74.51 & 22.87 & 58.66 & 7.05 & 25.36 \\\\\n%\\votenet{} (ours) & 29.96 & 82.87 & 78.66 & 76.32 & 53.10 & 29.64 & 23.57 & 33.59 & 2.37 & 36.92 & 57.76 & 33.71 & 33.68 & 45.14 & 86.98 & 37.94 & 79.77 & 19.58 & 46.75 \\\\\n\\votenet{} {ours} & 36.27 & 87.92 & 88.71 & 89.62 & 58.77 & 47.32 & 38.10 & 44.62 & 7.83 & 56.13 & 71.69 & 47.23 & 45.37 & 57.13 & 94.94 & 54.70 & 92.11 & 37.20 & 58.65 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.25 IoU.}\n\\label{tab:perclassscannet025}\n\\end{table*}",
            "tab:perclassscannet050": "\\begin{table*}[t!]\n\\begin{center}\n\\footnotesize\n\\setlength{\\tabcolsep}{3.2pt}\n\\begin{tabular}{l|cccccccccccccccccc|c}\n\\toprule\n& cab & bed & chair & sofa & tabl & door & wind & bkshf & pic & cntr & desk & curt & fridg & showr & toil & sink & bath & ofurn & mAP \\\\ \\midrule\n3DSIS 5views~\\cite{hou20183d} & 5.73 & 50.28 & 52.59 & 55.43 & 21.96 & 10.88 & 0.00 & 13.18 & 0.00 & 0.00 & 23.62 & 2.61 & 24.54 & 0.82 & 71.79 & 8.94 & 56.40 & 6.87 & 22.53 \\\\\n3DSIS Geo~\\cite{hou20183d} & 5.06 & 42.19 & 50.11 & 31.75 & 15.12 & 1.38 & 0.00 & 1.44 & 0.00 & 0.00 & 13.66 & 0.00 & 2.63 & 3.00 & 56.75 & 8.68 & 28.52 & 2.55 & 14.60 \\\\\n%\\votenet{} (ours) & 6.42 & 71.26 & 45.14 & 50.19 & 31.88 & 6.15 & 4.04 & 22.32 & 0.08 & 5.14 & 21.32 & 9.36 & 14.27 & 7.75 & 65.56 & 16.50 & 62.86 & 3.53 & 24.65 \\\\\n\\votenet{} (ours) & 8.07 & 76.06 & 67.23 & 68.82 & 42.36 & 15.34 & 6.43 & 28.00 & 1.25 & 9.52 & 37.52 & 11.55 & 27.80 & 9.96 & 86.53 & 16.76 & 78.87 & 11.69 & 33.54 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.5 IoU.}\n\\label{tab:perclassscannet050}\n\\end{table*}",
            "tab:vote_context": "\\begin{table}[h]\n    \\begin{center}\n    \\begin{tabular}{l|cccccc}\n    \\toprule\n         Seed layer & SA2 & SA3 & SA4 & FP1 & FP2 & FP3 \\\\ \\midrule\n         mAP & 51.2 & 56.3 & 55.1 & 56.6 & \\textbf{57.7} & 57.1 \\\\ \\bottomrule\n    \\end{tabular}\n    \\end{center}\n    \\caption{\\textbf{Effects of seed context for 3D detection.} Evaluation metric is mAP@0.25 on SUN RGB-D.}\n    \\label{tab:vote_context}\n\\end{table}",
            "tab:vote_num": "\\begin{table}[]\n    \\begin{center}\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n         Vote factor & 1 & 3 & 3 \\\\ \\hline\n         Random number & N & N & Y \\\\ \\hline\n         mAP & \\textbf{57.7} & 55.8 & 55.8 \\\\ \\bottomrule\n    \\end{tabular}\n    \\end{center}\n    \\caption{\\textbf{Effects of number of votes per seed.} Evaluation metric is mAP@0.25 on SUN RGB-D. If random number is on, we concatenate a random number to the seed feature before voting, which helps break symmetry in the case of multiple votes per seed.}\n    \\label{tab:vote_num}\n\\end{table}",
            "tab:sampling": "\\begin{table}[]\n    \\begin{center}\n    \\begin{tabular}{l|c}\n    \\toprule\n        Proposal sampling & mAP \\\\ \\midrule\n        Random sampling & 57.5\\\\ % 0.577005, 0.573915\n        Farthest point sampling on votes & 57.2\\\\ % 0.569167 0.575852\n        Farthest point sampling on seeds & \\textbf{57.7}\\\\ \n        \\bottomrule\n    \\end{tabular}\n    \\end{center}\n    \\caption{\\textbf{Effects of proposal sampling.} Evaluation metric is mAP@0.25 on SUN RGB-D. $256$ proposals are used for all evaluations. Our method is not sensitive to how we choose centers for vote groups/clusters.}\n    \\label{tab:sampling}\n\\end{table}",
            "tab:height": "\\begin{table}[]\n    \\centering\n    \\begin{tabular}{l|c|c}\n    \\toprule\n         Dataset & with height & without height \\\\ \\midrule\n         SUN RGB-D & 57.7 & 57.0 \\\\ \\hline\n         ScanNet & 58.6 & 58.1 \\\\ \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effects of the height feature.} Evaluation metric is mAP@0.25 on both datasets.}\n    \\label{tab:height}\n\\end{table}"
        },
        "figures": {
            "fig:teaser": "\\begin{figure}%[h]\n    \\centering\n    \\vspace{14pt}\n    \\begin{overpic}\n    [trim=0cm 0cm 0cm 0cm,clip,width=0.95\\linewidth]{fig/teaser.pdf}\n    \\put(2,43){\\small Voting from input point cloud}\n    \\put(60,43){\\small 3D detection output}\n    \\end{overpic}\n    % \\includegraphics[width=\\linewidth]{fig/teaser.pdf}\n    % \\includegraphics[width=1\\linewidth]{fig/placeholder.pdf}\n    \\caption{\\textbf{3D object detection in point clouds with a deep Hough voting model.} Given a point cloud of a 3D scene, our \\votenet{} votes to object centers and then groups and aggregates the votes to predict 3D bounding boxes and semantic classes of objects. Our code is open sourced at \\url{https://github.com/facebookresearch/votenet}\n    }\n    \\label{fig:teaser}\n\\end{figure}",
            "fig:votingnet": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{./fig/votingnet4.pdf}\n    \\caption{\\textbf{Illustration of the \\votenet~architecture} for 3D object detection in point clouds. Given an input point cloud of $N$ points with XYZ coordinates, a backbone network (implemented with PointNet++~\\cite{qi2017pointnetplusplus} layers) subsamples and learns deep features on the points and outputs a subset of $M$ points but extended by $C$-dim features. This subset of points are considered as seed points. Each seed independently generates a vote through a voting module. Then the votes are grouped into clusters and processed by the proposal module to generate the final proposals. The classified and NMSed proposals become the final 3D bounding boxes output. Image best viewed in color.}\n    \\label{fig:votingnet}\n\\end{figure*}",
            "fig:voting_vs_novote": "\\begin{figure}[t!]\n    \\centering\n    \\begin{overpic}\n    [trim=0cm 0cm 0cm 0cm,clip,width=0.8\\linewidth]{fig/voteOrNot.pdf}\n    \\put(8,73){\\small \\boxnet{} (no voting)}\n    \\put(65,73){\\small \\votenet{}}\n    \\end{overpic}\n    % \\includegraphics[width=\\linewidth]{fig/voteOrNot.pdf}\n    \\caption{\\textbf{Voting helps increase detection contexts.} Seed points that generate good boxes (\\boxnet), or good votes (\\votenet) which in turn generate good boxes, are overlaid (in blue) on top of a representative ScanNet scene. As the voting step effectively increases context, \\votenet{} demonstrates a much denser cover of the scene, therefore increasing the likelihood of accurate detection.}\n    \\label{fig:voting_vs_novote}\n\\end{figure}",
            "fig:perclass": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{fig/acc_vs_dist.pdf}\n    \\caption{\\textbf{Voting helps more in cases where object points are far from object centers}. We show for each category: voting accuracy gain (in blue dots) of \\votenet{} w.r.t our direct proposal baseline \\boxnet{}; and (in red squares) average object-center distance, normalized by the mean class size.}\n    \\label{fig:perclass}\n\\end{figure}",
            "fig:vote_pooling": "\\begin{figure}[t!]\n\\begin{minipage}[c]{0.52\\linewidth}\n\\begin{center}\n\\includegraphics[width=\\linewidth]{./fig/vote_pooling2.pdf}\n\\end{center}\n\\end{minipage}%\n\\begin{minipage}[c]{0.48\\linewidth}\n\\begin{center}\n{\n\\fontsize{8pt}{1em}\\selectfont\n\\begin{tabular}{c|c}\n\\toprule\n  Aggregation method & mAP \\\\\n\\midrule\nFeature avg. & 47.2 \\\\ \nFeature max & 47.8 \\\\\nFeature RBF avg. & 49.0 \\\\\n\\midrule\nPointnet (avg.) & 56.5 \\\\\nPointnet (max) & 57.7 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\end{minipage}\n\\vspace{0.1in}\n    \\caption{\\textbf{Vote aggregation analysis.} \\emph{Left:} mAP@0.25 on SUN RGB-D for varying aggregation radii when aggregating via Pointnet (max). \\emph{Right:} Comparisons of different aggregation methods (radius = $0.3$ for all methods). Using a learned vote aggregation is far more effective than manually pooling the features in a local neighborhood.}\n    \\label{fig:vote_pooling}\n\\end{figure}",
            "fig:qualitative_results": "\\begin{figure*}[t!]\n    \\centering\n    \\begin{overpic}\n    [trim=0cm 0cm 0cm 0cm,clip,width=0.86\\linewidth]{fig/scannet_results_for_paper.pdf}\n    \\put(15,50){\\small \\votenet{} prediction}\n    \\put(70,50){\\small Ground truth}\n    \\end{overpic}\n    \\caption{\\textbf{Qualitative results of 3D object detection in ScanNetV2.} Left: our \\votenet{}, Right: ground-truth. See Section \\ref{sec:qualitative} for details.}\n    \\label{fig:qualitative_results}\n\\end{figure*}",
            "fig:qualitative_results2": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=0.86\\linewidth]{fig/sunrgbd_results.jpg}\n    % \\includegraphics[width=1\\linewidth]{fig/placeholder.pdf}\n    \\caption{\\textbf{Qualitative results on SUN RGB-D.} Both left and right panels show (from left to right): an image of the scene (not used by our network), 3D object detection by \\votenet{}, and ground-truth annotations. See Section \\ref{sec:qualitative} for details.}\n    \\label{fig:qualitative_results2}\n\\end{figure*}",
            "fig:ap_ar": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{./fig/ap_ar.pdf}\n    \\caption{\\textbf{Number of proposals per scene v.s. Average Precision (AP) and Average Recall (AR) on SUN RGB-D.} The AP and AR are averaged across the 10 classes. The recall is maximum recall given a fixed number of detection per scene. The ``joint proposal'' means that we assign each proposal to a single class (the class with the highest classification score); The ``per-class proposal'' means that we assign each proposal to all the 10 classes (the objectness score is multipled by the semantic classification probability).}\n    \\label{fig:ap_ar}\n\\end{figure}",
            "fig:showvotes": "\\begin{figure}[t!]\n    \\centering\n    % \\vspace{14pt}\n    \\begin{overpic}\n    [trim=0cm 0cm 0cm 0cm,clip,width=\\linewidth]{fig/votes_scene.pdf}\n    % \\put(2,43){\\small Voting from input point cloud}\n    % \\put(60,43){\\small 3D detection output}\n    \\end{overpic}\n    \\caption{\\textbf{Vote meeting point.} \\emph{Left:} ScanNet scene with votes coming from object points. \\emph{Right:} vote offsets from source seed-points to target-votes. Object votes are colored green, and non-object ones are colored red. See how object points from all-parts of the object vote to form a cluster near the center. Non-object points, however, either vote ``nowhere'' and therefore lack structure, or are near object and have gathered enough context to also vote properly. }\n    \\label{fig:showvotes}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    L_{\\text{vote-reg}} = \\frac{1}{M_{\\text{pos}}} \\sum_i  \\|\\Delta x_i - \\Delta x_i^*\\| \\mathds{1}[s_i \\text{ on object}], \n\\end{equation}",
            "eq:2": "\\begin{equation}\n    p(\\mathcal{C}) = \\text{MLP}_2 \\left\\{ \\underset{i=1,...,n}{\\mbox{max}}\\left\\{\\text{MLP}_1 ([z'_i; h_i])\\right\\} \\right\\}\n    \\label{eq:vote_aggregation}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    L_{\\text{\\votenet{}}} = L_{\\text{vote-reg}} + \\lambda_1 L_{\\text{obj-cls}} + \\lambda_2 L_{\\text{box}} + \\lambda_3 L_{\\text{sem-cls}}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\begin{split}\n    L_{\\text{box}} & = L_{\\text{center-reg}} + 0.1 L_{\\text{angle-cls}} + L_{\\text{angle-reg}} \\\\\n    & + 0.1 L_{\\text{size-cls}} + L_{\\text{size-reg}}\n\\end{split}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    L_{\\text{BoxNet}} = \\lambda_1 L_{\\text{obj-cls}} + \\lambda_2 L_{\\text{box}} + \\lambda_3 L_{\\text{sem-cls}}\n\\end{equation}"
        },
        "git_link": "https://github.com/facebookresearch/votenet"
    }
}