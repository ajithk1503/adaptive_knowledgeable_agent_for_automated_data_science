{
    "meta_info": {
        "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series  Modeling",
        "abstract": "Modeling continuous-time dynamics on irregular time series is critical to\naccount for data evolution and correlations that occur continuously.\nTraditional methods including recurrent neural networks or Transformer models\nleverage inductive bias via powerful neural architectures to capture complex\npatterns. However, due to their discrete characteristic, they have limitations\nin generalizing to continuous-time data paradigms. Though neural ordinary\ndifferential equations (Neural ODEs) and their variants have shown promising\nresults in dealing with irregular time series, they often fail to capture the\nintricate correlations within these sequences. It is challenging yet demanding\nto concurrently model the relationship between input data points and capture\nthe dynamic changes of the continuous-time system. To tackle this problem, we\npropose ContiFormer that extends the relation modeling of vanilla Transformer\nto the continuous-time domain, which explicitly incorporates the modeling\nabilities of continuous dynamics of Neural ODEs with the attention mechanism of\nTransformers. We mathematically characterize the expressive power of\nContiFormer and illustrate that, by curated designs of function hypothesis,\nmany Transformer variants specialized in irregular time series modeling can be\ncovered as a special case of ContiFormer. A wide range of experiments on both\nsynthetic and real-world datasets have illustrated the superior modeling\ncapacities and prediction performance of ContiFormer on irregular time series\ndata. The project link is https://seqml.github.io/contiformer/.",
        "author": "Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, Dongsheng Li",
        "link": "http://arxiv.org/abs/2402.10635v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Neurips 2023 Poster"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\nIrregular time series are prevalent in real-world applications like disease prevention, financial decision-making, and earthquake prediction~\\cite{zuo2020transformer, bauer2016arrow, jia2019neural}. Their distinctive properties set them apart from regular time series data.\nFirst, irregular time series data are characterized by irregularly generated or non-uniformly sampled observations with variable time intervals, as well as missing data due to technical issues, or data quality problems, which pose challenges for traditional time series analysis techniques~\\cite{che2018recurrent, yue2022ts2vec}.\nSecond, even though the observations are irregularly sampled, the underlying data-generating process is assumed to be continuous~\\cite{rubanova2019latent, kidger2020neural, park2022neural}.\nThird, the relationships among the observations can be intricate and continuously evolving.\nAll these characteristics require elaborate modeling approaches for better understanding these data and making accurate predictions.\n\nThe continuity and intricate dependency of these data samples pose significant challenges for model design. \nSimply dividing the timeline into equally sized intervals can severely damage the continuity of the data~\\cite{lipton2016directly}. \nRecent works have suggested that underlying continuous-time process is appreciated for irregular time series modeling~\\cite{rubanova2019latent, kidger2020neural, che2018recurrent, mei2017neural}, which requires the modeling procedure to capture the continuous dynamic of the system. \nFurthermore, due to the continuous nature of data flow, we argue that the correlation within the observed data is also constantly changing over time.\nFor instance, stock prices of tech giants (e.g., MSFT and GOOG) show consistent evolving trends, yet they are affected by short-term events (e.g., large language model releases) and long-term factors. \n\nTo tackle these challenges, researchers have pursued two main branches of solutions. \nNeural ordinary differential equations (Neural ODEs) and state space models (SSMs) have illustrated promising abilities for capturing the dynamic change of the system over time~\\cite{smith2022simplified, kidger2022neural, chen2018neural, gu2021efficiently}. \nHowever, these methods overlook the intricate relationship between observations and their recursive nature can lead to cumulative errors if the number of iterations is numerous~\\cite{chen2018neural}.\nAnother line of research capitalizes on the powerful inductive bias of neural networks, such as various recurrent neural networks~\\cite{che2018recurrent, mei2017neural, schirmer2022modeling} and Transformer models~\\cite{xu2019self, shukla2021multi, ma2022non, zhang2019attain}. \nHowever, their use of fixed-time encoding or learning upon certain kernel functions fails to capture the complicated input-dependent dynamic systems.\n\nBased on the above analysis, modeling the relationship between observations while capturing the temporal dynamics is a challenging task. \nTo address this problem, we propose a Continuous-Time Transformer, namely \\textit{ContiFormer}, which incorporates the modeling abilities of continuous dynamics of Neural ODEs within the attention mechanism of Transformers and breaks the discrete nature of Transformer models.\nSpecifically, to capture the dynamics of the observations, ContiFormer begins by defining latent trajectories for each observation in the given irregularly sampled data points. \nNext, to capture the intricate yet continuously evolving relationship, it extends the discrete dot-product in Transformers to a continuous-time domain, where attention is calculated between continuous dynamics. \nWith the proposed attention mechanism and Transformer architecture, ContiFormer effectively models complex continuous-time dynamic systems. \n\nThe contributions of this paper can be summarized below.\n\\begin{itemize}[leftmargin=5mm]\n    \\item \\textit{Continuous-Time Transformer.} To the best of our knowledge, we are the first to incorporate a continuous-time mechanism into attention calculation in Transformer, which is novel and captures the continuity of the underlying system of the irregularly sampled time-series data. \n    \\item \\textit{Parallelism Modeling.} To tackle the conflicts between continuous-time calculation and the parallel calculation property of the Transformer model, we propose a novel reparameterization method, allowing us to parallelly execute the continuous-time attention in the different time ranges. \n    \\item \\textit{Theoretical Analysis.} We mathematically characterize that various Transformer variants~\\cite{xu2019self, li2020time, zhang2019attain, shukla2021multi, ma2022non} can be viewed as special instances of ContiFormer. Thus, our approach offers a broader scope that encompasses Transformer variants.\n    \\item \\textit{Experiment Results.} We examine our method on various irregular time series settings, including time-series interpolation, classification, and prediction. The extensive experimental results have illustrated the superior performance of our method against strong baseline models.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\n\\paragraph{Time-Discretized Models.} \nThere exists a branch of models based on the time-discrete assumption which transforms the time-space into a discrete one.\nRecurrent Neural Networks (RNN)~\\cite{hochreiter1997long, chung2014empirical} and Transformers~\\cite{vaswani2017attention} are powerful time-discrete sequence models, which have achieved great success in natural language processing~\\cite{devlin2018bert, liu2019roberta, zhang2021continuous}, computer vision~\\cite{zhu2020deformable, guo2022cmt} and time series forecasting~\\cite{wu2021autoformer, zhou2021informer, li2023towards, zhou2022fedformer, kitaev2020reformer, wu2022timesnet, zeng2023transformers}. \nUtilizing them for irregular time series data necessitates discretizing the timeline into time bins or padding missing values, potentially resulting in information loss and disregarding inter-observation dynamics.~\\cite{choi2018mime, mozer2017discrete, lipton2016directly}.\nAn alternative is to construct models that can utilize these various time intervals~\\cite{zhang2019attain, li2020time}. \nAdditionally, several recent approaches have also \nencoded time information into features to model irregularly sampled time series, including time representation approaches~\\cite{xu2019self, zhang2019attain} and kernel-based methods~\\cite{ma2022non}, which allow learning for time dynamics.\nFor instance, Mercer~\\cite{xu2019self} concatenates the event embedding with the time representation.\nDespite these advances, these models are still insufficient in capturing input-dependent dynamic systems. \n\n\\paragraph{Continuous-Time Models.} \nOther works shift to the continuous-time modeling paradigm.\nTo overcome the discrete nature of RNNs, \na different strategy involves the exponential decay of the hidden state between observations~\\cite{che2018recurrent, mei2017neural}. \nNevertheless, the applicability of these techniques is restricted to decaying dynamics specified between successive observations~\\cite{chen2018neural}. \nNeural ODEs are a class of models that leverage the theory of ordinary differential equations to define the dynamics of neural networks~\\cite{chen2018neural, jia2019neural}.\nNeural CDE~\\cite{kidger2020neural} and Neural RDE~\\cite{morrill2021neural} further utilize the well-defined mathematics of controlled differential equations~\\cite{arribas2018derivatives, kidger2019deep} and rough path theory~\\cite{boutaib2013dimension} to construct continuous-time RNN. \nODE-RNN~\\cite{rubanova2019latent} combines Neural ODE with Gated Recurrent Unit (GRU)~\\cite{cho2014properties} to model continuous-time dynamics and input changes.\nWhereas these approaches usually fail to capture the evolving relationship between observations~\\cite{schirmer2022modeling}. CADN~\\cite{chien2022learning} and ANCDE~\\cite{jhin2023attentive} extend ODE-RNN and Neural CDE by incorporating an attention mechanism to capture the temporal relationships, respectively.\nAnother line of research explores the use of state space models (SSMs) to represent continuous-time dynamic systems~\\cite{gu2021efficiently, smith2022simplified, gu2022parameterization,ansari2023neural}. However, these models adopt a recursive schema, which hampers its efficiency and can lead to cumulative errors~\\cite{chen2018neural}.\nIn contrast, our proposed model, ContiFormer leverages the parallelism advantages of the Transformer architecture and employs a non-autoregressive paradigm. \nFurthermore, extending the Transformer to the continuous-time domain remains largely unexplored by researchers. Such extension provides a more flexible and powerful modeling ability for irregular time series data.\n\n"
            },
            "section 3": {
                "name": "Method",
                "content": "\n\nThe ContiFormer is a Transformer \\cite{vaswani2017attention} model for processing time series data with irregular time intervals. \nFormally, an irregular time series is defined as $\\Gamma=[(X_1, t_1), \\ldots, (X_N, t_N)]$, where observations may occur at any time and the observation time points $\\boldsymbol{\\omega}=(t_1, \\ldots, t_N)$ are with irregular intervals. $X_i$ is the input feature for the $i$-th observation.\nWe denote $X=[X_1; X_2; ..., X_N] \\in \\mathbb{R}^{N \\times d}$.\n\nAs aforementioned, it is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. \nTo tackle the challenge, we propose a Continuous-Time Transformer architecture as shown in Figure ~\\ref{fig:ctsa}. \nEach layer of ContiFormer takes as input an irregular time series $X$ and the sampled time $\\boldsymbol{\\omega}$ and outputs a latent continuous trajectory that captures the dynamic change of the underlying system. \nWith such a design, ContiFormer transforms the discrete observation sequence into the continuous-time domain.\nAt each layer, the core attention module takes a continuous perspective and expands the dot-product operation in the vanilla Transformer to the continuous-time domain, which not only models the underlying continuous dynamics but also captures the evolving input-dependent process.\nFor better understanding, the description of each layer will omit the layer index, without causing confusion.\n\nThroughout the remaining section, we adopt the notation $\\boldsymbol{\\omega}$ to denote a sequence of (reference) time points, while $t$ is the random variable representing a query time point. \nMatrices are donated using uppercase letters, and lowercase letters except $t$ represent continuous functions.\n\n\n\n",
                "subsection 3.1": {
                    "name": "Continuous-Time Attention Mechanism",
                    "content": " \\label{sec:machamism}\n\nThe core of the ContiFormer layer is the proposed continuous-time multi-head attention (CT-MHA) module, as shown in Figure~\\ref{fig:ctsa}.\nAt each layer, we first transform the input irregular time series $X$ into $Q=[Q_1; Q_2; \\ldots; Q_N]$ for queries, $K=[K_1; K_2; \\ldots; K_N]$ for keys, and $V=[V_1; V_2; \\ldots; V_N]$ for values. \nAt a high level, the CT-MHA module transforms the irregular time series inputs to latent trajectories and outputs a continuous dynamic system that captures the time-evolving relationship between observations. \nTo accomplish this, it utilizes ordinary differential equations to define the latent trajectories for each observation.\nWithin the latent state, it assumes that the underlying dynamics evolve following linear ODEs.\nSubsequently, it constructs a continuous query function by approximating the underlying sample process of the input.\nUltimately, it produces a continuous-time function that captures the evolving relationship and represents the complex dynamic system.\n\n\\paragraph{Continuous Dynamics from Observations} \n\nTransformer calculates scaled dot-product attention for queries, keys, and values ~\\cite{vaswani2017attention}. In continuous form, we first employ ordinary differential equations to define the latent trajectories for each observation. Specifically, assuming that the first observation and last observation come at time point $t_1$ and $t_N$ respectively, we define continuous keys and values as\n\\begin{equation}\n    \\begin{aligned}\n        \\boldsymbol{\\mathrm{k}}_i(t_i) & = K_i ~, \\quad   \\boldsymbol{\\mathrm{k}}_i(t) = \\boldsymbol{\\mathrm{k}}_i(t_i) + \\int_{t_i}^{t} f\\,\\Big( \\tau, \\boldsymbol{\\mathrm{k}}_i(\\tau); \\theta_k \\Big)\\, \\text{d} \\tau ~,\\\\\n        \\boldsymbol{\\mathrm{v}}_i(t_i) & = V_i ~, \\quad   \\boldsymbol{\\mathrm{v}}_i(t) = \\boldsymbol{\\mathrm{v}}_i(t_i) + \\int_{t_i}^{t} f\\,\\Big( \\tau, \\boldsymbol{\\mathrm{v}}_i(\\tau); \\theta_v \\Big)\\, \\text{d} \\tau ~, \\\\\n    \\end{aligned}\n\\label{eq:kv}\n\\end{equation}\nwhere $t\\in [t_1, t_N]$, $\\boldsymbol{\\mathrm{k}}_i(\\cdot), \\boldsymbol{\\mathrm{v}}_i(\\cdot) \\in \\mathbb{R}^{d}$ represent the ordinary differential equation for the $i$-th observation with parameters $\\theta_k$ and $\\theta_v$, and the initial state of $\\boldsymbol{\\mathrm{k}}_i(t_i)$ and $\\boldsymbol{\\mathrm{v}}_i(t_i)$ respectively and the function $f\\left( \\cdot \\right) \\in \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}^{d}$ controls the change of the dynamics.\n\n\\paragraph{Query Function} \\label{ms:q}\n\nWhile keys and values are associated with the input and output of the attention mechanism, a query specifies what information to extract from the input. \nTo model a dynamic system, queries can be modeled as a function of time that represents the overall changes in the input to the dynamic system at a specific time point $t$, i.e., $\\boldsymbol{\\mathrm{q}}(t)$. \nSpecifically, we adopt a common assumption that irregular time series is a discretization of an underlying continuous-time process. \nThus, similar to ~\\cite{kidger2020neural}, we define a closed-form continuous-time interpolation function (e.g., natural cubic spline) with knots at $t_1, \\ldots, t_N$ such that $\\boldsymbol{\\mathrm{q}}(t_i) = Q_i$ as an approximation of the underlying process. \n\n\\paragraph{Scaled Dot Product}\n\nThe self-attention mechanism is the key component in Transformer architecture. \nAt its core, self-attention involves calculating the correlation between queries and keys. \nThis is achieved through the inner product of two matrices in discrete form, i.e., $Q \\cdot K^\\top$. \nExtending the discrete inner-product to its continuous-time domain, given two real functions $f(x)$ and $g(x)$, we define the inner product of two functions in a closed interval $[a, b]$ as\n\\begin{equation}\n    \\left< f, g \\right> = \\int_{a}^b f(x) \\cdot g(x) \\text{d} x ~.\n\\end{equation}\nIntuitively, it can be thought of as a way of quantifying how much the two functions ``align'' with each other over the interval. \nInspired by the formulation of the inner product in the continuous-time domain, we model the evolving relationship between the $i$-th sample and the dynamic system at time point $t$ as the inner product of $\\boldsymbol{\\mathrm{q}}$ and $\\boldsymbol{\\mathrm{k}}_i$ in a closed interval $[t_i, t]$, i.e.,\n\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_{i}(t) =  \\frac{\\int_{t_i}^{t} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{t - t_i} ~.\n\\label{eq:attn}\n\\end{equation}\nDue to the nature of sequence data, it is common to encounter abnormal points, such as events with significantly large time differences. To avoid numeric instability during training, we divide the integrated solution by the time difference. As a consequence, Eq. (\\ref{eq:attn}) exhibits discontinuity at $\\boldsymbol{\\mathrm{\\alpha}}_i(t_i)$. To ensure the continuity of the function $\\boldsymbol{\\mathrm{\\alpha}}_i(\\cdot)$, we define $\\boldsymbol{\\mathrm{\\alpha}}_i(t_i)$ as\n\\begin{equation}\n\\boldsymbol{\\mathrm{\\alpha}}_i(t_i) = \\lim_{\\epsilon \\rightarrow 0} \\frac{\\int_{t_i}^{t_i + \\epsilon} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{\\epsilon} = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot \\boldsymbol{\\mathrm{k}}_i(t_i)^\\top ~.\n\\label{eq:ii}\n\\end{equation}\n\n\\paragraph{Expected Values}\n\nGiven a query time $t \\in [t_1, t_N]$, the value of an observation at time point $t$ is defined as the expected value from $t_i$ to $t$. \nWithout loss of generality, it holds that the expected values of an observation sampled at time $t_i$ is $\\boldsymbol{\\mathrm{v}}_i(t_i)$ or $V_i$. \nFormally, the expected value is defined as\n\\begin{equation}\n    \\widehat{\\boldsymbol{\\mathrm{v}}}_{i}(t) = \\mathbb{E}_{t \\sim [t_i, t]} \\left[ \\boldsymbol{\\mathrm{v}}_i(t) \\right] = \\frac{\\int_{t_i}^{t} \\boldsymbol{\\mathrm{v}}_i(\\tau) \\text{d} \\tau}{t - t_i} ~.\n    \\label{eq:vv}\n\\end{equation}\n\n\\paragraph{Multi-Head Attention} \n\nThe continuous-time attention mechanism is a powerful tool used in machine learning that allows for the modeling of complex, time-varying relationships between keys, queries, and values. Unlike traditional attention mechanisms that operate on discrete time steps, continuous-time attention allows for a more fine-grained analysis of data by modeling the input as a continuous function of time.\nSpecifically, given the forehead-defined queries, keys, and values in continuous-time space, the continuous-time attention given a query time $t$ can be formally defined as\n\\begin{equation}\\\n\\begin{aligned}\n    \\operatorname{CT-ATTN}( Q, K, V, \\boldsymbol{\\omega})(t) & \n    = \\sum_{i=1}^{N} \\widehat{\\boldsymbol{\\mathrm{\\alpha}}}_i(t) \\cdot \\widehat{\\boldsymbol{\\mathrm{v}}}_i(t) ~,\\\\\n    \\text { where } \\widehat{\\boldsymbol{\\mathrm{\\alpha}}}_i(t) & = \\frac{\\operatorname{exp} \\left( \\boldsymbol{\\mathrm{\\alpha}}_i(t) / \\sqrt{d_k} \\right)}{\\sum_{j=1}^{N} \\operatorname{exp} \\left( \\boldsymbol{\\mathrm{\\alpha}}_j(t) / \\sqrt{d_k} \\right)} ~. \\\\\n\\end{aligned}\n\\label{eqn:msha}\n\\end{equation}\nMulti-head attention, an extension of the attention mechanism~\\cite{vaswani2017attention}, allows simultaneous focus on different input aspects.\nIt stabilizes training by reducing attention weight variance.\nWe extend Eq. (\\ref{eqn:msha}) by incorporating multiple sets of attention weights, i.e.,\n\\begin{equation}\n\\begin{aligned}\n\\operatorname{CT-MHA}(Q, K, V, \\boldsymbol{\\omega})(t) & =\\operatorname{Concat}\\left(\\operatorname{head}_{(1)}(t), \\ldots, \\operatorname{head}_{(\\mathrm{H})}(t)\\right) W^O ~,\\\\\n\\text { where } \\operatorname{head}_{(\\mathrm{h})}(t) & =\\operatorname{CT-ATTN}\\left(Q W_{(\\mathrm{h})}^Q, K W_{(\\mathrm{h})}^K, V W_{(\\mathrm{h})}^V, \\boldsymbol{\\omega} \\right)(t) ~,\n\\end{aligned}\n\\label{eq:contiattn}\n\\end{equation}\nwhere $W^O$, $W_{(\\mathrm{h})}^Q$, $W_{(\\mathrm{h})}^K$ and $W_{(\\mathrm{h})}^V$ are parameter matrices, $\\mathrm{h} \\in [1,\\mathrm{H}]$ and $\\mathrm{H}$ is the head number.\n\n"
                },
                "subsection 3.2": {
                    "name": "Continuous-Time Transformer",
                    "content": "\n\nDespite the widespread adoption of Transformers~\\cite{vaswani2017attention} in various research fields, their extension for modeling continuous-time dynamic systems is underexplored.\nWe propose ContiFormer that directly builds upon the original implementation of vanilla Transformer while extending it to the continuous-time domain.\nSpecifically, we apply layer normalization (LN) after both the multi-head self-attention (MHA) and the feed-forward blocks (FFN). \nWe formally characterize the ContiFormer layer below\n\\begin{equation}\n\\begin{aligned}\n \\tilde{\\boldsymbol{\\mathrm{z}}}^{l}(t) &= \\operatorname{LN}\\Big( \\operatorname{CT-MHA}(X^{l}, X^{l}, X^{l}, \\boldsymbol{\\omega}^{l})(t)  + \\boldsymbol{\\mathrm{x}}^{l}(t) \\Big) ~,\\\\\n \\boldsymbol{\\mathrm{z}}^{l}(t) & = \\operatorname{LN}\\Big( \\operatorname{FFN}(\\tilde{\\boldsymbol{\\mathrm{z}}}^{l}(t) ) + \\tilde{\\boldsymbol{\\mathrm{z}}}^{l}(t) \\Big) ~,\\\\\n\\end{aligned}\n\\label{eq:layer}\n\\end{equation}\nwhere $\\boldsymbol{\\mathrm{z}}^{l}(t)$ is the output from the $l$-th ContiFormer layer at time point $t$. \nAdditionally, we adopt residual connection to avoid potential gradient vanishing~\\cite{he2016deep}. \nTo incorporate the residual connection with the continuous-time output, we approximate the underlying process of the discrete input $X^l$ using a continuous function $\\boldsymbol{\\mathrm{x}}^l(t)$ based on the closed-form continuous-time interpolation function.\n\n\\paragraph{Sampling Process} \n\nAs described before, each ContiFormer layer derives a continuous function $\\boldsymbol{\\mathrm{z}}^l(t)$ w.r.t. time as the output, while receiving discrete sequence $X^l$ as input.\nHowever, $\\boldsymbol{\\mathrm{z}}^l(t)$ can not be directly incorporated into neural network architectures that expect inputs in the form of fixed-dimensional vectors and sequences~\\cite{shukla2021multi}, which places obstacles when stacking layers of ContiFormer.\nTo address this issue, we establish reference time points for the output of each layer. These points are used to discretize the layer output, and can correspond to either the input time points (i.e., $\\boldsymbol{\\omega}$) or task-specific time points. \nSpecifically, assume that the reference points for the $l$-th layer is $\\boldsymbol{\\omega}^{l}=[t_1^l, t_2^l, ..., t_{\\beta_l}^l]$, the input to the next layer $X^{l+1}$ can be sampled as $\\{ \\boldsymbol{\\mathrm{z}}^{l}(t_j^l) | j \\in [1, \\beta_l] \\}$.\n\n\\newpage\n"
                },
                "subsection 3.3": {
                    "name": "Complexity Analysis",
                    "content": "\\label{sec:complexity}\n\n\n\nWe consider an autoregressive task where the output of each observation is required for a particular classification or regression task. \nTherefore, the reference points for each layer are defined as $\\boldsymbol{\\omega}^{l} = \\boldsymbol{\\omega}$. \n\nTo preserve the parallelization of Transformer architecture and meanwhile implement the continuous-time attention mechanism in Eq. (\\ref{eqn:msha}), we first adopt time variable ODE~\\cite{chen2021neuralstpp} to reparameterize ODEs into a single interval $[-1, 1]$, followed by numerical approximation method to approximate the integrals, i.e.,\n\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_i(t_j) = \\frac{\\int_{t_i}^{t_j} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{t_j - t_i} = \\frac{1}{2} \\int_{-1}^{1} \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\tau) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\tau)^\\top \\text{d} \\tau \\approx  \\frac{1}{2} \\sum_{p=1}^{P} \\gamma_p \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_p) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p)^{\\top} ~,\n\\label{eq:app}\n\\end{equation}\nwhere $P$ denotes the number of intermediate steps to approximate an integral, $\\xi_p \\in [-1, 1]$ and $\\gamma_p \\geq 0$, $\\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(s) = \\boldsymbol{\\mathrm{k}}_i\\left((s(t_j - t_i) + t_i + t_j)/2\\right)$, and $\\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(s) = \\boldsymbol{\\mathrm{q}}_i\\left((s(t_j - t_i) + t_i + t_j)/2\\right)$. Finally, $\\boldsymbol{\\mathrm{\\alpha}}_i(t_j)$ can be solved with one invokes of ODESolver that contains $N^2$ systems, i.e., \n\\begin{equation}\n    \\underbrace{\\begin{bmatrix}\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{1, 1}(\\xi_{p-1}) \\\\\n    \\vdots \\\\\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{N,N}(\\xi_{p-1})\n    \\end{bmatrix}}_{\\boldsymbol{\\mathrm{K}}(\\xi_{p-1})} + \\int_{\\xi_{p-1}}^{\\xi_{p}} f(s, \\boldsymbol{\\mathrm{K}}(s); \\theta_k) ds =\n    \\underbrace{\\begin{bmatrix}\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{1, 1}(\\xi_{p}) \\\\\n    \\vdots \\\\\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{N, N}(\\xi_{p})\n    \\end{bmatrix}}_{\\boldsymbol{\\mathrm{K}}(\\xi_{p})} ~,\n\\label{eq:solve}\n\\end{equation}\nwhere $p \\in [1, ..., P]$ and we further define $\\xi_0=-1$. Additionally, $\\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_p)$ can be obtained by interpolating on the close-form continuous-time query function. Similar approach is adopted for solving $\\widehat{\\boldsymbol{\\mathrm{v}}}_i(t_j)$ in Eq. (\\ref{eq:vv}).\nThe detailed implementation is deferred to Appendix \\ref{sec:complex}.\n\nWe consider three criteria to analyze the complexity of different models, i.e., per-layer complexity, minimum number of sequential operations, and maximum path lengths~\\cite{vaswani2017attention}. \nComplexity per layer refers to the computational resources required to process within a single layer of a neural network. \nSequential operation refers to the execution of operations that are processed iteratively or in a sequential manner. \nMaximum path length measures the longest distance information must traverse to establish relationships, reflecting the model's ability to learn long-range dependencies.\n\nAs noted in Table \\ref{tab:complex}, \nvanilla Transformer is an efficient model for sequence learning with $O(1)$ sequential operations and $O(1)$ path length because of the parallel attention mechanism, whereas the recurrent nature of RNN and Neural ODE rely on autoregressive property and suffer from a large number of sequential operations.\nUtilizing Eq. (\\ref{eq:app}) and leveraging the parallelism inherent in the Transformer architecture, our model, ContiFormer, achieves $O(S)$  sequential operations and $O(1)$ path length, while also enjoying the advantage of capturing complex continuous-time dynamic systems. We have $S \\ll T$ and we generally set $S < N$ in our experiments.\n\n"
                },
                "subsection 3.4": {
                    "name": "Representation Power of ContiFormer",
                    "content": "\n\nIn the previous subsections, we introduce a novel framework for modeling dynamic systems. Then a natural question is: \\textit{How powerful is ContiFormer?} \nWe observe that by choosing proper weights, ContiFormer can be an extension of vanilla Transformer~\\cite{vaswani2017attention}. \nFurther, many Transformer variants tailored for irregular time series, including time embedding methods~\\cite{xu2019self, shukla2021multi} and kernelized attention methods~\\cite{ma2022non, li2020time}, can be special cases of our model. \nWe provide an overview of the main theorem below and defer the proof to Appendix ~\\ref{sec:proof}.\n\n\\begin{theorem}[Universal Attention Approximation Theorem]\nGiven query ($Q$) and key ($K$) matrices, such that $\\|Q_i\\|_2 < \\infty, \\|Q_i\\|_0 = d$ for $i \\in [1, ..., N]$.\nFor certain attention matrix, i.e., $\\operatorname{Attn}(Q, K) \\in \\mathbb{R}^{N \\times N}$ (see Appendix \\ref{sec:variants} for more information), there always exists a family of continuously differentiable vector functions $\\boldsymbol{\\mathrm{k}}_1(\\cdot), \\boldsymbol{\\mathrm{k}}_2(\\cdot), \\ldots, \\boldsymbol{\\mathrm{k}}_N(\\cdot)$, such that the discrete definition of the continuous-time attention formulation, i.e., $[\\boldsymbol{\\mathrm{\\alpha}}_1(\\cdot), \\boldsymbol{\\mathrm{\\alpha}}_2(\\cdot), ..., \\boldsymbol{\\mathrm{\\alpha}}_N(\\cdot)]$ in Eq. (\\ref{eq:ii}), given by\n\\begin{equation}\n\\operatorname{\\widetilde{Attn}}(Q, K)=\\left[\\begin{array}{cccc}\n\\boldsymbol{\\mathrm{\\alpha}}_1(t_1) & \\boldsymbol{\\mathrm{\\alpha}}_2(t_1) & \\cdots & \\boldsymbol{\\mathrm{\\alpha}}_N(t_1) \\\\\n\\boldsymbol{\\mathrm{\\alpha}}_1(t_2) & \\boldsymbol{\\mathrm{\\alpha}}_2(t_2) & \\cdots & \\boldsymbol{\\mathrm{\\alpha}}_N(t_2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\boldsymbol{\\mathrm{\\alpha}}_1(t_N) & \\boldsymbol{\\mathrm{\\alpha}}_2(t_N) & \\cdots & \\boldsymbol{\\mathrm{\\alpha}}_N(t_N)\n\\end{array}\\right] ~,\n\\label{eq:proof}\n\\end{equation}\nsatisfies that $\\operatorname{\\widetilde{Attn}}(Q, K) = \\operatorname{Attn}(Q, K)$.\n\\end{theorem}\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n\nIn this section, we evaluate ContiFormer on three types of tasks \non irregular time series data, \ni.e., interpolation and extrapolation, classification, event prediction, and forecasting. Additionally, we conduct experiments on pendulum regression task~\\cite{schirmer2022modeling, gu2021efficiently}, the results are listed in Appendix \\ref{sec:pendulum}.\n\n\\paragraph{Implementation Details}\n\nBy default, we use the natural cubic spline to construct the continuous-time query function. The vector field in ODE is defined as $f(t, \\textbf{x})=\\text{Actfn}(\\text{LN}(\\text{Linear}^{d, d}(\\text{Linear}^{d, d}(\\textbf{x}) + \\text{Linear}^{1, d}(t))))$, where $\\text{Actfn}(\\cdot)$ is either \\textit{tanh} or \\textit{sigmoid} activation function, $\\text{Linear}^{a,b}(\\cdot): \\mathbb{R}^a \\rightarrow \\mathbb{R}^b$ is a linear transformation from dimension $a$ to dimension $b$, $\\text{LN}$ denotes the layer normalization. We adopt the Gauss-Legendre Quadrature approximation to implement Eq. (\\ref{eq:app}). In the experiment, we choose the Runge-Kutta-4~\\cite{runge1895numerische} (RK4) algorithm to solve the ODE with a fixed step of fourth order and a step size of $0.1$. Thus, the number of forward passes to integrate from $-1$ to $1$, i.e., $S$ in Table \\ref{tab:complex}, is $80$.  All the experiments were carried out on a single 16GB NVIDIA Tesla V100 GPU. \n\n",
                "subsection 4.1": {
                    "name": "Modeling Continuous-Time Function",
                    "content": "\n\n\n\n\n\nThe first experiment studies the effectiveness of different models from different categories on continuous-time function approximation.\nWe generated a dataset of 300 2-dimensional \nspirals, sampled at 150 equally-spaced time points. \nTo generate irregular time points, we randomly sample 50 points from each sub-sampled trajectory without replacement. \nWe visualize the interpolation and extrapolation results of the irregular time series for different models, namely Latent ODE (w/ RNN encoder)~\\cite{chen2018neural}, Transformer~\\cite{vaswani2017attention} and our proposed ContiFormer. \nBesides, we list the interpolation and extrapolation results in Table ~\\ref{tab:interp} using rooted mean squared error (RMSE) and mean absolute error (MAE) metrics. \nMore visualization results and experimental settings can be found in Appendix \\ref{sec:interpolate}.\nAs shown in Figure ~\\ref{fig:interpolate}, both Latent ODE and ContiFormer can output a smooth and continuous function approximation, while Transformer fails to interpolate it given the noisy observations (\\textcolor[rgb]{0.648, 0.203, 0.476}{\\textcircled{\\small\\textbf{1}}}). \nFrom Table~\\ref{tab:interp}, we can observe that our model outperforms both Transformer and Latent ODE by a large margin. \nThe improvement lies in two aspects. \nFirst, compared to Transformer, our ContiFormer can produce an almost\\footnote{The output of ContiFormer can be considered ``continuous'' only if we overlook the approximation and numerical errors in the ODESolver. Additionally, traditional Transformers are commonly trained with dropout to prevent overfitting, which can result in discontinuous outputs. To mitigate this issue, a straightforward approach is to set the dropout rate to 0, thereby producing an ``almost'' continuous output for ContiFormer.} continuous-time output, making it more suitable for modeling continuous-time functions. \nSecond, compared to Latent ODE, our ContiFormer excels at retaining long-term information (\\textcolor[rgb]{0.296, 0.339, 0.629}{\\textcircled{\\small\\textbf{2}}}), leading to lower prediction error in extrapolating unseen time series. \nConversely, Latent ODE is prone to cumulative errors (\\textcolor[rgb]{1, 0.7617, 0} {\\textcircled{\\small\\textbf{3}}}), resulting in poorer performance in extrapolation tasks. \nTherefore, we conclude that ContiFormer is a more suitable approach for modeling continuous-time functions on irregular time series.\n\n\n"
                },
                "subsection 4.2": {
                    "name": "Irregularly-Sampled Time Series Classification",
                    "content": " \\label{istsc}\n\nThe second experiment evaluates our model on real-world irregular time series data.\nTo this end, we first examine the effectiveness of different models for irregular time series classification. \nWe select 20 datasets from UEA Time Series Classification Archive~\\cite{bagnall2018uea} with diverse characteristics in terms of the number, dimensionality, and length of time series samples. \nMore detailed information and experimental results can be found in Appendix ~\\ref{sec:classify}. \nTo generate irregular time series data, we follow the setting of ~\\cite{kidger2020neural} to randomly drop either 30\\%, 50\\% or 70\\% observations.\n\nWe compare the performance of ContiFormer with RNN-based methods (GRU-$\\Delta$t~\\cite{kidger2020neural}, GRU-D~\\cite{che2018recurrent}), Neural ODE-based methods (ODE-RNN~\\cite{rubanova2019latent}, CADN~\\cite{chien2021continuous}, Neural CDE~\\cite{kidger2020neural}), SSM-based models (S5~\\cite{smith2022simplified}), and attention-based methods (TST~\\cite{zerveas2021transformer}, mTAN~\\cite{shukla2021multi}).\n\n\n\n\n\nTable ~\\ref{tab:classify} presents the average accuracy and rank number under different drop ratios. \nContiFormer outperforms all the baselines on all three settings. The complete results are shown in Appendix ~\\ref{sec:full}. \nMoreover, attention-based methods perform better than both RNN-based methods and ODE-based methods, underscoring the significance of effectively modeling the inter-correlation of the observations.\nAdditionally, we find that our method is more robust than other models. \nContiFormer exhibits the smallest performance gap between 30\\% and 50\\%. \n\nBesides, we study the learned attention patterns with different models. \nAttention captures the impact of previous observations on the future state of a system by measuring the correlation between different observations at different time points. Consequently, it is expected that a model effectively capturing the underlying dynamics would exhibit continuous, smooth, and input-dependent intermediate outcomes. As illustrated in Figure~\\ref{fig:attn}, \nContiFormer and TST are both capable of capturing input-dependent patterns effectively, while mTAN struggles due to the fact that its attention mechanism depends solely on the time information.\nIn addition, ContiFormer excels in capturing complex and smooth patterns compared to TST, showcasing its ability to model time-evolving correlations effectively and potentially more robust to noisy data. \nConsequently, we believe that ContiFormer better captures the nature of irregular time series.\n\n"
                },
                "subsection 4.3": {
                    "name": "Predicting Irregular Event Sequences",
                    "content": " \\label{piel}\n\n\n\n\nNext, we evaluate different models for predicting the type and occurrence time of the next event with irregular event sequences~\\cite{shchur2019intensity, shchur2020fast, chen2021learning, gupta2022proactive}, a.k.a, marked temporal point process (MTPP) task.\nTo this end, we use one synthetic dataset and five real-world datasets for evaluation, namely Synthetic, Neonate~\\cite{stevenson2019dataset}, Traffic~\\cite{lai2018modeling},\nMIMIC~\\cite{du2016recurrent}, BookOrder~\\cite{du2016recurrent} and StackOverflow~\\cite{leskovec2014snap}. \nWe use the 4-fold cross-validation scheme for Synthetic, Neonate, and Traffic datasets following~\\cite{fang2023learning}, and the 5-fold cross-validation scheme for the other three datasets following~\\cite{mei2017neural, zuo2020transformer}.\nWe repeat the experiment 3 times and report the mean and standard deviations in Table ~\\ref{tab:exp}. More information about the task description, dataset information, and training algorithm can be found in Appendix \\ref{sec:mtpp}.\n\nWe first compare ContiFormer with 3 models on irregular time series, i.e., GRU-$\\Delta$t, ODE-RNN, and mTAN, which respectively belong to RNN-based, ODE-based, and attention-based approaches. \nAdditionally, we compare ContiFormer with baselines specifically designed for the MTPP task. These models include parametric method (HP~\\cite{hawkes1971spectra}), RNN-based methods (RMTPP ~\\cite{du2016recurrent}, NeuralHP~\\cite{mei2017neural}), and attention-based methods (SAHP~\\cite{zhang2020self}, THP~\\cite{zuo2020transformer}, NSTKA~\\cite{ma2022non}). \n\nThe experimental results, as presented in Table ~\\ref{tab:exp}, demonstrate the overall statistical superiority of ContiFormer to the compared baselines. Notably, the utilization of specific kernel functions, e.g., NSTKA and mTAN, results in subpar performance on certain datasets, highlighting their limitations in modeling complex data patterns in real-world scenarios.\nFurthermore, parametric methods like HP, perform poorly on most datasets, indicating their drawbacks in capturing real-life complexities. \n\n"
                },
                "subsection 4.4": {
                    "name": "Regular Time Series Forecasting",
                    "content": "\n\nLastly, we evaluate ContiFormer's performance in the context of regular time series modeling. To this end, we conducted an extensive experiment following the experimental settings from~\\cite{wu2021autoformer}. \nWe employed the ETT, Exchange, Weather, and ILI datasets for time series forecasting tasks. \nWe compare ContiFormer with the recent state-of-the-art model (TimesNet~\\cite{wu2022timesnet}), Transformer-based models (FEDformer\\cite{zhou2022fedformer}, Autoformer\\cite{wu2021autoformer}) and MLP-based model (DLinear~\\cite{zeng2023transformers}). \n\n\n\nTable ~\\ref{tab:regular2} summarizes the experimental results. Contiformer achieves the best performance on ILI dataset. Specifically, it gives \\textbf{10\\%} MSE reduction (2.874 $\\rightarrow$ 2.632). Also, Contiformer outperforms Autoformer and FEDformer on Exchange, ETTm2, and Weather datasets. Therefore, it demonstrates that Contiformer performs competitively with the state-of-the-art model on regular time series forecasting. For a more comprehensive exploration of the results, please refer to Appendix \\ref{sec:regular}.\n\n"
                }
            },
            "section 5": {
                "name": "Efficiency Analysis",
                "content": "\n\n\n\nEmploying continuous-time modeling in ContiFormer often results in substantial time and GPU memory overhead. Hence, we conduct a comparative analysis of ContiFormer's actual time costs against those of Transformer-based models (TST), ODE-based models (ODE-\nRNN, Neural CDE), and SSM-based models (S5). \n\nTable \\ref{tab:timecost} shows that both TST and S5 are effective models. Additionally, ContiFormer leverages a parallel mechanism for faster inference, resulting in faster inference speed as compared to ODE-RNN and Neural CDE. Furthermore, ContiFormer demonstrates competitive performance even with a larger step size, as detailed in Appendix \\ref{sec:tolerance}. Therefore, ContiFormer's robustness to step size allows it to achieve a superior balance between time cost and performance. Please refer to Appendix \\ref{sec:cost} for more details.\n\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\nWe propose ContiFormer, a novel continuous-time Transformer for modeling dynamic systems of irregular time series data. Compared with the existing methods, ContiFormer is more flexible since it abandons explicit closed-form assumptions of the underlying dynamics,\nleading to\nbetter generalization in various circumstances. \nBesides, similar to Neural ODE, ContiFormer can produce a continuous and smooth outcome. \nTaking advantage of the self-attention mechanism, ContiFormer can also capture long-term dependency in irregular observation sequences, \nachieving superior performance\non a variety of irregular time series modeling tasks.\n\n"
            },
            "section 7": {
                "name": "Acknowledgement",
                "content": "\nThis research is supported in part by the National Natural Science Foundation of China under grant 62172107.\n\n\\bibliographystyle{plain}\n\\bibliography{reference}\n\n\\newpage\n\n\\appendix\n"
            },
            "section 8": {
                "name": "Implement Details and Complexity Analysis",
                "content": " \\label{sec:complex}\n\n",
                "subsection 8.1": {
                    "name": "Background: Time Variable ODE",
                    "content": "\n\nA numerical ODE solver integrates a single ODE system $\\frac{d \\textbf{x}}{d t}=f(t, \\textbf{x})$, where $\\textbf{x} \\in \\mathbb{R}^d$ and $f: \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}^d$. Given an initial state $\\textbf{x}_{t_0}$, the ODE solver produces $\\textbf{x}_{t_1}, \\textbf{x}_{t_2}, ..., \\textbf{x}_{t_N}$, which represents the latent state for each time point~\\cite{chen2018neural}, i.e.,\n\\begin{equation}\n    \\textbf{x}_{t_1}, \\textbf{x}_{t_2}, \\ldots, \\mathbf{z}_{t_N}=\\operatorname{ODESolve}\\left(\\textbf{x}_{t_0}, f, t_0, \\ldots, t_N\\right) ~.\n\\label{eq:ode}\n\\end{equation}\nTo obtain the solution of Eq. (\\ref{eq:ode}), it requires the ODE solver to integrate over the interval $[t_0, t_N]$.\n\n\\paragraph{Varied time intervals} Now, we suppose that we need to solve $M$ dynamic system that has different time intervals. Let denote for the $i$-th system, the start point is $t_0^{(i)}$ and the endpoint is $t_1^{(i)}$ and the ODE system is defined by $\\frac{d \\textbf{x}}{d t}=f(t, \\textbf{x})$. We can construct a dummy variable so that the integral interval always starts from $-1$ and ends at $1$ and perform a change of variables to transform every system to use this dummy variable.\n\nFor simplifying purposes, we assume that a single system that integrates from $t_0$ to $t_1$ with $f(t, \\textbf{x})$ can control the differential equation and the solution of $\\textbf{x}(t)$. We introduce a dummy variable $s=\\frac{2t - t_0 - t_1}{t_1 - t_0}$ and a solution $\\mathbf{\\tilde{x}}(s)$ in a fix interval $[-1, 1]$ satisfies\n\\begin{equation}\n    \\tilde{\\textbf{x}}(s) = \\textbf{x}(\\frac{s(t_1 - t_0) + t_0 + t_1}{2}) ~.\n\\end{equation}\nThe differential equation that solve $\\mathbf{x}$ follows that\n\\begin{equation}\n\\begin{aligned}\n\\tilde{f}(s, \\tilde{\\textbf{x}}(s)) = \\frac{d \\tilde{\\textbf{x}}(s)}{d s} & =\\left.\\frac{d \\textbf{x}(t)}{d t}\\right|_{t=\\frac{s(t_1 - t_0) + t_0 + t_1}{2}} \\frac{d t}{d s} \\\\\n& =\\left.f(t, \\textbf{x}(t))\\right|_{t=\\frac{s(t_1 - t_0) + t_0 + t_1}{2}}\\left(\\frac{t_1 - t_0}{2}\\right) \\\\\n& =f\\left(\\frac{s(t_1 - t_0) + t_0 + t_1}{2}, \\tilde{\\textbf{x}}(s)\\right)\\left(\\frac{t_1 - t_0}{2}\\right) ~.\n\\end{aligned}\n\\end{equation}\nSince $\\tilde{\\textbf{x}}(-1)=\\textbf{x}(t_0)$ and $\\tilde{\\textbf{x}}(1)=x_{t_1}$, thus we have\n\\begin{equation}\n    \\textbf{x}(t_1)= \\operatorname{ODESolve}(\\textbf{x}(t_0), f, t_0, t_1) = \\operatorname{ODESolve}(\\tilde{\\textbf{x}}(-1), \\tilde{f}, -1, 1) ~.\n\\end{equation}\n\n"
                },
                "subsection 8.2": {
                    "name": "Background: Approximating Definite Integral of a Function",
                    "content": " \\label{sec:approximating}\n\nGiven a function $f$ that integrates over an interval $[-1, 1]$, a simple method to approximate it is to assume that the change is linear over time, i.e.,\n\\begin{equation}\n\\int_{-1}^{1} f(x) d x \\approx \\frac{f(-1) + f(1)}{2} ~.\n\\end{equation}\nBesides, in numerical analysis, Gauss\u2013Legendre Quadrature provides a more accurate way to approximate an integral over an interval $[-1, 1]$, i.e.,\n\\begin{equation}\n\\int_{-1}^1 f(x) \\mathrm{d} x \\approx \\sum_j \\omega_j f\\left(\\xi_j\\right) ~,\n\\end{equation}\nwhere $\\left\\{\\omega_j\\right\\}$ and $\\left\\{\\xi_j\\right\\}$ are quadrature weights and nodes such that $-1 \\leq \\xi_1 < ... < \\xi_k \\leq 1$.\n\n"
                },
                "subsection 8.3": {
                    "name": "Putting it all together",
                    "content": "\n\nGiven an irregular-sampled sequence, $\\Gamma=[ (x_1, t_1), (x_2, t_2), ..., (x_N, t_N)]$, where $x_i \\in \\mathbb{R}^{d}$ is the input feature of the $i$-th tokens happen at time $t_i$ and $N$ is the length of the sequence. We analyze the time and space complexity for an autoregressive task where the output of each observation is required for a particular classification or regression task. Specifically, for Eq. (\\ref{eq:attn}), $\\boldsymbol{\\mathrm{\\alpha}}_i(t_1), \\boldsymbol{\\mathrm{\\alpha}}_i(t_2), \\ldots, \\boldsymbol{\\mathrm{\\alpha}}_i(t_N)$ is required to form the output of Transformer. Therefore, the key difficulty is to solve Eq. (\\ref{eq:attn}). By using the numerical approximation method, we have\n\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_i(t_j) = \\frac{\\int_{t_i}^{t_j} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{t_j - t_i} = \\frac{1}{2} \\int_{-1}^{1} \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\tau) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\tau)^\\top \\text{d} \\tau \\approx  \\frac{1}{2} \\sum_{p=1}^{P} \\gamma_p \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_p) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p)^{\\top} ~,\n\\end{equation}\nwhere $\\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}$ and $\\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}$ follows that:\n\\begin{equation}\n\\begin{aligned}\n  \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(s) & = \\boldsymbol{\\mathrm{q}}\\left(\\frac{s(t_i - t_j) + t_i + t_j}{2}\\right) ~,  \\\\\n  \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(s) & = \\boldsymbol{\\mathrm{q}}_j\\left(\\frac{s(t_i - t_j) + t_i + t_j}{2}\\right) ~.\n\\end{aligned}\n\\end{equation}\nWe use $\\hat{\\boldsymbol{\\mathrm{k}}}_{i,j}$ to denote $\\frac{d \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(s)}{d s}$. Thus, $\\boldsymbol{\\mathrm{\\alpha}}_i(t_j)$ can be solved by paralleling $N^2$ ODE solvers. Specifically, $\\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p)$ is given by\n\\begin{equation}\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_1), \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_2), ..., \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p) = \\operatorname{ODESolver} (\\boldsymbol{\\mathrm{k}}_j(t_j), \\hat{\\boldsymbol{\\mathrm{k}}}_{i,j}, -1, \\xi_1, \\xi_2, ..., \\xi_p) ~.\n\\end{equation}\n\nAs for $\\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_k)$, since the input sequence is irregularly sampled and discrete in time, we propose to use interpolation methods~\\cite{kidger2020neural} (e.g., linear interpolation or cubic interpolation) method on $Q$ to construct a continuous-time query. Therefore $\\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_k)$ can be easily calculated in $O(d)$ time complexity.\n\n\n"
                }
            },
            "section 9": {
                "name": "Representation Power of ContiFormer",
                "content": " \\label{sec:proof}\n\nIn the section, we mathematically prove that by curated designs of function hypothesis, many Transformer variants can be covered as a special case of ContiFormer. For simplification, we only discuss the self-attention mechanism, where $Q, K, V \\in \\mathbb{R}^{N \\times d}$.\n\n",
                "subsection 9.1": {
                    "name": "Transformer Variants",
                    "content": " \\label{sec:variants}\n\nWe first show several Transformer-based methods for irregular time series. Most of these methods shown below calculate $N \\times N$ attention matrix and multiply it with $V$\\footnote{Throughout the proof section, we define $\\boldsymbol{\\mathrm{v}}_i(t_i)=\\boldsymbol{\\mathrm{v}}_i(t_i)=V_i$, and focus on the proof of attention matrix.}. \n\n\\paragraph{Vanilla Transformer}\n\nGiven query $Q \\in \\mathbb{R}^{N \\times d}$ and $V \\in \\mathbb{R}^{N \\times d}$, the attention matrix of a vanilla transformer~\\cite{vaswani2017attention} is calculated as \n\\begin{equation}\n\\operatorname{Attn}(Q, K)=\\left[\\begin{array}{cccc}\nQ_1 \\cdot K_1^\\top & Q_1 \\cdot K_2^\\top & \\cdots & Q_1 \\cdot K_N^\\top \\\\\nQ_2 \\cdot K_1^\\top & Q_2 \\cdot K_2^\\top & \\cdots & Q_2 \\cdot K_N^\\top \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nQ_N \\cdot K_1^\\top & Q_N \\cdot K_2^\\top & \\cdots & Q_N \\cdot K_N^\\top\n\\end{array}\\right] ~,\n\\end{equation}\nand the output of the self-attention mechanism is obtained by\\footnote{For simplification, we assume single-head attention is adopted throughout the proof.}\n\\begin{equation}\nZ_i = \\operatorname{Softmax}(\\operatorname{Attn}(Q, K)_i) V ~.\n\\end{equation}\n\n\\paragraph{Kernelized Attention}\n\nKernelized attention mechanism \\footnote{Not to be confused with methods that regard kernels as a form of approximation of the attention matrix.} incorporate time information through defining specific kernel function $k(x, x')$ and replace the attention matrix with\n\\begin{equation}\n\\operatorname{Attn}(Q, K)=\\left[\\begin{array}{cccc}\nQ_1 \\cdot K_1^\\top * k(t_1, t_1) & Q_1 \\cdot K_2^\\top * k(t_1, t_2) & \\cdots & Q_1 \\cdot K_N^\\top * k(t_1, t_N) \\\\\nQ_2 \\cdot K_1^\\top * k(t_2, t_1) & Q_2 \\cdot K_2^\\top * k(t_2, t_2) & \\cdots & Q_2 \\cdot K_N^\\top * k(t_2, t_N) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nQ_N \\cdot K_1^\\top * k(t_N, t_1) & Q_N \\cdot K_2^\\top * k(t_N, t_2) & \\cdots & Q_N \\cdot K_N^\\top * k(t_N, t_N)\n\\end{array}\\right] ~.\n\\end{equation}\nKernel functions are typically symmetric and hold that $\\phi(x, x)=1$ for any $x$. For instance, NSTKA~\\cite{ma2022non} propose a learnable Generalized Spectral Mixture (GSM) kernel, i.e.,\n\\begin{equation}\n\\begin{aligned}\nk_\\text{GSM}\\left(x, x'\\right) &=\n \\sum_i \\phi_i(x) \\phi_i\\left(x'\\right) k_{\\text{Gibbs}, i}\\left(x, x'\\right) \\cos \\left(2 \\pi\\left(\\mu_i(x) x-\\mu_i\\left(x'\\right) x'\\right)\\right) ~, \\\\\n\\text{ where } k_{\\text{Gibbs}, i}\\left(x, x'\\right) &= \\sqrt{\\frac{2 l_i(x) l_i\\left(x'\\right)}{l_i(x)^2+l_i\\left(x'\\right)^2}} \\exp \\left(-\\frac{\\left(x-x'\\right)^2}{l_i(x)^2+l_i\\left(x'\\right)^2}\\right) .\n\\end{aligned}\n\\end{equation}\nwhere $\\mu_i(x)$ denotes the mean, lengthscale as $l_i(x)$ and input-dependent weight as $\\phi_i(x)$. \n\n\\paragraph{Time Embedding Methods}\n\nTime embedding methods learn a time representation, adding or concatenating it with the input embedding, i.e.,\n\\begin{equation}\n\\operatorname{\\widehat{Attn}}(Q, K)=\\left[\\begin{array}{cccc}\n(Q_1, \\Phi(t_1)) \\cdot (K_1, \\Phi(t_1))^\\top & \\cdots & (Q_1, \\Phi(t_1)) \\cdot (K_N, \\Phi(t_N))^\\top  \\\\ \n(Q_2, \\Phi(t_2)) \\cdot (K_1, \\Phi(t_1))^\\top  & \\cdots & (Q_2, \\Phi(t_2)) \\cdot (K_N, \\Phi(t_N))^\\top \\\\ \n\\vdots & \\ddots & \\vdots \\\\\n(Q_N, \\Phi(t_N)) \\cdot (K_1, \\Phi(t_1))^\\top & \\cdots & (Q_N, \\Phi(t_N)) \\cdot (K_N, \\Phi(t_N))^\\top\n\\end{array}\\right] ~,\n\\end{equation}\nwhere $(\\cdot, \\cdot)$ is the concatenation of two vectors, $\\Phi(\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}^{d}$ is a learnable time representation. For simplification, we assume that time embedding is concatenated with the input embedding and ignore linear transformation on the time embedding.\\footnote{Using the concatenation operation, we have $(Q_i, \\Phi(t_i)) \\cdot (K_j, \\Phi(t_j))^\\top=Q_i \\cdot K_j^\\top + \\Phi(t_i) \\cdot \\Phi(t_j)^\\top$.} \n\nFor instance, Mercer~\\cite{xu2019self} transforms the periodic kernel function into the high-dimensional space spanned by truncated Fourier basis under certain frequencies ($\\{\\omega_i\\}$) and coefficients $\\{c_i\\}$, i.e.,\n\\begin{equation}\n\\begin{aligned}\n\\Phi(t) \\mapsto \\Phi_d^{\\mathcal{M}} & =\\left[\\Phi_{\\omega_1, d}^{\\mathcal{M}}(t), \\ldots, \\Phi_{\\omega_k, d}^{\\mathcal{M}}(t)\\right]^{\\top} ~,\\\\\n\\text{ where } \\Phi_\\omega^{\\mathcal{M}}(t) & =\\left[\\sqrt{c_1}, \\ldots, \\sqrt{c_{2 j}} \\cos \\left(\\frac{j \\pi t}{\\omega}\\right), \\sqrt{c_{2 j+1}} \\sin \\left(\\frac{j \\pi t}{\\omega}\\right), \\ldots\\right] ~.\n\\end{aligned}\n\\end{equation}\nmTAN~\\cite{shukla2021multi} generalizes the notion of a positional encoding used in Transformer-based models to continuous-time domain, i.e., \n\\begin{equation}\n\\Phi(t) = \\phi(t) \\cdot \\textbf{w} ~,\n\\end{equation}\nwhere $\\textbf{w} \\in \\mathbb{R}^{d \\times d}$ is a linear transformation \\footnote{In the original paper, the equation is defined as $\\Phi(t_i, t_j) = \\phi(t_i) \\textbf{w}\\textbf{v}^\\top \\phi(t_j)$. For simplicity purpose, we assume that $\\textbf{w} = \\textbf{v}$.} and \n\\begin{equation}\n\\phi(t)_i = \\begin{cases}\n    \\omega_0 \\cdot t + \\phi_0, & \\text{if } i = 0\\\\\n    \\sin (\\omega_i \\cdot t + \\phi_i), & \\text{if } 0 < i \\leq d\n    \\end{cases} ~,\n\\end{equation}\nwhere $\\omega$ and $\\phi$ are learnable weights. Furthermore, mTAN assumes that $Q=K=\\textbf{0}$.\n\nBesides, since $\\operatorname{Softmax}(\\textbf{x} + c)=\\operatorname{Softmax}(\\textbf{x})$, where $\\textbf{x}$ is a vector and $c$ is a constant. The following equation holds,\n\\begin{equation}\n\\operatorname{Softmax}(\\operatorname{Attn}(Q, K)) = \\operatorname{Softmax}(\\operatorname{\\widehat{Attn}}(Q, K)) ~,\n\\end{equation}\nwhere the matrix form of $\\operatorname{Softmax}$ normalize each row of the matrix and \n\\begin{equation}\n\\begin{aligned}\n\\operatorname{Attn}& (Q, K)= \\\\\n& \\left[\\begin{array}{cccc}\nQ_1 \\cdot K_1^\\top & \\cdots & Q_1 \\cdot K_N + \\Phi(t_1) \\cdot (\\Phi(t_N) - \\Phi(t_1))^\\top  \\\\ \nQ_2 \\cdot K_1 + \\Phi(t_2) \\cdot (\\Phi(t_2) - \\Phi(t_1))^\\top  & \\cdots & Q_2 \\cdot K_N + \\Phi(t_2) \\cdot (\\Phi(t_N) - \\Phi(t_2))^\\top \\\\ \n\\vdots & \\ddots & \\vdots \\\\\nQ_N \\cdot K_1 + \\Phi(t_N) \\cdot (\\Phi(t_1) - \\Phi(t_N))^\\top & \\cdots & Q_N \\cdot K_N^\\top\n\\end{array}\\right] ~.\n\\end{aligned}\n\\end{equation}\n\n"
                },
                "subsection 9.2": {
                    "name": "Proof of Extension and Universal Approximation",
                    "content": " \n\n\\begin{lemma}[Existence of Continuously Differentiable Vector Function] \nGiven a continuously differentiable vector function $u: \\mathbb{R} \\rightarrow \\mathbb{R}^d \\in \\mathcal{C}^1$, such that $\\forall x \\in \\mathbb{R}, \\|u(x)\\|_2 < \\infty, \\|u(x)\\|_0 = d$ and a continuously differentiable function $w: \\mathbb{R} \\rightarrow \\mathbb{R} \\in \\mathcal{C}^1$, there always exists another continuously differentiable vector function $v: \\mathbb{R} \\rightarrow \\mathbb{R}^d \\in \\mathcal{C}^1$ with a fixed point $p$ satisfying $u(p) \\cdot v(p)^\\top = w(p)$, such that $\\forall x \\in \\mathbb{R}, u(x) \\cdot v(x)^\\top = w(x)$ holds. \\footnote{Consider an open set $\\mathcal{U}$ on the real line and a function $f$ defined on $\\mathcal{U}$ with real values. Let $k$ be a non-negative integer. The function $f$ is said to be of differentiability class $\\mathcal{C}^{k}$ if the derivatives  $f',f'',\\dots ,f^{(k)}$ exist and are continuous on $\\mathcal{U}$.}\n\\end{lemma}\n\n\\begin{proof}\nThroughout the proof, we define $\\xi := v(p)$.\n\nSince $\\forall x \\in \\mathbb{R}, 0 <\\|u(x)\\|_2 < \\infty$, we define\n\\begin{equation}\nv(x) = \\frac{u(x)}{\\|u(x)\\|_2^2} \\cdot w(x) + k(x) ~,\n\\end{equation}\nwhere $k(x) \\in \\mathcal{C}^1$ such that $\\forall x \\in \\mathbb{R}, u(x) \\cdot k(x)^\\top=0$ holds. Therefore, for $\\forall x  \\in \\mathbb{R}$, we have\n\\begin{equation}\nu(x) \\cdot v(x)^\\top = u(x) \\cdot \\left( \\frac{u(x)}{\\|u(x)\\|_2^2} \\cdot w(x) + k(x) \\right) = w(x) + u(x) \\cdot k(x) = w(x) ~.\n\\end{equation}\nThus, we only need to prove the existence of $k(x)$. Since $v(x)$ has a fixed point at $p$, therefore,\n\\begin{equation}\nk(p) = \\xi - \\frac{u(p)}{\\|u(p)\\|_2^2} \\cdot w(p) ~.\n\\label{eq:cons}\n\\end{equation}\nLet $z[i]$ denote the $i$-th element of a vector $z$, thus, Eq. (\\ref{eq:cons}) is equivalent to\n\\begin{equation}\nk(p)[i] = \\xi[i] - \\frac{u(p)[i]}{\\|u(p)\\|_2^2} * w(p)[i] ~,\n\\end{equation}\nfor $i \\in [1, d]$. Since we have $u(p) \\cdot v(p)^\\top = w(p)$, thus,\n\\begin{equation}\nk(p) \\cdot u(p)^\\top = \\xi \\cdot u(p)^\\top - w(p) = 0 ~.\n\\label{eq:kf}\n\\end{equation}\n\nWe denote $c_i := k(p)[i] * u(p)[i]$. Since Eq. (\\ref{eq:kf}) holds, therefore $\\sum_{i=1}^{d} c_i=0$. Finally, we can construct $k(x)$ as\n\\begin{equation}\nk(x) = \\left[\\begin{array}{cccc}\nk(x)[1]  \\\\ \nk(x)[2] \\\\ \n\\vdots  \\\\\nk(x)[d] \\end{array}\\right]^\\top = \\left[\\begin{array}{cccc}\nc_1 / u(x)[1]  \\\\ \nc_2 / u(x)[2] \\\\ \n\\vdots  \\\\\nc_d / u(x)[d] \\end{array}\\right]^\\top ~.\n\\end{equation}\nSince $u, w \\in \\mathcal{C}^1$, therefore $k \\in \\mathcal{C}^1$, thus $v \\in \\mathcal{C}^1$.\n\\end{proof}\n\n\n\\begin{lemma}[Existence of $\\boldsymbol{k}_i(t)$]\nGiven a continuously differentiable vector function $\\boldsymbol{\\mathrm{q}}: \\mathbb{R} \\rightarrow \\mathbb{R}^d \\in \\mathcal{C}^1$ such that $\\forall x \\in \\mathbb{R}, \\|\\boldsymbol{\\mathrm{q}}(x)\\|_2 < \\infty, \\|\\boldsymbol{\\mathrm{q}}(x)\\|_0 = d$ and a continuously differentiable function $f: \\mathbb{R} \\rightarrow \\mathbb{R} \\in \\mathcal{C}^2$ and a vector $K_i \\in \\mathbb{R}^{d}$ and satisfies $f(t_i) = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot K_i^\\top$, there always exist a continuously differentiable vector function $\\boldsymbol{\\mathrm{k}}_i: \\mathbb{R} \\rightarrow \\mathbb{R}^d$, such that the following statements hold:\n\\begin{itemize}[leftmargin=5mm]\n    \\item $\\boldsymbol{\\mathrm{k}}_i(t_i)=K_i$,\n    \\item $\\forall t \\in \\mathbb{R}, t \\neq t_i, \\boldsymbol{\\mathrm{\\alpha}}_i(t) := \\frac{\\int_{t_i}^t \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d} \\tau}{t-t_i} = f(t)$.\n\\end{itemize}\n\\end{lemma}\n\n\\begin{proof}\nSince $t \\neq t_i$, we have\n\\begin{equation}\n\\begin{gathered}\n\\begin{aligned}\n& \\boldsymbol{\\mathrm{\\alpha}}_i(t) = f(t) \\\\\n \\iff & \\boldsymbol{\\mathrm{\\alpha}}_i(t) (t - t_i) = f(t) (t - t_i) \\\\\n \\stackrel{\\textcircled{\\tiny{1}}}{\\iff} & \\frac{\\text{d} \\boldsymbol{\\mathrm{\\alpha}}_i(t) (t - t_i)}{\\text{d} t} = \\frac{\\text{d} f(t) (t - t_i)}{\\text{d} t} \\\\ \n \\iff & \\boldsymbol{\\mathrm{q}}(t) \\cdot  \\boldsymbol{\\mathrm{k}}_i(t)^\\top = \\frac{\\text{d} f}{\\text{d} t}(t - t_i) + f(t) ~, \\\\\n\\end{aligned}\n\\end{gathered}\n\\label{eq:pp}\n\\end{equation}\nwhere $\\textcircled{\\tiny{1}}$ holds since we can define $\\boldsymbol{\\mathrm{\\alpha}}_i(t_i)=f(t_i)$. Besides, since\n\\begin{equation}\n\\lim_{|\\epsilon| \\rightarrow 0} \\boldsymbol{\\mathrm{\\alpha}}_i(t_i + \\epsilon) = \\lim_{|\\epsilon| \\rightarrow 0} \\frac{\\int_{t_i}^{t_i + \\epsilon} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau) \\text{d} \\tau}{\\epsilon} = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot \\boldsymbol{\\mathrm{k}}_i(t_i)^\\top = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot K_i^\\top ~,\n\\end{equation}\nsuch a definition does not break the continuity of $\\boldsymbol{\\mathrm{\\alpha}}$.\n\nSince $f \\in \\mathcal{C}^2$, therefore we denote $g(t) := \\frac{\\text{d} f}{\\text{d} t}(t - t_i) + f(t) \\in \\mathcal{C}^1$. Besides, \n\\begin{equation}\ng(t_i) = f(t_i) = \\boldsymbol{\\mathrm{q}}(t_i)\\cdot K_i^\\top   =  \\boldsymbol{\\mathrm{q}}(t_i) \\cdot \\boldsymbol{\\mathrm{k}}_i(t_i)^\\top \n\\end{equation}\n\nBy using \\textbf{Lemma 1} with $g$ corresponding to $w$, $\\boldsymbol{\\mathrm{q}}$ to $u$, $\\boldsymbol{\\mathrm{k}}_i$ to $v$ and $p=t_i$, we can prove the existence of $\\boldsymbol{\\mathrm{k}}_i$ that satisfies Eq. (\\ref{eq:pp}).\n\\end{proof}\n\n\\begin{theorem}[Universal Attention Approximation Theorem]\nGiven query ($Q$) and key ($K$), such that $\\|Q_i\\|_2 < \\infty, \\|Q_i\\|_0 = d$ for $i \\in [1, ..., N]$.\nFor any attention matrix with formulation defined in Appendix \\ref{sec:variants}, i.e., $\\operatorname{Attn}(Q, K)$, there always exists a family of continuously differentiable vector functions $k_1(\\cdot), k_2(\\cdot), \\ldots, k_N(\\cdot)$, such that the discrete formulation of $[\\boldsymbol{\\mathrm{\\alpha}}_1(\\cdot), \\boldsymbol{\\mathrm{\\alpha}}_2(\\cdot), ..., \\boldsymbol{\\mathrm{\\alpha}}_N(\\cdot)]$ defined by\n\\begin{equation}\n\\operatorname{\\widetilde{Attn}}(Q, K)=\\left[\\begin{array}{cccc}\n\\boldsymbol{\\mathrm{\\alpha}}_1(t_1) & \\boldsymbol{\\mathrm{\\alpha}}_2(t_1) & \\cdots & \\boldsymbol{\\mathrm{\\alpha}}_N(t_1) \\\\\n\\boldsymbol{\\mathrm{\\alpha}}_1(t_2) & \\boldsymbol{\\mathrm{\\alpha}}_2(t_2) & \\cdots & \\boldsymbol{\\mathrm{\\alpha}}_N(t_2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\boldsymbol{\\mathrm{\\alpha}}_1(t_N) & \\boldsymbol{\\mathrm{\\alpha}}_2(t_N) & \\cdots & \\boldsymbol{\\mathrm{\\alpha}}_N(t_N)\n\\end{array}\\right] ~,\n\\label{eq:proof}\n\\end{equation}\nsatisfies that $\\operatorname{\\widetilde{Attn}}(Q, K) = \\operatorname{Attn}(Q, K)$, where \n\\begin{equation}\n\\boldsymbol{\\mathrm{\\alpha}}_i(t) = \n\\frac{\\int_{t_i}^t \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d} \\tau}{t-t_i} ,  \\text{if } t \\neq t_i ~,\n\\end{equation} \nand $\\boldsymbol{\\mathrm{\\alpha}}_i(t_i) = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot \\boldsymbol{\\mathrm{k}}_i(t_i)^\\top$, $\\boldsymbol{\\mathrm{q}}(t)$ satisfies $\\boldsymbol{\\mathrm{q}}(t_i)=Q_i$.\n\\end{theorem}\n\n\\begin{proof}\nFirst, we show that the diagonal elements in Eq. (\\ref{eq:proof}), i.e., \n\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_i(t_i) = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot \\boldsymbol{\\mathrm{k}}_i(t_i)^\\top = Q_i \\cdot K_i^\\top ~,\n\\end{equation}\nholds for different forms of attention matrix defined in Appendix \\ref{sec:variants}.\n\nNext, for a given $i \\in [1, N]$, there exists a continuous cubic spline $h_i: \\mathbb{R} \\rightarrow \\mathbb{R}$ satisfies $h_i(t_j)=\\operatorname{Attn}(Q, K)_{j, i}$ for $\\forall j \\in [1, \\ldots, N]$ and $h_i \\in \\mathcal{C}^2$. By using \\textbf{Lemma 2} with $h_i$ corresponding to $f$, there exists a function $\\boldsymbol{\\mathrm{k}}_i$ such that $\\boldsymbol{\\mathrm{\\alpha}}_i(t) = h_i(t)$. Therefore, $\\boldsymbol{\\mathrm{\\alpha}}_i(t_j) = h_i(t_j)$ for $j \\in [1, ..., N]$, and we finish the proof.\n\\end{proof}\n\n"
                }
            },
            "section 10": {
                "name": "Experiment Details",
                "content": "\n\n",
                "subsection 10.1": {
                    "name": "Modeling Continuous-Time Function",
                    "content": " \\label{sec:interpolate}\n\n",
                    "subsubsection 10.1.1": {
                        "name": "Background: 2D Spiral",
                        "content": "\n\nA spiral is a curve that starts from a central point and gradually moves away from it while turning around it. We consider two types of spirals that start from time $s$ and end at time $e$. The first type of spiral is defined as\n\\begin{equation}\n    \\begin{aligned}\n        x(t) & = (a + bt) \\cdot cos(t) ~,\\\\\n        y(t) & = (a + bt) \\cdot sin(t) ~,\n    \\end{aligned}\n\\end{equation}\nwhile the second type of spiral is defined as\n\\begin{equation}\n    \\begin{aligned}\n        x(t) & = \\left(a + 50b / \\left(e-t\\right)\\right) \\cdot cos(e-t) ~,\\\\\n        y(t) & = \\left(a + 50b / \\left(e-t\\right)\\right) \\cdot sin(e-t) ~,\\\\\n    \\end{aligned}\n\\end{equation}\nwhere $a$ and $b$ are parameters of the Archimedean spiral.\n\n"
                    },
                    "subsubsection 10.1.2": {
                        "name": "Experiment Setting",
                        "content": "\n\nWe generate 300 spirals and 200/100 spirals are used for training/testing respectively. For each spiral, it randomly belongs to either a clockwise spiral or a counter-clockwise spiral, we sample $a \\sim \\mathcal{N}(0, \\alpha), b \\sim \\mathcal{N}(0.3, \\alpha)$ and we set $\\alpha=0.02$ for Table ~\\ref{tab:interp}. Also, we add Gaussian noise from $\\mathcal{N}(0, \\beta)$ to all training samples and we set $\\beta=0.1$. We test different models for interpolation and extrapolation. Specifically, each spiral is sampled at 150 equally-spaced time points. To generate an irregular time series, we randomly sample 30 points from the first half of the trajectory. Thus, the first half of the trajectories are used for the interpolation task while the second half of the trajectories, which is totally unobserved during both training and testing, are used for the extrapolation task. All the models are trained for 5000 iterations with a batch size equal to 64.\n\n"
                    },
                    "subsubsection 10.1.3": {
                        "name": "Baseline Implementation",
                        "content": "\n\nWe compare ContiFormer with Neural ODE-based models and Transformer. \n\n\\begin{itemize}[leftmargin=5mm]\n    \\item \\textit{Neural ODE\\cite{chen2018neural}.} This model uses the hidden vectors from the last observation as input, and obtain both the interpolation and extrapolation result from a single ODE solver. We train the model by minimizing the mean square loss.\n    \\item \\textit{Latent ODE (w/ RNN Enc)\\cite{chen2018neural}.} We run an RNN encoder through the time series and infer the parameters for a posterior over $z_{t_0}$. Next, we sample $\\mathbf{z}_{t_0} \\sim q\\left(\\mathbf{z}_{t_0} \\mid\\left\\{\\mathbf{x}_{t_i}, t_i\\right\\}\\right)$. Finally, we obtain both the interpolation and extrapolation result from a single ODE solver, i.e., $\\operatorname{ODESolve}(\\mathbf{z}_{t_0}, f, \\theta_f , t_0, . . . , t_M)$, where $(t_0, t_1, ..., t_M)$ is the time of the trajectory. We train the model by maximizing the evidence lower bound (ELBO). \n    \\item \\textit{Latent ODE (w/ ODE Enc)\\cite{rubanova2019latent}.} We replace the RNN encoder in Latent ODE (w/ RNN Enc) with the ODE-RNN encoder following \\cite{rubanova2019latent}. Again, we train the model by maximizing the evidence lower bound (ELBO).\n    \\item \\textit{Neural CDE\\cite{park2022neural}.} We follow the official implementation to construct the CDE function. For the interpolation, the hidden vectors are directly obtained from the continuous-time path from Neural CDE. For the extrapolation, we input the hidden vectors from the last timestamp of the Neural CDE to a single ODE solver. We train the model by minimizing the mean square loss.\n    \\item \\textit{Transformer\\cite{vaswani2017attention}.} In order to obtain the prediction for both seen and unseen time points, we use a learnable mask for unseen inputs. Instead of using position embedding, the input is added with a temporal encoding before forwarding it to the transformer layer. We train the model by minimizing the mean square loss.\n    \\item \\textit{ContiFormer.} We use linear interpolation to deal with unseen observations to avoid the numerical error caused by constructing the continuous-time query. Similar to Transformer, temporal encoding is adopted and the model is trained by minimizing the mean square loss.\n\\end{itemize}\n\n\n\n"
                    },
                    "subsubsection 10.1.4": {
                        "name": "Experimental Results with Different $\\alpha$",
                        "content": "\n\nIn this subsection, we test Transformer, Neural ODE and ContiFormer with different $\\alpha$ to show the robustness of these models. As shown in Table \\ref{tab:int2}, our model can achieve the best result on different parameters.\n\n"
                    },
                    "subsubsection 10.1.5": {
                        "name": "More Visualization Results",
                        "content": "\n\nMore visual results with $\\alpha$=0.02 can be found in Figure ~\\ref{fig:int2}.\n\n\n\n"
                    }
                },
                "subsection 10.2": {
                    "name": "Irregular Time Series Classification",
                    "content": " \\label{sec:classify}\n\n",
                    "subsubsection 10.2.1": {
                        "name": "Training Details",
                        "content": "\n\nFor both our model and the baseline models, we adopted a fixed learning rate of $10^{-2}$ and a batch size of $64$. To ensure the reliability of our results, all experiments were conducted three times, and the reported values represent the mean. The training process for all models lasted for 1000 epochs. However, if the training accuracy failed to show any improvement for 100 consecutive epochs, the training was stopped to avoid unnecessary computation. \n\n"
                    },
                    "subsubsection 10.2.2": {
                        "name": "Dataset Information",
                        "content": "\n\nWe select 20 datasets from UEA Time Series Classification Archieve with diverse characteristics in terms of the number, dimensionality and length of time series samples, as well as the number of classes. Detailed information is listed in Table \\ref{tab:ueadata}.\n\n\n\n"
                    },
                    "subsubsection 10.2.3": {
                        "name": "Full Experimental Results",
                        "content": " \\label{sec:full}\n\nWe follow the setting of \\cite{kidger2020neural} to randomly drop either 30\\%, 50\\% or 70\\% observations. Experimental results are shown in Table \\ref{tab:uea30}, Table \\ref{tab:uea50} and Table \\ref{tab:uea70}. We can find that ContiFormer outperforms RNN-based methods, ODE-based methods and Transformer-based methods. Also, we observe that ContiFormer outperforms all the baselines by a large margin when 70\\% observations are dropped, which confirms that ContiFormer is more suitable for irregular time series modeling.\n\n\n\n\n\n\n\n"
                    }
                },
                "subsection 10.3": {
                    "name": "Predicting Irregular-Sampled Sequences",
                    "content": " \\label{sec:mtpp}\n\n",
                    "subsubsection 10.3.1": {
                        "name": "Background: Marked Temporal Point Process (MTPP)",
                        "content": "\n\nThe marked Temporal Point Process (MTPP) has been widely studied in recent years. We define a sequence of events and types as $\\Gamma=[(t_1, k_1), (t_2, k_2), \\ldots, (t_N, k_N)]$, where $t_i$ is the time point when the $i$-th event occurs, with $t_1 < t_2 < \\ldots < t_N$, $k_i \\in \\{ 1, 2, \\ldots, \\mathcal{K} \\}$ is the type of the $i$-th event and $\\mathcal{K}$ is the total number of event types. A history event is the event sequence up to but not including time $t$, denoted by $\\mathcal{H}_t={(t_j, k_j): t_j<t}$. The goal of the temporal point process is to model the conditional intensity function (CIF), denoted by $\\lambda_{k}^{*}(t)$, which represents the likelihood of an event occurring at time $t$, given the historical events. Models are trained so that the log probability of an event sequence, i.e., \n\n\\begin{equation}\n    \\log p(\\Gamma)=\\sum_{j=1}^N \\log \\lambda_{k_j}^*\\left(t_j\\right)-\\int_{t_1}^{t_N} \\lambda^*(\\tau) d \\tau ~.\n\\end{equation}\n\n"
                    },
                    "subsubsection 10.3.2": {
                        "name": "Dataset Description",
                        "content": " \n\nWe use one synthetic dataset and five real-world datasets, namely Synthetic, Neonate~\\cite{stevenson2019dataset}, Traffic~\\cite{lai2018modeling},\nMIMIC~\\cite{du2016recurrent}, BookOrder~\\cite{du2016recurrent} and StackOverflow~\\cite{leskovec2014snap} to evaluate our model. \n\n\\paragraph{Synthetic}\n\nTo evaluate the effectiveness of the proposed method, we create a synthetic dataset. Specifically, we consider $10$ types of event where each type of event contain three properties, i.e., $(X_i, V_i, D_i)$. Hence, given an event $\\textbf{e}$ with type $j$ is represented as $\\textbf{e} = (e.x, e.v, e.d, e.k)$, where $e.x \\sim \\mathcal{N}(X_j, \\mu_1)$, $e.v \\sim \\mathcal{N}(V_j, \\mu_2)$, $e.d=D_j > 0 $ and $e.k$ is the type of the event. We set $\\mu_1=\\mu_2=0.01$ and $X, V, D \\in \\mathbb{R}^{10}$ is randomly generated and pre-defined.\n\nTo model both the correlation between events and the dynamic changes of events within the system. We first define the dynamic attention (weight) for each event, which is composed of inter-event dependency modeling and intra-event dynamic modeling. For inter-event dependency, it aims to calculate time-aware attention (weight) between each pair of events. Specifically, we use the following equation to define the weight between event $\\textbf{e}$ and $\\hat{\\textbf{e}}$,\n\\begin{equation}\n    \\omega(\\textbf{e}, \\hat{\\textbf{e}}) = (e.t - \\hat{e}.t) \\cdot (e.x * \\hat{e}.v - \\hat{e}.x * e.v) ~.\n\\end{equation}\nNext, the define $\\lambda_{k}^{*}(t)$ and $\\lambda^{*}(t)$ in which we assume that only the last event attributes to the decision of the next event, following Markov decision process (MDP), i.e.\n\n\\begin{equation}\n\\begin{aligned}\n\\lambda_{k}^{*}(t) & = P(k | \\hat{e}.k) \\cdot \\text{Softplus} \\left(\\sum_{\\textbf{e} \\in \\mathcal{H}_t} \\omega(\\textbf{e}, \\hat{\\textbf{e}}) \\cdot \\text{exp} \\left( \\frac{- (t - e.t)}{e.d} \\right) \\right) ~,\\\\\n\\lambda^{*}(t) & = \\text{Softplus} \\left(\\sum_{\\textbf{e} \\in \\mathcal{H}_t} \\omega(\\textbf{e}, \\hat{\\textbf{e}}) \\cdot \\text{exp} \\left( \\frac{- (t - e.t)}{e.d} \\right) \\right) ~.\\\\\n\\end{aligned}\n\\label{eq:lam}\n\\end{equation}\nwhere $\\text{exp} \\left( - (t - e.t) / e.d \\right)$ is the exponential decay effect for each event, $\\hat{\\textbf{e}}$ is the last event as of time $t$. The existence of the exponential decay function ensures that all the events will eventually disappear in the system as time goes to infinity so as to stabilize the conditional intensify function. $P(k | \\hat{k})$ is a pre-defined matrix that means the probability of the next event to with type $k$ given that the type of the previous event is $\\hat{\\textbf{e}}$.\n\nNext, we predict the occurrence time of the next event with\n\\begin{equation}\n    \\begin{aligned}\n        \\widehat{t}_{j+1} & =\\int_{t_j}^{\\infty} t \\cdot p\\left(t \\mid \\mathcal{H}_t\\right) d t ~,\\\\\n        \\text{ where \\quad} p\\left(t \\mid \\mathcal{H}_t\\right) & =\\lambda^{*}(t) \\exp \\left(-\\int_{t_j}^t \\lambda^{*}(t) d t\\right) ~.\\\\\n    \\end{aligned}\n    \\label{eq:pre}\n\\end{equation}\nTo predict the type of the next event, we sample the type of the next event from $P(\\cdot | \\hat{e}.k)$, where $\\hat{e}.k$ is the type of the previous event.\n\nFinally, we use 500 different random seeds to generate a total of 500 event sequences. Also, we use 4-fold cross-validation for evaluation. The generation process is shown in Algorithm \\ref{algo:gen}.\n\n\\begin{algorithm}\n\\caption{Generate Synthetic Dataset with Time-Aware Dynamic Kernel}\n\\label{algo:gen}\n\\begin{algorithmic} \n\\STATE \\textbf{Input} $\\textit{seed} \\in [0, 500)$, $\\textit{tmax}=20$, $X = [0.78, 0.11, -1.0, -0.56, -0.78, 1.0, 0.56, -0.33, 0.3, $ $-0.11]$, $V=[-0.17, 0.5, -0.28, 0.28, 0.17, 0.39, -0.05, 0.05, -0.39, -0.5]$, $D=[0.5, 0., 0.28, $ $ 0.17, 0.22, 0.44, 0.33, 0.11, 0.05, 0.39]$ \n\n\\STATE \\textbf{Output} A generated event sequence $E$. \n\n\\STATE Initialize the random state of Numpy using $\\textit{seed}$ and defined the probabilistic matrix $P$.  \n\\STATE Initialize the event sequence $E$ to be an empty array, and set current time $T=0$.\n\\STATE Random generate the type of the first event $e.k$ and sample $e.x, e.v$ and $e.d$.\n\\STATE Add the event to the event sequence.\n\\WHILE {$ t \\leq \\textit{tmax}$}\n    \\STATE Calculate the conditional intensity function using Eq. (\\ref{eq:lam}).\n    \\STATE Predict the time of the next event $e.t$ using Eq. (\\ref{eq:pre}).\n    \\STATE Assume the type of previous event is $\\hat{k}$, sample the type of the next event $e.k$ from $P(\\cdot | \\hat{k})$.\n    \\STATE Sample sample $e.x, e.v$ and $e.d$ given $e.k$.\n    \\STATE Add the generated event to the event sequence $E$.\n    \\STATE Update $T=e.t$.\n\\ENDWHILE\n\\RETURN $E$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\paragraph{Neonate.} Neonate dataset is a clinical dataset from 79 patients, among which 48 patients have incurred at least two seizures during the observation. The dataset contains only one type of event, i.e., the start point of a seizure (in minutes). The task is to predict when the next seizure will happen. Note that we only clear the start point of the seizure. Thus, we only have one type of event.\n\n\\paragraph{Traffic.} Traffic dataset is a benchmark dataset for time-series forecasting collected by the Caltrans Performance Measurement System (PeMS). This dataset contains the traffic state monitoring of 861 sensors in a two-year period. The data is collected every 1 hour. We extract time points with fluctuations higher than 50\\% as events, whose types are up and down according to the direction. We consider two events that are within 6 hours as duplicated events and remove the latter one from the dataset. Besides, for each sensor, it contains a long sequence, we partition these event sequences by month, i.e., for each event sequence, it contains the event of a sensor in one month.\n\n\\paragraph{MIMIC.} MIMIC dataset is a clinical dataset that collects patients' visits to a hospital's ICU in a seven-year period. We treat the visits of each patient as a separate sequence.\n\n\\paragraph{BookOrder.} BookOrder dataset is a financial dataset that contains transaction records of stock in one day. We record the time (in milliseconds) and the action that was taken in each transaction. The dataset is a single long sequence with only two types of events: \u201cbuy\u201d and \u201csell\u201d. We clip the event sequences so that the maximum length is 500 in both the train and test sets. Furthermore, the mean $\\mu$ and standard derivative $\\sigma$ of the time difference between two events in the dataset is 1.32 and 20.24 respectively. Therefore, we clip the maximum time difference to 70.\n\n\\paragraph{StackOverflow.} StackOverflow dataset is a question-answering website. The website rewards users with badges to promote engagement in the community, and the same badge can be rewarded multiple times to the same user. We collect data in a two-year period, and we treat each user\u2019s reward history as a sequence.\n\nFor MIMIC, Bookorder, and StackOverflow datasets, we use the dataset from the Google Drive\\footnote{\\url{https://drive.google.com/drive/folders/0BwqmV0EcoUc8UklIR1BKV25YR1U?resourcekey=0-OrlU87jyc1m-dVMmY5aC4w}}. Statistic information can be found in Table ~\\ref{tab:data}. We use the 4-fold cross-validation scheme for Synthetic, Neonate, and Traffic datasets following~\\cite{fang2023learning}, and the 5-fold cross-validation scheme for the other three datasets following~\\cite{mei2017neural, zuo2020transformer}.\n\n\n\n\n"
                    },
                    "subsubsection 10.3.3": {
                        "name": "Implementation Details",
                        "content": " \n\n\\paragraph{Input Embedding}\n\nGiven the history event $\\mathcal{H}_t=\\left\\{\\left(t_j, k_j\\right): t_j<t\\right\\}$ with a total of $\\mathcal{K}$ events, the input of Transformer-based model and RNN-based model is obtained through event embedding and temporal encoding, i.e., for the $i$-th event $x_i = \\text{emb}(k) + \\text{enc}(t)$, where $\\text{emb}(k)$ is the $k$-th column of the embedding table $\\mathcal{E} \\in \\mathbb{R}^{\\mathcal{K} \\times d}$ and temporal encoding is defined by a combination of sine and cosine function~\\cite{zuo2020transformer}.\n\n\\paragraph{Conditional Intensity Function}\n\nGiven the output from the model, i.e. $\\mathbf{h}\\left(t_1\\right), \\mathbf{h}\\left(t_2\\right),  .., \\mathbf{h}\\left(t_N\\right)$. The dynamics of the temporal point process are described by a continuous conditional intensity function. To reduce the memory cost and accelerate the training process. Instead of using the continuous-time output from ContiFormer, we only use ContiFormer to generate a hidden representation for discrete time points, i.e., $t_1, t_2, ..., t_j, ..., t_N$. Following ~\\cite{zuo2020transformer}, the conditional intensity function is defined by\n\\begin{equation}\n\\lambda\\left(t \\mid \\mathcal{H}_t\\right)=\\sum_{k=1}^K \\lambda_k\\left(t \\mid \\mathcal{H}_t\\right) ~,\n\\end{equation}\nwhere each of the type-specific conditional intensity function is defined by\n\\begin{equation}\n\\lambda_k\\left(t \\mid \\mathcal{H}_t\\right)=f_k\\left(\\alpha_k \\frac{t-t_j}{t_j}+\\mathbf{w}_k^{\\top} \\mathbf{h}\\left(t_j\\right)+b_k\\right) ~,\n\\end{equation}\nwhere on interval $t \\in [t_j, t_{j+1})$ and $f_k(x)=\\beta_k \\log \\left(1+\\exp \\left(x / \\beta_k\\right)\\right)$ is the softplus function with parameter $\\beta_k$. The use of such a closed-form intensity function allows for $O(1)$ interpolation.\n\n\\paragraph{Training Loss} \\label{sec:loss}\n\nWe train our model and all the baseline models by jointing maximizing the log-likelihood of the sequence and minimizing the prediction losses.\n\nSpecifically, for a sequence $\\mathcal{S}$ over an interval $[t_i, t_N]$, given the conditional intensity function $\\lambda\\left(t \\mid \\mathcal{H}_t\\right)$, the log-likelihood is given by\n\\begin{equation}\n\\ell_{LL}(\\mathcal{S})=\\underbrace{\\sum_{j=1}^N \\log \\lambda\\left(t_j \\mid \\mathcal{H}_j\\right)}_{\\text {event log-likelihood }}-\\underbrace{\\int_{t_1}^{t_N} \\lambda\\left(t \\mid \\mathcal{H}_t\\right) d t}_{\\text {non-event log-likelihood }} .\n\\end{equation}\nOne challenge to compute the log-likelihood is to solve the integral, i.e., $\\Lambda=\\int_{t_1}^{t_N} \\lambda\\left(t \\mid \\mathcal{H}_t\\right) d t$. Since \\textit{softplus} function has no closed-form computation, we apply the Monte Carlo integration method, i.e.,\n\\begin{equation}\n\\widehat{\\Lambda}_{\\mathrm{MC}}=\\sum_{j=2}^P\\left(t_j-t_{j-1}\\right)\\left(\\frac{1}{P} \\sum_{i=1}^P \\lambda\\left(u_i\\right)\\right) ~,\n\\end{equation}\nwhere $u_i \\sim \\operatorname{Unif}(t_{j-1}, t_j)$ for $i \\in [1, ..., P]$ is sampled from a uniform distribution over an interval $[t_{j-1}, t_j]$.\n\nBesides the log-likelihood loss, we also use the cross-entropy loss and regression loss for predicting the type and time of the next events.\n\nSpecifically, given the conditional intensity function, the next time stamp prediction is given by\n\\begin{equation}\n\\begin{aligned}\n& p\\left(t \\mid \\mathcal{H}_t\\right)=\\lambda\\left(t \\mid \\mathcal{H}_t\\right) \\exp \\left(-\\int_{t_j}^t \\lambda\\left(\\tau \\mid \\mathcal{H}_\\tau\\right) d \\tau\\right) ~, \\\\\n& \\widehat{t}_{j+1}=\\int_{t_j}^{\\infty} t \\cdot p\\left(t \\mid \\mathcal{H}_t\\right) d t ~. \\\\\n\\end{aligned}\n\\end{equation}\nThus, the regression loss and the cross-entropy loss are given by\n\\begin{equation}\n\\ell_{reg}(\\mathcal{S}) = \\sum_{i=2}^{N} (\\widehat{t}_{i} - t_i)^2 ~, \n\\ell_{pred}(\\mathcal{S}) = \\sum_{i=2}^{N} -\\log \\left( \\frac{\\lambda_{k_i}\\left(\\widehat{t}_{i}\\right)}{\\lambda\\left(\\widehat{t}_{i} \\right)} \\right) ~.\n\\end{equation}\nOverall, the modeling is trained in a multi-task manner, i.e., \n\\begin{equation}\n    \\ell(\\mathcal{S}) = \\ell_{LL} + \\alpha_1 \\ell_{reg}(\\mathcal{S}) + \\alpha_2 \\ell_{pred}(\\mathcal{S}) ~,\n\\end{equation}\nwhere $\\alpha_1$ and $\\alpha_2$ are the pre-defined weight. By default, we set $\\alpha_1=0.01$ and $\\alpha_2=1$.\n\n"
                    },
                    "subsubsection 10.3.4": {
                        "name": "Significance test on Synthetic and BookOrder datasets",
                        "content": "\n\nTo demonstrate the statistical superiority of ContiFormer, we conducted a t-test on Synthetic and BookOrder datasets using 10 random seeds. The t-test results are listed in Table \\ref{tab:ttest}.\n\n\n\n"
                    }
                },
                "subsection 10.4": {
                    "name": "Regular Time Series Forecasting",
                    "content": " \\label{sec:regular}\n\nTime series forecasting has garnered considerable attention in recent years, as evidenced by various studies~\\cite{wu2021autoformer, zhou2021informer, li2023towards}. In this study, we assess ContiFormer against several models tailored for regular time series forecasting. \n\n\\paragraph{Datasets}\n\nWe evaluate ContiFormer on five experimental datasets. i) ETT dataset \\cite{zhou2021informer} contains the data collected from electricity transformers, including load and oil temperature that are recorded every 15 minutes between July 2016 and July 2018. Here, we use ETTm2 and ETTh2 datasets. ii) Exchange dataset \\cite{wu2021autoformer} records the daily exchange rates of eight different countries ranging from 1990 to 2016. iii) Weather dataset is recorded every 10 minutes for 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc. v) ILI dataset includes the weekly recorded influenza-like illness (ILI) patients data from Centers for\nDisease Control and Prevention of the United States between 2002 and 2021. \n\n\\paragraph{Baselines}\n\nWe compare ContiFormer with several transformer-based models for regular time series forecasting under the multivariate setting. These transformer-based models includes: FEDformer\\cite{zhou2022fedformer}, Autoformer\\cite{wu2021autoformer}, Informer\\cite{zhou2021informer} and Reformer\\cite{kitaev2020reformer}. Also, we chose one recent state-of-the-art model. i.e. TimesNet\\cite{wu2022timesnet} for comparison.\n\n\\paragraph{Results of Time Series Forecasting}\n\nThe experimental results are depicted in Table~\\ref{tab:regular}. ContiFormer achieves the best performance on ILI dataset. Specifically, it gives \\textbf{10\\%} MSE reduction (2.874 $\\rightarrow$ 2.632). Also, ContiFormer outperforms Autoformer and FEDformer on Exchange, ETTm2, and Weather datasets. Therefore, it demonstrates that ContiFormer performs competitively with the state-of-the-art model.\n\n\n\n"
                },
                "subsection 10.5": {
                    "name": "Pendulum Regression Task",
                    "content": " \\label{sec:pendulum}\n\nThe objective of pendulum regression is to assess the models' capability in accommodating observations obtained at irregular intervals~\\cite{schirmer2022modeling, smith2022simplified}. The input sequence comprises $50$ images, each measuring $24$ by $24$ pixels, and is afflicted by correlated noise. These images are sampled at irregular intervals from a continuous trajectory spanning a duration of $100$ units, while the targets correspond to the sine and cosine values of the pendulum's angle. It's important to note that the pendulum follows a nonlinear dynamical system, with the velocity remaining unobserved. Table \\ref{tab:pendulum} summarizes the results of this experiment. ContiFormer outperforms CRU and ODE-RNN.\n\n\n\n"
                }
            },
            "section 11": {
                "name": "Ablation Study and Parameter Study",
                "content": "\n\n",
                "subsection 11.1": {
                    "name": "Effects of Different Attention Mechanism",
                    "content": "\n\nWe study the impact of different attention mechanisms to show the significance of both continuous-evolving attention scores and continuous value functions. We claim that both of these two continuous designs attribute to the modeling of complex dynamic systems. We conduct the experiment on 6 datasets for event prediction task. As shown in Table. \\ref{tab:abl}, ContiFormer outperforms both of the two variants, which shows that jointly modeling the dynamic relationship between input observations and meanwhile capturing the dynamic change of the observations is critical for modeling complex event sequences in reality.\n\n\n\n"
                },
                "subsection 11.2": {
                    "name": "Effects of Different Numeric Approximation Methods",
                    "content": "\n\nAs discussed in Appendix ~\\ref{sec:approximating}, linear interpolation and Gauss-Legendre Quadrature are two methods for approximating an integral in a close interval $[-1, 1]$. We study the impact of these two methods and set $P$, i.e., the number of intermediate steps for integral approximation, in the Gauss-Legendre quadrature from $\\{ 2, 3, 4, 5\\}$. We conducted the experiment on 6 datasets for event prediction task. The experimental results can be found in Table ~\\ref{tab:ffr}. As shown in the table, our model exhibits low sensitivity to the choice of approximation methods.\n\n\n\n\n"
                },
                "subsection 11.3": {
                    "name": "Effects of Tolerance Errors in ODESolver",
                    "content": " \\label{sec:tolerance}\n\nODE solvers play a crucial role in ensuring that the model's output remains within a specified tolerance of the true solution~\\cite{chen2018neural}. By adjusting this tolerance, we can observe changes in the behavior of the neural network. Given our use of a fixed-step ODE solver, we sought to investigate the impact of the step size on prediction performance throughout the experiment. Specifically, we explored different step sizes from the set $\\{0.5, 0.1, 0.05, 0.01, 0.005\\}$. A smaller step size corresponds to a greater number of forward passes required to solve the ordinary differential equations, resulting in more accurate solutions at the expense of increased computational time. We conducted extensive experiments on 6 datasets for the event prediction task. The experimental results are presented in Table ~\\ref{tab:eff}. The table reveals that our model demonstrates low sensitivity to the tolerance error. Therefore, we set the tolerance error to $0.1$ in order to make a desirable balance between computational time and prediction accuracy. We explain the phenomena in two aspects. First, our framework, similar to the Transformer architecture, circumvents cumulative errors by eliminating the necessity of sequentially passing neural networks in a regressive manner. Second, since the output from the attention module is actually a weighted sum of the tokens, the total variance is lowered. For instance, assume that $X_1, X_2, ..., X_N \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n, then the total variance of the mean value $\\bar{x}=\\frac{1}{N} (X_1+X_2+...+X_N)$ is $\\frac{\\sigma^2}{N}$, and the variance is significantly lowered given the large \n. Overall, our model is not sensitive to tolerance error, which makes our model robust to different scenarios.\n\n\n"
                },
                "subsection 11.4": {
                    "name": "Effects of Vector Fields in ODE",
                    "content": "\n\nNeural ODEs are controlled by a vector field that defines the change of $x$ over time, i.e.,\n\\begin{equation}\n    \\frac{\\text{d} \\textbf{x}}{\\text{d} t} = f(\\textbf{x}, t) ~.\n\\end{equation}\nwhere $f: \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}^{d}$. We investigate two types of $f$ to implement Eq. (\\ref{eq:kv}). \n\nSpecifically, we denote $\\text{Concat}$ as\n\\begin{equation}\nf(\\textbf{x}, t)=\\text{Actfn}(\\text{Linear}^{d, d}(\\text{Linear}^{d, d}(\\textbf{x}) + \\text{Linear}^{1, d}(t))) ~,\n\\end{equation}\nand $\\text{ConcatNorm}$ as\n\\begin{equation}\nf(\\textbf{x}, t)=\\text{Actfn}(\\text{LN}(\\text{Linear}^{d, d}(\\text{Linear}^{d, d}(\\textbf{x}) + \\text{Linear}^{1, d}(t)))) ~, \n\\end{equation}\nwhere $\\text{Actfn}(\\cdot)$ is either $\\text{tanh}$ or $\\text{sigmoid}$ activation function, $\\text{Linear}^{a,b}(\\cdot): \\mathbb{R}^a \\rightarrow \\mathbb{R}^b$: is a linear transformer from dimension $a$ to dimension $b$, $\\text{LN}$ denotes the layer normalization. We conduct the experiment on 6 datasets for event prediction task. The experimental results are shown in Table ~\\ref{tab:func}. As shown in the table, incorporating normalization before the activation function leads to improved predictive performance.\n\n\n\n\n\n\n"
                },
                "subsection 11.5": {
                    "name": "Effects of Dropout Rate",
                    "content": "\n\n\n\nDropout in the Transformer can prevent overfitting and boost performance. However, with the increment of the dropout rate, we output of ContiFormer tends to be less continuous, leading to a sub-optimal solution for tasks that prefer a continuous-time output. To this end, we conduct experiments on both modeling continuous-time function and event prediction tasks. As for the task of modeling continuous-time function, it is desirable to have a dynamic system where outputs are ``almost'' continuous and smooth. As shown in Figure ~\\ref{fig:dropout}, the performance of both Transformer and ContiFormer drops significantly as the dropout rate increases. For prediction tasks, the dynamic system can be shaky, which reflects noise or other uncontrolled influences on the system. The experimental results on event prediction can be found in Table ~\\ref{tab:drp}. As shown in the table, the dropout rate has low sensitivity to the prediction results. \n\n"
                },
                "subsection 11.6": {
                    "name": "Effects of Time Normalization",
                    "content": "\n\nAs shown in Eq. (\\ref{eq:attn}) and Eq. (\\ref{eq:vv}), due to the nature of sequence data, it is common to encounter abnormal points, such as events with significantly large time differences. To avoid numeric instability during training, we divide the integrated solution by the time difference. To examine the effect of normalization, we conduct experiments on 6 datasets for event prediction task. The experimental results can be found in Table ~\\ref{tab:norm}, which shows that the normalization is significant on most datasets, including Synthetic, Neonate, MIMIC and StackOverflow datasets.\n\n\n\n\n\n\n"
                }
            },
            "section 12": {
                "name": "Time Cost v.s. Memory Cost v.s. Accuracy",
                "content": " \\label{sec:cost}\n\n\n\n\n\n\nUtilizing continuous-time modeling in ContiFormer often results in substantial time and GPU memory overhead. We have conducted a comparative analysis of ContiFormer's actual time costs in comparison to Transformer-based models (TST), ODE-based models (ODE-RNN, Neural CDE), and SSM-based models (S5). Throughout our analysis, we employ an RK4 solver for ODE-based models and our model, setting the step size to $0.1$ and $0.01$. It's worth noting that, for ODE-RNN and other ODE-based models, the input times have an impact on the number of forward passes when using a fixed-step ODE solver, thus, affecting the time costs.\n\nThe left figure in Figure \\ref{fig:cost} illustrates the influence of input times concerning input length. For the model with normalization (a.k.a., w/ norm), the input times consistently range from $[0,1]$, regardless of the input length. Conversely, for the model without normalization (a.k.a., w/o norm), the input times span from $[0, L]$, where $L$ represents the input length.\n\nAs depicted in the middle figure in Figure \\ref{fig:cost}, our model performs comparably to ODE-RNN (w/ norm) when the input length is up to $100$, but becomes approximately four times slower as the input length extends to $1000$. The right figure in Figure \\ref{fig:cost} displays GPU memory usage, revealing a significantly higher memory cost associated with our model.\n\nTable \\ref{tab:costfull} provides a more comprehensive overview of different models on UEA datasets. It is evident that our model strikes a better balance between accuracy and time cost. Specifically, our model achieves the best performance on 3 out of 6 selected datasets while incurring only twice the time cost compared to S5.\n\n\\newpage\n"
            },
            "section 13": {
                "name": "Broader Impact",
                "content": "\n\nThe proposed model, ContiFormer, presents several potential impacts within the field of machine learning and irregular time series analysis. These include:\n\\begin{itemize}[leftmargin=5mm]\n    \\item Enhanced Modeling Capability: ContiFormer extends the Transformer architecture to effectively capture complex continuous-time dynamic systems. By leveraging continuous-time modeling techniques, the model demonstrates improved performance in handling irregular time series data, offering a more flexible and powerful modeling ability.\n    \\item Practical Applications: Our proposed method achieves state-of-the-art performance in a variety of tasks and real-life datasets, which makes it more promising to tackle real-world applications. The ability to model continuous-time dynamics offers valuable insights into time-evolving systems, facilitating improved decision-making and predictive capabilities in diverse domains.\n\\end{itemize}\n"
            }
        },
        "tables": {
            "tab:complex": "\\begin{wraptable}{r}{0.6\\textwidth}\n    \\centering\n    \\vspace{-13pt}\n    \\caption{Per-layer complexity (Comp. per Layer), minimum number of sequential operations (Seq. Op.), and maximum path lengths (Max. Path Len.). $N$ is the sequence length, $d$ is the representation dimension, $T$ is the number of function evaluations (NFE) for the ODE solver in a single forward pass from $t_1$to $t_N$, and $S$ represents the NFE from $-1$ to $1$.}\n    \\resizebox{0.6\\textwidth}{!}{%\n    \\begin{tabular}{cccc}\n    \\toprule\n        Model & Comp. per Layer & Seq. Op. & Max. Path Len. \\\\\n    \\midrule\n        Transformer~\\cite{vaswani2017attention} & $O(N^2 \\cdot d)$ & $O(1)$ & $O(1)$ \\\\\n        RNN~\\cite{hopfield1982neural} & $O(N \\cdot d^2)$ & $O(N)$ & $O(N)$ \\\\\n        Neural ODE~\\cite{chen2018neural} & $O(T \\cdot d^2)$ & $O(T)$ & $O(T)$ \\\\\n    \\midrule\n        ContiFormer & $O(N^2 \\cdot S \\cdot d^2)$ & $O(S)$ & $O(1)$ \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\label{tab:complex}\n    \\vspace{-10pt}\n\\end{wraptable}",
            "tab:interp": "\\begin{wraptable}{r}{0.52\\textwidth}\n\\centering\n\\vspace{-10pt}\n\\caption{Interpolation and extrapolation results of different models on 2-dimensional spirals. $\\downarrow$ indicates the lower the better.}\n\\resizebox{0.52\\textwidth}{!}{%\n\\begin{tabular}{c|ccc}\n\\toprule\n     Metric & Transformer~\\cite{vaswani2017attention} & Latent ODE~\\cite{chen2018neural} & \\textbf{ContiFormer} \\\\\n\\midrule\n\\midrule\n\\multicolumn{4}{c}{Task = \\textit{Interpolation} ($\\times 10^{-2}$)} \\\\\n\\midrule\nRMSE ($\\downarrow$) & 1.37 \\small{$\\pm$ 0.14} & 2.09 \\small{$\\pm$ 0.22} & \\textbf{0.49 \\small{$\\pm$ 0.06}} \\\\\nMAE ($\\downarrow$) & 1.42 \\small{$\\pm$ 0.13} & 1.95 \\small{$\\pm$ 0.25} & \\textbf{0.52 \\small{$\\pm$ 0.06}} \\\\\n\\midrule\n\\midrule\n\\multicolumn{4}{c}{Task = \\textit{Extrapolation} ($\\times 10^{-2}$)} \\\\\n\\midrule\nRMSE ($\\downarrow$) & 1.36 \\small{$\\pm$ 0.10} & 1.59 \\small{$\\pm$ 0.05} & \\textbf{0.64 \\small{$\\pm$ 0.09}} \\\\\nMAE ($\\downarrow$) & 1.49 \\small{$\\pm$ 0.12} & 1.52 \\small{$\\pm$ 0.05} & \\textbf{0.65 \\small{$\\pm$ 0.08}} \\\\\n\\bottomrule\n\\end{tabular}}\n\\label{tab:interp}\n\\vspace{-10pt}\n\\end{wraptable}",
            "tab:classify": "\\begin{table*}[t]\n\\centering\n\\caption{Experimental results on irregular time series classification. Avg. ACC. stands for average accuracy over 20 datasets and Avg. Rank stands for average ranking over 20 datasets. $\\uparrow$ ($\\downarrow$) indicates the higher (lower) the better.}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{c|cc|cc|cc}\n\\toprule\n     Mask Ratio & \\multicolumn{2}{c}{30\\% Dropped} & \\multicolumn{2}{c}{50\\% Dropped} & \\multicolumn{2}{c}{70\\% Dropped} \\\\\n    \\midrule\n     Metric & Avg. ACC. ($\\uparrow$) & Avg. Rank ($\\downarrow$) & Avg. ACC. ($\\uparrow$) & Avg. Rank ($\\downarrow$) &Avg. ACC. ($\\uparrow$) & Avg. Rank ($\\downarrow$)  \\\\\n     \\cmidrule(l){1-1}\n    \\cmidrule(l){2-3}\n    \\cmidrule(l){4-5}\n    \\cmidrule(l){6-7}\n    GRU-D~\\cite{kidger2020neural} & 0.7284 & 6 & 0.7117 & 5.8 & 0.6725 & 6.15 \\\\\n    GRU-$\\Delta$t~\\cite{che2018recurrent} & 0.7298 & 5.75 & 0.7157 & 5.65 & 0.6795 & 5.7 \\\\\n    ODE-RNN~\\cite{rubanova2019latent} & 0.7304 & 5.45 & 0.7000 & 5.55 & 0.6594 & 6.4 \\\\\n    Neural CDE~\\cite{kidger2020neural} & 0.7142 & 6.85 & 0.6929 & 6.5 & 0.6753 & 6.3 \\\\\n    mTAN~\\cite{shukla2021multi} & 0.7381 & 5.7 & 0.7118 & 5.85 & 0.6955 & 5.4 \\\\\n    CADN~\\cite{chien2021continuous} & 0.7402 & 5.4 & 0.7211 & 5.55  & 0.7183 & 3.9\\\\\n    S5~\\cite{smith2022simplified} & 0.7854 & 4.4 & 0.7638 & 4.25 & 0.7401 & 4.45 \\\\\n    TST~\\cite{zerveas2021transformer} & 0.8089 & 2.75 & 0.7793 & 3.15 & 0.7297 & 4.2 \\\\\n    \\midrule\n    \\textbf{ContiFormer} & \\textbf{0.8126} & \\textbf{2.4} & \\textbf{0.7997} & \\textbf{1.9} & \\textbf{0.7749} & \\textbf{2.1} \\\\\n\\bottomrule\n\\end{tabular}}\n\\label{tab:classify}\n\\vspace{-10pt}\n\\end{table*}",
            "tab:exp": "\\begin{table}[t]\n    \\centering\n    \\caption{Prediction result of compared models for event prediction. LL for log-likelihood and ACC for accuracy. $\\uparrow$ ($\\downarrow$) indicates the higher (lower) the better. (bold values indicate the best performance and $^+$ indicates outperforming the best baseline by at least 3 standard deviations.)}\n    \\label{tab:exp}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cccccccc}\n    \\toprule\n        Model & Metric & Synthetic & Neonate & Traffic & MIMIC & StackOverflow & BookOrder \\\\ \n    \\midrule\n        \\multirow{3}{*}{HP~\\cite{hawkes1971spectra}} & LL ($\\uparrow$) & -3.084 \u00b1 .005 & -4.618 \u00b1 .005 & -1.482 \u00b1 .005 & -4.618 \u00b1 .005 & -5.794 \u00b1 .005 & -1.036 \u00b1 .000 \\\\ \n        ~ & Accuracy ($\\uparrow$) & 0.756 \u00b1 .000 & -- & 0.570 \u00b1 .000 & 0.795 \u00b1 .000 & 0.441 \u00b1 .000 & 0.604 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.953 \u00b1 .000 & 10.957 \u00b1 .012 & 0.407 \u00b1 .000 & 1.021 \u00b1 .000 & 1.341 \u00b1 .000 & 3.781 \u00b1 .000 \\\\ \n    \\midrule\n        \\multirow{3}{*}{RMTPP~\\cite{du2016recurrent}} & LL ($\\uparrow$) & -1.025 \u00b1 .030 & -2.817 \u00b1 .023 & -0.546 \u00b1 .012 & -1.184 \u00b1 .023 & -2.374 \u00b1 .001 & -0.952 \u00b1 .007 \\\\ \n        ~ & Accuracy ($\\uparrow$) & 0.841 \u00b1 .000 & -- & 0.805 \u00b1 .002 & 0.823 \u00b1 .004 & 0.461 \u00b1 .000 & 0.624 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.369 \u00b1 .014 & 9.517 \u00b1 .023 & 0.337 \u00b1 .001 & 0.864 \u00b1 .017 & 0.955 \u00b1 .000 & 3.647 \u00b1 .003 \\\\ \n    \\midrule\n        \\multirow{3}{*}{NeuralHP~\\cite{mei2017neural}} & LL ($\\uparrow$) & -1.371 \u00b1 .004 & -2.795 \u00b1 .012 & -0.643 \u00b1 .004 & -1.239 \u00b1 .027 & -2.608 \u00b1 .000 & -1.104 \u00b1 .005 \\\\\n        ~ & Accuracy ($\\uparrow$) & 0.841 \u00b1 .000 & -- & 0.759 \u00b1 .001 & 0.814 \u00b1 .001 & 0.450 \u00b1 .000 & 0.621 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.631 \u00b1 .002 & 9.614 \u00b1 .013 & 0.358 \u00b1 .001 & 0.846 \u00b1 .007 & 1.022 \u00b1 .000 & 3.734 \u00b1 .003 \\\\ \n    \\midrule\n        \\multirow{3}{*}{SAHP~\\cite{xu2019self}} & LL ($\\uparrow$) & -0.619 \u00b1 .063 & -2.646 \u00b1 .057 & 0.372 \u00b1 .022 & \\textbf{-1.110 \u00b1 .030} & -2.404 \u00b1 .002 & -0.304 \u00b1 .002 \\\\ \n        ~ & Accuracy ($\\uparrow$) & 0.841 \u00b1 .000 & -- & 0.780 \u00b1 .001 & 0.830 \u00b1 .004 & 0.455 \u00b1 .000 & 0.622 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.521 \u00b1 .055 & 9.403 \u00b1 .060 & 0.337 \u00b1 .000 & 0.851 \u00b1 .006 & 0.963 \u00b1 .000 & 3.676 \u00b1 .011 \\\\ \n    \\midrule\n        \\multirow{3}{*}{THP~\\cite{zuo2020transformer}} & LL ($\\uparrow$) & -0.589 \u00b1 .017 & -2.702 \u00b1 .045 & -0.569 \u00b1 .015 & -1.137 \u00b1 .038 & -2.354 \u00b1 .001 & -1.102 \u00b1 .052 \\\\ \n        ~ & Accuracy ($\\uparrow$) & 0.841 \u00b1 .000 & -- & 0.818 \u00b1 .001 & 0.834 \u00b1 .005 & -0.468 \u00b1 .000 & 0.622 \u00b1 .004 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.205 \u00b1 .006 & 9.471 \u00b1 .045 & 0.332 \u00b1 .000 & 0.843 \u00b1 .017 & 0.951 \u00b1 .000 & 3.688 \u00b1 .004 \\\\ \n    \\midrule\n        \\multirow{3}{*}{NSTKA~\\cite{ma2022non}} & LL ($\\uparrow$) & -1.001 \u00b1 .025 & -2.747 \u00b1 .061 & -0.667 \u00b1 .027 & -1.188 \u00b1 .012 & -2.406 \u00b1 .003 & -1.098 \u00b1 .019 \\\\ \n        ~ & Accuracy ($\\uparrow$) & 0.842 \u00b1 .000 & -- & 0.767 \u00b1 .001 & 0.833 \u00b1 .003 & 0.465 \u00b1 .000 & 0.621 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.379 \u00b1 .015 & 9.502 \u00b1 .049 & 0.346 \u00b1 .001 & 0.842 \u00b1 .008 & 0.956 \u00b1 .000 & 3.731 \u00b1 .007 \\\\ \n    \\midrule\n        \\multirow{3}{*}{GRU-$\\Delta$t~\\cite{chen2018neural}} & LL ($\\uparrow$) & -0.871 \u00b1 .050 & -2.736 \u00b1 .031 & -0.613 \u00b1 .062 & -1.164 \u00b1 .026 & -2.389 \u00b1 .002 & -0.915 \u00b1 .006 \\\\\n        ~ & Accuracy ($\\uparrow$) & 0.841 \u00b1 .000 & -- & 0.800 \u00b1 .004 & 0.832 \u00b1 .007 & 0.466 \u00b1 .000 & 0.627 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.249 \u00b1 .013 & 9.421 \u00b1 .050 & 0.335 \u00b1 .001 & 0.850 \u00b1 .010 & 0.950 \u00b1 .000 & 3.666 \u00b1 .016 \\\\\n    \\midrule\n        \\multirow{3}{*}{ODE-RNN~\\cite{rubanova2019latent}} & LL ($\\uparrow$) & -1.032 \u00b1 .102 & -2.732 \u00b1 .080 & -0.491 \u00b1 .011 & -1.183 \u00b1 .028 & -2.395 \u00b1 .001 & -0.988 \u00b1 .006 \\\\ \n        ~ & Accuracy ($\\uparrow$) & 0.841 \u00b1 .000 & -- & 0.812 \u00b1 .000 & 0.827 \u00b1 .006 & 0.467 \u00b1 .000 & 0.624 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.342 \u00b1 .030 & 9.289 \u00b1 .048 & 0.334 \u00b1 .000 & 0.865 \u00b1 .021 & 0.952 \u00b1 .000 & \\textbf{3.605 \u00b1 .004} \\\\ \n    \\midrule\n        \\multirow{3}{*}{mTAN~\\cite{shukla2021multi}} & LL ($\\uparrow$) & -0.920 \u00b1 .036 & -2.722 \u00b1 .026 & -0.548 \u00b1 .023 & -1.149 \u00b1 .029 & -2.391 \u00b1 .002 & -0.980 \u00b1 .004 \\\\\n        ~ & Accuracy ($\\uparrow$) & 0.842 \u00b1 .000 & -- & 0.811 \u00b1 .002 & 0.832 \u00b1 .009 & 0.466 \u00b1 .000 & 0.620 \u00b1 .000 \\\\ \n        ~ & RMSE ($\\downarrow$) & 0.286 \u00b1 .008 & 9.363 \u00b1 .042 & 0.334 \u00b1 .001 & 0.848 \u00b1 .006 & 0.950 \u00b1 .000 & 3.680 \u00b1 .015 \\\\\n    \\midrule\n        \\multirow{3}{*}{ContiFormer} & LL ($\\uparrow$) & \\textbf{-0.535 \u00b1 .028$^+$} & \\textbf{-2.550 \u00b1 .026} & \\textbf{0.635 \u00b1 .019$^+$} & -1.135 \u00b1 .023 & \\textbf{-2.332 \u00b1 .001$^+$} & \\textbf{-0.270 \u00b1 .010$^+$} \\\\ \n        ~ & Accuracy ($\\uparrow$) & \\textbf{0.842 \u00b1 .000} & -- & \\textbf{0.822 \u00b1 .001$^+$} & \\textbf{0.836 \u00b1 .006} & \\textbf{0.473 \u00b1 .000$^+$} & \\textbf{0.628 \u00b1 .001$^+$} \\\\ \n        ~ & RMSE ($\\downarrow$) & \\textbf{0.192 \u00b1 .005} & \\textbf{9.233 \u00b1 .033} & \\textbf{0.328 \u00b1 .001$^+$} & \\textbf{0.837 \u00b1 .007} & \\textbf{0.948 \u00b1 .000$^+$} & 3.614 \u00b1 .020 \\\\ \n    \\bottomrule\n    \\end{tabular}}\n\\end{table}",
            "tab:regular2": "\\begin{table}[t]\n    \\centering\n    \\caption{Results of regular time series forecasting. Input length for ILI is 36 and 96 for the others. Prediction lengths for ETTm2 and ETTh2 are $\\{24, 48, 168, 336\\}$, $\\{96, 192, 336, 540\\}$ for Exchange and Weather, and $\\{24, 36, 48, 60\\}$ for ILI. Results are averaged over the four prediction lengths. Bold/underlined values indicate the best/second-best. MSE stands for mean squared error.} \n \\resizebox{\\columnwidth}{!}{%\n    \\begin{tabular}{c|cccccccccccc}\n    \\toprule\n         Model & \\multicolumn{2}{c}{ContiFormer} & \\multicolumn{2}{c}{TimesNet\\cite{wu2022timesnet}} & \\multicolumn{2}{c}{DLinear\\cite{zeng2023transformers}} & \\multicolumn{2}{c}{FEDformer\\cite{zhou2022fedformer}} & \\multicolumn{2}{c}{Autoformer\\cite{wu2021autoformer}} & \\multicolumn{2}{c}{Transformer\\cite{vaswani2017attention}} \\\\ \n    \\cmidrule(lr){2-3}\n    \\cmidrule(lr){4-5}\n    \\cmidrule(lr){6-7}\n    \\cmidrule(lr){8-9}\n    \\cmidrule(lr){10-11}\n    \\cmidrule(lr){12-13}\n         Metric & MSE  & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\\\\n    \\midrule\n        Exchange & 0.339  & 0.399  & \\underline{0.320}  & \\underline{0.396}  & \\textbf{0.277}  & \\textbf{0.377}  & 0.392  & 0.446  & 0.440  & 0.477  & 1.325  & 0.949  \\\\ \n    \\midrule\n        ETTm2 &  0.226  & \\underline{0.300}  & \\textbf{0.214}  & \\textbf{0.276}  & \\underline{0.220}  & 0.305  & 0.226  & \\underline{0.300}  & 0.231  & 0.307  & 0.689  & 0.633 \\\\ \n    \\midrule    \n        ETTh2 & 0.356  & 0.395  & \\textbf{0.321}  & \\textbf{0.364}  & 0.358  & 0.396  & \\underline{0.351}  & \\underline{0.393}  & 0.352  & 0.394  & 2.081  & 1.179  \\\\ \n    \\midrule\n        ILI & \\textbf{2.632}  & \\textbf{1.042}  & \\underline{2.874}  & \\underline{1.066}  & 4.188  & 1.493  & 4.467  & 1.547  & 4.046  & 1.446  & 5.130  & 1.507 \\\\ \n    \\midrule\n        Weather & \\textbf{0.257}  & \\underline{0.290}  & \\underline{0.259}  & \\textbf{0.285}  & 0.260  & 0.311  & 0.315  & 0.360  & 0.318  & 0.359  & 0.672  & 0.579 \\\\ \n    \\bottomrule\n    \\end{tabular}}\n    \\vspace{-10pt}\n    \\label{tab:regular2}\n\\end{table}",
            "tab:timecost": "\\begin{wraptable}{r}{0.5\\textwidth}\n\\centering\n\\vspace{-10pt}\n\\caption{Actual time cost during training for different models on RacketSports dataset (0\\% dropped) from UEA dataset. We adopt RK4 solver for ODE-RNN, Neural CDE, and ContiFormer. Step size refers to the amount of time increment for the RK4 solver. $\\uparrow$ ($\\downarrow$) indicates the higher (lower) the better.}\n\\resizebox{0.52\\textwidth}{!}{%\n\\begin{tabular}{cccc}\n\\toprule\n     Model & Step Size & Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$) \\\\\n\\midrule\nTST~\\cite{zerveas2021transformer} & -- & $0.14 \\times$ & 0.826 \\\\\nS5~\\cite{smith2022simplified} & -- & $1 \\times$ & 0.772 \\\\\n\\midrule\n\\multirow{2}{*}{ODE-RNN~\\cite{rubanova2019latent}} & 0.1 & $2.47 \\times$ &  0.827 \\\\\n~ & 0.01 & $14.64 \\times$ & 0.796 \\\\\n\\midrule\n\\multirow{2}{*}{Neural CDE~\\cite{park2022neural}} & 0.1 & $3.48 \\times$ & 0.743 \\\\\n~ & 0.01 & $25.03 \\times$ & 0.748 \\\\\n\\midrule\n\\multirow{2}{*}{ContiFormer} & 0.1 & $1.88 \\times$ & \\textbf{0.836} \\\\\n& 0.01 & $5.87 \\times$ & \\textbf{0.836} \\\\\n\\bottomrule\n\\end{tabular}}\n\\label{tab:timecost}\n\\vspace{-5pt}\n\\end{wraptable}",
            "tab:int2": "\\begin{table*}[]\n    \\centering\n    \\caption{More experimental results on spiral 2d.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{c|c|cccccc}\n    \\toprule\n        \\multirow{2}{*}{$\\alpha$} & \\multirow{2}{*}{Metric} & \\multirow{2}{*}{Transformer\\cite{vaswani2017attention}} & \\multirow{2}{*}{Neural ODE\\cite{chen2018neural}} & Latent ODE\\cite{chen2018neural} & Latent ODE\\cite{rubanova2019latent} & \\multirow{2}{*}{Neural CDE\\cite{park2022neural}} & \\multirow{2}{*}{\\textbf{ContiFormer}} \\\\\n        & & & & (w/ RNN Enc) & (w/ ODE Enc) & & \\\\\n    \\midrule\n    \\midrule\n    \\multicolumn{8}{c}{Task = \\textit{Interpolation} ($\\times 10^{-2}$)} \\\\\n    \\midrule\n        \\multirow{2}{*}{0.02} & RMSE ($\\downarrow$) & 1.37 $\\pm$ 0.15 & 1.90 $\\pm$ 0.17 & 2.09 $\\pm$ 0.22 & 4.53 $\\pm$ 1.34 & 4.80 $\\pm$ 0.50 &  \\textbf{0.50 $\\pm$ 0.06} \\\\\n        & MAE ($\\downarrow$) & 1.42 $\\pm$ 0.14 & 1.90 $\\pm$ 0.17 & 1.95 $\\pm$ 0.26 & 4.25 $\\pm$ 1.48 & 5.12 $\\pm$ 0.47 & \\textbf{0.53 $\\pm$ 0.07} \\\\\n    \\midrule\n        \\multirow{2}{*}{0.2} & RMSE ($\\downarrow$) & 2.30 $\\pm$ 0.51 & 1.88 $\\pm$ 0.09 & 3.34 $\\pm$ 0.10 & 4.82 $\\pm$ 1.97 & 4.60 $\\pm$ 1.08 & \\textbf{0.42 $\\pm$ 0.04} \\\\\n        & MAE ($\\downarrow$) & 1.75 $\\pm$ 0.18 & 1.58 $\\pm$ 0.12 & 3.16 $\\pm$ 0.43 & 4.73 $\\pm$ 1.21 & 4.58 $\\pm$ 0.93 & \\textbf{0.42 $\\pm$ 0.01} \\\\\n    \\midrule\n        \\multirow{2}{*}{2} & RMSE ($\\downarrow$) & 3.75 $\\pm$ 0.50 & 2.1 $\\pm$ 0.21 & 3.04 $\\pm$ 0.48 & 4.49 $\\pm$ 1.26 & 2.92 $\\pm$ 0.32 & \\textbf{0.45 $\\pm$ 0.11} \\\\\n        & MAE ($\\downarrow$) & 2.33 $\\pm$ 0.60 & 2.49 $\\pm$ 0.30 & 2.88 $\\pm$ 0.54 & 4.12 $\\pm$ 1.00 & 2.96 $\\pm$ 0.23 & \\textbf{0.41 $\\pm$ 0.07} \\\\\n    \\midrule\n    \\midrule\n    \\multicolumn{8}{c}{Task = \\textit{Extrapolation} ($\\times 10^{-2}$)} \\\\\n    \\midrule\n    \\multirow{2}{*}{0.02} & RMSE ($\\downarrow$) & 1.37 $\\pm$ 0.11 & 2.07 $\\pm$ 0.02 & 1.59 $\\pm$ 0.05 & 2.69 $\\pm$ 0.82 & 2.24 $\\pm$ 0.03 & \\textbf{0.64 $\\pm$ 0.10} \\\\\n        & MAE ($\\downarrow$) & 1.49 $\\pm$ 0.12 & 2.02 $\\pm$ 0.07 & 1.52 $\\pm$ 0.09 & 2.83 $\\pm$ 0.88 & 2.38 $\\pm$ 0.04 & \\textbf{0.65 $\\pm$ 0.08} \\\\\n    \\midrule\n        \\multirow{2}{*}{0.2} & RMSE ($\\downarrow$) & 2.96 $\\pm$ 1.46 & 4.38 $\\pm$ 0.35 & 4.78 $\\pm$ 0.15 & 5.03 $\\pm$ 1.96 & 3.81 $\\pm$ 1.02 & \\textbf{0.74 $\\pm$ 0.10} \\\\\n        & MAE ($\\downarrow$) & 2.79 $\\pm$ 1.11 & 3.35 $\\pm$ 0.43 & 4.02 $\\pm$ 0.59 & 4.72 $\\pm$ 1.96 & 3.37 $\\pm$ 0.78 & \\textbf{0.69 $\\pm$ 0.09} \\\\\n    \\midrule\n        \\multirow{2}{*}{2} & RMSE ($\\downarrow$) & 5.36 $\\pm$ 0.97 & 6.02 $\\pm$ 0.58 & 5.70 $\\pm$ 0.57 & 4.68 $\\pm$ 1.70 & 3.44 $\\pm$ 1.06 & \\textbf{0.75 $\\pm$ 0.07} \\\\\n        & MAE ($\\downarrow$) & 4.48 $\\pm$ 0.82 & 4.84 $\\pm$ 0.43 & 4.46 $\\pm$ 0.60 & 4.29 $\\pm$ 1.34 & 3.32 $\\pm$ 0.85 & \\textbf{0.69 $\\pm$ 0.07} \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\label{tab:int2}\n\\end{table*}",
            "tab:ueadata": "\\begin{table*}[htbp]\n    \\centering\n    \\caption{Dataset Information of UEA.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{ccccccc}\n    \\toprule\n         Dataset & Ablation & \\# Train & \\# Test & Length & \\# Classes & \\# Variates    \\\\\n    \\midrule\nArticularyWordRecognition & AWR &  275 &\t300 &\t144 &\t25 &\t9  \\\\\nBasicMotions & BM & 40 & \t40 &\t100 &\t4 &\t6 \\\\\nCharacterTrajectories & CT & 1422 &\t1436 &\t182 &\t20 &\t3 \\\\\nDuckDuckGeese & DDG & 60 &\t40 &\t270 &\t5 &\t1345 \\\\\nEpilepsy & EP & 137 &\t138 &\t207 &\t4 &\t3 \\\\\nERing & ER & 30 &\t270 &\t65 &\t6 &\t4 \\\\\nFingerMovements & FM & 316 &\t100 &\t50 &\t2 &\t28\\\\\nHandMovementDirection & HMD & 160 &\t74 &\t400 &\t4 &\t10 \\\\\nHandwriting & HW & 150 & \t850 & \t152 & \t26 &\t3 \\\\\nHeartbeat & HB & 204 &\t205 &\t405 &\t2 &\t61 \\\\\nJapaneseVowels & JV & 270 &\t370 &\t29 &\t9 &\t12 \\\\\nLibras & LB & 180 &\t180 &\t45 &\t15 &\t2 \\\\\nLSST & LSST & 2459 &\t2466 &\t36 &\t14 &\t6\\\\\nNATOPS & NATOPS & 180 &\t180 &\t51 &\t6 & \t24 \\\\\nPEMS-SF & PEMS & 267 &\t173 &\t144 &\t7 &\t963 \\\\\nPenDigits & PD & 7494 &\t3498 &\t8 &\t10 &\t2 \\\\\nRacketSports & RS & 151 &\t152 &\t30 &\t4 &\t6 \\\\\nSelfRegulationSCP1 & SR & 268 &\t293 &\t896 &\t2 &\t6 \\\\\nSpokenArabicDigits & SA & 6599 &\t2199 &\t93 &\t10 &\t13 \\\\\nUWaveGestureLibrary & UGL & 2238 &\t2241 &\t315 &\t8 &\t3 \\\\\n\\bottomrule\n    \\end{tabular}}\n    \\label{tab:ueadata}\n\\end{table*}",
            "tab:uea30": "\\begin{table*}[htbp]\n\\centering\n\\caption{Experimental result on UEA when 30\\% observations are dropped. (ODE. stands for ODE-RNN, N. CDE stands for Neural CDE, ACC. stands for Accuracy.)}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{cccccccccc}\n\\toprule\nDataset & GRU-D & GRU-$\\Delta$t      & ODE. & CADN    & N. CDE &  S5  & mTAN & TST        & \\textbf{ContiFormer} \\\\\n\\midrule\n        AWR & 0.8533  & 0.8956  & 0.8789  & 0.8056  & 0.9089  & 0.9011  & 0.8067  & \\textbf{0.9722}  & 0.9633  \\\\ \n        BM & 0.9333  & 0.9417  & 0.8250  & 0.9583  & \\textbf{0.9917}  & 0.9833  & \\textbf{0.9917}  & 0.9667  & 0.9750  \\\\ \n        CT & 0.9325  & 0.8558  & 0.9415  & 0.9575  & 0.9276  & 0.9610  & 0.9529  & 0.9742  & \\textbf{0.9833}  \\\\ \n        DDG & 0.5867  & 0.5933  & 0.5067  & 0.4600  & 0.3400  & 0.5667  & 0.6000  & 0.6533  & \\textbf{0.7267}  \\\\ \n        EP & 0.7995  & 0.8043  & 0.8454  & 0.8123  & 0.7657  & 0.9074  & 0.9203  & \\textbf{0.9589}  & 0.9324  \\\\ \n        ER & 0.7062  & 0.7148  & 0.6296  & 0.8623  & 0.7543  & 0.8346  & 0.7123  & 0.9160  & \\textbf{0.9210}  \\\\\n        FM & 0.6167  & 0.6167  & 0.5667  & \\textbf{0.6233}  & 0.5867  & 0.6033  & 0.5433  & 0.6133  & 0.6000  \\\\ \n        HMD & 0.3559  & 0.4099  & 0.5450  & 0.4505  & 0.3333  & \\textbf{0.6171}  & 0.4414  & 0.5946  & 0.4550  \\\\ \n        HW & 0.0722  & 0.1251  & 0.0675  & 0.0925  & 0.1831  & 0.1302  & 0.0937  & \\textbf{0.2196}  & 0.2000  \\\\ \n        HB & 0.7577  & 0.7593  & 0.7805  & 0.7431  & 0.7333  & 0.7431  & \\textbf{0.7789}  & 0.7398  & 0.7561  \\\\ \n        JV & 0.9703  & 0.9550  & 0.9432  & 0.9703  & 0.9225  & 0.9676  & 0.9595  & 0.9865  & \\textbf{0.9919}  \\\\ \n        LB & 0.5037  & 0.4741  & 0.5222  & 0.5938  & 0.7481  & \\textbf{0.9216}  & 0.5426  & 0.7519  & 0.8778  \\\\\n        LSST & 0.5589  & 0.5615  & 0.5715  & 0.5574  & 0.3940  & \\textbf{0.6389}  & 0.5307  & 0.5520  & 0.6004  \\\\ \n        NATOPS & 0.9204  & 0.9315  & 0.8704  & 0.8944  & 0.8148  & 0.8500  & 0.9019  & \\textbf{0.9352}  & 0.9222  \\\\ \n        PEMS & 0.7938  & 0.7669  & 0.8208  & 0.7958  & 0.7572  & 0.9133  & 0.8189  & \\textbf{0.8420}  & 0.8401  \\\\ \n        PD & 0.9309  & 0.9222  & 0.9433  & 0.9402  & 0.8422  & 0.8110  & 0.9029  & \\textbf{0.9520}  & 0.9517  \\\\ \n        RS & 0.7434  & 0.7281  & 0.7654  & 0.7325  & 0.6798  & 0.7500  & 0.7325  & 0.8158  & \\textbf{0.8487}  \\\\ \n        SR & 0.8942  & \\textbf{0.9124}  & 0.9101  & 0.8259  & 0.8783  & 0.8737  & 0.8646  & 0.9044  & 0.9101  \\\\ \n        SA & 0.9663  & 0.9660  & 0.9830  & 0.9809  & 0.8998  & 0.9291  & 0.9321  & 0.9744  & \\textbf{0.9836}  \\\\ \n        UGL & 0.6729  & 0.6625  & 0.6917  & 0.7479  & 0.8219  & 0.8042  & 0.7344  & \\textbf{0.8552}  & 0.8135  \\\\ \n    \\midrule\n        \\textbf{Acc.} ($\\uparrow$) & 0.7284  & 0.7298  & 0.7304  & 0.7402  & 0.7142  & 0.7854  & 0.7381  & 0.8089  & \\textbf{0.8126}  \\\\\n        \\textbf{Rank}  ($\\downarrow$) & 6 & 5.75 & 5.45 & 5.4 & 6.85 & 4.4 & 5.7 & 2.75 & \\textbf{2.4} \\\\ \n\\midrule\n\\bottomrule\n\\end{tabular}}\n\\label{tab:uea30}\n\\end{table*}",
            "tab:uea50": "\\begin{table*}[htbp]\n\\centering\n\\caption{Experimental result on UEA when 50\\% observations are dropped. (ODE. stands for ODE-RNN, N. CDE stands for Neural CDE, ACC. stands for Accuracy.)}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{cccccccccc}\n\\toprule\nDataset & GRU-D & GRU-$\\Delta$t      & ODE.  & CADN   & N. CDE & S5  & mTAN & TST        & \\textbf{ContiFormer} \\\\\n\\midrule\nAWR & 0.8811  & 0.8978  & 0.8989  & 0.8067  & 0.9133  & 0.9078  & 0.8067  & \\textbf{0.9633}  & \\textbf{0.9633}  \\\\ \nBM & 0.9417  & 0.8500  & 0.7667  & 0.9417  & 0.9583  & \\textbf{0.9917}  & 0.9583  & 0.9500  & 0.9583  \\\\ \nCT & 0.8872  & 0.9088  & 0.9506  & \\textbf{0.9798}  & 0.9285  & 0.9638  & 0.9431  & 0.9601  & \\textbf{0.9798}  \\\\ \nDDG & 0.5467  & 0.6000  & 0.4933  & 0.5467  & 0.3467  & 0.5667  & 0.6067  & 0.6533  & \\textbf{0.6867}  \\\\\nEP & 0.7295  & 0.7899  & 0.7512  & 0.6728  & 0.7101  & 0.9012  & 0.9106  & \\textbf{0.9589}  & 0.9469  \\\\ \nER & 0.7358  & 0.7432  & 0.5704  & 0.8527  & 0.7691  & 0.8284  & 0.6432  & \\textbf{0.9012}  & 0.8901  \\\\ \nFM & 0.5800  & 0.5967  & 0.5933  & 0.5933  & 0.6000  & 0.6033  & 0.5700  & \\textbf{0.6067}  & \\textbf{0.6067}  \\\\\nHMD & 0.4099  & 0.4054  & 0.4685  & 0.4550  & 0.3243  & \\textbf{0.5811}  & 0.4099  & 0.5270  & 0.4595  \\\\ \nHW & 0.0769  & 0.0945  & 0.0588  & 0.1004  & 0.1855  & 0.1357  & 0.0737  & 0.1600  & \\textbf{0.1757}  \\\\ \nHB & 0.7642  & 0.7593  & \\textbf{0.7707}  & 0.7512  & 0.7333  & 0.7463  & 0.7789  & 0.7350  & 0.7610  \\\\ \nJV & 0.9604  & 0.9532  & 0.9126  & 0.9649  & 0.9180  & 0.9631  & 0.9595  & 0.9775  & \\textbf{0.9856}  \\\\ \nLB & 0.3796  & 0.4352  & 0.4537  & 0.5680  & 0.7315  & 0.6352  & 0.5259  & 0.6556  & \\textbf{0.8537}  \\\\ \nLSST & 0.5203  & 0.5292  & 0.5223  & 0.5222  & 0.3969  & \\textbf{0.7204}  & 0.5126  & 0.5151  & 0.5661  \\\\ \nNATOPS & 0.9185  & 0.9167  & 0.8444  & 0.8741  & 0.8019  & 0.8315  & 0.8778  & 0.8778  & \\textbf{0.9222}  \\\\ \nPEMS & 0.7823  & 0.7861  & 0.8170  & 0.7611  & 0.7534  & \\textbf{0.9114 } & 0.8015  & 0.7669  & 0.8401  \\\\\nPD & 0.8245  & 0.7804  & 0.8563  & 0.8369  & 0.6389  & 0.6544  & 0.6898  & \\textbf{0.8827}  & 0.8799  \\\\ \nRS & 0.7412  & 0.7368  & 0.7127  & 0.7632  & 0.5526  & 0.7193  & 0.6601  & 0.7807  & \\textbf{0.8355}  \\\\ \nSR & 0.9135  & \\textbf{0.9170}  & 0.9044  & 0.8123  & 0.8760  & 0.8885  & 0.8658  & 0.8987  & 0.9067  \\\\ \nSA & 0.9583  & 0.9556  & 0.9759  & 0.9663  & 0.8907  & 0.9218  & 0.9254  & 0.9667  & \\textbf{0.9767}  \\\\ \nUGL & 0.6833  & 0.6583  & 0.6781  & 0.6531  & 0.8281  & 0.8042  & 0.7167  & \\textbf{0.8490}  & 0.8000  \\\\ \n\\midrule\n\\textbf{ACC.} ($\\uparrow$) & 0.7117  & 0.7157  & 0.7000  & 0.7211  & 0.6929  & 0.7638  & 0.7118  & 0.7793  & \\textbf{0.7997} \\\\ \n\\textbf{Rank}  ($\\downarrow$) & 5.8 & 5.65 & 5.75 & 5.55 & 6.5 & 4.25 & 5.85 & 3.15 & \\textbf{1.9} \\\\ \n\\bottomrule\n\\end{tabular}}\n\\label{tab:uea50}\n\\end{table*}",
            "tab:uea70": "\\begin{table*}[t]\n\\centering\n\\caption{Experimental result on UEA when 70\\% observations are dropped. (ODE. stands for ODE-RNN, N. CDE stands for Neural CDE, ACC. stands for Accuracy)}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{cccccccccc}\n\\toprule\nDataset & GRU-D & GRU-$\\Delta$t      & ODE.  & CADN   & N. CDE  & S5 & mTAN & TST  & \\textbf{ContiFormer} \\\\\n\\midrule\nAWR & 0.8689  & 0.8822  & 0.8433  & 0.7911  & 0.9178  & 0.8956  & 0.7956  & 0.9089  & \\textbf{0.9511}  \\\\ \n        BM & 0.7583  & 0.7917  & 0.7167  & \\textbf{0.9750}  & 0.9000  & 0.9167  & 0.9417  & 0.9167  & 0.9167  \\\\ \n        CT & 0.8433  & 0.8496  & 0.9471  & 0.9582  & 0.9241  & 0.9547  & 0.9009  & 0.9313  & \\textbf{0.9675}  \\\\ \n        DDG & 0.5200  & 0.6000  & 0.4800  & 0.5667  & 0.3533  & 0.5667  & 0.6067  & 0.5867  & \\textbf{0.7267}  \\\\ \n        EP & 0.7464  & 0.7222  & 0.6643  & 0.6667  & 0.6667  & 0.8802  & 0.9058  & \\textbf{0.9420}  & 0.9275  \\\\ \n        ER & 0.6741  & 0.6617  & 0.5383  & \\textbf{0.8647}  & 0.7630  & 0.8025  & 0.7407  & 0.8198  & 0.8580  \\\\ \n        FM & 0.5467  & 0.5933  & 0.5733  & 0.6000  & 0.5767  & 0.6133  & 0.5700  & 0.5300  & \\textbf{0.6233}  \\\\\n        HMD & 0.3829  & 0.3784  & 0.4459  & 0.4324  & 0.3378  & \\textbf{0.5991}  & 0.3694  & 0.4324  & 0.4144  \\\\\n        HW & 0.0784  & 0.0831  & 0.0541  & 0.0957  & \\textbf{0.1533}  & 0.1078  & 0.0604  & 0.1255  & 0.1247  \\\\\n        HB & 0.7610  & 0.7528  & 0.7593  & 0.7610  & 0.7561  & 0.7512  & \\textbf{0.7691}  & 0.7593  & 0.7593  \\\\ \n        JV & 0.9514  & 0.9414  & 0.9261  & 0.9685  & 0.8964  & 0.9559  & 0.9613  & 0.9595  & \\textbf{0.9766}  \\\\ \n        LB & 0.3648  & 0.3722  & 0.3463  & 0.5412  & 0.6796  & 0.6278  & 0.4926  & 0.5500  & \\textbf{0.7648}  \\\\ \n        LSST & 0.4803  & 0.4895  & 0.4505  & \\textbf{0.5278}  & 0.4093  & 0.6778  & 0.4992  & 0.4874  & 0.5149  \\\\ \n        NATOPS & 0.8722  & 0.8981  & 0.7889  & 0.9074  & 0.7944  & 0.8259  & 0.8463  & 0.8315  & \\textbf{0.9204}  \\\\ \n        PEMS & 0.7418  & 0.7611  & 0.7842  & 0.7726  & 0.7303  & \\textbf{0.8902}  & 0.7861  & 0.6840  & 0.8227  \\\\ \n        PD & 0.7451  & 0.6670  & 0.7802  & 0.7596  & 0.5201  & 0.5197  & 0.6063  & \\textbf{0.8078}  & 0.8069  \\\\ \n        RS & 0.6711  & 0.6776  & 0.6316  & 0.7018  & 0.5570  & 0.6382  & 0.6228  & 0.6930  & \\textbf{0.7697}  \\\\ \n        SR & 0.9090  & \\textbf{0.9170}  & 0.9158  & 0.8515  & 0.8726  & 0.8874  & 0.8635  & 0.8612  & 0.8953  \\\\ \n        SA & 0.9304  & 0.9295  & 0.9523  & 0.9513  & 0.8622  & 0.9024  & 0.8925  & 0.9447  & \\textbf{0.9600}  \\\\\n        UGL & 0.6031  & 0.6219  & 0.5896  & 0.6729  & \\textbf{0.8344}  & 0.7896  & 0.6792  & 0.8219  & 0.7969  \\\\\n    \\midrule\n        \\textbf{ACC.} ($\\uparrow$) & 0.6725  & 0.6795  & 0.6594  & 0.7183  & 0.6753  & 0.7401  & 0.6955  & 0.7297  & \\textbf{0.7749}  \\\\ \n        \\textbf{Rank}  ($\\downarrow$) & 6.15 & 5.7 & 6.4 & 3.9 & 6.3 & 4.45 & 5.4 & 4.2 & \\textbf{2.1} \\\\\n\\bottomrule\n\\end{tabular}}\n\\label{tab:uea70}\n\\end{table*}",
            "tab:data": "\\begin{table*}[htbp]\n    \\centering\n    \\caption{Dataset statistics: name of the dataset, number of event types, number of events in the dataset, average length per sequence, and number of sequences in training/test sets.}\n    \\begin{tabular}{c|c|c|c|c|c}\n    \\toprule\n         Dataset & K & \\# Events & Avg. Length & \\# Train Seqs & \\# Test Seqs  \\\\\n    \\midrule\n         Synthetic & 10 & 8, 618 & 17 & 375 & 125\\\\\n         Neonate & 1 & 534 & 11 & 38 & 10 \\\\\n         Traffic & 2 & 154, 747 & 60 & 1, 938 & 647 \\\\\n         MIMIC & 75 & 2, 419 & 4 & 527 & 65 \\\\\n         BookOrder & 2 & 414, 800 & 2, 074 & 90 & 100 \\\\\n         StackOverflow & 22 & 480, 413 & 72 & 4, 777 & 1, 326 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:data}\n\\end{table*}",
            "tab:ttest": "\\begin{table}[htbp]\n    \\centering\n    \\caption{Significance test on Synthetic and BookOrder dataset with 10 repeats. The P-value represents the significant value between the ContiFormer and the baseline model. Bolder values represent P-value $< 10^{-6}$ in significance test.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{ccccccccccccc}\n    \\toprule\n        Dataset & \\multicolumn{6}{c}{Synthetic} & \\multicolumn{6}{c}{BookOrder} \\\\\n        \\cmidrule(lr){2-7}\n        \\cmidrule(lr){8-13}\n        Metric & \\multicolumn{2}{c}{LL ($\\uparrow$)} & \\multicolumn{2}{c}{Accuracy ($\\uparrow$)} & \\multicolumn{2}{c}{RMSE ($\\downarrow$)} & \\multicolumn{2}{c}{LL ($\\uparrow$)} & \\multicolumn{2}{c}{Accuracy ($\\uparrow$)} & \\multicolumn{2}{c}{RMSE ($\\downarrow$)} \\\\ \n        \\cmidrule(lr){2-3}\n        \\cmidrule(lr){4-5}\n        \\cmidrule(lr){6-7}\n        \\cmidrule(lr){8-9}\n        \\cmidrule(lr){10-11}\n        \\cmidrule(lr){12-13}\n        Statistic & Mean & P-value & Mean & P-value & Mean & P-value & Mean & P-value & Mean & P-value & Mean & P-value \\\\ \n    \\midrule\n        GRU-dt & -0.8611 & 5.38E-08 & 0.8420 & 0.003 & 0.2556 & 3.49E-09 & -1.0137 & 1.07E-31 & 0.6274 & 3.11E-08 & 3.6680 & 1.32E-05 \\\\ \n        NSTKA & -1.0088 & 1.27E-21 & 0.8419 & 0.0003 & 0.3863 & 1.74E-12 & -1.0916 & 2.95E-27 & 0.6223 & 5.48E-15 & 3.7175 & 1.88E-08 \\\\ \n        ODE-RNN & -0.8432 & 2.39E-11 & 0.8420 & 0.098 & 0.2401 & 3.77E-08 & -0.9458 & 1.44E-26 & 0.6278 & 1.17E-07 & 3.5898 & 1.94E-03 \\\\ \n        RMTPP & -1.0132 & 3.57E-19 & 0.8420 & 0.0143 & 0.3711 & 6.70E-18 & -0.9551 & 9.70E-26 & 0.6245 & 1.45E-13 & 3.6475 & 9.76E-04 \\\\ \n        SAHP & -0.6247 & 2.21E-05 & 0.8420 & 0.1778 & 0.5218 & 2.37E-10 & -0.3053 & 1.83E-07 & 0.6227 & 1.45E-13 & 3.6810 & 9.55E-07 \\\\ \n        THP & -0.6035 & 6.28E-06 & 0.8420 & 0.1354 & 0.2100 & 4.85E-06 & -1.0788 & 1.55E-17 & 0.6251 & 1.96E-06 & 3.6907 & 8.02E-07 \\\\\n    \\midrule\n        ContiFormer & \\textbf{-0.5445} & N/A & 0.8421 & N/A & \\textbf{0.1943} & N/A & \\textbf{-0.2745} & N/A & \\textbf{0.6295} & N/A & 3.6171 & N/A \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\label{tab:ttest}\n\\end{table}",
            "tab:regular": "\\begin{table}[t]\n    \\centering\n    \\caption{Results of regular time series forecasting. We set the\ninput length as 36 for ILI and 96 for the others. Avg. stands for Average. Bold values indicate the best performance and underlined values indicate the second-best performance (lower the better).}\n \\resizebox{\\columnwidth}{!}{%\n    \\begin{tabular}{c|c|cccccccccccccccc}\n    \\toprule\n         \\multicolumn{2}{c}{Models} & \\multicolumn{2}{c}{ContiFormer} & \\multicolumn{2}{c}{TimesNet\\cite{wu2022timesnet}} & \\multicolumn{2}{c}{DLinear\\cite{zeng2023transformers}} & \\multicolumn{2}{c}{FEDformer\\cite{zhou2022fedformer}} & \\multicolumn{2}{c}{Autoformer\\cite{wu2021autoformer}} & \\multicolumn{2}{c}{Transformer\\cite{vaswani2017attention}} & \\multicolumn{2}{c}{Informer\\cite{zhou2021informer}} &  \\multicolumn{2}{c}{Reformer\\cite{kitaev2020reformer}} \\\\ \n    \\cmidrule(lr){3-4}\n    \\cmidrule(lr){5-6}\n    \\cmidrule(lr){7-8}\n    \\cmidrule(lr){9-10}\n    \\cmidrule(lr){11-12}\n    \\cmidrule(lr){13-14}\n    \\cmidrule(lr){15-16}\n    \\cmidrule(lr){17-18}\n         \\multicolumn{2}{c}{Metrics} & MSE  & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\\\\n    \\midrule\n\n        \\multirow{5}{*}{\\rotatebox{90}{Exchange}} & 96 & \\underline{0.105} & \\underline{0.232} & 0.107 & 0.238 & \\textbf{0.088} & \\textbf{0.217} & 0.153 & 0.284 & 0.153 & 0.283 & 0.891 & 0.743 & 1.348 & 0.966 & 1.208 & 0.961 \\\\ \n        ~ & 192 & 0.215 & 0.339 & \\underline{0.201} & \\underline{0.327} & \\textbf{0.175} & \\textbf{0.313} & 0.264 & 0.375 & 0.327 & 0.413 & 1.338 & 0.955 & 1.296 & 0.941 & 1.329 & 1.001 \\\\ \n        ~ & 336 & \\underline{0.336} & \\underline{0.427} & 0.354 & 0.435 & \\textbf{0.310} & \\textbf{0.421} & 0.437 & 0.485 & 0.555 & 0.560 & 1.553 & 1.056 & 1.641 & 1.072 & 3.023 & 1.427 \\\\ \n        ~ & 540 & 0.698 & 0.598 & \\underline{0.614} & \\underline{0.582} & \\textbf{0.534} & \\textbf{0.555} & 0.710 & 0.639 & 0.724 & 0.648 & 1.515 & 1.042 & 2.430 & 1.284 & 1.801 & 1.159 \\\\\n        ~ & \\textbf{Avg.} & 0.339  & 0.399  & \\underline{0.320}  & \\underline{0.396}  & \\textbf{0.277}  & \\textbf{0.377}  & 0.392  & 0.446  & 0.440  & 0.477  & 1.325  & 0.949  & 1.679  & 1.066  & 1.841  & 1.137  \\\\ \n    \\midrule\n        \\multirow{5}{*}{\\rotatebox{90}{ETTm2}} & 24 & 0.126  & 0.226  & \\textbf{0.104}  & \\textbf{0.200}  & \\underline{0.109}  & \\underline{0.213}  & 0.149  & 0.253  & 0.157  & 0.262  & 0.282  & 0.397  & 0.391  & 0.475  & 0.405  & 0.499 \\\\ \n        ~ & 48 & 0.175  & 0.276  & \\textbf{0.143}  & \\textbf{0.238}  & \\underline{0.145}  & \\underline{0.249}  & 0.172  & 0.268  & 0.177  & 0.276  & 0.501  & 0.535  & 0.420  & 0.484  & 0.541  & 0.569 \\\\\n        ~ & 168 & 0.272  & 0.336  & \\textbf{0.255}  & \\underline{0.308}  & 0.262  & 0.345  & \\textbf{0.255}  & 0.318  & 0.262  & 0.324  & 0.824  & 0.730  & 1.068  & 0.869  & 1.149  & 0.846 \\\\ \n        ~ & 336 & 0.331  & \\underline{0.361}  & 0.352  & \\textbf{0.359}  & 0.366  & 0.414  & \\textbf{0.326}  & \\underline{0.361}  & \\underline{0.328}  & 0.365  & 1.149  & 0.871  & 1.379  & 0.983  & 2.258  & 1.153 \\\\\n        ~ & \\textbf{Avg.} & 0.226  & \\underline{0.300}  & \\textbf{0.214}  & \\textbf{0.276}  & \\underline{0.220}  & 0.305  & 0.226  & \\underline{0.300}  & 0.231  & 0.307  & 0.689  & 0.633  & 0.815  & 0.703  & 1.088  & 0.767 \\\\ \n    \\midrule    \n        \\multirow{5}{*}{\\rotatebox{90}{ETTh2}} & 24 & 0.237  & 0.327  & \\underline{0.196}  & \\underline{0.281}  & \\textbf{0.179}  & \\textbf{0.277}  & 0.262  & 0.340  & 0.266  & 0.343  & 0.946  & 0.800  & 1.408  & 1.028  & 0.762  & 0.703 \\\\ \n        ~ & 48 & 0.287  & 0.355  & \\textbf{0.244}  & \\textbf{0.317}  & \\underline{0.247}  & \\underline{0.329}  & 0.291  & 0.354  & 0.294  & 0.358  & 1.009  & 0.826  & 1.642  & 1.100  & 1.101  & 0.856  \\\\ \n        ~ & 168 & 0.417  & 0.426  & \\textbf{0.397}  & \\textbf{0.409}  & 0.430  & 0.448  & \\underline{0.403}  & 0.420  & \\underline{0.403}  & \\underline{0.419}  & 3.014  & 1.483  & 3.261  & 1.511  & 2.076  & 1.144 \\\\ \n        ~ & 336 & 0.484  & 0.471  & \\underline{0.446}  & \\textbf{0.449}  & 0.577  & 0.532  & 0.448  & 0.458  & \\textbf{0.445}  & \\underline{0.456}  & 3.357  & 1.606  & 3.155  & 1.481  & 2.633  & 1.297 \\\\ \n        ~ & \\textbf{Avg.} & 0.356  & 0.395  & \\textbf{0.321}  & \\textbf{0.364}  & 0.358  & 0.396  & \\underline{0.351}  & \\underline{0.393}  & 0.352  & 0.394  & 2.081  & 1.179  & 2.366  & 1.280  & 1.643  & 1.000 \\\\ \n    \\midrule\n        \\multirow{5}{*}{\\rotatebox{90}{ILI}} & 24 & \\textbf{2.391}  & \\textbf{1.004}  & \\underline{2.925}  & \\underline{1.061}  & 4.412  & 1.576  & 4.979  & 1.668  & 4.334  & 1.527  & 4.789  & 1.431  & 5.718  & 1.623  & 4.678  & 1.455 \\\\\n        ~ & 36 & \\textbf{2.673}  & \\textbf{1.006}  & \\underline{3.145}  & \\underline{1.127}  & 4.314  & 1.529  & 4.812  & 1.628  & 4.084  & 1.438  & 4.995  & 1.474  & 5.353  & 1.564  & 4.801  & 1.490 \\\\\n        ~ & 48 & \\textbf{2.536}  & \\underline{1.039}  & \\underline{2.716}  & \\textbf{1.027}  & 3.910  & 1.424  & 4.112  & 1.465  & 3.870  & 1.408  & 5.283  & 1.545  & 5.120  & 1.537  & 4.946  & 1.510 \\\\ \n        ~ & 60 & \\underline{2.930}  & \\underline{1.117}  & \\textbf{2.711}  & \\textbf{1.049}  & 4.114  & 1.446  & 3.965  & 1.428  & 3.895  & 1.411  & 5.454  & 1.578  & 5.556  & 1.613  & 5.134  & 1.545 \\\\ \n        ~ & \\textbf{Avg.} & \\textbf{2.632}  & \\textbf{1.042}  & \\underline{2.874}  & \\underline{1.066}  & 4.188  & 1.493  & 4.467  & 1.547  & 4.046  & 1.446  & 5.130  & 1.507  & 5.437  & 1.584  & 4.890  & 1.500 \\\\ \n    \\midrule\n        \\multirow{5}{*}{\\rotatebox{90}{Weather}} & 96 & \\textbf{0.170}  & \\textbf{0.218}  & \\underline{0.171}  & \\underline{0.221}  & 0.194  & 0.250  & 0.254  & 0.323  & 0.253  & 0.317  & 0.579  & 0.539  & 0.599  & 0.541  & 0.940  & 0.675 \\\\ \n        ~ & 192 & \\textbf{0.229}  & \\underline{0.271}  & \\underline{0.230}  & \\textbf{0.269}  & 0.237  & 0.297  & 0.287  & 0.344  & 0.293  & 0.347  & 0.738  & 0.613  & 0.459  & 0.453  & 1.061  & 0.735 \\\\ \n        ~ & 336 & \\underline{0.288}  & \\underline{0.317}  & 0.289  & \\textbf{0.302}  & \\textbf{0.282}  & 0.334  & 0.336  & 0.374  & 0.346  & 0.380  & 0.464  & 0.466  & 0.575  & 0.495  & 1.201  & 0.802 \\\\ \n        ~ & 540 & \\underline{0.341}  & \\underline{0.355}  & 0.345  & \\textbf{0.348}  & \\textbf{0.325}  & 0.365  & 0.384  & 0.400  & 0.379  & 0.394  & 0.909  & 0.700  & 0.862  & 0.625  & 0.958  & 0.702 \\\\ \n        ~ & \\textbf{Avg.} & \\textbf{0.257}  & \\underline{0.290}  & \\underline{0.259}  & \\textbf{0.285}  & 0.260  & 0.311  & 0.315  & 0.360  & 0.318  & 0.359  & 0.672  & 0.579  & 0.624  & 0.529  & 1.040  & 0.729 \\\\ \n    \\bottomrule\n    \\end{tabular}}\n    \\vspace{-10pt}\n    \\label{tab:regular}\n\\end{table}",
            "tab:pendulum": "\\begin{table}[htbp]\n    \\centering\n    \\caption{Regression MSE $\\times 10^{-3}$ (mean $\\pm$ std) on pendulum regression task.}\n    \\begin{tabular}{cc}\n    \\toprule\n        Model & Regression MSE ($\\times 10^{-3}$) \\\\\n    \\midrule\n        mTAN~\\cite{shukla2021multi} & 65.64 (4.05) \\\\\n        ODE-RNN~\\cite{rubanova2019latent} & 7.26 (0.41) \\\\\n        CRU~\\cite{schirmer2022modeling} & 4.63 (1.07) \\\\\n        S5~\\cite{smith2022simplified} & \\textbf{3.41 (0.27)} \\\\\n    \\midrule\n        ContiFormer & 4.21 (0.24) \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:pendulum}\n\\end{table}",
            "tab:abl": "\\begin{table*}[t]\n    \\centering\n    \\caption{Effects of different attention mechanism design on event prediction task. Attn. stands for Attention. {\\scriptsize \\CheckmarkBold} refers to a continuous version of attention/value, while {\\scriptsize \\XSolidBrush} refers to a static version. It is important to note that when both attention and value are static, the model is equivalent to Transformer. Conversely, when both are continuous, the model is referred to as ContiFormer.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{C{0.053\\textwidth}C{0.053\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}|C{0.135\\textwidth}C{0.135\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}}\n    \\toprule\n         \\multicolumn{2}{c}{Dataset} & \\multicolumn{3}{c}{Synthetic} & \\multicolumn{2}{c}{Neonate} & \\multicolumn{3}{c}{Traffic} \\\\\n    \\cmidrule(l){1-2}\n    \\cmidrule(l){3-5}\n    \\cmidrule(l){6-7}\n    \\cmidrule(l){8-10}\n         Attn. & Value & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$)  & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\XSolidBrush} & -0.589 & 0.841 & 0.205 & -2.702 & 9.471 & 0.569 & 0.818 & 0.332 \\\\\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\CheckmarkBold} & -0.576 & \\textbf{0.842} & 0.202 & -2.634 & 9.249 & 0.598 & 0.819 & \\textbf{0.327} \\\\\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\XSolidBrush} & -0.563 & 0.842 & 0.200 & -2.590 & 9.267 & 0.544 & 0.814 & 0.329 \\\\\n    \\midrule\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\CheckmarkBold}  & \\textbf{-0.535} & \\textbf{0.842} & \\textbf{0.192} & \\textbf{-2.550}  & \\textbf{9.233} & \\textbf{0.635} & \\textbf{0.822} & 0.328 \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cc|ccc|ccc|ccc}\n    \\toprule\n         \\multicolumn{2}{c}{Dataset} & \\multicolumn{3}{c}{MIMIC} & \\multicolumn{3}{c}{BookOrder} & \\multicolumn{3}{c}{StackOverflow} \\\\\n    \\cmidrule(l){1-2}\n    \\cmidrule(l){3-5}\n    \\cmidrule(l){6-8}\n    \\cmidrule(l){9-11}\n         Attn. & Value & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) &  LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\XSolidBrush} & -1.137 & 0.834 & 0.843 & -0.302 & 0.622 & 3.688 & -2.354 & 0.468 & 0.951 \\\\\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\CheckmarkBold}  & -1.147 & 0.832 & \\textbf{0.837} & -0.312 & 0.627 & 3.690 & -2.354 & 0.468 & 0.950 \\\\\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\XSolidBrush} & -1.137 & \\textbf{0.837} & 0.838 & -0.272 & \\textbf{0.629} & 3.632 & -2.337 & \\textbf{0.472} & \\textbf{0.948} \\\\\n    \\midrule\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\CheckmarkBold} & \\textbf{-1.135} & 0.836 & \\textbf{0.837} & \\textbf{-0.270} & 0.628 & \\textbf{3.614} & \\textbf{-2.334} & \\textbf{0.472} & \\textbf{0.948} \\\\\n    \\bottomrule\n    \\end{tabular}}\n\n    \\label{tab:abl}\n\\end{table*}",
            "tab:ffr": "\\begin{table*}[t]\n    \\centering\n    \\caption{Effects of difference numerical approximation methods on event prediction task. (default to linear approximation instead of the Gauss-Legendre quadrature method).}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{C{0.20\\textwidth}C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}|C{0.135\\textwidth}C{0.135\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{$P$}{Dataset}} & \\multicolumn{3}{c}{Synthetic} & \\multicolumn{2}{c}{Neonate} & \\multicolumn{3}{c}{Traffic} \\\\\n    \\cmidrule(l){2-4}\n    \\cmidrule(l){5-6}\n    \\cmidrule(l){7-9}\n          & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$)  & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    \\midrule\n    \\multicolumn{9}{c}{Approxiamation Method = \\textit{Linear}} \\\\\n    \\midrule\n    -- & -0.535 & \\textbf{0.842} & \\textbf{0.192} & \\textbf{-2.550}  & \\textbf{9.233} & \\textbf{0.635} & \\textbf{0.822} & \\textbf{0.328} \\\\\n    \\midrule\n    \\midrule\n    \\multicolumn{9}{c}{Approxiamation Method = \\textit{Gauss-Legendre Quadrature}} \\\\\n    \\midrule\n    2 & -0.545 & \\textbf{0.842} & 0.198 & -2.571 & 9.235 & 0.599 & 0.817 & 0.330 \\\\\n    3 & \\textbf{-0.531} & \\textbf{0.842} & 0.194 & -2.594 & 9.236 & 0.610 & 0.820 & \\textbf{0.328} \\\\\n    4 & -0.556 & \\textbf{0.842} & 0.199 & -2.590 & 9.253 & 0.589 & 0.819 & \\textbf{0.328} \\\\\n    5 & -0.550 & \\textbf{0.842} & 0.193 & -2.602 & 9.258 & 0.601 & 0.820 & \\textbf{0.328} \\\\\n    \\bottomrule\n    \\end{tabular}}  \n\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cccc|ccc|ccc}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{$P$}{Dataset}} & \\multicolumn{3}{c}{MIMIC} & \\multicolumn{3}{c}{BookOrder} & \\multicolumn{3}{c}{StackOverflow} \\\\\n    \\cmidrule(l){2-4}\n    \\cmidrule(l){5-7}\n    \\cmidrule(l){8-10}\n           & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) &  LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    \\midrule\n    \\multicolumn{10}{c}{Approxiamation Method = \\textit{Linear}} \\\\\n    \\midrule\n    -- & -1.135 & 0.836 & \\textbf{0.837} & \\textbf{-0.270} & \\textbf{0.628} & 3.614 & \\textbf{-2.332} & \\textbf{0.473} & \\textbf{0.948} \\\\\n    \\midrule\n    \\midrule\n    \\multicolumn{10}{c}{Approxiamation Method = \\textit{Gauss-Legendre Quadrature}} \\\\\n    \\midrule\n    2 & -1.144 & 0.834 & 0.840 & -0.291 & 0.627 & 3.653 & -2.337 & 0.472 & \\textbf{0.948} \\\\\n    3 & -1.142 & \\textbf{0.837} & 0.839 & -0.294 & 0.627 & 3.667 & -2.337 & 0.471 & \\textbf{0.948} \\\\\n    4 & -1.143 & \\textbf{0.837} & 0.839 & -0.287 & 0.627 & 3.647 & -2.340 & 0.471 & \\textbf{0.948} \\\\\n    5 & \\textbf{-1.132} & 0.834 & 0.838 & -0.290 & \\textbf{0.628} & \\textbf{3.563} & -2.338 & 0.471 & \\textbf{0.948} \\\\\n    \\bottomrule\n    \\end{tabular}} \n    \\label{tab:ffr}\n\\end{table*}",
            "tab:eff": "\\begin{table*}[t]\n    \\centering\n    \\caption{Effects of tolerance errors in ODESolver on event prediction task (default to 0.5 for Neonate dataset and 0.1 for other datasets). (The use of a double dash (--) indicates that the experiment requires an excessively long duration of time.)}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{C{0.21\\textwidth}C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}|C{0.135\\textwidth}C{0.135\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{$P$}{Dataset}} & \\multicolumn{3}{c}{Synthetic} & \\multicolumn{2}{c}{Neonate} & \\multicolumn{3}{c}{Traffic} \\\\\n    \\cmidrule(l){2-4}\n    \\cmidrule(l){5-6}\n    \\cmidrule(l){7-9}\n          & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$)  & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    5e-1 & -0.559 & 0.841 & 0.196 & \\textbf{-2.550} & 9.233 & 0.614 & 0.820 & \\textbf{0.328} \\\\\n    1e-1 & \\textbf{-0.535} & \\textbf{0.842} & \\textbf{0.192} & -2.561 & \\textbf{9.229} & \\textbf{0.635} & \\textbf{0.822} & \\textbf{0.328} \\\\\n    5e-2 & -0.549 & \\textbf{0.842} & 0.195 & -2.568 & 9.236 & 0.620 & 0.819 & 0.329 \\\\\n    1e-2 & -0.539 & \\textbf{0.842} & 0.195 & -2.573 & 9.233 & 0.619 & 0.820 & 0.329 \\\\\n    5e-3 & -0.549 & \\textbf{0.842} & 0.199 & -2.559 & 9.238 & 0.613 & 0.819 & 0.329 \\\\\n    \\bottomrule\n    \\end{tabular}}\n\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cccc|ccc|ccc}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{Stepsize}{Dataset}} & \\multicolumn{3}{c}{MIMIC} & \\multicolumn{3}{c}{BookOrder} & \\multicolumn{3}{c}{StackOverflow} \\\\\n    \\cmidrule(l){2-4}\n    \\cmidrule(l){5-7}\n    \\cmidrule(l){8-10}\n          & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) &  LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    5e-1 & -1.138 & \\textbf{0.837} & \\textbf{0.837} & -0.268 & \\textbf{0.629} & 3.619 & -2.333 & \\textbf{0.473} & \\textbf{0.948} \\\\\n    1e-1 & \\textbf{-1.135} & 0.836 & \\textbf{0.837} & -0.270 & 0.628 & 3.614 & \\textbf{-2.332} & \\textbf{0.473} & \\textbf{0.948} \\\\\n    5e-2 & -1.137 & 0.835 & \\textbf{0.837} & \\textbf{-0.263} & \\textbf{0.629} & \\textbf{3.609} & \\textbf{-2.332} & \\textbf{0.473} & \\textbf{0.948} \\\\\n    1e-2 & -1.143 & 0.836 & 0.838 & \\textbf{-0.263} & \\textbf{0.629} & 3.612 & -- & -- & --  \\\\\n    5e-3 & -1.147 & \\textbf{0.837} & 0.838 & -- & -- & -- & -- & -- & --  \\\\\n    \\bottomrule\n    \\end{tabular}} \n    \\label{tab:eff}\n\\end{table*}",
            "tab:func": "\\begin{table*}[t]\n    \\centering\n    \\caption{Effects of different vector field designs on event prediction task. Attn. stands for Attention. {\\scriptsize \\CheckmarkBold} refers to $\\text{ConcatNorm}$, while {\\scriptsize \\XSolidBrush} refers to $\\text{Concat}$.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{C{0.06\\textwidth}C{0.08\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}|C{0.135\\textwidth}C{0.135\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}}\n    \\toprule\n         \\multicolumn{2}{c}{Dataset} & \\multicolumn{3}{c}{Synthetic} & \\multicolumn{2}{c}{Neonate} & \\multicolumn{3}{c}{Traffic} \\\\\n    \\cmidrule(l){1-2}\n    \\cmidrule(l){3-5}\n    \\cmidrule(l){6-7}\n    \\cmidrule(l){8-10}\n         Norm & Actfn & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$)  & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    {\\scriptsize \\XSolidBrush} & tanh & -0.552 & \\textbf{0.842} & 0.198 & \\textbf{-2.550} & 9.233 & \\textbf{0.639} & 0.820 & 0.329  \\\\\n    {\\scriptsize \\XSolidBrush} & sigmoid & -0.561 & \\textbf{0.842} & 0.201 & -2.602 & 9.263 & 0.635 & \\textbf{0.822} & \\textbf{0.328}  \\\\\n    {\\scriptsize \\CheckmarkBold} & tanh & -0.556 & \\textbf{0.842} & 0.199 & -2.584 & 9.242 & 0.628 & 0.820 & \\textbf{0.328} \\\\\n    {\\scriptsize \\CheckmarkBold} & sigmoid & \\textbf{-0.535} & \\textbf{0.842} & \\textbf{0.192} & -2.601 & \\textbf{9.232} & 0.616 & 0.819 & 0.329  \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cc|ccc|ccc|ccc}\n    \\toprule\n         \\multicolumn{2}{c}{Dataset} & \\multicolumn{3}{c}{MIMIC} & \\multicolumn{3}{c}{BookOrder} & \\multicolumn{3}{c}{StackOverflow} \\\\\n    \\cmidrule(l){1-2}\n    \\cmidrule(l){3-5}\n    \\cmidrule(l){6-8}\n    \\cmidrule(l){9-11}\n         Norm & Actfn  & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) &  LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    {\\scriptsize \\XSolidBrush} & tanh & -1.162 & 0.831 & \\textbf{0.837} & -0.274 & 0.627 & 3.615 & -2.337 & 0.472 & 0.948   \\\\\n    {\\scriptsize \\XSolidBrush} & sigmoid & -1.144 & \\textbf{0.836} & 0.838 & \\textbf{-0.270} & \\textbf{0.628} & \\textbf{3.614} & -2.334 & 0.472 & 0.948  \\\\\n    {\\scriptsize \\CheckmarkBold} & tanh & -1.157 & 0.833 & 0.840 & -0.307 & 0.622 & 3.681 & -2.335 & \\textbf{0.473} & \\textbf{0.947}  \\\\\n    {\\scriptsize \\CheckmarkBold} & sigmoid & \\textbf{-1.135} & \\textbf{0.836} & \\textbf{0.837} & -0.302 & 0.625 & 3.689 & \\textbf{-2.332} & \\textbf{0.473} & 0.948  \\\\\n    \\bottomrule\n    \\end{tabular}}\n\n    \\label{tab:func}\n\\end{table*}",
            "tab:drp": "\\begin{table*}[t]\n    \\centering\n    \\caption{Effects of dropout rate in ContiFormer on event prediction task (default to 0.1).}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{C{0.21\\textwidth}C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}C{0.135\\textwidth}C{0.135\\textwidth}C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{Dropout}{Dataset}} & \\multicolumn{3}{c}{Synthetic} & \\multicolumn{2}{c}{Neonate} & \\multicolumn{3}{c}{Traffic} \\\\\n    \\cmidrule(l){2-4}\n    \\cmidrule(l){5-6}\n    \\cmidrule(l){7-9}\n          & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$)  & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    0 & -0.557 & \\textbf{0.842} & \\textbf{0.191} & -2.588 & \\textbf{9.215} & 0.633 & \\textbf{0.824} & \\textbf{0.328}  \\\\\n    0.1 & \\textbf{-0.535} & \\textbf{0.842} & 0.192 & \\textbf{-2.550} & 9.233 & \\textbf{0.635} & 0.822 & \\textbf{0.328} \\\\\n    0.3 & -0.644 & \\textbf{0.842} & 0.223 & -2.583 & 9.242 & 0.577 & 0.814 & 0.332 \\\\\n    0.5 & -0.696 & 0.841 & 0.230 & -2.579 & 9.280 & 0.525 & 0.813 & 0.331 \\\\\n    \\bottomrule\n    \\end{tabular}}\n\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cccccccccc}\n    \\toprule\n         \\multirow{2}{*}{\\diagbox{Dropout}{Dataset}} & \\multicolumn{3}{c}{MIMIC} & \\multicolumn{3}{c}{BookOrder} & \\multicolumn{3}{c}{StackOverflow} \\\\\n    \\cmidrule(l){2-4}\n    \\cmidrule(l){5-7}\n    \\cmidrule(l){8-10}\n          & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) &  LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    0 & -1.193 & 0.825 & 0.839 & -0.268 & 0.629 & 3.616 & -2.338 & 0.472 & 0.949 \\\\\n    0.1 & \\textbf{-1.135} & \\textbf{0.836} & 0.837 & -0.270 & 0.628 & 3.614 & \\textbf{-2.332} & \\textbf{0.473} &\\textbf{ 0.948} \\\\\n    0.3 & -1.142 & 0.833 & \\textbf{0.836} & \\textbf{-0.266} & \\textbf{0.630} & \\textbf{3.608} & -2.336 & 0.472 & \\textbf{0.948} \\\\\n    0.5 & -1.149 & 0.834 & 0.844 & -0.270 & 0.629 & 3.624 & -2.342 & 0.471 & 0.948 \\\\\n    \\bottomrule\n    \\end{tabular}} \n\n    \\label{tab:drp}\n\\end{table*}",
            "tab:norm": "\\begin{table*}[t]\n    \\centering\n    \\caption{Effects of normalization on event prediction task. Attn. stands for Attention. {\\scriptsize \\CheckmarkBold} refers to normalizing the attention/value as shown in Eq. (\\ref{eq:attn})/Eq. (\\ref{eq:vv}), while {\\scriptsize \\XSolidBrush} refers to removing the normalization, i.e. $t - t_i$. Note that ContiFormer use both normalization for attention and values.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{C{0.053\\textwidth}C{0.053\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}|C{0.135\\textwidth}C{0.135\\textwidth}|C{0.07\\textwidth}C{0.09\\textwidth}C{0.11\\textwidth}}\n    \\toprule\n         \\multicolumn{2}{c}{Dataset} & \\multicolumn{3}{c}{Synthetic} & \\multicolumn{2}{c}{Neonate} & \\multicolumn{3}{c}{Traffic} \\\\\n    \\cmidrule(l){1-2}\n    \\cmidrule(l){3-5}\n    \\cmidrule(l){6-7}\n    \\cmidrule(l){8-10}\n         Attn. & Value & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$)  & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\XSolidBrush} & -0.618 & \\textbf{0.842} & 0.215 & -2.612 & 9.277 & \\textbf{0.721} & \\textbf{0.822} & 0.329 \\\\\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\CheckmarkBold} & -0.601 & 0.841 & 0.206 & -2.576 & 9.240 & 0.536 & 0.814 & 0.336 \\\\\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\XSolidBrush} & -0.584 & \\textbf{0.842} & 0.205 & -2.601 & 9.272 & 0.576 & 0.818 & \\textbf{0.327} \\\\\n    \\midrule\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\CheckmarkBold}  & \\textbf{-0.535} & \\textbf{0.842} & \\textbf{0.192} & \\textbf{-2.550}  & \\textbf{9.233} & 0.635 & \\textbf{0.822} & 0.328 \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cc|ccc|ccc|ccc}\n    \\toprule\n         \\multicolumn{2}{c}{Dataset} & \\multicolumn{3}{c}{MIMIC} & \\multicolumn{3}{c}{BookOrder} & \\multicolumn{3}{c}{StackOverflow} \\\\\n    \\cmidrule(l){1-2}\n    \\cmidrule(l){3-5}\n    \\cmidrule(l){6-8}\n    \\cmidrule(l){9-11}\n         Attn. & Value & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) &  LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) & LL ($\\uparrow$) & ACC ($\\uparrow$) & RMSE ($\\downarrow$) \\\\\n    \\midrule\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\XSolidBrush} & -1.167 & 0.826 & 0.848 & -0.311 & 0.627 & 3.710 & -2.346 & 0.470 & \\textbf{0.948} \\\\\n    {\\scriptsize \\XSolidBrush} & {\\scriptsize \\CheckmarkBold} & -1.149 & 0.833 & 0.840 & \\textbf{-0.269} & \\textbf{0.630} & 3.634 & -2.341 & 0.471 & \\textbf{0.948} \\\\\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\XSolidBrush} & -1.172 & 0.826 & 0.844 & -0.277 & 0.629 & 3.654 & -2.336 & \\textbf{0.472} & \\textbf{0.948} \\\\\n    \\midrule\n    {\\scriptsize \\CheckmarkBold} & {\\scriptsize \\CheckmarkBold} & \\textbf{-1.135} & \\textbf{0.836} & \\textbf{0.837} & -0.270 & 0.628 & \\textbf{3.614} & \\textbf{-2.334} & \\textbf{0.472} & \\textbf{0.948} \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \n    \\label{tab:norm}\n\\end{table*}",
            "tab:costfull": "\\begin{table}[t]\n    \\centering\n    \\caption{Time Cost v.s. Accuracy for different models on UEA classification datasets (0\\% dropped). For all ODE-based models and our model, we adopt the RK4 solver and step size refers to the amount of time increment for the RK4 solver. $\\uparrow$ ($\\downarrow$) indicates the higher (lower) the better. Note that since the results are aggregated over 3 random seeds, achieving the same results doesn't indicate the same outcome for each random seed.}\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cccccccc}\n    \\toprule\n        Dataset & & \\multicolumn{2}{c}{BasicMotions} & \\multicolumn{2}{c}{JapaneseVowels} &  \\multicolumn{2}{c}{Libras}  \\\\ \n        Model & Step Size & Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$) &  Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$) & Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$)  \\\\ \n    \\midrule\n        TST & - & $0.56 \\times$  & 0.975  & $0.10 \\times$  & 0.986  & $0.36 \\times$  & 0.843  \\\\ \n        S5 & - & $1 \\times$  & 0.958  & $1 \\times$  & 0.926  & $1 \\times$  & 0.655  \\\\ \n    \\midrule\n        \\multirow{2}{*}{ODE-RNN (w/o norm)} & 0.1 & $3.06 \\times$  & 0.917  & $0.79 \\times$  & 0.952  & $2.67 \\times$  & 0.602  \\\\ \n        ~ & 0.01 & $18.94 \\times$  & 0.925  & $5.26 \\times$  & 0.955  & $16.29 \\times$  & 0.611  \\\\ \n    \\midrule\n        \\multirow{2}{*}{ODE-RNN (w/ norm)} & 0.1 & $1.46 \\times$  & \\textbf{1.000}  & $0.32 \\times$  & 0.980  & $1.24 \\times$  & 0.637  \\\\ \n        ~ & 0.01 & $1.59 \\times$  & \\textbf{1.000}  & $0.47 \\times$  & 0.980  & $1.59 \\times$  & 0.619  \\\\ \n    \\midrule\n        \\multirow{2}{*}{Neural CDE} & 0.1 & $4.47 \\times$  & 0.958  & $1.16 \\times$  & 0.932  & $3.87 \\times$  & 0.763  \\\\ \n        ~ & 0.01 & $34.26 \\times$  & 0.958  & $9.57 \\times$  & 0.942  & $28.48 \\times$  & 0.744  \\\\ \n    \\midrule\n        \\multirow{2}{*}{ContiFormer} & 0.1 & $2.89 \\times$  & 0.975  & $0.49 \\times$  & 0.990  & $2.26 \\times$  & \\textbf{0.870}  \\\\ \n        ~ & 0.01 & $13.75 \\times$  & 0.975  & $1.87 \\times$  & \\textbf{0.991}  & $8.74 \\times$  & \\textbf{0.870} \\\\ \n    \\bottomrule\n    \\end{tabular}}\n\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{cccccccc}\n    \\toprule\n        Dataset & & \\multicolumn{2}{c}{NATOPS} & \\multicolumn{2}{c}{PEMS-SF} &  \\multicolumn{2}{c}{RacketSports}  \\\\ \n        Model & Step Size & Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$) &  Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$) & Time Cost ($\\downarrow$) & Accuracy ($\\uparrow$)  \\\\ \n    \\midrule\n        TST & - & $0.32 \\times$  & \\textbf{0.963}  & $0.15 \\times$  & 0.890  & $0.14 \\times$ & 0.826  \\\\ \n        S5 & - & $1 \\times$  & 0.839  & $1 \\times$  & \\textbf{0.896}  & $1 \\times$  & 0.772  \\\\ \n    \\midrule\n        \\multirow{2}{*}{ODE-RNN (w/o norm)} & 0.1 & $2.67 \\times$  & 0.909  & $4 \\times$  & 0.778  & $2.47 \\times$  & 0.827  \\\\ \n        ~ & 0.01 & $16.52 \\times$  & 0.907  & $28.45 \\times$  & 0.761  & $14.65 \\times$  & 0.796  \\\\ \n    \\midrule\n        \\multirow{2}{*}{ODE-RNN (w/ norm)} & 0.1 & $1.28 \\times$  & 0.898  & $1.47 \\times$  & 0.775  & $1.15 \\times$  & 0.785  \\\\ \n        ~ & 0.01 & $1.45 \\times$  & 0.898  & $1.44 \\times$  & 0.775  & $1.57 \\times$  & 0.787  \\\\ \n    \\midrule\n        \\multirow{2}{*}{Neural CDE} & 0.1 & $3.83 \\times$  & 0.789  & $6.04 \\times$  & 0.763  & $3.48 \\times$  & 0.743  \\\\ \n        ~ & 0.01 & $29.24 \\times$ & 0.787  & $50.36 \\times$  & 0.763  & $25.04 \\times$  & 0.748  \\\\ \n    \\midrule\n        \\multirow{2}{*}{ContiFormer} & 0.1 & $2.32 \\times$  & 0.935  & $5.45 \\times$  & 0.823  & $1.87 \\times$  & \\textbf{0.836}  \\\\ \n        ~ & 0.01 & $9.76 \\times$  & 0.935  & $37.08 \\times$  & 0.823  & $5.88 \\times$  & \\textbf{0.836} \\\\ \n    \\bottomrule\n    \\end{tabular}}\n    \n    \\label{tab:costfull}\n\\end{table}"
        },
        "figures": {
            "fig:ctsa": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.97\\textwidth]{fig/ctsa.pdf}\n    \\caption{Architecture of the ContiFormer layer. ContiFormer takes an irregular time series and its corresponding sampled time points as input. Queries, keys, and values are obtained in continuous-time form. The attention mechanism (CT-MHA) performs a scaled inner product in a continuous-time manner to capture the evolving relationship between observations, resulting in a complex continuous dynamic system. Feedforward and layer normalization are adopted, similar to the Transformer. Finally, a sampling trick is employed to make ContiFormer stackable. Note that the highlighted trajectories in purple indicate the part of functions that are involved in the calculation of the output. \n    }\n    \\label{fig:ctsa}\n\\end{figure*}",
            "fig:interpolate": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/interpolation_mark.pdf}\n    \\caption{Interpolation and extrapolation of spirals with irregularly-samples time points by Transformer, Neural ODE, and our model. }\n    \\label{fig:interpolate}\n    \\vspace{-10pt}\n\\end{figure*}",
            "fig:attn": "\\begin{wrapfigure}{r}{0.5\\textwidth}\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{fig/attention.pdf}\n  \\caption{Visualization of attention scores on UWaveGestureLibrary dataset. Colors indicate the attention scores for different instances at time $t=0$. Observations at time $t=0$ are observed and normalize the time interval to $[0, 1]$.}\n  \\label{fig:attn}\n  \\vspace{-10pt}\n\\end{wrapfigure}",
            "fig:int2": "\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/interpolation_full.pdf}\n    \\caption{More visualization results with $\\alpha=0.02$. Here, Latent ODE refers to Latent ODE w/ RNN Encoder~\\cite{chen2018neural}.}\n    \\label{fig:int2}\n\\end{figure}",
            "fig:dropout": "\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/dropout.pdf}\n    \\caption{Interpolation results on 2D spiral under different dropout rates.}\n    \\label{fig:dropout}\n\\end{figure}",
            "fig:cost": "\\begin{figure}[htbp]\n    \\centering\n    \\vspace{20pt}\n    \\includegraphics[width=\\textwidth]{fig/cost.pdf}\n    \\caption{Time Cost v.s. Memory Cost v.s. Accuracy.}\n    \\label{fig:cost}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\begin{aligned}\n        \\boldsymbol{\\mathrm{k}}_i(t_i) & = K_i ~, \\quad   \\boldsymbol{\\mathrm{k}}_i(t) = \\boldsymbol{\\mathrm{k}}_i(t_i) + \\int_{t_i}^{t} f\\,\\Big( \\tau, \\boldsymbol{\\mathrm{k}}_i(\\tau); \\theta_k \\Big)\\, \\text{d} \\tau ~,\\\\\n        \\boldsymbol{\\mathrm{v}}_i(t_i) & = V_i ~, \\quad   \\boldsymbol{\\mathrm{v}}_i(t) = \\boldsymbol{\\mathrm{v}}_i(t_i) + \\int_{t_i}^{t} f\\,\\Big( \\tau, \\boldsymbol{\\mathrm{v}}_i(\\tau); \\theta_v \\Big)\\, \\text{d} \\tau ~, \\\\\n    \\end{aligned}\n\\label{eq:kv}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\left< f, g \\right> = \\int_{a}^b f(x) \\cdot g(x) \\text{d} x ~.\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_{i}(t) =  \\frac{\\int_{t_i}^{t} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{t - t_i} ~.\n\\label{eq:attn}\n\\end{equation}",
            "eq:4": "\\begin{equation}\n\\boldsymbol{\\mathrm{\\alpha}}_i(t_i) = \\lim_{\\epsilon \\rightarrow 0} \\frac{\\int_{t_i}^{t_i + \\epsilon} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{\\epsilon} = \\boldsymbol{\\mathrm{q}}(t_i) \\cdot \\boldsymbol{\\mathrm{k}}_i(t_i)^\\top ~.\n\\label{eq:ii}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\widehat{\\boldsymbol{\\mathrm{v}}}_{i}(t) = \\mathbb{E}_{t \\sim [t_i, t]} \\left[ \\boldsymbol{\\mathrm{v}}_i(t) \\right] = \\frac{\\int_{t_i}^{t} \\boldsymbol{\\mathrm{v}}_i(\\tau) \\text{d} \\tau}{t - t_i} ~.\n    \\label{eq:vv}\n\\end{equation}",
            "eq:6": "\\begin{equation}\\\n\\begin{aligned}\n    \\operatorname{CT-ATTN}( Q, K, V, \\boldsymbol{\\omega})(t) & \n    = \\sum_{i=1}^{N} \\widehat{\\boldsymbol{\\mathrm{\\alpha}}}_i(t) \\cdot \\widehat{\\boldsymbol{\\mathrm{v}}}_i(t) ~,\\\\\n    \\text { where } \\widehat{\\boldsymbol{\\mathrm{\\alpha}}}_i(t) & = \\frac{\\operatorname{exp} \\left( \\boldsymbol{\\mathrm{\\alpha}}_i(t) / \\sqrt{d_k} \\right)}{\\sum_{j=1}^{N} \\operatorname{exp} \\left( \\boldsymbol{\\mathrm{\\alpha}}_j(t) / \\sqrt{d_k} \\right)} ~. \\\\\n\\end{aligned}\n\\label{eqn:msha}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n\\begin{aligned}\n\\operatorname{CT-MHA}(Q, K, V, \\boldsymbol{\\omega})(t) & =\\operatorname{Concat}\\left(\\operatorname{head}_{(1)}(t), \\ldots, \\operatorname{head}_{(\\mathrm{H})}(t)\\right) W^O ~,\\\\\n\\text { where } \\operatorname{head}_{(\\mathrm{h})}(t) & =\\operatorname{CT-ATTN}\\left(Q W_{(\\mathrm{h})}^Q, K W_{(\\mathrm{h})}^K, V W_{(\\mathrm{h})}^V, \\boldsymbol{\\omega} \\right)(t) ~,\n\\end{aligned}\n\\label{eq:contiattn}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\begin{aligned}\n \\tilde{\\boldsymbol{\\mathrm{z}}}^{l}(t) &= \\operatorname{LN}\\Big( \\operatorname{CT-MHA}(X^{l}, X^{l}, X^{l}, \\boldsymbol{\\omega}^{l})(t)  + \\boldsymbol{\\mathrm{x}}^{l}(t) \\Big) ~,\\\\\n \\boldsymbol{\\mathrm{z}}^{l}(t) & = \\operatorname{LN}\\Big( \\operatorname{FFN}(\\tilde{\\boldsymbol{\\mathrm{z}}}^{l}(t) ) + \\tilde{\\boldsymbol{\\mathrm{z}}}^{l}(t) \\Big) ~,\\\\\n\\end{aligned}\n\\label{eq:layer}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_i(t_j) = \\frac{\\int_{t_i}^{t_j} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{t_j - t_i} = \\frac{1}{2} \\int_{-1}^{1} \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\tau) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\tau)^\\top \\text{d} \\tau \\approx  \\frac{1}{2} \\sum_{p=1}^{P} \\gamma_p \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_p) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p)^{\\top} ~,\n\\label{eq:app}\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\underbrace{\\begin{bmatrix}\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{1, 1}(\\xi_{p-1}) \\\\\n    \\vdots \\\\\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{N,N}(\\xi_{p-1})\n    \\end{bmatrix}}_{\\boldsymbol{\\mathrm{K}}(\\xi_{p-1})} + \\int_{\\xi_{p-1}}^{\\xi_{p}} f(s, \\boldsymbol{\\mathrm{K}}(s); \\theta_k) ds =\n    \\underbrace{\\begin{bmatrix}\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{1, 1}(\\xi_{p}) \\\\\n    \\vdots \\\\\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{N, N}(\\xi_{p})\n    \\end{bmatrix}}_{\\boldsymbol{\\mathrm{K}}(\\xi_{p})} ~,\n\\label{eq:solve}\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\textbf{x}_{t_1}, \\textbf{x}_{t_2}, \\ldots, \\mathbf{z}_{t_N}=\\operatorname{ODESolve}\\left(\\textbf{x}_{t_0}, f, t_0, \\ldots, t_N\\right) ~.\n\\label{eq:ode}\n\\end{equation}",
            "eq:12": "\\begin{equation}\n    \\tilde{\\textbf{x}}(s) = \\textbf{x}(\\frac{s(t_1 - t_0) + t_0 + t_1}{2}) ~.\n\\end{equation}",
            "eq:13": "\\begin{equation}\n\\begin{aligned}\n\\tilde{f}(s, \\tilde{\\textbf{x}}(s)) = \\frac{d \\tilde{\\textbf{x}}(s)}{d s} & =\\left.\\frac{d \\textbf{x}(t)}{d t}\\right|_{t=\\frac{s(t_1 - t_0) + t_0 + t_1}{2}} \\frac{d t}{d s} \\\\\n& =\\left.f(t, \\textbf{x}(t))\\right|_{t=\\frac{s(t_1 - t_0) + t_0 + t_1}{2}}\\left(\\frac{t_1 - t_0}{2}\\right) \\\\\n& =f\\left(\\frac{s(t_1 - t_0) + t_0 + t_1}{2}, \\tilde{\\textbf{x}}(s)\\right)\\left(\\frac{t_1 - t_0}{2}\\right) ~.\n\\end{aligned}\n\\end{equation}",
            "eq:14": "\\begin{equation}\n    \\textbf{x}(t_1)= \\operatorname{ODESolve}(\\textbf{x}(t_0), f, t_0, t_1) = \\operatorname{ODESolve}(\\tilde{\\textbf{x}}(-1), \\tilde{f}, -1, 1) ~.\n\\end{equation}",
            "eq:15": "\\begin{equation}\n\\int_{-1}^{1} f(x) d x \\approx \\frac{f(-1) + f(1)}{2} ~.\n\\end{equation}",
            "eq:16": "\\begin{equation}\n\\int_{-1}^1 f(x) \\mathrm{d} x \\approx \\sum_j \\omega_j f\\left(\\xi_j\\right) ~,\n\\end{equation}",
            "eq:17": "\\begin{equation}\n    \\boldsymbol{\\mathrm{\\alpha}}_i(t_j) = \\frac{\\int_{t_i}^{t_j} \\boldsymbol{\\mathrm{q}}(\\tau) \\cdot \\boldsymbol{\\mathrm{k}}_i(\\tau)^\\top \\text{d}\\tau}{t_j - t_i} = \\frac{1}{2} \\int_{-1}^{1} \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\tau) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\tau)^\\top \\text{d} \\tau \\approx  \\frac{1}{2} \\sum_{p=1}^{P} \\gamma_p \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(\\xi_p) \\cdot \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p)^{\\top} ~,\n\\end{equation}",
            "eq:18": "\\begin{equation}\n\\begin{aligned}\n  \\tilde{\\boldsymbol{\\mathrm{q}}}_{i,j}(s) & = \\boldsymbol{\\mathrm{q}}\\left(\\frac{s(t_i - t_j) + t_i + t_j}{2}\\right) ~,  \\\\\n  \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(s) & = \\boldsymbol{\\mathrm{q}}_j\\left(\\frac{s(t_i - t_j) + t_i + t_j}{2}\\right) ~.\n\\end{aligned}\n\\end{equation}",
            "eq:19": "\\begin{equation}\n    \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_1), \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_2), ..., \\tilde{\\boldsymbol{\\mathrm{k}}}_{i,j}(\\xi_p) = \\operatorname{ODESolver} (\\boldsymbol{\\mathrm{k}}_j(t_j), \\hat{\\boldsymbol{\\mathrm{k}}}_{i,j}, -1, \\xi_1, \\xi_2, ..., \\xi_p) ~.\n\\end{equation}",
            "eq:20": "\\begin{equation}\n\\operatorname{Attn}(Q, K)=\\left[\\begin{array}{cccc}\nQ_1 \\cdot K_1^\\top & Q_1 \\cdot K_2^\\top & \\cdots & Q_1 \\cdot K_N^\\top \\\\\nQ_2 \\cdot K_1^\\top & Q_2 \\cdot K_2^\\top & \\cdots & Q_2 \\cdot K_N^\\top \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nQ_N \\cdot K_1^\\top & Q_N \\cdot K_2^\\top & \\cdots & Q_N \\cdot K_N^\\top\n\\end{array}\\right] ~,\n\\end{equation}",
            "eq:21": "\\begin{equation}\nZ_i = \\operatorname{Softmax}(\\operatorname{Attn}(Q, K)_i) V ~.\n\\end{equation}",
            "eq:22": "\\begin{equation}\n\\operatorname{Attn}(Q, K)=\\left[\\begin{array}{cccc}\nQ_1 \\cdot K_1^\\top * k(t_1, t_1) & Q_1 \\cdot K_2^\\top * k(t_1, t_2) & \\cdots & Q_1 \\cdot K_N^\\top * k(t_1, t_N) \\\\\nQ_2 \\cdot K_1^\\top * k(t_2, t_1) & Q_2 \\cdot K_2^\\top * k(t_2, t_2) & \\cdots & Q_2 \\cdot K_N^\\top * k(t_2, t_N) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nQ_N \\cdot K_1^\\top * k(t_N, t_1) & Q_N \\cdot K_2^\\top * k(t_N, t_2) & \\cdots & Q_N \\cdot K_N^\\top * k(t_N, t_N)\n\\end{array}\\right] ~.\n\\end{equation}",
            "eq:23": "\\begin{equation}\n\\begin{aligned}\nk_\\text{GSM}\\left(x, x'\\right) &=\n \\sum_i \\phi_i(x) \\phi_i\\left(x'\\right) k_{\\text{Gibbs}, i}\\left(x, x'\\right) \\cos \\left(2 \\pi\\left(\\mu_i(x) x-\\mu_i\\left(x'\\right) x'\\right)\\right) ~, \\\\\n\\text{ where } k_{\\text{Gibbs}, i}\\left(x, x'\\right) &= \\sqrt{\\frac{2 l_i(x) l_i\\left(x'\\right)}{l_i(x)^2+l_i\\left(x'\\right)^2}} \\exp \\left(-\\frac{\\left(x-x'\\right)^2}{l_i(x)^2+l_i\\left(x'\\right)^2}\\right) .\n\\end{aligned}\n\\end{equation}",
            "eq:24": "\\begin{equation}\n\\operatorname{\\widehat{Attn}}(Q, K)=\\left[\\begin{array}{cccc}\n(Q_1, \\Phi(t_1)) \\cdot (K_1, \\Phi(t_1))^\\top & \\cdots & (Q_1, \\Phi(t_1)) \\cdot (K_N, \\Phi(t_N))^\\top  \\\\ \n(Q_2, \\Phi(t_2)) \\cdot (K_1, \\Phi(t_1))^\\top  & \\cdots & (Q_2, \\Phi(t_2)) \\cdot (K_N, \\Phi(t_N))^\\top \\\\ \n\\vdots & \\ddots & \\vdots \\\\\n(Q_N, \\Phi(t_N)) \\cdot (K_1, \\Phi(t_1))^\\top & \\cdots & (Q_N, \\Phi(t_N)) \\cdot (K_N, \\Phi(t_N))^\\top\n\\end{array}\\right] ~,\n\\end{equation}",
            "eq:25": "\\begin{equation}\n\\begin{aligned}\n\\Phi(t) \\mapsto \\Phi_d^{\\mathcal{M}} & =\\left[\\Phi_{\\omega_1, d}^{\\mathcal{M}}(t), \\ldots, \\Phi_{\\omega_k, d}^{\\mathcal{M}}(t)\\right]^{\\top} ~,\\\\\n\\text{ where } \\Phi_\\omega^{\\mathcal{M}}(t) & =\\left[\\sqrt{c_1}, \\ldots, \\sqrt{c_{2 j}} \\cos \\left(\\frac{j \\pi t}{\\omega}\\right), \\sqrt{c_{2 j+1}} \\sin \\left(\\frac{j \\pi t}{\\omega}\\right), \\ldots\\right] ~.\n\\end{aligned}\n\\end{equation}",
            "eq:26": "\\begin{equation}\n\\Phi(t) = \\phi(t) \\cdot \\textbf{w} ~,\n\\end{equation}",
            "eq:27": "\\begin{equation}\n\\phi(t)_i = \\begin{cases}\n    \\omega_0 \\cdot t + \\phi_0, & \\text{if } i = 0\\\\\n    \\sin (\\omega_i \\cdot t + \\phi_i), & \\text{if } 0 < i \\leq d\n    \\end{cases} ~,\n\\end{equation}",
            "eq:28": "\\begin{equation}\n\\operatorname{Softmax}(\\operatorname{Attn}(Q, K)) = \\operatorname{Softmax}(\\operatorname{\\widehat{Attn}}(Q, K)) ~,\n\\end{equation}",
            "eq:29": "\\begin{equation}\n\\begin{aligned}\n\\operatorname{Attn}& (Q, K)= \\\\\n& \\left[\\begin{array}{cccc}\nQ_1 \\cdot K_1^\\top & \\cdots & Q_1 \\cdot K_N + \\Phi(t_1) \\cdot (\\Phi(t_N) - \\Phi(t_1))^\\top  \\\\ \nQ_2 \\cdot K_1 + \\Phi(t_2) \\cdot (\\Phi(t_2) - \\Phi(t_1))^\\top  & \\cdots & Q_2 \\cdot K_N + \\Phi(t_2) \\cdot (\\Phi(t_N) - \\Phi(t_2))^\\top \\\\ \n\\vdots & \\ddots & \\vdots \\\\\nQ_N \\cdot K_1 + \\Phi(t_N) \\cdot (\\Phi(t_1) - \\Phi(t_N))^\\top & \\cdots & Q_N \\cdot K_N^\\top\n\\end{array}\\right] ~.\n\\end{aligned}\n\\end{equation}",
            "eq:30": "\\begin{equation}\n    \\begin{aligned}\n        x(t) & = (a + bt) \\cdot cos(t) ~,\\\\\n        y(t) & = (a + bt) \\cdot sin(t) ~,\n    \\end{aligned}\n\\end{equation}",
            "eq:31": "\\begin{equation}\n    \\begin{aligned}\n        x(t) & = \\left(a + 50b / \\left(e-t\\right)\\right) \\cdot cos(e-t) ~,\\\\\n        y(t) & = \\left(a + 50b / \\left(e-t\\right)\\right) \\cdot sin(e-t) ~,\\\\\n    \\end{aligned}\n\\end{equation}",
            "eq:32": "\\begin{equation}\n    \\log p(\\Gamma)=\\sum_{j=1}^N \\log \\lambda_{k_j}^*\\left(t_j\\right)-\\int_{t_1}^{t_N} \\lambda^*(\\tau) d \\tau ~.\n\\end{equation}",
            "eq:33": "\\begin{equation}\n    \\omega(\\textbf{e}, \\hat{\\textbf{e}}) = (e.t - \\hat{e}.t) \\cdot (e.x * \\hat{e}.v - \\hat{e}.x * e.v) ~.\n\\end{equation}",
            "eq:34": "\\begin{equation}\n\\begin{aligned}\n\\lambda_{k}^{*}(t) & = P(k | \\hat{e}.k) \\cdot \\text{Softplus} \\left(\\sum_{\\textbf{e} \\in \\mathcal{H}_t} \\omega(\\textbf{e}, \\hat{\\textbf{e}}) \\cdot \\text{exp} \\left( \\frac{- (t - e.t)}{e.d} \\right) \\right) ~,\\\\\n\\lambda^{*}(t) & = \\text{Softplus} \\left(\\sum_{\\textbf{e} \\in \\mathcal{H}_t} \\omega(\\textbf{e}, \\hat{\\textbf{e}}) \\cdot \\text{exp} \\left( \\frac{- (t - e.t)}{e.d} \\right) \\right) ~.\\\\\n\\end{aligned}\n\\label{eq:lam}\n\\end{equation}",
            "eq:35": "\\begin{equation}\n    \\begin{aligned}\n        \\widehat{t}_{j+1} & =\\int_{t_j}^{\\infty} t \\cdot p\\left(t \\mid \\mathcal{H}_t\\right) d t ~,\\\\\n        \\text{ where \\quad} p\\left(t \\mid \\mathcal{H}_t\\right) & =\\lambda^{*}(t) \\exp \\left(-\\int_{t_j}^t \\lambda^{*}(t) d t\\right) ~.\\\\\n    \\end{aligned}\n    \\label{eq:pre}\n\\end{equation}",
            "eq:36": "\\begin{equation}\n\\lambda\\left(t \\mid \\mathcal{H}_t\\right)=\\sum_{k=1}^K \\lambda_k\\left(t \\mid \\mathcal{H}_t\\right) ~,\n\\end{equation}",
            "eq:37": "\\begin{equation}\n\\lambda_k\\left(t \\mid \\mathcal{H}_t\\right)=f_k\\left(\\alpha_k \\frac{t-t_j}{t_j}+\\mathbf{w}_k^{\\top} \\mathbf{h}\\left(t_j\\right)+b_k\\right) ~,\n\\end{equation}",
            "eq:38": "\\begin{equation}\n\\ell_{LL}(\\mathcal{S})=\\underbrace{\\sum_{j=1}^N \\log \\lambda\\left(t_j \\mid \\mathcal{H}_j\\right)}_{\\text {event log-likelihood }}-\\underbrace{\\int_{t_1}^{t_N} \\lambda\\left(t \\mid \\mathcal{H}_t\\right) d t}_{\\text {non-event log-likelihood }} .\n\\end{equation}",
            "eq:39": "\\begin{equation}\n\\widehat{\\Lambda}_{\\mathrm{MC}}=\\sum_{j=2}^P\\left(t_j-t_{j-1}\\right)\\left(\\frac{1}{P} \\sum_{i=1}^P \\lambda\\left(u_i\\right)\\right) ~,\n\\end{equation}",
            "eq:40": "\\begin{equation}\n\\begin{aligned}\n& p\\left(t \\mid \\mathcal{H}_t\\right)=\\lambda\\left(t \\mid \\mathcal{H}_t\\right) \\exp \\left(-\\int_{t_j}^t \\lambda\\left(\\tau \\mid \\mathcal{H}_\\tau\\right) d \\tau\\right) ~, \\\\\n& \\widehat{t}_{j+1}=\\int_{t_j}^{\\infty} t \\cdot p\\left(t \\mid \\mathcal{H}_t\\right) d t ~. \\\\\n\\end{aligned}\n\\end{equation}",
            "eq:41": "\\begin{equation}\n\\ell_{reg}(\\mathcal{S}) = \\sum_{i=2}^{N} (\\widehat{t}_{i} - t_i)^2 ~, \n\\ell_{pred}(\\mathcal{S}) = \\sum_{i=2}^{N} -\\log \\left( \\frac{\\lambda_{k_i}\\left(\\widehat{t}_{i}\\right)}{\\lambda\\left(\\widehat{t}_{i} \\right)} \\right) ~.\n\\end{equation}",
            "eq:42": "\\begin{equation}\n    \\ell(\\mathcal{S}) = \\ell_{LL} + \\alpha_1 \\ell_{reg}(\\mathcal{S}) + \\alpha_2 \\ell_{pred}(\\mathcal{S}) ~,\n\\end{equation}",
            "eq:43": "\\begin{equation}\n    \\frac{\\text{d} \\textbf{x}}{\\text{d} t} = f(\\textbf{x}, t) ~.\n\\end{equation}",
            "eq:44": "\\begin{equation}\nf(\\textbf{x}, t)=\\text{Actfn}(\\text{Linear}^{d, d}(\\text{Linear}^{d, d}(\\textbf{x}) + \\text{Linear}^{1, d}(t))) ~,\n\\end{equation}",
            "eq:45": "\\begin{equation}\nf(\\textbf{x}, t)=\\text{Actfn}(\\text{LN}(\\text{Linear}^{d, d}(\\text{Linear}^{d, d}(\\textbf{x}) + \\text{Linear}^{1, d}(t)))) ~, \n\\end{equation}"
        }
    }
}