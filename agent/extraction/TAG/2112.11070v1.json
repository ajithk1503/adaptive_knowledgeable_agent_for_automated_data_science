{
    "meta_info": {
        "title": "An Inference Approach To Question Answering Over Knowledge Graphs",
        "abstract": "Knowledge Graphs (KG) act as a great tool for holding distilled information\nfrom large natural language text corpora. The problem of natural language\nquerying over knowledge graphs is essential for the human consumption of this\ninformation. This problem is typically addressed by converting the natural\nlanguage query to a structured query and then firing the structured query on\nthe KG. Direct answering models over knowledge graphs in literature are very\nfew. The query conversion models and direct models both require specific\ntraining data pertaining to the domain of the knowledge graph. In this work, we\nconvert the problem of natural language querying over knowledge graphs to an\ninference problem over premise-hypothesis pairs. Using trained deep learning\nmodels for the converted proxy inferencing problem, we provide the solution for\nthe original natural language querying problem. Our method achieves over 90%\naccuracy on MetaQA dataset, beating the existing state-of-the-art. We also\npropose a model for inferencing called Hierarchical Recurrent Path\nEncoder(HRPE). The inferencing models can be fine-tuned to be used across\ndomains with less training data. Our approach does not require large\ndomain-specific training data for querying on new knowledge graphs from\ndifferent domains.",
        "author": "Aayushee Gupta, K. M. Annervaz, Ambedkar Dukkipati, Shubhashis Sengupta",
        "link": "http://arxiv.org/abs/2112.11070v1",
        "category": [
            "cs.LG",
            "cs.CL"
        ],
        "additionl_info": "10 pages, 4 figures, 4 tables"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction \\& Motivation",
                "content": "\n\n Knowledge Graphs ~\\citep{nickel2016review} \\footnote{https://googleblog.blogspot.in/2012/05/introducing-knowledge-graph-things-not.html}\n represent information in the form of fact triplets, consisting of a subject entity, relation and an object entity (example: $<$\\textit{Italy, capital, Rome}$>$). The entities represent the nodes of the graph and the relationships between them act as edges. A fact triple $($subject entity, relation, object relation$)$ is represented as $(h,r,t)$.  Practical knowledge graphs congregate information from secondary databases or extract facts from unstructured text using various statistical learning mechanisms, examples of such a system is NELL~\\citep{mitchell2015never}. There are human created knowledge bases as well, like Freebase (FB15k)~\\citep{bollacker2008freebase}. Knowledge graphs are a great tool to hold distilled information for human consumption as well as using this information for machine processing. For human consumption, natural language querying capability over these graphs is quite essential.\n A natural language query can vary from very simple forms like \\textit{what is the capital of Italy?}, which can be answered from a single fact, to complex forms which needs to be reasoned over a set of connected facts. Figure~\\ref{fig:google} shows an example of a complex query unanswered by Google Knowledge Graph. The query requires reasoning based on multiple facts collected from the knowledge graph involving the query and answer entities.\n\n\n\nWe approach the problem of natural language querying from the perspective of inference. The main motivation is that a Knowledge Graph contains facts to support a given answer. So the facts from KG acting as a premise, should be able to tell whether an answer is right or wrong. Also, there has been considerable work done in the domain of inferencing, with multitudes of datasets. If we convert the original problem to inferencing, we could leverage that volume of work. The problem of Natural Language Inference (NLI) is to identify whether a statement (hypothesis: $\\mathcal{H}$) in natural language can be supported or contradicted in the context of another statement (premise: $\\mathcal{P}$) in natural language. If it can neither be inferred nor contradicted, we say the hypothesis is `neutral' to the premise. NLI is one of the most important components for natural language understanding systems~\\citep{benthem2008brief,maccartney2009extended}. For the original problem, given the query we create hypothesis facts based on potential answers to the query and populate a set of premises for each based on the facts related to them in the KG. For the earlier example, two potential hypotheses are \\textit{Milan is the capital of Italy} and \\textit{Rome is the capital of Italy}. Separate premises for each hypothesis are created based on various facts available in KG related to \\textit{Italy, Milan} and \\textit{Italy, Rome}. These premise-hypothesis pairs are tested for entailment or contradiction using the inference models. The above is a simple example, the premises and hypotheses get increasingly long and complex based on the complexity of the query.\n\nSome of the state of the art models for NLI are ESIM (Enhanced LSTM model)\\citep{chen2016enhanced,lan2018neural} and Transformer\\citep{radford2018improving}.  We used ESIM model in our work. It is a combination of the Bi-LSTM~\\citep{greff2015lstm} and Tree LSTM~\\citep{tai2015improved} models which encode the premises and hypotheses sentences and their parse trees, respectively. Further, local inferences for words and their context, along with local information between (linguistic) phrases and clauses are collected through an attention model~\\citep{luong2015effective,bahdanau2014neural}.\n\n\nThe main contributions of our work are as follows,\n\\begin{enumerate}\n\t\\item An approach to convert \\textit{Question Answering}(QA) problem over a Knowledge Graph to that of \\textit{Inference}.\n\t\\item Hierarchical Recurrent Path Encoder(HRPE), which is a custom model for the inference problem at hand.\n\t\\item Using the approach and leveraging the work on Natural Language Inference, we achieved state of the art accuracy on MetaQA~\\citep{zhang2018variational} dataset.\n\t\\item The proposed approach is amenable to domain adaptation, and this property can be utilized for Knowledge Graphs from new domains with fewer training data.\n\\end{enumerate}\n\n    \n"
            },
            "section 2": {
                "name": "Our Approach and Model",
                "content": "\n\\label{sec:main}\n\nFor the original problem, given a Knowledge Graph $\\mathcal{K}$ and a natural language query $Q$ with entities mentioned $\\mathcal{Q}_e = \\{e_1,\\dots,e_n\\}$, the problem is to retrieve the correct set of answer entities $\\mathcal{A}_c$ from nodes of $\\mathcal{K}$. We first populate a potential answer set $\\mathcal{A}_p$ from the connected entities of $Q_e$ in $\\mathcal{K}$. For each $a_i \\in \\mathcal{A}_p $, the various paths in  $\\mathcal{K}$ from $a_i$ to each element of $\\mathcal{Q}_e$ is populated and considered a part of the premise $\\mathcal{P}$. Similarly, for each $a_i \\in \\mathcal{A}_p$, WH-word in the query is replaced with $a_i$ to form the hypothesis $\\mathcal{H}_i$. The premise-hypothesis pair $(\\mathcal{P},\\mathcal{H}_i)$, is then processed using inferencing models to check for Entailment and Contradiction. Entailment implies that the answer $a_i$ is in the correct answer set $\\mathcal{A}_c$, else discarded as wrong answer. The set $\\mathcal{A}_c$ is finally given as the result for the original natural language query.\n\n\n\nA path between two entities $e_i$ and $e_j$ in $\\mathcal{K}$ is an \\textit{ordered set} of triples $\\mathcal{P}_{ij}$. If they are directly connected, then $P_{ij}$ contains only a single triple, $\\{(e_i,r_{ij},e_j)\\}$, where $r_{ij}$ stands for the relationship between $e_i$ and $e_j$. If the length of the path connecting them is $n$, then $P_{ij}$ will contain $n$ such triples. Since there can be multiple paths between two entities in $\\mathcal{K}$, all paths upper bounded by a hyper parameter for length are considered in the premise path set, $\\mathcal{P}$.  After forming the set of connected paths that form the premise, we process the premise in two different ways. \n\nEach fact(a single triplet in single path) in the premise is converted to natural language form using templates. For example \\textit{(Donald Trump,presidentOf,USA)} is converted to \\textit{Donald Trump is the president of the USA}. These type of templates can be easily generated for relationship types in $\\mathcal{K}$. In this method, the premise is a set of sentences and hypothesis is a single sentence. This premise-hypothesis pairs dataset is processed using ESIM(Enhanced LSTM model)\\citep{chen2016enhanced,lan2018neural} model akin to standard natural language inference. For the entities and relationship tokens, the TransE~\\citep{bordes2013translating} embeddings of $\\mathcal{K}$ are used and for the other template tokens, pre-trained GloVe~\\citep{pennington2014glove} embeddings were used. Our model does not assume any specific KG embeddings or its properties. For the current work, we have used the simplest of the lot (TransE). We use the TransE model implementation from OpenKE\\citep{han2018openke} to generate 300 dimensional embeddings for entities and relations in the KG. \n\n\n\n\\paragraph{Hierarchical Recurrent Path Encoder Model(HRPE):}\nThe conversion of premise path triplets was done to align the problem to that of natural language inference. But we realized that may not be the best approach to create a good representation for the premise, for one the NLI models were discarding the granularity of a single path, which is important in the final inference. Also the sequence lengths were getting large, forcing the model to discard many samples from processing. Hence we came up with Hierarchical Recurrent Path Encoder Model(HRPE). We first encoded each path $P_j \\in \\mathcal{P}$ using an LSTM, equation~\\ref{eq:path}. The hypothesis $\\mathcal{H}_i$ was encoded to $H$, using a bi-LSTM, equation~\\ref{eq:hypo}. This was used to generate attention over each path encodings, and creating a single path encoding($p$) based on the attention weights, equation~\\ref{eq:path2}. This allowed the model to focus on the premise component important from the hypothesis perspective,i.e., the correct path between query and answer entity. The path encodings were also further encoded by another LSTM, in hierarchical fashion to generate $P$, equation~\\ref{eq:path2}. This was required to allow interaction between paths for the final inference. The concatenated vector $H:p:P$ was fed through a Feed Forward network to do the classification prediction, equations~\\ref{eq:class},~\\ref{eq:loss}. \n\n\\begin{equation}\n\\forall  P_j \\in \\mathcal{P},\\  p_j = (p_{j1}, \\cdots, p_{jn}) \\leftarrow \\mathrm{LSTM}(P_j)\n\\label{eq:path}\n\\end{equation}\n\n\\begin{equation}\nH = (h_{i1}, \\cdots, h_{in}) \\leftarrow \\mathrm{bi-LSTM}(\\mathcal{H}_i)\n\\label{eq:hypo}\n\\end{equation}\n\n\n\\begin{equation}\np = \\Sigma_j \\frac{H.(p_j)^T}{\\Sigma_k H.(p_k)^T} (p_{j1}, \\cdots, p_{jn})\n\\label{eq:path2}\n\\end{equation}\n\n\\begin{equation}\nP = (P_1, \\cdots, P_n) \\leftarrow \\mathrm{LSTM}(p_1,\\cdots,p_j)\n\\label{eq:path3}\n\\end{equation}\n\nThe aggregated vector $\\mathcal{A} = H:p:P$ was then used to do the\nfinal label prediction of entailment or contradiction, \n\\begin{equation}\n\\mathrm{label} = \\mathrm{softmax}(W^T \\mathcal{A} )),\n\\label{eq:class}\n\\end{equation}\nand\n\\begin{equation}\n\\mathrm{loss} = C(\\mathrm{label}_{\\mathrm{gold}}, \\mathrm{label}),\n\\label{eq:loss}\n\\end{equation}\n    where $W$ is the model parameter and $C(p,q)$ denotes the\n    cross-entropy between $p$ and $q$. We minimized this loss averaged\n    across the training samples, to learn the various model parameters\n    using Stochastic Gradient Descent~\\citep{stochastic-gradient-tricks} algorithm. \n\nOur complete solution framework consists of several components for working with knowledge graphs, extraction and linking of entities between queries and knowledge graphs, converting queries to a multiple choice query-answer format followed by conversion to premise-hypothesis-label format and finally training the inference models on this data to predict query answers. The whole framework and dependency between components is shown in Figure~\\ref{fig:framework}.\n\n",
                "subsection 2.1": {
                    "name": "Domain Adaptability of the Model",
                    "content": "\n\n\n\nIn our approach and proposed model, two kinds of tokens are input to the model, the entity-relationship tokens and template/question word tokens. We used generated TransE~\\citep{bordes2013translating} KG embeddings for entities and relationship tokens while pre-trained Glove ~\\citep{pennington2014glove} 300D embeddings for rest of the words in the vocabulary. The template/question word tokens are very few in number compared to entity relationship tokens. The KG embeddings can be generated in an unsupervised fashion given a new Knowledge Graph. The concept of inferencing has less gravity on the domain and the learnings are thus transferable across domains. Hence, we postulate that our approach can be used to create QA models for domains where KG is available, but have fewer QA training data. As such, the models trained on one source domain QA training data $\\mathcal{D}_s$, can be used with minimal fine tuning on new target domain $\\mathcal{D}_t$. A transformation of the KG embeddings between source and target domain can also be learned in an unsupervised manner and can be fine tuned with fewer training data from the target domain, equation~\\ref{eq:domain1}. This will help in better results for domain adaptation.\n\n\\begin{equation}\n f( W, E(\\mathcal{K}_s)) = E(\\mathcal{K}_t)\n \\label{eq:domain1}\n\\end{equation}\n\nHere, $W$ are the model parameters for the transformation function $f$(learned unsupervised and then fine tuned), $E$ is the KG embeddings generation function and $\\mathcal{K}_s, \\mathcal{K}_t$ are source and target domain KGs respectively.\n\n\\begin{equation}\n\\mathcal{I}_s \\leftarrow \\mathrm{Train}(\\mathcal{D}_s,E(\\mathcal{K}_s)))\n\\end{equation}\n\nHere, $\\mathcal{I}_s$ is the trained inference model from the source domain. For creating the model on the target domain $\\mathcal{I}_t$, we use the trained model from the source domain, learned transformation function for KG embeddings and fine tuned training data from target domain.\n\n\\begin{equation}\n\\mathcal{I}_t \\leftarrow \\mathrm{Train}(\\mathcal{D}_t,f(W,E(\\mathcal{K}_s))) : \\mathcal{I}_s\n\\end{equation}\n\n"
                }
            },
            "section 3": {
                "name": "Experiments \\& Results",
                "content": "\n\\label{sec:exp}\n\n\n\\paragraph{Dataset and Preprocessing:}\n\\begin{itemize}\n    \n\\item MetaQA\n\nWe used MetaQA dataset~\\citep{zhang2018variational} anchored over a movie knowledge graph having complex queries. The dataset contains 400K QA pairs, 100k of which can be answered using single path length, 150K require paths of lengths 2 in premise and 150K require paths of length 3 in premise.The anchored WikiMovies\\footnote{https://research.fb.com/downloads/babi/} knowledge graph contains over 1 million triples having 38340 entities and 6 relationship types covering directors,writers,actors,languages,release years and languages associated with movies. For each query, we populated a total of $n$ answer choices including the set of correct and incorrect answers. The premise-hypothesis pairs were then labelled as entailment for the correct answers and contradiction for the wrong answers. An illustrative example of the generated PHL triplet is given in Table \\ref{table:example}. Premise-Hypothesis-Label(PHL) triplet data was then further split to test and train the inference models. Certain questions were discarded from the original dataset, since the sequence length for processing premise was exorbitantly high. The dataset statistics after cleaning up are given in the Table~\\ref{table:stats}. Entity identification from query was done using bi-LSTM-CRF model\\citep{lample2016neural}. The identified entities were linked to the entities in the knowledge graph using Jaro Winkler~\\citep{deza2009encyclopedia}similarity measure. \n\\item PathQuestions\n\nPathQuestions (PQ)~\\citep{zhou2018interpretable} is another smaller multi-hop question answering dataset developed from a subset of Freebase Knowledge Graph of 2 million triples having 2215 entities and 14 relationships. Each path question consists of natural language question related to a topic entity whose answer can be found in the KG by following the correct answer path. The PQ dataset contains a total of 1908 and 5198 questions for paths of lengths 2 and 3 respectively which we use for our experiments. We generate the set of correct and incorrect answers followed by creation of PHL triplets in the same way as mentioned above. The dataset statistics for the same are shown in Table \\ref{table:stats}. We follow the same dataset split as mentioned in ~\\citep{zhou2018interpretable}.\n\n\\end{itemize}\n\n\\paragraph{Implementation, Training \\& Hyper-parameters:}   The model was implemented in TensorFlow~\\citep{tensorflow2015-whitepaper} - an open-source library for numerical computation for Deep Learning. All experiments were carried on a Dell Precision Tower 7910 server with Nvidia Titan X GPU. The models were trained using the Adam's Optimizer~\\citep{kingma2014adam} in a stochastic gradient descent~\\citep{stochastic-gradient-tricks} fashion. We used batch normalization~\\citep{ioffe2015batch} while training and dropout for regularization. The ESIM model gave best results when hidden layer of LSTM was of size 300, equivalent to the word embedding dimension. On increasing the sequence length for ESIM model, the model became too heavy and was unable to process all the questions in the 3-length-path dataset having long premises. For HRPE model, the LSTM hidden state size of 150 gave the best performance and a sequence length of 20 and 250 were used in the hierarchical levels of LSTM.\n\n\n \\paragraph{Metrics:} The classification accuracy of each model was calculated as usual based on the  predicted class and the gold label for each PHL sample. For QA accuracy, the set of predicted answers are populated based on the inference prediction with the hypothesis created from potential answer set. The aggregated correct answers for each question are matched with the gold labeled answers for each query and if both the sets match, then a question is considered to have been answered as correct, otherwise incorrect. \n \n\n\n",
                "subsection 3.1": {
                    "name": "Results",
                    "content": "\n\nThe results for the experiments with the two inference models for 2 and 3 length path QA pairs are presented in Table \\ref{table:res}. We have obtained state of the art accuracy on both 2 and 3 path length QA with both ESIM and HRPE models, beating the variational model(VRN)\\citep{zhang2018variational} published along with the MetaQA dataset.Table \\ref{table:res} presents accuracy calculated via Hit@1 metric used in the VRN model while we use an Exact Answer Set Match metric for the QA accuracy calculation. Our models clearly perform better, especially in the 3 length path case. However with the smaller PQ dataset, we have less accuracy compared to the baseline IRN \\citep{zhou2018interpretable} model for 2-length path while our HRPE model performs better in the 3-length path case.\nWe also present domain adaptability results in Table \\ref{table:domain} by pre-training both HRPE and ESIM models with (source domain) MetaQA dataset and fine-tune them on the PQ dataset (target domain). The results indicate the feasibility of our approach across different domain KGs although the ESIM model is better suited to domain adaptability compared to the HRPE model. \n\n%\\begin{table*}[]\n%\\centering\n%\\begin{tabular}{|l|l|l|l|l|l|}\n%\\hline\n%\\begin{tabular}[c]{@{}l@{}}Data\\\\ Type\\end{tabular} & Model & \\begin{tabular}[c]{@{}l@{}}Classification\\\\ Accuracy\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}QA \\\\ Accuracy\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Variational\\\\ Model\\\\ Accuracy\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}\\#Test QA\\\\ Pairs\\end{tabular} \\\\ \\hline\n%\\multirow{2}{*}{2-length path} & ESIM & 99.8 & 99.29 & 91.9 & 10045 \\\\ \\cline{2-6} \n %& HRPE & 98.4 & 95 & 91.9 & 10045 \\\\ \\hline\n%\\multirow{2}{*}{3-length path} & ESIM & 97.1 & 89.42 & 59.7 & 2665 \\\\ \\cline{2-6} \n %& HRPE & 97.66 & 84.62 & 58 & 4969 \\\\ \\hline\n%\\end{tabular}\n%\\caption{Results from inference models along with benchmark comparison {[}Variational Model Accuracy\\citep{zhang2018variational}{]}}\n%\\label{table:res}\n%\\end{table*}\n\n\n% \\usepackage{multirow}\n\n\n\n\n\n%\\begin{table}[h]\n%\\centering\n%\\begin{tabular}{|l|l|l|l|}\n%\\hline\n%\\textbf{\\begin{tabular}[c]{@{}l@{}}Question\\\\ Type\\end{tabular}} &  & \\textbf{Questions} & \\textbf{PHLs} \\\\ \\hline\n%2-length & \\begin{tabular}[c]{@{}l@{}}Train\\\\ Test\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}80024\\\\ 10045\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}320096\\\\ 40180\\end{tabular} \\\\ \\hline\n%3-length & \\begin{tabular}[c]{@{}l@{}}Train\\\\ Test\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}41575\\\\ 4969\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}166300\\\\ 19876\\end{tabular} \\\\ \\hline\n%\\end{tabular}\n%\\caption{Data statistics after conversion from MetaQA}\n%\\label{table:statsold}\n%\\end{table}\n\n\n\n\n\n\n\n\n\n\nAs variational model(VRN)\\citep{zhang2018variational} reported Hit@1 metric, we performed a model ablation study by modifying the number of potential answers considered per question. We vary the potential answer choices from 4 to 24 to see the effect on inference models accuracy  Figures~\\ref{fig:ablation}. Classification accuracy of the inference models is not affected much but we observe a  drop in QA accuracy in case of 3 length path case while the 2 length path cases still performs better. Also, the HRPE model can be seen less inclined towards loss in accuracy compared to the ESIM model. Even at 24 answers considered, our numbers are better than VRN model. Our model can discard more wrong answers effectively, as wrong answer entities have lesser chance of good premise connectivity to the entities in the question.\n\n\n"
                }
            },
            "section 4": {
                "name": "Related Work",
                "content": "\n\\label{sec:related}\n\n\\paragraph{Inferencing over Knowledge Graphs:} Typically in literature, Inferencing over Knowledge Graphs is an entirely different problem. Knowledge Graphs are largely constructed from text using statistical techniques ~\\citep{mitchell2015never,niu2012deepdive}. Such Knowledge Graphs are generally incomplete. Inferencing over KGs generally refers to predicting the missing links in these incomplete KGs, also referred in literature as KG Completion solved using Embedding and Path ranking based models \\citep{xiong2017deeppath,das2017go,chen2018variational}.\nThese methods perform inference using probabilistic or reinforcement learning models trained over existing KG triples and predict links missing from the graph by going over similar paths and creating new relations for non-neighbor entity pairs. Our problem is entirely different, we assume the KG is complete and then do Question Answering on KG, reasoned over multiple facts available. \n\n\n\\paragraph{Question Answering over Knowledge Graphs:} One major approach for natural language querying on Knowledge Graphs or other structured sources is converting the query into a structured query language and firing it on the KG. Existing models use a parse tree to convert query to a structured form ~\\citep{liang2016learning}. Latest works treat this as a machine translation problem, with source language as the natural language and target language as the structured query language~\\citep{wang2015building},~\\citep{zhong2017seq2sql}, ~\\citep{xu2017sqlnet},~\\citep{mccann2018natural}. Our approach is different, since we do not convert the query into intermediate structured form and generate the answer directly from the KG. The closest work in literature, with respect to what we proposed here is Variational Reasoning Network\\citep{zhang2018variational}. It is a probabilistic inference model for answering movie based questions that require reasoning over KG triples for answering them. We use the dataset published in this work and their results as benchmark. However, our approach and model is completely different from this work. The PathQuestions~\\citep{zhou2018interpretable} dataset that we use in our experiments had a benchmark IRN model wherein annotations are used for every question with correct relation chain or path, topic and answer entities already known while we derive the paths between question and answer entity from the KG in our approach.  Question Answering over text has been attempted from inference perspective in works like KG2~\\citep{zhang2018kg}. However this is being done over text, and their model uses KG created from text hypothesis and premise to do the inferencing. In contrast, we are doing Question Answering over KG and our model is completely different.\n\n\nThere are plethora of question answering datasets available in which questions are answered by retrieving information from a corpus. We particularly worked with complex natural language queries in our study. Such non-factoid questions are not easily answered by a retrieval engine and require some reasoning model that can combine multiple facts from a KG leading to the correct answers.~\\citep{talmor2018web} provide a set of complex web questions based on the Freebase KG and a seq-to-seq model to obtain answers to these questions via decomposition to simple question answers assuming the simple questions can be answered via a retrieval engine. WikiHop \\citep{welbl2018constructing} and HotPotQA\\citep{yang2018hotpotqa} are based on questions which can be answered by reading multiple Wikipedia pages. These datasets follow a reading comprehension based question answering task requiring reasoning over multiple text documents. ARC \\citep{clark2018think} is another multiple-choice science based question answering dataset which requires complex reasoning from scientific knowledge text corpus to answer questions prepared with the help of school instructors of grades 6-8. Scitail\\citep{khot2018scitail} is an entailment dataset developed from Science based QA and has been shown to be useful for answering the complex multiple choice ARC science questions. Most of these works are  based on reasoning or retrieval over a text corpus.\\footnote{Our work stopped in late 2019. And relevant literature post that is not considered or cited here.}\n\n\n"
            },
            "section 5": {
                "name": "Conclusion \\& Future Work",
                "content": "\n\\label{sec:future}\n\nWe presented a simple approach for converting Question Answering over Knowledge Graphs into an inference problem. Leveraging existing models of Natural Language Inference, as well as proposing a new model, we have shown state of the art results on MetaQA dataset. Our model is simple and amenable for domain adaptation, to solve the problem of QA over KGs from newer domains with lesser training data. To the best of our knowledge, this is the first attempt in treating QA over KG as an inferencing problem and the results are exciting. The work is preliminary and provides a good starting point for discussion and further research. \n\nIn the related work, we mentioned about KG completion problem and there is a considerable volume of work in that space. Although we positioned the work as QA reasoned over known facts from the KG, QA over KG can involve reasoning over unknown facts as well. We can leverage the models for KG completion and attempt to solve the problem of QA over KG reasoned over both known and unknown facts. The problem area is relatively new, and datasets require correspondence between the QA training data and the Knowledge Graph. There are not many datasets available to quantify the results of domain adaptation yet. We are currently working on creating a new dataset for KG question answering based on ComplexWebQA dataset~\\citep{talmor2018web} which has Freebase~\\citep{bollacker2008freebase} as the anchored KG. The creation of this dataset is essential to test the model capabilities to reason over larger fact sets since the existing MetaQA dataset is simple in that regard. We have considered the potential answers(correct and incorrect) from the model and generated them in the preprocessing step. Inclusion of generating potential answers using a mechanism such as attention over the graph, within the model, is an adaptation that can be taken up. We hope our work and initial results will inspire the community to conduct further investigation in this area.\n\n\\newpage\n%\\bibliographystyle{splncs04}\n\\bibliographystyle{acl_natbib}\n\\bibliography{IAQAKG}\n\n\n"
            }
        },
        "tables": {
            "table:example": "\\begin{table}\n\\centering\n\\begin{tabular}{|l|l|l|} \n\\hline\n\\multicolumn{3}{|l|}{Question: Which person directed the films acted by the actors in Kid Millions?}       \\\\ \n\\hline\n\\multicolumn{3}{|l|}{Potential Answers: David Butler, Douglas Sirk, Tom Hooper, Franck Khalfoun}                \\\\ \n\\hline\nPremise  & Hypothesis     & Label  \\\\ \n\\hline\n\\begin{tabular}[c]{@{}l@{}}Kid Millions starred actors Eddie Cantor and \\\\ Thank Your Lucky Stars starred actors Eddie Cantor and \\\\ Thank Your Lucky Stars directed by David Butler\\\\ \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}David Butler directed the films \\\\acted by the actors in Kid Millions\\end{tabular}     & 0      \\\\ \n\\hline\n\\begin{tabular}[c]{@{}l@{}}Kid Millions release year 1934 and\\\\Imitation of Life release year 1934 and\\\\Imitation of Life directed by Douglas Sirk\\end{tabular}                                & \\begin{tabular}[c]{@{}l@{}}Douglas Sirk directed the films \\\\acted by the actors in Kid Millions \\end{tabular}    & 1      \\\\ \n\\hline\n\\begin{tabular}[c]{@{}l@{}}Kid Millions release year 1934 and\\\\Les Miserables release year 1934 and\\\\Les Miserables directed by Tom Hooper\\end{tabular}                                      & \\begin{tabular}[c]{@{}l@{}}Tom Hooper directed the films \\\\acted by the actors in Kid Millions \\end{tabular}      & 1      \\\\ \n\\hline\n\\begin{tabular}[c]{@{}l@{}}Kid Millions release year 1934 and \\\\ Maniac release year 1934 and \\\\ Maniac directed by Franck Khalfoun\\\\ \\end{tabular}                                                  & \\begin{tabular}[c]{@{}l@{}}Franck Khalfoun directed the films \\\\acted by the actors in Kid Millions \\end{tabular} & 1      \\\\\n\\hline\n\\end{tabular}\n\\caption{Sample example query conversion to NLI format: Named entity in the question (Kid Millions) is linked to the correct answer entity generating an Entail (label 0) PHL triplet while incorrect potential answers found at n-path length away from the question entity in the KG are used to generate Contradiction (label 1) PHL triplet.}\n\\label{table:example}\n\\end{table}",
            "table:res": "\\begin{table}\n\\centering\n\n\\begin{tabular}{|l|l|l|l|l|l|l|l|} \n\\hline\n\\multicolumn{2}{|l|}{Dataset Type}                                                                                       & \\multicolumn{3}{l|}{MetaQA}                                   & \\multicolumn{3}{l|}{PQ}                                         \\\\ \n\\hline\n\\begin{tabular}[c]{@{}l@{}}Path length\\\\\\end{tabular} & Model                                                            & ESIM  & HRPE  & \\begin{tabular}[c]{@{}l@{}}VRN \\end{tabular} & ESIM  & HRPE   & \\begin{tabular}[c]{@{}l@{}}IRN \\end{tabular}  \\\\ \n\\hline\n\\multirow{3}{1cm}{two length path}           &\\begin{tabular}[c]{@{}l@{}}Classification\\\\Accuracy\\end{tabular} & 99.8  & 98.4  & {-}                                             & 73.3  & 77.75  & {-}                                              \\\\ \n\\cline{2-8}\n                                                      & \\begin{tabular}[c]{@{}l@{}}QA\\\\Accuracy\\end{tabular}             & 99.29 & 95    & 91.9                                          & 51.31 & ~56.54 & 96                                             \\\\ \n\\cline{2-8}\n                                                      & \\#Test QA                                                        & 10045 & 10045 & 10045                                         & 191   & 191~   & 191                                            \\\\ \n\\hline\n\\multirow{3}{1cm}{three length path}                        & \\begin{tabular}[c]{@{}l@{}}Classification\\\\ Accuracy\\end{tabular} & 97.1  & 97.71 & -                                             & 92.27 & 94.18  & -                                              \\\\ \n\\cline{2-8}\n                                                      & \\begin{tabular}[c]{@{}l@{}}QA\\\\Accuracy\\end{tabular}             & 89.43 & 84.77 & 58/59.7                                       & 84.93 & 88.96  & 87.7                                           \\\\ \n\\cline{2-8}\n                                                      & \\#Test QA                                                        & 2665  & 4969  & 2665/4969                                     & 272   & 335    & 520                                            \\\\\n\\hline\n\\end{tabular}\n\\caption{Results from inference models along with benchmark models comparison}\n\\label{table:res}\n\\end{table}",
            "table:stats": "\\begin{table}\n\\centering\n\\begin{tabular}{|l|l|l|l|l|l|} \n\\hline\n\\multicolumn{2}{|l|}{\\textbf{Dataset }}                                                                                            & \\multicolumn{2}{l|}{\\textbf{MetaQA }}                                                                            & \\multicolumn{2}{l|}{\\textbf{PQ}}                                                                       \\\\ \n\\hline\n\\begin{tabular}[c]{@{}l@{}}\\textbf{Question}\\\\\\textbf{ Type} \\end{tabular} &                                                       & \\textbf{Questions}                                     & \\textbf{PHLs}                                           & \\textbf{Questions}                                & \\textbf{PHLs}                                      \\\\ \n\\hline\n2-length                                                                   & \\begin{tabular}[c]{@{}l@{}}Train\\\\ Test \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}80024\\\\ 10045 \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}320096\\\\ 40180 \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}1526\\\\191\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}3052\\\\382\\end{tabular}  \\\\ \n\\hline\n3-length                                                                   & \\begin{tabular}[c]{@{}l@{}}Train\\\\ Test \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}41575\\\\ 4969 \\end{tabular}  & \\begin{tabular}[c]{@{}l@{}}166300\\\\ 19876 \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}2730\\\\335\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}5460\\\\670\\end{tabular}  \\\\\n\\hline\n\\end{tabular}\n\\caption{Data statistics after conversion from MetaQA and PQ}\n\\label{table:stats}\n\n\\end{table}",
            "table:domain": "\\begin{table}\n\\centering\n\\begin{tabular}{|l|l|l|l|l|l|} \n\\hline\n\\multirow{2}{*}{Path Length} & \\multirow{2}{*}{Model} & \\multicolumn{2}{l|}{\\begin{tabular}[c]{@{}l@{}}Without Domain\\\\Adaptation \\end{tabular}}                                & \\multicolumn{2}{l|}{\\begin{tabular}[c]{@{}l@{}}With Domain\\\\Adaptation \\end{tabular}}                                    \\\\ \n\\cline{3-6}\n                             &                        & \\begin{tabular}[c]{@{}l@{}}Classification\\\\Accuracy\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}QA\\\\Accuracy\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Classification\\\\Accuracy\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}QA\\\\Accuracy\\end{tabular}  \\\\ \n\\hline\n\\multirow{2}{*}{2-length}    & ESIM                   & 73.3                                                             & 51.31                                                & 76.7                                                             & 58.64                                                 \\\\ \n\\cline{2-6}\n                             & HRPE                   & 77.75                                                            & 56.54                                                & 78.27                                                            & 56.02                                                 \\\\ \n\\hline\n\\multirow{2}{*}{3-length}    & ESIM                   & 92.27                                                            & 84.93                                                & 93.9                                                             & 87.87                                                 \\\\ \n\\cline{2-6}\n                             & HRPE                   & 94.18                                                            & 88.96                                                & 94.33                                                            & 88.66                                                 \\\\\n\\hline\n\\end{tabular}\n\\caption{Results from Domain Adaptation on PQ dataset}\n\\label{table:domain}\n\\end{table}"
        },
        "figures": {
            "fig:framework": "\\begin{figure*}\n\\includegraphics[width=\\textwidth,height=0.4\\textheight]{{Model.png}}\n\\caption{Proposed solution framework for the problem}\n\\label{fig:framework}\n\\end{figure*}",
            "fig:ablation": "\\begin{figure*}\n    \\centering\n    \\subfigure[Classification Accuracy vs Number of Answers]{{\\includegraphics[width=0.45\\textwidth]{Classification.png} }}%\n    \\hfill\n    \\subfigure[QA Accuracy vs Number of Answers]{{\\includegraphics[width=0.45\\textwidth]{QA.png} }}%\n    \\hfill\n    \\caption{Effect on model accuracy (classification and QA) on varying Number of potential answers per question for 2 and 3 length path QA. ESIM-2 and ESIM-3 refer to the NLI model for 2 and 3 path length QA while HRPE-2 and HRPE-3 refer to the path encoder model}%\n    \\label{fig:ablation}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\forall  P_j \\in \\mathcal{P},\\  p_j = (p_{j1}, \\cdots, p_{jn}) \\leftarrow \\mathrm{LSTM}(P_j)\n\\label{eq:path}\n\\end{equation}",
            "eq:2": "\\begin{equation}\nH = (h_{i1}, \\cdots, h_{in}) \\leftarrow \\mathrm{bi-LSTM}(\\mathcal{H}_i)\n\\label{eq:hypo}\n\\end{equation}",
            "eq:3": "\\begin{equation}\np = \\Sigma_j \\frac{H.(p_j)^T}{\\Sigma_k H.(p_k)^T} (p_{j1}, \\cdots, p_{jn})\n\\label{eq:path2}\n\\end{equation}",
            "eq:4": "\\begin{equation}\nP = (P_1, \\cdots, P_n) \\leftarrow \\mathrm{LSTM}(p_1,\\cdots,p_j)\n\\label{eq:path3}\n\\end{equation}",
            "eq:5": "\\begin{equation}\n\\mathrm{label} = \\mathrm{softmax}(W^T \\mathcal{A} )),\n\\label{eq:class}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n\\mathrm{loss} = C(\\mathrm{label}_{\\mathrm{gold}}, \\mathrm{label}),\n\\label{eq:loss}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n f( W, E(\\mathcal{K}_s)) = E(\\mathcal{K}_t)\n \\label{eq:domain1}\n\\end{equation}",
            "eq:8": "\\begin{equation}\n\\mathcal{I}_s \\leftarrow \\mathrm{Train}(\\mathcal{D}_s,E(\\mathcal{K}_s)))\n\\end{equation}",
            "eq:9": "\\begin{equation}\n\\mathcal{I}_t \\leftarrow \\mathrm{Train}(\\mathcal{D}_t,f(W,E(\\mathcal{K}_s))) : \\mathcal{I}_s\n\\end{equation}"
        }
    }
}