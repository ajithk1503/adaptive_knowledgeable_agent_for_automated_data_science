{
    "meta_info": {
        "title": "Enhancing Link Prediction with Fuzzy Graph Attention Networks and  Dynamic Negative Sampling",
        "abstract": "Link prediction is crucial for understanding complex networks but traditional\nGraph Neural Networks (GNNs) often rely on random negative sampling, leading to\nsuboptimal performance. This paper introduces Fuzzy Graph Attention Networks\n(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative\nsampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)\nsystematically selects high-quality negative edges based on fuzzy similarities,\nimproving training efficiency. FGAT layer incorporates fuzzy rough set\nprinciples, enabling robust and discriminative node representations.\nExperiments on two research collaboration networks demonstrate FGAT's superior\nlink prediction accuracy, outperforming state-of-the-art baselines by\nleveraging the power of fuzzy rough sets for effective negative sampling and\nnode feature learning.",
        "author": "Jinming Xing, Ruilin Xing, Chang Xue, Dongwen Luo",
        "link": "http://arxiv.org/abs/2411.07482v3",
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ],
        "additionl_info": "Accepted to ISMSI'25"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nLink prediction has emerged as a crucial task in network analysis with extensive applications across diverse domains. In medical sciences, it aids in predicting protein-protein interactions and drug-target associations; in financial systems, it helps detect fraudulent transactions and assess credit risks; and in chemistry, it facilitates the discovery of novel molecular structures and chemical reactions. The ability to accurately predict potential connections in these complex networks has significant implications for scientific advancement and practical applications.\n\nGraph Neural Networks (GNNs) have demonstrated remarkable success in link prediction tasks, primarily due to their inherent capability to capture and process structural information in graph-structured data. However, a critical limitation in existing GNN-based approaches lies in their negative sampling methodology. Contemporary methods typically employ random sampling strategies to select negative edges, disregarding the rich semantic and structural information encoded in node representations. This oversight significantly hampers the training process, resulting in slower convergence rates and suboptimal model performance. An ideal negative sampling mechanism should not only leverage node embeddings effectively but also adaptively select high-quality negative samples based on the model's current state, ensuring both dynamic responsiveness and sampling accuracy.\n\nWhile various methodologies have been explored to enhance link prediction accuracy, the potential of fuzzy rough sets\u2014a mathematical framework for measuring fuzzy relations and handling uncertainty\u2014remains largely unexplored in the context of GNNs and link prediction. This theoretical framework offers unique advantages in capturing imprecise relationships and handling ambiguous data structures, making it particularly suitable for network analysis tasks.\n\nTo address these limitations and leverage the untapped potential of fuzzy rough sets, we propose a novel fuzzy rough sets-based negative sampling strategy called Fuzzy Negative Sampling (FNS). This approach systematically evaluates candidate negative edges through their fuzzy lower approximation values, selecting the top K candidates as negative training instances. Furthermore, we introduce Fuzzy Graph Attention Network (FGAT), an enhanced graph neural architecture designed to aggregate neighboring node information in a more robust and effective manner.\n\nThe main contributions of this work can be summarized as follows:\n\\begin{itemize}\n    \\item We introduce FNS, a novel negative sampling framework that leverages fuzzy rough sets theory to identify high-quality negative edges, significantly improving the effectiveness of the training process in link prediction tasks.\n    \\item We propose FGAT, an innovative graph attention network that incorporates fuzzy rough set principles to achieve more robust and discriminative node representations.\n    \\item We conduct comprehensive experiments across two real-world datasets, demonstrating the effectiveness of our proposed framework.\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\textbf{Graph Neural Networks} have demonstrated remarkable versatility across various graph-based learning tasks. In node classification, seminal works like GraphSAGE \\cite{hamilton2017inductive} and Graph Attention Networks (GAT) \\cite{velivckovic2017graph} have established foundational approaches for learning node representations through neighborhood aggregation. GCN \\cite{kipf2016semi} introduced convolutional operations on graphs, enabling efficient feature propagation across network structures. For graph classification tasks, hierarchical pooling mechanisms have been developed, with DiffPool \\cite{ying2018hierarchical} and TopKPool \\cite{diehl2019edge} proposing learnable strategies to generate graph-level representations. In the context of link prediction, SEAL \\cite{zhang2018link} pioneered the use of local subgraphs for edge existence prediction, while VGAE \\cite{kipf2016variational} employed variational autoencoders for learning edge formation patterns. Recent advances include NGNN \\cite{zhang2021nested}, which introduces neural architecture improvements specifically designed for link prediction tasks.\n\n\\textbf{Fuzzy rough sets theory}, initially proposed by Dubois and Prade \\cite{dubois1990rough}, has evolved into a powerful framework for handling uncertainty and imprecision in data analysis. In feature selection, Jensen and Shen \\cite{jensen2004fuzzy} developed fuzzy-rough attribute reduction algorithms that significantly outperform traditional approaches in identifying relevant features while maintaining information fidelity. The application of fuzzy rough sets in medical diagnosis has been exemplified by works such as \\cite{xing2022weighted}, where they effectively handle the inherent uncertainty in patient data for more accurate disease classification. For uncertainty measurement, the framework has been extensively studied in theoretical works by \\cite{gao2022parameterized} and \\cite{ye2021novel}, establishing mathematical foundations for quantifying various types of uncertainty in data relationships. Recent developments include hybrid approaches combining fuzzy rough sets with deep learning and applications in big data analytics \\cite{ji2021fuzzy} , demonstrating the framework's adaptability to modern computational challenges.\n% Particularly noteworthy is the work by [x] that established connections between fuzzy rough sets and graph-theoretic concepts, though their application to graph neural networks remains unexplored.\n\nExcept the innovative applications of Graph Neural Networks and fuzzy rough set theory in handling complex data and multi-task learning, against the backdrop of rapid advancements in machine learning, applications like knowledge graph technology in intelligent question-answering systems have become a key area of development for graph learning methods, effectively integrating multiple data sources to support flexible knowledge processing \\cite{lin2021research}. Similarly, multi-model integration technology applied to automated generation systems has significantly enhanced content generation flexibility and quality, providing valuable insights for representation learning based on graph data \\cite{yang2021research}. In network security, Graph Neural Networks have demonstrated strong generalization capabilities in supporting Botnet detection through machine learning, precisely identifying abnormal behaviors and enhancing network defense levels \\cite{yang2022botnet}.\n\nIn recent years, the scope of machine learning applications has continuously expanded \\cite{cheng24patch,cheng25unifying,cheng22deep,cheng22estimation,xing24traffic,cheng23shapnn}, especially in image processing and VR fields. Blockchain-enhanced image retrieval systems, for example, have brought revolutionary improvements in data security and retrieval efficiency \\cite{zhao2022image}. In VR and robotic interaction, AI-vision-powered intelligent systems have explored new methods for balancing real-world interaction and virtual immersion, making human-computer interaction more natural and seamless \\cite{yang2023balancing}. Furthermore, the integration of fuzzy rough set theory with deep learning has also achieved breakthroughs in traffic data prediction, enabling more accurate short-term forecasting through multi-source data fusion, effectively adapting to the dynamic demands of complex data environments \\cite{deng2021short}. Overall, these technological innovations not only showcase the broad applicability of graph learning methods and fuzzy rough sets across different task scenarios but also reinforce their theoretical and practical value in big data and intelligent applications.\n\n"
            },
            "section 3": {
                "name": "Methodology",
                "content": "\nFigure \\ref{fig:framework} illustrates our proposed framework, which consists of two main components:\n\\begin{itemize}\n    \\item \\textbf{Fuzzy Negative Sampling:} A mechanism that selects high-quality negative edges based on fuzzy similarities, where negative edges with high fuzzy similarity are dynamically selected for the FGAT framework's training.\n    \\item \\textbf{FGAT Convolution Layer:} A specially designed layer for effective neighbor node information aggregation. Multiple FGAT convolution layers are stacked to capture multi-hop information.\n\\end{itemize}\n\nIn the following sections, we first detail the computation of fuzzy similarities using fuzzy rough sets for high-quality negative edge selection. Subsequently, we elaborate on the FGAT architecture, followed by a comprehensive framework summary.\n\n\n",
                "subsection 3.1": {
                    "name": "Fuzzy Negative Sampling",
                    "content": "\nA fuzzy information system is defined as a tuple $(U,A,V,f)$, where $U$ represents a non-empty finite set of samples, $A$ denotes the finite set of sample attributes, $V$ represents the domain of all attributes in $A$, expressed as $V=\\bigcup_i V_i$ where $V_i$ is the domain of attribute $i$, and $f$ is a mapping function $U\\times A\\rightarrow V$ \\cite{xing2022weighted}.\n\nFor an attribute set $B\\subseteq A$ and a fuzzy equivalence relation $R$, we can compute a coverage of the universe $U$. For a sample $x$, we denote its coverage under the fuzzy equivalence relation $R$ as $[x]_R$. The membership of a sample $y$ to the coverage $[x]_R$ is defined as $[x]_R(y)=R(x,y)$, where $R(x,y)$ quantifies the similarity between samples $x$ and $y$ under relation $R$. For any sample $x\\in U$ and subset $X\\subseteq A$, the fuzzy lower and upper approximations of sample $x$ to $X$ are defined as \\cite{xing2022weighted}:\n\\begin{equation}\n    \\begin{split}\n        \\underline{R_S}X(x)&=\\inf_{y\\in U}S(N(R(x,y)),X(y)),\\\\\n        \\overline{R_T}X(x)&=\\sup_{y\\in U}T(R(x,y),X(y))\n    \\end{split}\n    \\label{eq:fuzzy approximations}\n\\end{equation}\nwhere $S$ and $T$ represent fuzzy triangular conorm (S-norm) and fuzzy triangular norm (T-norm) respectively, and $N(x)=1-x$.\n\nUsing the conventional min-max version of $T$ and $S$ norms, for a set of samples $d_i$ of class $i$ and corresponding attribute set $B\\subseteq A$, Equation \\ref{eq:fuzzy approximations} can be reformulated as:\n\\begin{equation}\n    \\begin{split}\n        \\underline{R_B}d_i(x)&=\\inf_{y\\in U}\\max(1-R(x,y),d_i(y)),\\\\\n        \\overline{R_B}d_i(x)&=\\sup_{y\\in U}\\min(R(x,y),d_i(y))\n    \\end{split}\n    \\label{eq:fuzzy approximations min-max}\n\\end{equation}\n\nTo capture non-linear high-level similarities, $R$ typically employs kernel functions, including the Gaussian kernel: $k_G(x,y)=\\exp(-\\frac{||x-y||^2}{\\delta})$, exponential kernel: $k_E(x,y)=\\exp(-\\frac{||x-y||}{\\delta})$, and rational quadratic kernel: $k_R(x,y)=1-\\frac{||x-y||^2}{||x-y||^2+\\delta}$.\n\nDuring each training epoch, negative links are dynamically selected based on their quality scores. For any potential negative link with end nodes $(x,y)$, the quality score is computed as:\n\\begin{equation}\n    Score(x,y)=\\alpha\\times\\underline{R_B}d_y(x)+(1-\\alpha)\\times\\underline{R_B}d_x(y)\n\\end{equation}\nwhere $\\alpha$ is a hyperparameter.\n\nWhile computing quality scores for all possible negative edges and selecting the top k would be optimal, this approach becomes computationally intractable for large dense graphs. For a graph with $N$ nodes and $E$ edges, there exist $N\\times (N-1)-E$ potential directed negative edges. To address this computational challenge, we randomly select $2E$ negative edges and select the top $E$ edges among them. This strategy reduces the computational complexity from $N\\times (N-1)-E$ to $2E$ while maintaining near-optimal performance.\n\nThe selected top $E$ negative edges are combined with the original positive edges to form the training dataset. To prevent class imbalance issues, we maintain an equal number of selected negative edges and original positive edges.\n\n"
                },
                "subsection 3.2": {
                    "name": "FGAT Convolution Layer",
                    "content": "\nThe FGAT convolution layer integrates GAT convolution layers with linear layers, incorporating layer normalization for training acceleration and dropout mechanisms for effective regularization.\n\nGiven an undirected graph $G = (V, E)$, where $V$ represents the set of nodes and $E$ denotes the set of edges, each node $v \\in V$ is associated with a feature vector $\\mathbf{h}_v \\in \\mathbb{R}^F$, where $F$ represents the dimension of input features per node. The FGAT layer aims to compute updated node representations $\\mathbf{h}_v' \\in \\mathbb{R}^{F'}$, where $F'$ denotes the output feature dimension, by performing weighted aggregation of features from each node's neighborhood.\n\nFor a node pair consisting of node $v$ and its neighbor $u$, the attention coefficient $e_{vu}$ is computed through:\n\\begin{equation}\n    e_{vu} = \\text{LeakyReLU} \\left( \\mathbf{a}^T \\left[ \\mathbf{W} \\mathbf{h}_v \\parallel \\mathbf{W} \\mathbf{h}_u \\right] \\right)\n\\end{equation}\nwhere:\n\\begin{itemize}\n    \\item $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ represents a learnable weight matrix that transforms node features linearly.\n    \\item $\\parallel$ indicates vector concatenation.\n    \\item $\\mathbf{a} \\in \\mathbb{R}^{2F'}$ denotes a learnable weight vector.\n    \\item LeakyReLU serves as the activation function, typically configured with a small negative slope (e.g., 0.2).\n\\end{itemize}\n\nThe attention coefficients then undergo normalization across each node's neighborhood using the softmax function:\n\\begin{equation}\n    \\alpha_{vu} = \\frac{\\exp(e_{vu})}{\\sum_{k \\in \\mathcal{N}(v)} \\exp(e_{vk})}\n\\end{equation}\nwhere $\\mathcal{N}(v)$ represents the neighborhood set of node $v$.\n\nThe normalized attention scores $\\alpha_{vu}$ facilitate the computation of updated node features $\\mathbf{h}_v'$ through weighted aggregation:\n\\begin{equation}\n    \\mathbf{h}_v' = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} \\mathbf{W} \\mathbf{h}_u \\right)\n\\end{equation}\nwhere $\\sigma$ represents a non-linear activation function, typically implemented as ReLU.\n\nTo enhance model robustness and representational capacity, the GAT layers employ multi-head attention mechanisms. Specifically, K independent attention heads operate in parallel, each generating distinct attention coefficients and feature representations. These representations are subsequently concatenated to produce the final output:\n\\begin{equation}\n    \\mathbf{h}_v' = \\parallel_{k=1}^K \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu}^{(k)} \\mathbf{W}^{(k)} \\mathbf{h}_u \\right)\n\\end{equation}\nwhere $\\alpha_{vu}^{(k)}$ and $\\mathbf{W}^{(k)}$ correspond to the attention coefficient and weight matrix of the k-th attention head, respectively.\n\nLayer normalization \\cite{xiong2020layer} is incorporated to stabilize and expedite the training process by normalizing layer inputs. For an input vector $\\mathbf{h} = [h_1, h_2, \\dots, h_d]$ with $d$ features, the normalized output $\\mathbf{\\hat{h}} = [\\hat{h}_1, \\hat{h}_2, \\dots, \\hat{h}_d]$ is computed as:\n\\begin{equation}\n    \\hat{h}_i = \\frac{h_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\end{equation}\nwhere $\\mu = \\frac{1}{d} \\sum_{i=1}^d h_i$ represents the mean, $\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^d (h_i - \\mu)^2$ denotes the variance, and $\\epsilon$ is a small constant ensuring numerical stability. The final output $y$ is obtained through the application of learnable scaling parameter $\\gamma$ and bias term $\\beta$:\n\\begin{equation}\n    y_i = \\gamma \\hat{h}_i + \\beta\n\\end{equation}\n\n"
                },
                "subsection 3.3": {
                    "name": "The FGAT Framework",
                    "content": "\nAs illustrated in Figure \\ref{fig:framework}, the FGAT framework operates through a systematic process that begins with dynamic negative edge selection during each training epoch, utilizing the given adjacency matrix. These dynamically selected negative edges, along with the existing positive edges, are subsequently processed by the FGAT layer in conjunction with their corresponding node embeddings. To effectively capture long-range dependencies within the graph structure, multiple FGAT layers are cascaded, with residual connections implemented to enhance training stability and information flow. Following the iterative processing through these layers, we obtain updated node representations $H=\\{h_1,h_2,\\dots,h_N\\}$. The probability of link existence between any pair of nodes $x$ and $y$ is then computed as:\n\\begin{equation}\n    P_r^{link}(x,y)=\\text{Sigmoid}(h_xh_y^T)\n\\end{equation}\n\nThe framework's effectiveness stems from two key components: the fuzzy negative sampling technique, which efficiently identifies and selects high-quality negative edges, and the FGAT layer architecture, which performs iterative neighbor information aggregation. The empirical validation of this framework's performance is documented in the experiments section, demonstrating its effectiveness in link prediction tasks.\n%\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\nWe conduct comparative evaluations of FGAT against several state-of-the-art baselines using two research collaboration network datasets. The experimental results demonstrate the superior performance of FGAT. In this section, we present detailed information about the datasets, experimental settings, evaluation metrics, and analysis of results.\n\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\nOur evaluation utilizes two research collaboration network datasets, summarized in Table \\ref{tab:datasets summary}.\n%\n\nThe Ca-netscience dataset comprises 379 nodes and 914 edges, with an average node degree of 2.4. In contrast, Ca-sandi-auths exhibits a more sparse structure with fewer nodes, edges, and a lower average node degree. Both datasets are directed networks. For experimental purposes, we employ a 70-10-20 split ratio, where 70\\% of the data is used for training, 10\\% for validation (early stopping), and 20\\% for testing.\n\n"
                },
                "subsection 4.2": {
                    "name": "Experiment Settings and Evaluation Metrics",
                    "content": "\nWe benchmark FGAT against several prominent baseline models: MLP, GCN \\cite{kipf2016semi}, GraphSAGE \\cite{hamilton2017inductive}, and GAT \\cite{velivckovic2017graph}. All baseline models maintain their default parameter configurations. For FGAT implementation, we configure the embedding dimension to 128 and employ a stack of 4 FGAT convolution layers. The dataset partitioning follows the aforementioned 0.7:0.1:0.2 ratio for training, validation, and testing, respectively.\n\nTo ensure a comprehensive performance assessment, we employ multiple evaluation metrics: Precision, Recall, F1 score, and ROC score. This diverse set of metrics provides a multifaceted evaluation, enabling thorough analysis of each model's capabilities across different performance aspects.\n\n"
                },
                "subsection 4.3": {
                    "name": "Results",
                    "content": "\nThe experimental results are presented in Table \\ref{tab:experiment results}, yielding several significant observations:\n\\begin{itemize}\n    \\item On the Ca-netscience dataset, MLP demonstrates the poorest performance, attributable to its inability to capture spatial information encoded in the adjacency matrix. GCN, GraphSAGE, and GAT exhibit comparable performance levels, with GraphSAGE achieving superior Recall scores and GAT excelling in F1 metrics. For the Ca-sandi-auths dataset, MLP achieves notable Recall but relatively inferior Precision, suggesting overfitting tendencies and limited generalization capability. GAT, leveraging its attention mechanism, achieves the highest ROC scores among baseline methods.\n    \\item FGAT outperforms baseline methods across both datasets in terms of the average of four evaluation metrics. Specifically, it demonstrates an average improvement of 7.11\\% across all metrics on Ca-netscience, and a more substantial 15.55\\% improvement on Ca-sandi-auths. This superior performance can be attributed to two key factors: the fuzzy negative sampling mechanism, which enables focused learning on error-prone edges, and the FGAT layer architecture, which provides robust message aggregation capabilities.\n\\end{itemize}\n\n"
                }
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\nThe proposed FGAT framework, combining FNS and a novel graph attention layer, significantly improves link prediction performance compared to existing methods. FNS effectively identifies informative negative edges by leveraging fuzzy rough sets, leading to more focused and efficient model training. The FGAT layer, integrating fuzzy set concepts, captures complex relationships in graph data, resulting in superior node representations for accurate link prediction. The paper's findings highlight the potential of fuzzy rough sets in advancing GNNs for link prediction tasks and pave the way for future research exploring fuzzy set theory in graph-based learning.\n\n\\begin{thebibliography}{00}\n\n    \\bibitem{hamilton2017inductive}\n    W.~Hamilton, Z.~Ying, and J.~Leskovec, ``Inductive representation learning on\n    large graphs,'' \\emph{Advances in neural information processing systems},\n    vol.~30, 2017.\n\n    \\bibitem{velivckovic2017graph}\n    P.~Veli{\\v{c}}kovi{\\'c}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Lio, and\n    Y.~Bengio, ``Graph attention networks,'' \\emph{arXiv preprint\n        arXiv:1710.10903}, 2017.\n\n    \\bibitem{kipf2016semi} \n    T.~N. Kipf and M.~Welling, ``Semi-supervised classification with graph\n    convolutional networks,'' \\emph{arXiv preprint arXiv:1609.02907}, 2016.\n\n    \\bibitem{ying2018hierarchical}\n    Z.~Ying, J.~You, C.~Morris, X.~Ren, W.~Hamilton, and J.~Leskovec,\n    ``Hierarchical graph representation learning with differentiable pooling,''\n    \\emph{Advances in neural information processing systems}, vol.~31, 2018.\n\n    \\bibitem{diehl2019edge}\n    F.~Diehl, ``Edge contraction pooling for graph neural networks,'' \\emph{arXiv\n        preprint arXiv:1905.10990}, 2019.\n\n    \\bibitem{zhang2018link}\n    M.~Zhang and Y.~Chen, ``Link prediction based on graph neural networks,''\n    \\emph{Advances in neural information processing systems}, vol.~31, 2018.\n\n    \\bibitem{kipf2016variational}\n    T.~N. Kipf and M.~Welling, ``Variational graph auto-encoders,'' \\emph{arXiv\n        preprint arXiv:1611.07308}, 2016.\n\n    \\bibitem{zhang2021nested}\n    M.~Zhang and P.~Li, ``Nested graph neural networks,'' \\emph{Advances in Neural\n        Information Processing Systems}, vol.~34, pp. 15\\,734--15\\,747, 2021.\n\n    \\bibitem{dubois1990rough}\n    D.~Dubois and H.~Prade, ``Rough fuzzy sets and fuzzy rough sets,''\n    \\emph{International Journal of General System}, vol.~17, no. 2-3, pp.\n    191--209, 1990.\n\n    \\bibitem{jensen2004fuzzy}\n    R.~Jensen and Q.~Shen, ``Fuzzy--rough attribute reduction with application to\n    web categorization,'' \\emph{Fuzzy sets and systems}, vol. 141, no.~3, pp.\n    469--485, 2004.\n\n    \\bibitem{xing2022weighted}\n    J.~Xing, C.~Gao, and J.~Zhou, ``Weighted fuzzy rough sets-based tri-training\n    and its application to medical diagnosis,'' \\emph{Applied Soft Computing},\n    vol. 124, p. 109025, 2022.\n\n    \\bibitem{gao2022parameterized}\n    C.~Gao, J.~Zhou, J.~Xing, and X.~Yue, ``Parameterized maximum-entropy-based\n    three-way approximate attribute reduction,'' \\emph{International Journal of\n        Approximate Reasoning}, vol. 151, pp. 85--100, 2022.\n\n    \\bibitem{ye2021novel}\n    J.~Ye, J.~Zhan, W.~Ding, and H.~Fujita, ``A novel fuzzy rough set model with\n    fuzzy neighborhood operators,'' \\emph{Information Sciences}, vol. 544, pp.\n    266--297, 2021.\n\n    \\bibitem{ji2021fuzzy}\n    W.~Ji, Y.~Pang, X.~Jia, Z.~Wang, F.~Hou, B.~Song, M.~Liu, and R.~Wang, ``Fuzzy\n    rough sets and fuzzy rough neural networks for feature selection: A review,''\n    \\emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},\n    vol.~11, no.~3, p. e1402, 2021.\n\n    \\bibitem{lin2021research}\n    C.~Lin, S.~Chen, X.~Yang, C.~Li, C.~Qu, and Q.~Chen, ``Research and application\n    of knowledge graph technology for intelligent question answering,'' in\n    \\emph{2021 12th International Symposium on Parallel Architectures, Algorithms\n        and Programming (PAAP)}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE, 2021,\n    pp. 152--156.\n\n    \\bibitem{yang2021research}\n    X.~Yang, ``Research on automatic composition based on multiple machine learning\n    models,'' in \\emph{2021 3rd International Conference on Artificial\n        Intelligence and Advanced Manufacture}, 2021, pp. 1206--1209.\n\n    \\bibitem{yang2022botnet}\n    X.~Yang, Z.~Guo, and Z.~Mai, ``Botnet detection based on machine learning,'' in\n    \\emph{2022 International Conference on Blockchain Technology and Information\n        Security (ICBCTIS)}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE, 2022, pp.\n    213--217.\n    \n    \\bibitem{cheng24patch} Cheng, Qisen, Shuhui Qu, and Janghwan Lee. \"Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual Defect Detection.\" In *2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)*, pp. 586-592. IEEE, 2024.\n    \\bibitem{cheng25unifying} Cheng, Qisen, Jinming Xing, Chang Xue, and Xiaoran Yang. \"Unifying Prediction and Explanation in Time-Series Transformers via Shapley-based Pretraining.\" *arXiv preprint arXiv:2501.15070* (2025).\n    \\bibitem{cheng22deep} Cheng, Qisen, Shuhui Qu, and Janghwan Lee. \"72\u20103: Deep Learning Based Visual Defect Detection in Noisy and Imbalanced Data.\" In *SID Symposium Digest of Technical Papers*, vol. 53, no. 1, pp. 971-974. 2022.\n    \\bibitem{cheng22estimation} Cheng, Qisen, Chang Zhang, and Xiang Shen. \"Estimation of Energy and Time Usage in 3D Printing With Multimodal Neural Network.\" In *2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)*, pp. 900-903. IEEE, 2022.\n    \\bibitem{xing24traffic} Xing, Jinming, Zhaomin Xiao, Yingyi Wu, Jinran Zhang, Zhuoer Xu, and Zhelu Mai. \"Network Traffic Forecasting via Fuzzy Spatial-Temporal Fusion Graph Neural Networks.\" In *2024 11th International Conference on Soft Computing \\& Machine Intelligence (ISCMI)*, pp. 282-286. IEEE, 2024.\n    \\bibitem{cheng23shapnn} Cheng, Qisen, Shuhui Qu, and Janghwan Lee. \"SHAPNN: Shapley Value Regularized Tabular Neural Network.\" *arXiv preprint arXiv:2309.08799* (2023).\n    \n    \\bibitem{zhao2022image}\n    S.~Zhao, J.~Xie, C.~Lin, X.~Nie, J.~Ye, X.~Yang, and P.~Xu, ``Image retrieval\n    based on blockchain,'' in \\emph{2022 International Conference on Blockchain\n        Technology and Information Security (ICBCTIS)}.\\hskip 1em plus 0.5em minus\n    0.4em\\relax IEEE, 2022, pp. 210--212.\n\n    \\bibitem{yang2023balancing}\n    X.~Yang, Y.~Zhan, Y.~Iwasaki, M.~Shi, S.~Tang, and H.~Iwata, ``Balancing\n    real-world interaction and vr immersion with ai vision robotic arm,'' in\n    \\emph{2023 IEEE International Conference on Mechatronics and Automation\n        (ICMA)}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE, 2023, pp. 2051--2057.\n\n    \\bibitem{deng2021short}\n    X.~Deng, H.~Zhou, X.~Yang, and C.~Ye, ``Short-term traffic condition prediction\n    based on multi-source data fusion,'' in \\emph{International Conference on\n        Data Mining and Big Data}.\\hskip 1em plus 0.5em minus 0.4em\\relax Springer,\n    2021, pp. 327--335.\n\n    \\bibitem{xiong2020layer}\n    R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,\n    L.~Wang, and T.~Liu, ``On layer normalization in the transformer\n    architecture,'' in \\emph{International Conference on Machine Learning}.\\hskip\n    1em plus 0.5em minus 0.4em\\relax PMLR, 2020, pp. 10\\,524--10\\,533.\n\n\\end{thebibliography}\n\n% \\vspace{12pt}\n"
            }
        },
        "tables": {
            "tab:experiment results": "\\begin{table*}[htbp]\n    \\centering\n    \\caption{Experiment Results}\n    \\begin{tabular}{lcccccccc}\n        \\toprule\n        \\multirow{2}[2]{*}{Methods} & \\multicolumn{4}{c}{Ca-netscience} & \\multicolumn{4}{c}{Ca-sandi-auths}                                                                                                                                                                    \\\\\n        \\cmidrule(lr){2-5} \\cmidrule(lr){6-9}\n                                    & \\multicolumn{1}{c}{Precision}     & \\multicolumn{1}{c}{Recall}         & \\multicolumn{1}{c}{F1} & \\multicolumn{1}{c}{ROC} & \\multicolumn{1}{c}{Precision} & \\multicolumn{1}{c}{Recall} & \\multicolumn{1}{c}{F1} & \\multicolumn{1}{c}{ROC} \\\\\n        \\midrule\n        MLP                         & 0.4931                            & 0.5879                             & 0.5363                 & 0.5044                  & 0.5581                        & \\textbf{1.0000}            & 0.7164                 & 0.6181                  \\\\\n        GCN                         & 0.5903                            & 0.7363                             & 0.6553                 & 0.7078                  & 0.5417                        & 0.5417                     & 0.5417                 & 0.5304                  \\\\\n        GraphSAGE                   & 0.5502                            & \\textbf{0.8132}                    & 0.6563                 & 0.6277                  & 0.4651                        & 0.8333                     & 0.5970                 & 0.5885                  \\\\\n        GAT                         & 0.6034                            & 0.7692                             & \\textbf{0.6763}        & 0.6916                  & 0.6053                        & 0.9583                     & 0.7419                 & \\textbf{0.7240}         \\\\\n        FGAT                        & \\textbf{0.6667}                   & 0.6593                             & 0.6630                 & \\textbf{0.7422}         & \\textbf{0.6216}               & 0.9583                     & \\textbf{0.7541}        & 0.7170                  \\\\\n        \\bottomrule\n    \\end{tabular}%\n    \\label{tab:experiment results}%\n\\end{table*}",
            "tab:datasets summary": "\\begin{table}[htbp]\n    \\centering\n    \\caption{Datasets Summary}\n    \\begin{tabular}{lcc}\n        \\toprule\n                           & Ca-netscience & Ca-sandi-auths \\\\\n        \\midrule\n        \\#Nodes            & 379           & 86             \\\\\n        \\#Edges            & 914           & 124            \\\\\n        \\#AvgDegree        & 2.4           & 1.4            \\\\\n        Directed           & TRUE          & TRUE           \\\\\n        \\%Training Edges   & 0.7           & 0.7            \\\\\n        \\%Validation Edges & 0.1           & 0.1            \\\\\n        \\%Testing Edges    & 0.2           & 0.2            \\\\\n        \\bottomrule\n    \\end{tabular}%\n    \\label{tab:datasets summary}%\n\\end{table}"
        },
        "figures": {
            "fig:framework": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{FGAT.png}\n    \\caption{The FGAT Framework}\n    \\label{fig:framework}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n    \\begin{split}\n        \\underline{R_S}X(x)&=\\inf_{y\\in U}S(N(R(x,y)),X(y)),\\\\\n        \\overline{R_T}X(x)&=\\sup_{y\\in U}T(R(x,y),X(y))\n    \\end{split}\n    \\label{eq:fuzzy approximations}\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\begin{split}\n        \\underline{R_B}d_i(x)&=\\inf_{y\\in U}\\max(1-R(x,y),d_i(y)),\\\\\n        \\overline{R_B}d_i(x)&=\\sup_{y\\in U}\\min(R(x,y),d_i(y))\n    \\end{split}\n    \\label{eq:fuzzy approximations min-max}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    Score(x,y)=\\alpha\\times\\underline{R_B}d_y(x)+(1-\\alpha)\\times\\underline{R_B}d_x(y)\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    e_{vu} = \\text{LeakyReLU} \\left( \\mathbf{a}^T \\left[ \\mathbf{W} \\mathbf{h}_v \\parallel \\mathbf{W} \\mathbf{h}_u \\right] \\right)\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\alpha_{vu} = \\frac{\\exp(e_{vu})}{\\sum_{k \\in \\mathcal{N}(v)} \\exp(e_{vk})}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\mathbf{h}_v' = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} \\mathbf{W} \\mathbf{h}_u \\right)\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathbf{h}_v' = \\parallel_{k=1}^K \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu}^{(k)} \\mathbf{W}^{(k)} \\mathbf{h}_u \\right)\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    \\hat{h}_i = \\frac{h_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    y_i = \\gamma \\hat{h}_i + \\beta\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    P_r^{link}(x,y)=\\text{Sigmoid}(h_xh_y^T)\n\\end{equation}"
        }
    }
}