{
    "meta_info": {
        "title": "N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",
        "abstract": "Recent progress in neural forecasting accelerated improvements in the\nperformance of large-scale forecasting systems. Yet, long-horizon forecasting\nremains a very difficult task. Two common challenges afflicting the task are\nthe volatility of the predictions and their computational complexity. We\nintroduce N-HiTS, a model which addresses both challenges by incorporating\nnovel hierarchical interpolation and multi-rate data sampling techniques. These\ntechniques enable the proposed method to assemble its predictions sequentially,\nemphasizing components with different frequencies and scales while decomposing\nthe input signal and synthesizing the forecast. We prove that the hierarchical\ninterpolation technique can efficiently approximate arbitrarily long horizons\nin the presence of smoothness. Additionally, we conduct extensive large-scale\ndataset experiments from the long-horizon forecasting literature, demonstrating\nthe advantages of our method over the state-of-the-art methods, where N-HiTS\nprovides an average accuracy improvement of almost 20% over the latest\nTransformer architectures while reducing the computation time by an order of\nmagnitude (50 times). Our code is available at bit.ly/3VA5DoT",
        "author": "Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski",
        "link": "http://arxiv.org/abs/2201.12886v6",
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "additionl_info": "Accepted at the Thirty-Seventh AAAI Conference on Artificial  Intelligence (AAAI-23)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": " \\label{section1:introduction}\nLong-horizon forecasting is critical in many important applications including risk management and planning. Notable examples include power plant maintenance scheduling \\citep{hyndman2009long_electricity} and planning for infrastructure construction~\\citep{ziel2018long_epf}, as well as early warning systems that help mitigate vulnerabilities due to extreme weather events~\\citep{reid2006early_warning_systems, field2012risk_management_climate}. In healthcare, predictive monitoring of vital signs enables detection of preventable adverse outcomes and application of life-saving interventions~\\citep{churpek2016long_healthcare}. \n\n\n\nRecently, neural time series forecasting has progressed in a few promising directions. First, the architectural evolution included adoption of the attention mechanism and the rise of Transformer-inspired approaches~\\citep{li2019logtrans,fan2019multimodal_transformer,ahmed2019attentive_state_space,lim2021temporal_fusion_transformer}, as well as introduction of attention-free architectures composed of deep stacks of fully connected layers~\\citep{oreshkin2020nbeats, olivares2021nbeatsx}. Both of these approaches are relatively easy to scale up in terms of capacity, compared to LSTMs, and have proven to be capable of capturing long-range dependencies. The attention-based approaches are very generic as they can explicitly model direct interactions between every pair of input-output elements. Unsurprisingly, they happen to be the most computationally expensive. The architectures based on fully connected stacks capture input-output relationships implicitly, however they tend to be more compute-efficient. Second, the recurrent forecast generation strategy has been replaced with the multi-step prediction strategy in both of these approaches. Aside from its convenient bias-variance benefits and robustness~\\citep{marcellino2006multi_step_forecasting,atiya2016multi_step_forecasting}, the multi-step strategy has enabled the models to efficiently predict long sequences in a single forward pass~\\citep{wen2017mqrcnn,zhou2021informer,lim2021temporal_fusion_transformer}.\n\nDespite all the recent progress, long-horizon forecasting remains challenging for neural networks, because their unbounded expressiveness translates directly into \\emph{excessive computational complexity} and \\emph{forecast volatility}, both of which become especially pronounced in this context. For instance, both attention and fully connected layers scale quadratically in memory and computational cost with respect to the forecasting horizon length. Fig.~\\ref{fig:motivation} illustrates how forecasting errors and computation costs inflate dramatically with growing forecasting horizon in the case of the fully connected architecture electricity consumption predictions. Attention-based predictions show similar behavior.\n\nNeural long-horizon forecasting research has mostly focused on attention efficiency making self-attention sparse~\\citep{child2019sparse_transformer,li2019logtrans,zhou2021informer} or local~\\citep{li2019logtrans}. In the same vain, attention has been cleverly redefined through locality-sensitive hashing~\\citep{kitaev2020reformer} or FFT~\\citep{wu2021autoformer}. Although that research has led to incremental improvements in compute cost and accuracy, the silver bullet long-horizon forecasting solution is yet to be found. In this paper we make a bold step in this direction by developing a novel forecasting approach that cuts long-horizon compute cost by an order of magnitude while simultaneously offering \\multivarMSEgains\\% accuracy improvements on a large array of multi-variate forecasting datasets compared to existing state-of-the-art Transformer-based techniques. We redefine existing fully-connected \\NBEATS{} architecture~\\citep{oreshkin2020nbeats} by enhancing its input decomposition via multi-rate data sampling and its output synthesizer via multi-scale interpolation. Our extensive experiments show the importance of the proposed novel architectural components and validate significant improvements in accuracy and computational complexity of the proposed algorithm. \\\\\n\nOur contributions are summarized below:\n\\begin{enumerate}%[label=(\\roman*)]\n    \\item \\textbf{Multi-Rate Data Sampling}: We incorporate sub-sampling layers in front of fully-connected blocks, significantly reducing the memory footprint and the amount of computation needed, while maintaining the ability to model long-range dependencies. \n    \\item \\textbf{Hierarchical Interpolation}: We enforce smoothness of the multi-step predictions by reducing the dimensionality of neural network's prediction and matching its time scale with that of the final output via multi-scale hierarchical interpolation. This novel technique is not unique to our proposed model, and can be incorporated in different architectures.\n    \\item \\textbf{\\ourstext \\ architecture}: A novel way of hierarchically synchronizing the rate of input sampling with the scale of output interpolation across blocks, which induces each block to specialize on forecasting its own frequency band of the time-series signal.\n    \\item \\textbf{State-of-the-art results} on six large-scale benchmark datasets from the long-horizon forecasting literature: electricity transformer temperature, exchange rate, electricity consumption, San Francisco bay area highway traffic, weather and influenza-like illness. % on six well-established multivariate forecasting benchmarks.\n\\end{enumerate}\n\n%Our code is available at\n%\\url{https://anonymous.4open.science/r/n-hits-C430}\n% \\href{https://github.com/cchallu/deep-midas}{\\textcolor{blue}{\\ours's repository}}.\n\nThe remainder of this paper is structured as follows. Section~\\ref{section2:literature} reviews relevant literature, Section~\\ref{section3:model} introduces notation and describes the methodology, Sections~\\ref{section4:experiments} and~\\ref{section:discussion_of_findings} describe and analyze our empirical findings. Finally, Section \\ref{section5:conclusion} concludes the paper.\n\n"
            },
            "section 2": {
                "name": "Related Work",
                "content": " \\label{section2:literature}\n\n\n\\textbf{Neural forecasting.} Over the past few years, deep forecasting methods have become ubiquitous in industrial forecasting systems, with examples in optimal resource allocation and planning in transportation \\citep{laptev2017neural_forecasting_uber}, large e-commerce retail \\citep{wen2017mqrcnn, olivares2021probabilistic_hierarchical_dpm, paria2021hierarchical_hired, rangapuram2021hierarchical_e2e}, or financial trading \\citep{banushev_barclay2021neural_forecasting_aws}. The evident success of the methods in recent forecasting competitions \\citep{makridakis2020m4_competition, makridakis2021m5_competition} has renovated the interest within the academic community \\citep{benidis2020dl_timeseries_review2}. In the context of multi-variate long-horizon forecasting, Transformer-based approaches have dominated the landscape in the recent years, including \\Autoformer~\\citep{wu2021autoformer}, an encoder-decoder model with decomposition capabilities and an approximation to attention based on Fourier transform, \\Informer~\\citep{zhou2021informer}, Transformer with MLP based multi-step prediction strategy, that approximates self-attention with sparsity, \\Reformer~\\citep{kitaev2020reformer}, Transformer that approximates attention with locality-sensitive hashing and \\LogTrans~\\citep{li2019logtrans}, Transformer with local/log-sparse attention.\n\n\\textbf{Multi-step forecasting.} Investigations of the bias/variance trade-off in multi-step forecasting strategies reveal that the \\emph{direct} strategy, which allocates a different model for each step, has low bias and high variance, avoiding error accumulation across steps, exhibited by the classical \\emph{recursive} strategy, but losing in terms of net model parsimony. Conversely, in the \\emph{joint} forecasting strategy, a single model produces forecasts for all steps in one shot, striking the perfect balance between variance and bias, avoiding error accumulation and leveraging shared model parameters~\\citep{bao2014msvr, atiya2016multi_step_forecasting, wen2017mqrcnn}. \n\n\\textbf{Multi-rate input sampling.} Previous forecasting literature recognized challenges of extremely long horizon predictions, and proposed \\emph{mixed data sampling regression} (\\MIDAS; \\citealt{ghysels2007midas_regressions, armesto2010ForecastingWMF}) to ameliorate the problem of parameter proliferation while preserving high frequency temporal information. \\MIDAS\\ regressions maintained the classic \\emph{recursive} forecasting strategy of linear auto-regressive models, but defined a parsimonious fashion of feeding the inputs. \n\n\\textbf{Interpolation.} Interpolation has been extensively used to augment the resolution of modeled signals in many fields such as signal and image processing~\\citep{meijering2002interpolation_history}. In time-series forecasting, its applications range from completing unevenly sampled data and noise filters \\cite{chow1971interpolation_extrapolation_time_series, roque1981interpolation_time_series_note, shukla2019interpolation_prediction_networks, rubanova2019latent_odes_irregular_sample} to fine-grained quantile-regressions with recurrent networks \\citep{gasthaus2019spline_quantile_function}. To our knowledge, temporal interpolation has not been used to induce multi-scale hierarchical time-series forecasts.\n\n"
            },
            "section 3": {
                "name": "\\ours\\ Methodology",
                "content": " \\label{section3:model}\nIn this section, we describe our proposed approach, \\ours, whose high-level diagram and main principles of operation are depicted in Fig.~\\ref{fig:nhits_architecture}. Our method extends the \\emph{Neural Basis Expansion Analysis} approach (\\NBEATS; \\citealt{oreshkin2020nbeats}) in several important respects, making it more accurate and computationally efficient, especially in the context of long-horizon forecasting. In essence, our approach uses multi-rate sampling of the input signal and multi-scale synthesis of the forecast, resulting in a hierarchical construction of forecast, greatly reducing computational requirements and improving forecasting accuracy. \n%\\NBEATS\n\nSimilarly to \\NBEATS, \\ours\\ performs local nonlinear projections onto basis functions across multiple blocks. Each block consists of a \\emph{multilayer perceptron} (\\MLP), which learns to produce coefficients for the backcast and forecast outputs of its basis. The backcast output is used to clean the inputs of subsequent blocks, while the forecasts are summed to compose the final prediction. The blocks are grouped in stacks, each specialized in learning a different characteristic of the data using a different set of basis functions. The overall network input, $\\mathbf{y}_{t-L:t}$, consists of $L$ lags. % of the target time-series $\\mathbf{y}$. \n\n\\ours\\ is composed of $S$ stacks, $B$ blocks each. Each block contains an \\MLP\\ predicting forward and backward basis coefficients. The next subsections describe the novel components of our architecture. Note that in the following, we skip the stack index $s$ for brevity.\n\n",
                "subsection 3.1": {
                    "name": "Multi-Rate Signal Sampling",
                    "content": "\n\\label{section:multirate_sampling}\nAt the input to each block $\\ell$, we propose to use a MaxPool layer with kernel size $k_{\\ell}$ to help it focus on analyzing components of its input with a specific scale. Larger $k_{\\ell}$ will tend to cut more high-frequency/small-time-scale components from the input of the \\MLP, forcing the block to focus on analyzing large scale/low frequency content. We call this \\emph{multi-rate signal sampling}, referring to the fact that the \\MLP\\ in each block faces a different effective input signal sampling rate. Intuitively, this helps the blocks with larger pooling kernel size $k_{\\ell}$ focus on analyzing large scale components critical for producing consistent long-horizon forecasts. \n\n% \\begin{figure}[ht!]\n% \\centering\n% \\includegraphics[width=0.65\\linewidth]{images/linear_interpolation.pdf}\n% \\vskip -0.15in\n% \\caption{The interpolation enhancement to the multi-step forecasting strategy allows \\ours\\ to reduce the volatility of the predictions through smoothness.} \\label{fig:linear_interpolation}\n% \\end{figure}\n\nAdditionally, multi-rate processing reduces the width of the \\MLP\\ input for most blocks, limiting the memory footprint and the amount of computation as well as  reducing the number of learnable parameters and hence alleviating the effects of overfitting, while maintaining the original receptive field. Given block $\\ell$ input $\\mathbf{y}_{t-L:t,\\ell}$ (the input to the first block $\\ell=1$ is the network-wide input, $\\mathbf{y}_{t-L:t, 1} \\equiv \\mathbf{y}_{t-L:t}$), this operation can be formalized as follows:\n\\begin{equation} \\label{equation:nhits_mixed_datasampling}\n    \\mathbf{y}^{(p)}_{t-L:t, \\ell} = \\mathbf{MaxPool}\\left(\\mathbf{y}_{t-L:t, \\ell},\\; k_{\\ell}\\right)\n\\end{equation}\n\n"
                },
                "subsection 3.2": {
                    "name": "Non-Linear Regression",
                    "content": "\n\nFollowing subsampling, block $\\ell$ looks at its input and non-linearly regresses forward $\\mathbf{\\theta}^{f}_{\\ell}$ and backward $\\mathbf{\\theta}^{b}_{\\ell}$ interpolation \\MLP\\ coefficients that learns hidden vector $\\mathbf{h}_{\\ell} \\in \\mathbb{R}^{N_{h}}$, which is then linearly projected:\n\\begin{align} \\label{equation:nhits_projections}\n\\begin{split}\n    \\mathbf{h}_{\\ell}  &= \\mathbf{MLP}_{\\ell}\\left(\\mathbf{y}^{(p)}_{t-L:t,\\ell}\\right) \\\\\n    \\btheta^{f}_{\\ell} &= \\textbf{LINEAR}^{f}\\left(\\mathbf{h}_{\\ell}\\right) \\\\\n    \\btheta^{b}_{\\ell} &= \\textbf{LINEAR}^{b}\\left(\\mathbf{h}_{\\ell}\\right)\n\\end{split}\n\\end{align}\nThe coefficients are then used to synthesize backcast $\\mathbf{\\tilde{y}}_{t-L:t,\\ell}$ and forecast $\\mathbf{\\hat{y}}_{t+1:t+H,\\ell}$ outputs of the block, via the process described below.\n\n"
                },
                "subsection 3.3": {
                    "name": "Hierarchical Interpolation",
                    "content": "\n\\label{section:hierarchical_interpolation}\n\nIn most multi-horizon forecasting models, the cardinality of the neural network prediction equals the dimensionality of horizon, $H$. For example, in \\NBEATSi\\ $|\\btheta^{f}_{\\ell}|= H$; in Transformer-based models, decoder attention layer cross-correlates $H$ output embeddings with $L$ encoded input embeddings ($L$ tends to grow with growing $H$). This leads to quick inflation in compute requirements and unnecessary explosion in model expressiveness as horizon $H$ increases.\n\nTo combat these issues, we propose to use \\emph{temporal interpolation}. We define the dimensionality of the interpolation coefficients in terms of the \\emph{expressiveness ratio} $r_{\\ell}$ that controls the number of parameters per unit of output time, $|\\btheta^{f}_{\\ell}|= \\lceil r_{\\ell} \\, H \\rceil$. To recover the original sampling rate and predict all $H$ points in the horizon, we use temporal interpolation via the interpolation function $g$:\n\n\\begin{align}\n\\begin{split}\n    \\hat{y}_{\\tau,\\ell}   &= g(\\tau, \\btheta^{f}_{\\ell}), \\quad \\forall \\tau \\in \\{t+1,\\dots,t+H\\}, \\\\\n    \\tilde{y}_{\\tau,\\ell} &= g(\\tau, \\btheta^{b}_{\\ell}), \\quad \\forall \\tau \\in \\{t-L,\\dots,t\\}. \n\\end{split}\n\\end{align}\n\nInterpolation can vary in \\emph{smoothness}, $g \\in \\mathcal{C}^{0}, \\mathcal{C}^{1}, \\mathcal{C}^{2}$. In Appendix G we explore the nearest neighbor, piece-wise linear and cubic alternatives. For concreteness, the linear interpolator $g \\in \\mathcal{C}^{1}$, \nalong with the time partition $\\mathcal{T} = \\{t+1, t+1+1/r_{\\ell}, \\ldots, t+H-1/r_{\\ell}, t+H \\}$, is defined as\n\n\\begin{align}\n% \\nonumber\n\\begin{split}\ng(\\tau, \\theta) &= \\theta[t_{1}] + \\left(\\frac{\\theta[t_{2}]-\\theta[t_{1}]}{t_{2}-t_{1}}\\right)(\\tau-t_{1}) \\\\\nt_1 &= \\arg\\min_{t \\in \\mathcal{T}: t \\leq \\tau} \\tau - t, \\quad t_2 = t_1 + 1/r_{\\ell}.\n\\end{split}\n\\end{align}\n\n\n\nThe \\emph{hierarchical} interpolation principle is implemented by distributing expressiveness ratios across blocks in a manner synchronized with multi-rate sampling. Blocks closer to the input have smaller $r_{\\ell}$ and larger $k_{\\ell}$, implying that input blocks generate low-granularity signals via more aggressive interpolation, being also forced to look at more aggressively sub-sampled (and smoothed) signals. The resulting hierarchical forecast $\\mathbf{\\hat{y}}_{t+1:t+H}$ is assembled by summing the outputs of all blocks, essentially composing it out of interpolations at different time-scale hierarchy levels.\n\nSince each block specializes on its own scale of input and output signal, this induces a clearly structured hierarchy of interpolation granularity, the intuition conveyed in Fig.~\\ref{fig:motivation} and \\ref{fig:nhits_intuition}. We propose to use \\emph{exponentially increasing expressiveness ratios} to handle a wide range of frequency bands while controlling the number of parameters. Alternatively, each stack can specialize in modeling a different known cycle of the time-series (weekly, daily etc.) using a matching $r_{\\ell}$ (see Table A.3). Finally, the backcast residual formed at previous hierarchy scale is subtracted from the input of the next hierarchy level to amplify the focus of the next level block on signals outside of the band that has already been handled by the previous hierarchy members.\n\\begin{equation}\n\\begin{split}\n\\mathbf{\\hat{y}}_{t+1:t+H} &= \\sum^{L}_{l=1} \\hat{\\mathbf{y}}_{t+1:t+H,\\ell} \\nonumber \\\\ \\mathbf{y}_{t-L:t,\\ell+1} &= \\mathbf{y}_{t-L:t,\\ell}-\\mathbf{\\tilde{y}}_{t-L:t,\\ell}   %\\nonumber\n\\end{split}\n\\end{equation}\n\nHierarchical interpolation has advantageous theoretical guarantees. We show in Appendix A, that it can approximate infinitely/dense horizons. As long as the interpolating function $g$ is characterized by projections to informed multi-resolution functions $V_{w}$, and the forecast relationships are smooth. \\\\\n\n\\textbf{Neural Basis Approximation Theorem.} Let a forecast mapping be $\\mathcal{Y}( \\cdot \\;|\\; \\ylag ): [0, 1]^L \\rightarrow \\mathcal{F}$, where the forecast functions $\\mathcal{F}=\\{\\mathcal{Y}(\\tau): [0,1] \\to \\mathbb{R}\\}=\\mathcal{L}^{2}([0,1])$ representing a infinite/dense horizon, are square integrable. If the multi-resolution functions $V_{w}=\\{\\phi_{w,h}(\\tau) = \\phi(2^{w}(\\tau-h)) \\;|\\; w \\in {\\mathbb{Z}}, h\\in2^{-w}\\times[0,\\dots,2^{w}]\\}$ can arbitrarily approximate $\\mathcal{L}^{2}([0,1])$. And the projection $\\mathrm{Proj}_{V_{w}}(\\mathcal{Y}(\\tau))$ varies smoothly on $\\ylag$. Then the forecast mapping $\\mathcal{Y}(\\cdot \\;|\\; \\ylag)$ can be arbitrarily approximated by a neural basis expansion learning a finite number of  multi-resolution coefficients $\\hat{\\theta}_{w,h}$. That is $\\forall \\epsilon>0$,\n\n\\begin{equation}\n    \\int |\\mathcal{Y}(\\tau \\;|\\; \\ylag) -\\sum_{w,h} \\hat{\\theta}_{w,h}(\\ylag) \\phi_{w,h}(\\tau) | d\\tau \\leq \\epsilon\n\\end{equation}\n\nExamples of multi-resolution functions $V_{w}=\\{\\phi_{w,h}(\\tau) = \\phi(2^{w}(\\tau-h)) \\;|\\; w \\in {\\mathbb{Z}}, h\\in2^{-w}\\times[0,\\dots,2^{w}]\\}$ include piece-wise constants, piece-wise linear functions and splines with arbitrary approximation capabilities.\n\n"
                }
            },
            "section 4": {
                "name": "Experimental Results",
                "content": " \\label{section4:experiments}\nWe follow the experimental settings from \\citep{wu2021autoformer, zhou2021informer} (NeurIPS 2021 and AAAI 2021 Best Paper Award). We first describe datasets, baselines and metrics used for the quantitative evaluation of our model. Table~\\ref{table:main_results_multivar} presents our key results, demonstrating SoTA performance of our method relative to existing work. We then carefully describe the details of training and evaluation setups. We conclude the section by describing ablation studies.  % In this section we present our empirical results. \n\n\n",
                "subsection 4.1": {
                    "name": "Datasets",
                    "content": "\n\\label{section:datasets}\n\nAll large-scale datasets used in our empirical studies are publicly available and have been used in neural forecasting literature, particularly in the context of long-horizon~\\citep{lai2018lstnet, zhou2019epf_heterogeneous_lstm, li2019logtrans, wu2021autoformer}. Table A1 summarizes their characteristics. Each set is normalized with the train data mean and standard deviation. \n\n% \\footnote{\\ETTm$_2$\\ is available at the \\href{https://github.com/zhouhaoyi/ETDataset}{\\textcolor{blue}{\\Informer's repository}}.\n% \\Exchange \\ is available at the \\href{https://github.com/laiguokun/multivariate-time-series-data}{\\textcolor{blue}{\\LSTNet's dataset repository}}.\n% \\Electricity \\ is available at the \\href{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}{\\textcolor{blue}{UCI ML repository}}.\n% \\TrafficL, \\ is reported by the \\href{https://pems.dot.ca.gov/}{\\textcolor{blue}{California Department of Transportation}}.\n% \\Weather \\ is published by \\href{https://www.bgc-jena.mpg.de/wetter/}{\\textcolor{blue}{Max Planck Institute}}.\n% \\href{https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html}{\\textcolor{blue}{US Centers for Disease Control and Prevention}} reports \\ILI.\n% }\n\n\\textbf{Electricity Transformer Temperature.} The \\ETTm$_{2}$\\ dataset measures an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at a fifteen minutes frequency. %\\citep{zhou2021informer}\n\\textbf{Exchange-Rate.} The \\Exchange\\ dataset is a collection of daily exchange rates of eight countries relative to the US dollar. The countries include Australia, UK, Canada, Switzerland, China, Japan, New Zealand and Singapore from 1990 to 2016. % \\citep{lai2018lstnet}\n\\textbf{Electricity.} The \\Electricity\\ dataset reports the fifteen minute electricity consumption (KWh) of 321 customers from 2012 to 2014. For comparability, we aggregate it hourly. %\\citep{zhou2021informer, li2019local_transformer}\n\\textbf{San Francisco Bay Area Highway Traffic.} This \\TrafficL\\ dataset was collected by the California Department of Transportation, it reports road hourly occupancy rates of 862 sensors, from January 2015 to December 2016. %\\citep{lai2018lstnet,wu2021autoformer}\n\\textbf{Weather.} This \\Weather\\ dataset contains the 2020 year of 21 meteorological measurements recorded every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in Jena, Germany. %\\citep{wu2021autoformer}\n\\textbf{Influenza-like illness.} The \\ILI\\ dataset reports weekly recorded influenza-like illness (ILI) patients from Centers for Disease Control and Prevention of the United States from 2002 to 2021. It is a ratio of ILI patients vs. the week's total. %%\\citep{wu2021autoformer}\n\n"
                },
                "subsection 4.2": {
                    "name": "Evaluation Setup",
                    "content": "\nWe evaluate the accuracy of our approach using \\emph{mean absolute error} (MAE) and \\emph{mean squared error} (MSE) metrics, which are well-established in the literature~\\citep{zhou2021informer,wu2021autoformer}, for varying horizon lengths $H$:\n\\begin{equation}\n    \\mathrm{MSE} = \\frac{1}{H} \\sum^{t+H}_{\\tau=t}\\left(\\mathbf{y}_{\\tau}-\\hat{\\mathbf{y}}_{\\tau}\\right)^{2},\\qquad \\mathrm{MAE} = \\frac{1}{H} \\sum^{t+H}_{\\tau=t} |\\mathbf{y}_{\\tau}-\\hat{\\mathbf{y}}_{\\tau}| %\\nonumber\n\\end{equation}\n\nNote that for multivariate datasets, our algorithm produces forecast for each feature in the dataset and metrics are averaged across dataset features. Since our model is univariate, each variable is predicted using only its own history, $\\mathbf{y}_{t-L:t}$, as input. Datasets are partitioned into train, validation and test splits. Train split is used to train model parameters, validation split is used to tune hyperparameters, and test split is used to compute metrics reported in Table~\\ref{table:main_results_multivar}. Appendix~\\ref{section:datasets} shows partitioning into train, validation and test splits: seventy, ten, and twenty percent of the available observations respectively, with the exception of \\ETTm$_2$\\ that uses twenty percent as validation.\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Key Results",
                    "content": "\n\\label{section:main_results}\n\nWe compare \\ours\\ to the following SoTA multivariate baselines: (1) \\FEDformer~\\citep{zhouFEDformer2022}, (2) \\Autoformer~\\citep{wu2021autoformer}, (3) \\Informer~\\citep{zhou2021informer}, (4) \\Reformer~\\citep{kitaev2020reformer} and (5) \\LogTrans~\\citep{li2019logtrans}. Additionally, we consider the univariate baselines: (6) \\DilRNN~\\citep{chang2017dilatedRNN} and (7) auto-\\ARIMA~\\citep{hyndman2008automatic_arima}.\n\n\n\n\\textbf{Forecasting Accuracy.} Table~\\ref{table:main_results_multivar} summarizes the multivariate forecasting results. \\ours\\ outperforms the best baseline, with average relative error decrease across datasets and horizons of \\multivarMAEgains\\% in MAE and \\multivarMSEgains\\% in MSE. \\ours\\ maintains a comparable performance to other state-of-the-art methods for the shortest measured horizon (96/24), while for the longest measured horizon (720/60) decreases multivariate MAE by \\multivarlongMAEgains\\% and MSE by \\multivarlongMSEgains\\%. We complement the key results in Table~\\ref{table:main_results_multivar}, with the additional univariate forecasting experiments in Appendix F, again demonstrating state-of-the-art performance against baselines.\n\n\n\n\\textbf{Computational Efficiency.} We measure the computational training time of \\ours, \\NBEATS\\ and Transformer-based methods in the multivariate setting and show compare in Figure~\\ref{fig:computational_cost_comparison}. The experiment monitors the whole training process for the \\ETTm$_2$ dataset. For the Transformer-based models we used hyperparameters reported in \\citep{wu2021autoformer}. Compared to the Transformer-based methods, \\ours\\ is \\inferenceGains$\\times$ faster than \\Autoformer. In terms of memory, \\ours\\ has less than \\memoryGains\\% \\ of the parameters of the second-best alternative, since it scales linearly with respect to the input's length. Compared to the original \\NBEATS, our method is \\inferenceGainsNBEATS$\\times$ faster and requires only \\memoryGainsNBEATS\\% \\ of the parameters. Finally, while \\ours\\ is an univariate model, it has \\textit{global} (shared) parameters for all time-series in the dataset. Just like \\cite{oreshkin2020nbeats}, our experiments (Appendix I) show that \\ours\\ maintains constant parameter/training computational complexity regarding dataset's size.\n\n"
                },
                "subsection 4.4": {
                    "name": "Training and Hyperparameter Optimization",
                    "content": "\n\\label{section:training_methodology}\n\nWe consider a minimal space of hyperparameters to explore configurations of the \\ours\\ architecture.\nFirst, we consider the kernel pooling size for multi-rate sampling from Equation~(\\ref{equation:nhits_mixed_datasampling}).\nSecond, the number of coefficients from Equation~(\\ref{equation:nhits_projections}) that we selected between several alternatives, some matching common seasonalities of the datasets and others exponentially increasing. We tune the random seed to escape underperforming local minima. Details are reported in Table A3 in Appendix D.\n\nDuring the \\emph{hyperparameter optimization phase}, we measure MAE performance on the validation set and use a Bayesian optimization library (\\HYPEROPT;\\,\\citealt{bergstra2011hyperopt}), with 20 iterations. We use the optimal configuration based on the validation loss to make prediction on the test set. We refer to the combination of hyperparameter optimization and test prediction as a \\emph{run}. \\ours\\ is implemented in \\PyTorch~\\citep{pytorch2019library} and trained using \\ADAM\\ optimizer~\\citep{kingma2014adam_method}, MAE loss, batch size 256 and initial learning rate of 1e-3, halved three times across the training procedure. All our experiments are conducted on a GeForce RTX 2080 GPU.\n\n"
                },
                "subsection 4.5": {
                    "name": "Ablation Studies",
                    "content": "\n\\label{appendix:ablation_main_contributions}\n\nWe believe that the advantages of the \\ours\\ architecture are rooted in its multi-rate hierarchical nature. Fig.~\\ref{fig:decomposition} shows a qualitative comparison of \\ours\\ with and without hierarchical interpolation/multi-rate sampling components. We clearly see \\ours\\ developing the ability to produce interpretable forecast decomposition providing valuable information about trends and seasonality in separate channels, unlike the control model. Appendix G presents the decomposition for the different interpolation techniques. We support our qualitative conclusion with quantitative results. We define the following set of alternative models: \\ours, our proposed model with both multi-rate sampling and hierarchical interpolation, \\ours$_{2}$ only hierarchical interpolation, \\ours$_{3}$ only multi-rate sampling, \\ours$_{4}$ no multi-rate sampling or interpolation (corresponds to the original \\NBEATSg~\\citep{oreshkin2020nbeats}), finally \\NBEATSi, the interpreatble version of the \\NBEATS\\ (\\citep{oreshkin2020nbeats}). Tab.~\\ref{table:ablation_nhits_contributions} clearly shows that the combination of both proposed components (hierarchical interpolation and multi-rate sampling) results in the best performance, emphasizing their complementary nature in long-horizon forecasting. We see that the original \\NBEATS\\ is consistently worse, especially the \\NBEATSi. The advantages of the proposed techniques for long-horizon forecasting, multi-rate sampling and interpolation, are not limited to the \\ours\\ architecture. In Appendix H we demonstrate how adding them to a \\DilRNN\\ improve its performance.\n\n\nAdditional \\emph{ablation studies} are reported in Appendix G. The MaxPool multi-rate sampling wins over AveragePool. Linear interpolation wins over nearest neighbor and cubic. Finally and most importantly, we show that the order in which hierarchical interpolation is implemented matters significantly. The best configuration is to have the low-frequency/large-scale components  synthesized and removed from analysis first, followed by more fine-grained modeling of high-frequency/intermittent signals.\n"
                }
            },
            "section 5": {
                "name": "Discussion of Findings",
                "content": " \\label{section:discussion_of_findings}\nOur results indicate the complementarity and effectiveness of multi-rate sampling and hierarchical interpolation for long-horizon time-series forecasting. Table~\\ref{table:ablation_nhits_contributions} indicates that these components enforce a useful inductive bias compared to both the free-form model $\\ours_4$ (plain fully connected architecture) and the parametric model \\NBEATSi\\  (polynomial trend and sinusoidal seasonality used as basis functions in two respective stacks). The latter obviously providing a detrimental inductive bias for long-horizon forecasting. Notwithstanding our current success, we believe we barely scratched the surface in the right direction and further progress is possible using advanced multi-scale processing approaches in the context of time-series forecasting, motivating further research.\n\n\\ours\\ outperforms SoTA baselines while simultaneously providing an interpretable non-linear decomposition. Fig.~\\ref{fig:motivation} and~\\ref{fig:decomposition} showcase \\ours\\ perfectly specializing and reconstructing latent harmonic signals from synthetic and real data respectively. This novel \\emph{interpretable} decomposition can provide insights to users, improving their confidence in high-stakes applications like healthcare. Finally, \\ours\\ hierarchical interpolation can be explored from the multi-resolution analysis perspective \\citep{daubechies1992ten}. Replacing the sequential projections from the interpolation functions onto these Wavelet induced spaces is an interesting line of research.\n\nOur study raises a question about the effectiveness of the existing long-horizon multi-variate forecasting approaches, as all of them are substantially outperformed by our univariate algorithm. If these approaches underperform due to problems with overfitting and model parsimony at the level of marginals, it is likely that the integration of our approach with Transformer-inspired architectures could form a promising research direction as the univariate results in Appendix F suggest. However, there is also a chance that the existing approaches underperform due to their inability to effectively integrate information from multiple variables, which clearly hints at possibly untapped research potential in this area. Whichever is the case, we believe our results provide a strong guidance signal and a valuable baseline for future research in the area of long-horizon multi-variate forecasting.\n"
            },
            "section 6": {
                "name": "Conclusions",
                "content": " \\label{section5:conclusion}\nWe proposed a novel neural forecasting algorithm \\ours\\ that combines two complementary techniques, multi-rate input sampling and hierarchical interpolation, to produce drastically improved, interpretable and computationally efficient long-horizon time-series predictions. Our model, operating in the univariate regime and accepting only the predicted time-series' history, significantly outperforms all previous Transformer-based multi-variate models using an order of magnitude less computation. This sets a new baseline for all ensuing multi-variate work on six popular datasets and motivates further research to effectively use information from multiple variables.\n\n\n"
            },
            "section 7": {
                "name": "Acknowledgements",
                "content": " \\label{section:acknowledgements}\nThis work was partially supported by the Defense Advanced Research Projects Agency (award FA8750-17-2-0130), the National Science Foundation (grant 2038612), the Space Technology Research Institutes grant from NASA\u2019s Space Technology Research Grants Program, the U.S. Department of Homeland Security (award 18DN-ARI-00031), and by the U.S. Army Contracting Command (contracts W911NF20D0002 and W911NF22F0014 delivery order \\#4). Thanks to Mengfei Cao for in-depth discussion and comments on the method, and Kartik Gupta for his insights on the connection of \\ours\\ with Wavelet's theory. The authors are also grateful to Stefania La Vattiata for her assistance in the upbeat visualization of the \\ourscomplete\\ method.\n\n\\bibliography{aaai23}\n\n\\clearpage\n\\appendix\n% \\section{Appendix}\n% \\label{section:appendix}\n\\setcounter{table}{0}\n\\setcounter{figure}{0}\n\\renewcommand{\\thetable}{A\\arabic{table}}\n\n"
            },
            "section 8": {
                "name": "Neural Basis Approximation Theorem",
                "content": "\n\\label{section:neural_basis_approximation}\n\n%\\clearpage\n"
            },
            "section 9": {
                "name": "Computational Complexity Analysis",
                "content": " \\label{section:complexity}\n\n%\\newpage\n% \\clearpage\n"
            },
            "section 10": {
                "name": "Datasets and Partition",
                "content": " \\label{appendix:datasets}\n\n%\\newpage\n% \\clearpage\n% \\breakpage\n"
            },
            "section 11": {
                "name": "Hyperparameter Exploration",
                "content": " \\label{section:benchmark_hyperparameters}\n\n% \\newpage\n"
            },
            "section 12": {
                "name": "Main results standard deviations",
                "content": "\n\\label{section:standard_deviations}\n\n% \\newpage\n"
            },
            "section 13": {
                "name": "Univariate Forecasting",
                "content": " \\label{section:univariate_forecasting}\n\n\\newpage\n%\\clearpage\n"
            },
            "section 14": {
                "name": "Ablation Studies",
                "content": " \\label{appendix:ablation}\n\n\\newpage\n"
            },
            "section 15": {
                "name": "Multi-rate sampling and Hierarchical Interpolation beyond \\ours",
                "content": " \\label{section:rnn}\n\n%\\newpage\n"
            },
            "section 16": {
                "name": "Hyperparameter Optimization Resources",
                "content": " \\label{section:hyperparameter_resources}\n\n"
            }
        },
        "figures": {
            "fig:motivation": "\\begin{figure}[!ht]\n    \\centering\n    \\subfigure[\\emph{Computational Cost}]{\n    \\label{fig:computational_cost}\n    \\includegraphics[width=0.46\\linewidth]{images/plots-motivation-params-speed.pdf} % 0.51\n    }\n    \\subfigure[\\emph{Prediction Errors}]{\n    \\label{fig:performance}\n    \\includegraphics[width=0.40\\linewidth]{images/plots-motivation-performance.pdf} % 0.435\n    }    \n    \\subfigure[\\emph{Neural Hierarchical Interpolation}]{\n    \\includegraphics[width=0.85\\linewidth]{images/figure1_example_2.pdf} % 0.9\n    }\n    \\caption{(a) The computational costs in time and memory (b) and \\emph{mean absolute errors} (MAE) of the predictions of a high capacity fully connected model exhibit evident deterioration with growing forecast horizons. (c) Specializing a flexible model's outputs in the different frequencies of the signal through hierarchical interpolation combined with multi-rate input processing offers a solution.}\n    \\label{fig:motivation}\n\\end{figure}",
            "fig:nhits_architecture": "\\begin{figure*}[t] %!ht\n\\centering\n\\includegraphics[width=0.9\\linewidth]{images/N_HiTS.pdf}\n\\caption{\\ours\\  architecture. The model is composed of several MLPs with ReLU nonlinearities. Blocks are connected via doubly residual stacking principle with the backcast $\\mathbf{\\tilde{y}}_{t-L:t,\\ell}$ and forecast $\\mathbf{\\hat{y}}_{t+1:t+H,\\ell}$ outputs of the $\\ell$-th block.\nMulti-rate input pooling, hierarchical interpolation and backcast residual connections together induce the specialization of the additive predictions in different signal bands, reducing memory footprint and compute time, improving architecture parsimony and accuracy.\n}\n\\label{fig:nhits_architecture}\n\\end{figure*}",
            "fig:nhits_intuition": "\\begin{figure}[!t] %t !h\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{images/nhits_intuition.pdf}\n    \\vskip -0.19in\n    \\caption{\\ours\\ composes its predictions hierarchically using blocks specialized in different frequencies based on controlled signal projections, through \\emph{expressiveness ratios}, and interpolation of each block. The coefficients are locally determined along the horizon, allowing \\ours\\ to reconstruct non-periodic/stationary signals, beyond constant Fourier transform projections.} \\label{fig:nhits_intuition}\n\\end{figure}",
            "fig:computational_cost_comparison": "\\begin{figure}[t] %!ht\n    \\centering\n    \\subfigure[\\emph{Time Efficiency}]{\\label{fig:inference_comparison}\n    \\includegraphics[width=0.4\\linewidth]{images/plots-comparison-time.pdf}} %0.472\n    \\hspace{10mm} \n    \\subfigure[\\emph{Memory Efficiency}]{\\label{fig:memory_comparison}\n    \\includegraphics[width=0.4\\linewidth]{images/plots-comparison-params.pdf}} % 0.495\n    %\\vspace{-0.5cm}\n    \\caption{Computational efficiency comparison. \\ours\\ exhibits the best training time compared to Transformer-based and fully connected models, and smallest memory footprint.}\n    \\label{fig:computational_cost_comparison}\n\\end{figure}",
            "fig:decomposition": "\\begin{figure*}[ht!]\n\\centering\n\\subfigure[H. interpolation, multi-rate sampling]{\\label{fig:decomposition_mf}\n\\includegraphics[width=0.3\\linewidth]{images/plots-interpetable-decomposition.pdf}}\n\\subfigure[No h. interpolation, multi-rate sampling]{\\label{fig:decomposition_g}\n\\includegraphics[width=0.3\\linewidth]{images/plots-interpetable-decomposition-generic.pdf}}\n\\vspace{-0.25cm}\n\\caption{ETTm2 and 720 ahead forecasts using \\ours\\ (left panel), \\ours\\ with hierarchical linear interpolation and multi-rate sampling removed (right panel). The top row shows the original signal and the forecast. The second, third and fourth rows show the forecast components for each stack. The last row shows the residuals, $y-\\hat{y}$. In (a), each block shows scale specialization, unlike (b), in which signals are not interpretable.} \n\\label{fig:decomposition} %\\textcolor{green}{FINAL}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation} \\label{equation:nhits_mixed_datasampling}\n    \\mathbf{y}^{(p)}_{t-L:t, \\ell} = \\mathbf{MaxPool}\\left(\\mathbf{y}_{t-L:t, \\ell},\\; k_{\\ell}\\right)\n\\end{equation}",
            "eq:2": "\\begin{align} \\label{equation:nhits_projections}\n\\begin{split}\n    \\mathbf{h}_{\\ell}  &= \\mathbf{MLP}_{\\ell}\\left(\\mathbf{y}^{(p)}_{t-L:t,\\ell}\\right) \\\\\n    \\btheta^{f}_{\\ell} &= \\textbf{LINEAR}^{f}\\left(\\mathbf{h}_{\\ell}\\right) \\\\\n    \\btheta^{b}_{\\ell} &= \\textbf{LINEAR}^{b}\\left(\\mathbf{h}_{\\ell}\\right)\n\\end{split}\n\\end{align}",
            "eq:3": "\\begin{align}\n\\begin{split}\n    \\hat{y}_{\\tau,\\ell}   &= g(\\tau, \\btheta^{f}_{\\ell}), \\quad \\forall \\tau \\in \\{t+1,\\dots,t+H\\}, \\\\\n    \\tilde{y}_{\\tau,\\ell} &= g(\\tau, \\btheta^{b}_{\\ell}), \\quad \\forall \\tau \\in \\{t-L,\\dots,t\\}. \n\\end{split}\n\\end{align}",
            "eq:4": "\\begin{align}\n% \\nonumber\n\\begin{split}\ng(\\tau, \\theta) &= \\theta[t_{1}] + \\left(\\frac{\\theta[t_{2}]-\\theta[t_{1}]}{t_{2}-t_{1}}\\right)(\\tau-t_{1}) \\\\\nt_1 &= \\arg\\min_{t \\in \\mathcal{T}: t \\leq \\tau} \\tau - t, \\quad t_2 = t_1 + 1/r_{\\ell}.\n\\end{split}\n\\end{align}",
            "eq:5": "\\begin{equation}\n\\begin{split}\n\\mathbf{\\hat{y}}_{t+1:t+H} &= \\sum^{L}_{l=1} \\hat{\\mathbf{y}}_{t+1:t+H,\\ell} \\nonumber \\\\ \\mathbf{y}_{t-L:t,\\ell+1} &= \\mathbf{y}_{t-L:t,\\ell}-\\mathbf{\\tilde{y}}_{t-L:t,\\ell}   %\\nonumber\n\\end{split}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\int |\\mathcal{Y}(\\tau \\;|\\; \\ylag) -\\sum_{w,h} \\hat{\\theta}_{w,h}(\\ylag) \\phi_{w,h}(\\tau) | d\\tau \\leq \\epsilon\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathrm{MSE} = \\frac{1}{H} \\sum^{t+H}_{\\tau=t}\\left(\\mathbf{y}_{\\tau}-\\hat{\\mathbf{y}}_{\\tau}\\right)^{2},\\qquad \\mathrm{MAE} = \\frac{1}{H} \\sum^{t+H}_{\\tau=t} |\\mathbf{y}_{\\tau}-\\hat{\\mathbf{y}}_{\\tau}| %\\nonumber\n\\end{equation}"
        },
        "git_link": "https://github.com/Nixtla/neuralforecast"
    }
}