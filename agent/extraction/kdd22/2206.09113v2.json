{
    "meta_info": {
        "title": "Pre-training Enhanced Spatial-temporal Graph Neural Network for  Multivariate Time Series Forecasting",
        "abstract": "Multivariate Time Series (MTS) forecasting plays a vital role in a wide range\nof applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have\nbecome increasingly popular MTS forecasting methods. STGNNs jointly model the\nspatial and temporal patterns of MTS through graph neural networks and\nsequential models, significantly improving the prediction accuracy. But limited\nby model complexity, most STGNNs only consider short-term historical MTS data,\nsuch as data over the past one hour. However, the patterns of time series and\nthe dependencies between them (i.e., the temporal and spatial patterns) need to\nbe analyzed based on long-term historical MTS data. To address this issue, we\npropose a novel framework, in which STGNN is Enhanced by a scalable time series\nPre-training model (STEP). Specifically, we design a pre-training model to\nefficiently learn temporal patterns from very long-term history time series\n(e.g., the past two weeks) and generate segment-level representations. These\nrepresentations provide contextual information for short-term time series input\nto STGNNs and facilitate modeling dependencies between time series. Experiments\non three public real-world datasets demonstrate that our framework is capable\nof significantly enhancing downstream STGNNs, and our pre-training model aptly\ncaptures temporal patterns.",
        "author": "Zezhi Shao, Zhao Zhang, Fei Wang, Yongjun Xu",
        "link": "http://arxiv.org/abs/2206.09113v2",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Accepted by SIGKDD 2022 (Research Track)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec_intro}\n\nMultivariate time series data is ubiquitous in our lives, from transportation and energy to economics.\nIt contains time series from multiple interlinked variables.\nPredicting future trends based on historical observations is of great value in helping to make better decisions.\n% For example, traffic forecasting plays an important role in intelligent transportation systems since it is one of the foundations of traffic scheduling optimization.\nThus, multivariate time series forecasting has remained an enduring research topic in both academia and industry for decades.\n\n\n\nIndeed, multivariate time series can be generally formalized as spatial-temporal graph data~\\cite{GWNet}.\nOn the one hand, multivariate time series have complex temporal patterns, \\eg multiple periodicities.\nOn the other hand, different time series can affect other's evolutionary processes because of the underlying interdependencies between variables, which is non-Euclidean and is reasonably modeled by the graph structure.\nTo illustrate, we take the traffic flow system as an example, where each sensor corresponds to a variable.\nFigure \\ref{Intro}(a) depicts the traffic flow time series generated from two sensors deployed on the road network.\nApparently, there are two repeating temporal patterns, \\ie daily and weekly periodicities.\nThe morning/evening peaks occur every day, while the weekdays and weekends exhibit different patterns.\nFurthermore, the two time series share very similar trends because the selected sensors 20 and 301 are closely connected in the traffic network.\nConsequently, accurate time series forecasting depends not only on the pattern of its temporal dimension but also on its interlinked time series.\nBesides, it is worth noting that we made the above analysis on the basis of observing a sufficiently long time series.\n\nTo make accurate predictions, Spatial-Temporal Graph Neural Networks~(STGNNs) have attracted increasing attention recently.\nSTGNNs combine Graph Neural Networks~(GNNs)~\\cite{2017GCN} and sequential models.\nThe former is used to deal with the dependencies between time series, and the latter is used to learn the temporal patterns.\n% \n% For example, DCRNN~\\cite{2017DCRNN} integrates diffusion graph convolution into Recurrent Neural Networks~(RNN).\n% \n% The two most representative works are DCRNN~\\cite{2017DCRNN} and Graph WaveNet~\\cite{GWNet}.\n% DCRNN integrates diffusion graph convolution into Recurrent Neural Networks~(RNN).\n% Graph WaveNet uses diffusion graph convolution and dilated Convolution Neural Network~(CNN)~\\cite{2016TCN} for achieving higher efficiency and performance.\nBenefitting from jointly modeling the spatial and temporal patterns, STGNNs have achieved state-of-the-art performance.\nIn addition, an increasing number of recent works are further exploring the joint learning of graph structures and STGNNs since the dependency graph between time series, which is handcrafted by prior knowledge, is often biased and incorrect, even missing in many cases.\nIn short, spatial-temporal graph neural networks have made significant progress for multivariate time series forecasting in many real-world applications.\nHowever, there is no free lunch.\nMore powerful models require more complex structures.\nThe computational complexity usually increases linearly or quadratically with the length of the input time series.\nFurther considering the number of time series (\\eg hundreds), it is not easy for STGNNs to scale to very long-term historical time series.\nIn fact, most models use historical data in a small window to make predictions, \\eg use the past twelve time steps (one hour) to predict the future twelve time steps~\\cite{2020MTGNN, 2021GTS, 2020GMAN, 2017DCRNN, GWNet}.\nThe inability to explicitly learn from long-term information brings up some intuitive concerns.\n\nFirstly, the STGNN model is blind to the context information beyond the window.\nConsidering that time series are usually noisy, it may be difficult for the model to distinguish short-term time series in different contexts.\nFor example, when observing data within two small windows of length twelve shown in Figure \\ref{Intro}(b), we find that the two time series in different contexts are similar.\nTherefore, it is difficult for models to make accurate predictions about their different future trends based on limited historical data.\nSecondly, short-term information is unreliable for modeling the dependency graph, which is represented by the similarity (or correlation) between time series.\nAs shown in Figure \\ref{Intro}(c), the two time series are not similar when we observe data within the small window, neither in number nor in trend.\nOn the contrary, long-term historical time series are beneficial for resisting noise, which facilitates obtaining more robust and accurate dependencies.\nAlthough long-term historical information is beneficial, as mentioned above, it is expensive for the STGNNs to scale to very long-term historical time series directly.\n% Furthermore, the optimization of the model can also become problematic with the increase of input sequence length.\nFurthermore, the optimization of the model can also become problematic as the length of the input sequence increases.\n\n% In order to solve the above problems, we hope to find another way and not directly design more complex STGNNs.\n% Instead, a model is designed to efficiently extract very long-term historical information, and can effectively provide context information as bias for the raw short-term input of the STGNN models, and at the same time can well represent ultra-long sequences to solve the problem of graph structure learning.\n\n% In order to solve the above problems, we hope to find another way and not directly design more complex STGNNs.\nTo address these challenges, we propose a novel framework, in which \\underline{S}\\underline{T}GNN is \\underline{E}nhanced by a scalable time series \\underline{P}re-training model~(STEP).\nThe pre-training model aims to efficiently learn the temporal patterns from very long-term historical time series and generate segment-level representations, which contain rich contextual information that is beneficial to address the first challenge.\nIn addition, the learned representations of these segments~(\\ie the short-term time series) are able to incorporate the information from the whole long historical time series to calculate the correlation between time series, thus solving the second challenge, the problem of missing the dependency graph.\nSpecifically, we design an efficient unsupervised pre-training model for \\underline{T}ime \\underline{S}eries based on Trans\\underline{Former} blocks~\\cite{2017Transformer}~(TSFormer), which is trained through the masked autoencoding strategy~\\cite{2021MAE}.\nTSFormer efficiently captures information over very long-term historical data over weeks, and produces segment-level representations that correctly reflect complex patterns in time series.\nSecond, we design a graph structure learner based on the representation of TSFormer, which learns discrete dependency graph and utilizes the $k$NN graph computed based on the representation of TSFormer as a regularization to guide the joint training of graph structure and STGNN.\nNotably, STEP is a general framework that can extend to almost arbitrary STGNNs.\nIn summary, the main contributions are the following:\n\\begin{itemize}\n    \\item We propose a novel framework for multivariate time series forecasting, where the STGNN is enhanced by a pre-training model.\n    Specifically, the pre-training model generates segment-level representations that contain contextual information to improve the downstream models.\n    \\item We design an efficient unsupervised pre-training model for time series based on Transformer blocks and train it by the masked autoencoding strategy. \n    % The representations of TSFormer contain rich contextual information that can act as a bias.\n    Furthermore, we design a graph structure learner for learning the dependency graph.\n    \\item Experimental results on three real-world datasets show that our method can significantly enhance the performance of downstream STGNNs, and our pre-training model aptly captures temporal patterns.\n\\end{itemize}\n\n\n\n% sparse discrete \n\n% In this paper, we propose to use pre-training model to efficiently learn from very long-term historical information, and enhance the STGNNs by effectively providing context information as bias for the short-term input of the STGNN models.\n% At the same time, it can well represent very long-term sequences to solve the problem of graph structure learning.\n% Specifically, we first design a efficient unsupervised pre-training model for time series based on transformer blocks~(named TSFormer), which is trained via the masked autoencoding strategy.\n% TSFormer is very good at efficiently capturing information over very long distances, such as thousands of time steps (\\eg history information over weeks), and it produces sub-sequence-level representations that correctly reflect complex patterns in time series, such as the multiple periodicities mentioned above.\n% Thus, STGNNs will not only take a short time series as input, but also consider the representation of this short time series.\n% Further, we design a discrete graph structure learner, which learns a dynamic and sparse dependency graph to facilitate model optimization.\n% Finally, we fuse the representations of TSFormer and STGNN together using MLPs as the semantic projection.\n% Our model is decoupled with the downstream STGNNs, and thus, it can easily extent to arbitrary STGNNs.\n% Ultimately, we can choose any STGNN as the backend without complex changes. \n% Experimental results on three real-world datasets show that our method can significantly enhance the performance of downstream STGNNs.\n% In summary, this work makes several major contributions:\n% \\begin{itemize}\n    % \\item \n% \\end{itemize}\n% 1. 背景\n%% 1.1 时间序列：周期性和模式\n%%% 在生活中无处不在，通常呈现出复杂的周期性和某些特定的模式。\n%%% 通常以“日”、“周”为周期变化[1]。例如Traffic Flow Sensor记录下的早晚高峰模式。\n\n%% 1.2 多变量时间序列：序列之间的关联\n%%% 多变量时间序列不只包含时间维度上的模式，不同的序列之间还相互影响着彼此的演进过程。\n%%% 由于现实世界是一个巨大的复杂系统，多变量时间序列的应用场景更加普遍，例如金融、交通、电力、零售等等。\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\nWe first define the concept of multivariate time series, the dependency graph.\nThen, we define the forecasting problem addressed.\n\n% Frequently used notations are summarized in Table 1.\n\\begin{definition}\n\\textbf{Multivariate Time Series.}\nA multivariate time series has multiple time-dependent variable, such as observations from multiple sensors.\nIt can be denoted as a tensor $\\mathcal{X}\\in\\mathbb{R}^{T\\times N\\times C}$, where $T$ is the number of time steps, $N$ is the number of variables, \\eg the sensors, and $C$ is the number of channels.\nWe additionaly denote the data of time series $i$ as $\\mathbf{S}^i\\in\\mathbb{R}^{T\\times C}$.\n\\end{definition}\n\n\\begin{definition}\n\\textbf{Dependency Graph.}\nEach variable depends not only on its past values but also on other variables. Such dependencies are captured by a dependency graph $\\mathcal{G}=(V, E)$, where $V$ is the set of $|V|=N$ nodes, and each node corresponds to a variable, \\eg a sensor. $E$ is the set of $|E|=M$ edges.\nThe graph can also be denoted as an adjacent matrix $\\mathbf{A}\\in\\mathbb{R}^{N\\times N}$.\n\\end{definition}\n\n\\begin{definition}\n\\textbf{Multivariate Time Series Forecasting.}\nGiven historical signals $\\mathcal{X}\\in\\mathbb{R}^{T_h\\times N\\times C}$ from the past $T_h$ time steps, multivariate time series forecasting aims to predict the values $\\mathcal{Y}\\in\\mathbb{R}^{T_f\\times N\\times C}$ of the $T_f$ nearest future time steps.\n\\end{definition}\n"
            },
            "section 3": {
                "name": "Model Architecture",
                "content": "\n\nAs shown in Figure \\ref{model}, STEP has two stages: the pre-training stage and the forecasting stage.\nIn the pre-training stage, we design a masked autoencoding model for \\underline{T}ime \\underline{S}eries based on Trans\\underline{Former} blocks~(TSFormer) to efficiently learn temporal patterns.\nTSFormer is capable of learning from the very long-term sequence and gives segment-level representations that contain rich context information.\nIn the forecasting stage, we use the pre-trained encoder to provide context information to enhance the downstream STGNN.\nFurthermore, based on the representations of the pre-training model, we further design a discrete and sparse graph learner to deal with the cases that the pre-defined graph is missing.\n\n\\vspace{-0.1cm}\n",
                "subsection 3.1": {
                    "name": "The Pre-training Stage",
                    "content": "\nIn this part, we aim at designing an efficient unsupervised pre-training model for time series.\nWhile the pre-training model has made significant progress in natural language processing~\\cite{2019BERT, 2020GPT, 2018ELMO}, progress in time series lags behind them.\nFirst, we would like to discuss the difference between time series and natural language, which will motivate the design of TSFormer.\nWe attempt to distinguish them from the following two perspectives:\n\n\\textbf{(i) Time series information density is lower.} \n% Compared with natural language, data points in time series give less semantic information.\nAs human-generated signals, each data point in natural language~(\\ie a word in a sentence) has rich semantics and is suitable as the data unit for model input.\nOn the contrary, isolated \n{\\fontsize{8.8pt}{\\baselineskip}\\selectfont \ndata points in time series} give less semantic information. \nSemantics only arise when we observe at least segment-level data, such as going up or down.\nOn the other hand, language models are usually trained by predicting only a few missing words per sentence.\nHowever, masked values in time series can often be trivially predicted by simple interpolation~\\cite{2021KDD_Transformer}, making the pre-training model only focuses on low-level information.\n% , \\ie, numerical information\nTo address this problem, a simple strategy that works well is to mask a very high portion of the model's input to encourage the learning of high-level semantics, motivated by recent development in computer vision~\\cite{2021MAE}. \n% A simple strategy of masking a very high portion of the model’s input will work well to encourage learning of high-level semantics, motivated by recent development in computer vision~\\cite{2021MAE}. \n% To encourage learning of high-level semantics, motivated by recent development in computer vision~\\cite{2021MAE}, a simple strategy of masking a very high portion of the model's input will work well. \nIt creates a challenging self-supervised task that forces the model to obtain holistic understanding of time series.\n\n\\textbf{(ii) Time series require longer sequences to learn the temporal patterns.}\nIn natural languages, sequences of hundreds of lengths have contained rich semantic information.\nThus, language pre-training models usually cut or pad the input sequence to hundreds~\\cite{2018ELMO, 2020GPT, 2019BERT}.\nHowever, although time series have relatively more straightforward semantics than natural languages, they require longer sequences to learn it. For example, traffic system records data every five seconds, and if we want to learn the weekly periodicity, we need at least consecutive 2016 time slices. \nAlthough sampling at a lower frequency is a possible solution, it inevitably loses information.\nFortunately, although longer time series will increase the model complexity, we can alleviate it by stacking fewer Transformer blocks and fix model parameters during the forecasting stage to reduce computational and memory overhead.\n% \\textbf{(iii) Frequency domain is important for obtain significant features.}\n% 不同的语义在时间序列中的时域中可能非常相似。\n% 例如，序列[70, 68, 66, 64] 和 [64, 66, 68, 70]在时域中相似度~(例如cosine相似度）较高，但他们代表上升和下降两种语义。\n% % 频率领域更容易获取到可区分的特征\n% There is little noise in language and images, which basically does affect the discrimination of patterns.\n% In non-adversarial situations, noise generally only appears in individual places (such as typos in natural language) or low-amplitude global noise (such as noise in images).\n% Due to the noise of the sensor and the stability of the system, the time series generally has global noise with high signal-to-noise ratio.\n\n% 基于以上分析以及CV预训练模型最新工作的启发，我们。。。\nMotivated by the above analyses and recent computer vision models~\\cite{2020ViT}, especially Masked AutoEncoder~(MAE)~\\cite{2021MAE}, we propose a masked autoencoding model for time series based on Transformer blocks~(\\ie TSFormer).\nTSFormer reconstructs the original signal based on the given partial observed signals.\nWe use an asymmetric design to largely reduce computation: the encoder operates on only partially visible signals, and the decoder uses a lightweight network on the full signals.\nThe model is shown in Figure \\ref{model}(left), and we will introduce each component in detail next.\n\n\\noindent \\textbf{Masking.} \n{\\color{black}\nWe divide the input sequence $\\mathbf{S}^{i}$ from node $i$ into $P$ non-overlapping patches of length $L$~(Input sequences are obtained over the original time series through a sliding window of length $P*L$).\nThe $j$th patch can be denoted as $\\mathbf{S}_j^i\\in\\mathbb{R}^{LC}$, where $C$ is the input channel.}\nWe assume $L$ is the commonly used length of input time series of STGNNs.\nWe randomly mask a subset of patches with masking ratio $r$ set to a high number of 75$\\%$ to create a challenging self-supervised task.\nHere, we emphasize that the strategy of using patches as input units serves multiple purposes.\nFirstly, segments~(\\ie patches) are more appropriate for explicitly providing semantics than separate points.\nSecondly, it facilitates the use of downstream models, as downstream STGNNs take a single segment as input.\nLast but not least, it significantly reduces the length of sequences input to the encoder, and the high masking ratio $r$ makes the encoder more efficient during the pre-training stage.\n\n% First, it significantly reduces the length of sequences input to the encoder.\n% Second, it facilitates the use of downstream models since downstream STGNNs also take a sequence of length $L$ as input.\n% Last but not least, the highly masking ratio $r$ makes the encoder very efficient during the pre-training stage.\n\n\\noindent \\textbf{Encoder.} \n% Standard Transformer Blocks\n% Embedding Layer\n% Only operates on observed pathces\n% Learnable positional encoding~(Different from MAE)\n    % Reason 1: better performance\n    % Reason 2: reflects the periodicity, which is demonstrated in Section X.\n% 4Layer\n% Our encoder is a series of Transformer blocks with a linear projection~(\\ie the input embedding layer) and a positional embedding layer.\n% The encoder only operates on unmasked patches.\n% As the semantics of time series are more straightforward, we use four layers of  Transformer blocks, far less than the depth of Transformer-based models in computer vision~\\cite{2020ViT, 2021MAE} and natural languages~\\cite{2019BERT, 2020GPT}. \n% Notably, the positional embeddings are added to all patches, although the mask tokens are not used in the encoder. \n% Moreover, unlike the deterministic, sinusoidal embeddings used in MAE~\\cite{2021MAE}, we use learnable positional embeddings. \n% On the one hand, in this work, learnable embeddings significantly outperform sinusoidal ones for all datasets. \n% On the other hand, we observed that learned positional embeddings is crucial in learning time series' periodic features, which will be demonstrated in Section \\ref{sec_inspecting}.\n Our encoder is a series of Transformer blocks~\\cite{2017Transformer} with an input embedding layer and a positional encoding layer. \nThe encoder only operates on unmasked patches.\nAs the semantics of time series are more straightforward than languages, we use four layers of  Transformer blocks, far less than the depth of Transformer-based models in computer vision~\\cite{2020ViT, 2021MAE} and natural languages~\\cite{2019BERT, 2020GPT}. \nSpecifically, the input embedding layer is a linear projection to transform the unmasked patches into latent space:\n\\begin{equation}\n\\setlength\\abovedisplayskip{0.2cm}\n\\setlength\\belowdisplayskip{0.2cm}\n    \\mathbf{U}_j^i = \\mathbf{W}\\cdot\\mathbf{S}^i_j + \\mathbf{b},\n\\end{equation}\nwhere $\\mathbf{W}\\in\\mathbb{R}^{d\\times(LC)}$ and $\\mathbf{b}\\in\\mathbb{R}^d$ are learnable parameters, $\\mathbf{U}^i_j\\in\\mathbb{R}^{d}$ are the model input vectors, and $d$ is the hidden dimension.\nFor masked patches, we use a shared learnable mask token to indicate the presence of a missing patch to be predicted.\nNext, the positional encoding layer is used to add sequential information.\nNotably, the positional encoding operates on all patches, although the mask tokens are not used in the encoder. \nMoreover, unlike the deterministic, sinusoidal embeddings used in MAE~\\cite{2021MAE}, we use learnable positional embeddings. \nOn the one hand, in this work, learnable embeddings significantly outperform sinusoidal ones for all datasets. \nOn the other hand, we observe that learned positional embeddings are crucial in learning time series' periodic features, which will be demonstrated in Section \\ref{sec_inspecting}.\nFinally, we obtain the latent representations $\\mathbf{H}_j^i\\in\\mathbf{R}^d$ through Transformer blocks for all unmasked patches $j$.\n\n\n\n\\noindent \\textbf{Decoder.} \nThe decoder is also a series of Transformer blocks that reconstruct the latent representations back to a lower semantic level, \\ie numerical information. \nThe decoder operates on the full set of patches, including the mask tokens.\n% We use a shared learnable mask token to indicate the presence of a missing patch to be predicted. \nUnlike MAE~\\cite{2021MAE}, we no longer add positional embeddings here since all patches already have positional information added in the encoder. \nNotably, the decoder is only used during the pre-training stage to perform the sequence reconstruction task, and can be designed independently of the encoder.\nWe use only a single layer of Transformer block for balancing efficiency and effectiveness.\n% Finally, we apply a Multi-Layer Perception~(MLP) to make predictions whose number of output dimensions equals the length of each patch, \\ie $\\text{L}$.\n{\\color{black}\nFinally, we apply Multi-Layer Perceptions~(MLPs) to make predictions whose number of output dimensions equals the length of each patch.\nSpecifically, given the latent representation $\\mathbf{H}^i_j\\in\\mathbb{R}^d$ of patch $j$, the decoder gives the reconstructed sequence $\\hat{\\mathbf{S}}^i_j\\in\\mathbb{R}^{LC}$.}\n\n\\noindent \\textbf{Reconstruction target.}\nOur loss function compute mean absolute error between the original sequence $\\mathbf{S}^i_j$ and reconstructed sequence $\\hat{\\mathbf{S}}^i_j$.\nKindly note that we only compute loss over the masked patches, which is in line with other pre-training models~\\cite{2021MAE, 2019BERT}. Moreover, all these operations are computed in parallel for all time series $i$.\n% After the projection layer, we will get a reconstructed time series.\n% Our model is trained by predicting sequence of each masked patches.\n% We use mean absolute error as the distance the reconstructed and original sequence.\n% We adopt mean absolute error as the loss function for the reconstructed and original sequence.\n% Kindly note that the loss is only computed on the masked patches.\n% Note that the loss is only computed on the masked patches.\n\nIn summary, TSFormer is efficient thanks to the high masking ratio and fewer Transformer blocks.\nTSFormer is capable of learning from the very long-term sequence (\\eg weeks) and can be trained on a single GPU.\nThe encoder generates representations for the input patches~(segments).\n% and their concatenation can represent the entire input sequence. \nFurthermore, another noteworthy difference from MAE~\\cite{2021MAE} is that we pay more attention to the representations of the patches. \nOn the one hand, we can use the representations to verify periodic patterns in the data, which will be demonstrated in Section \\ref{sec_inspecting}. \nMore importantly, they can conveniently act as contextual information for short-term input of downstream STGNNs, which will be introduced in the next.\n% More details and formulas can be found in Appendix \\ref{appendix_tsformer}.\n% downstream models since the STGNNs take the last patch as input.\n\n"
                },
                "subsection 3.2": {
                    "name": "The Forecasting Stage",
                    "content": "\n% 预测阶段：我们使用预训练模型增强时空图神经网络。\n% 输入输出：对于给定的时间片，STGNN通常选取较短的L长度的历史数据为输入。TSFormer将这个输入从L扩展到了P*L，产生了P个对应的表征H_i?。\n% 我们使用这些表征建模缺失的依赖图，并增强下游时空图神经网络。\nFor a given time series $i$, TSFormer takes its historical signals $\\mathbf{S}^i\\in\\mathbb{R}^{T_p\\times C}$ of the past $T_p=L\\times P$ time steps as input.\nWe divide it into $P$ non-overlapping patches of length $L$: $\\mathbf{S}_1^i, \\cdots, \\mathbf{S}_P^i$, where $\\mathbf{S}_j^i\\in\\mathbb{R}^{L\\times C}$.\nThe pre-trained TSFormer encoder generates representations $\\mathbf{H}_j^i\\in\\mathbb{R}^{d}$ for each $\\mathbf{S}_j^i$, where $d$ is the dimension of hidden states.\nConsidering that the computational complexity usually increases linearly or quadratically with the length of the input time series, STGNNs can only take the latest, \\ie the last patch $\\mathbf{S}_P^i\\in\\mathbb{R}^{L\\times C}$ for each time series $i$ as input.\nFor example, the most typical setting is $L=12$.\nIn the forecasting stage, we aim at enhancing the STGNNs based on the representations of the pre-trained TSFormer encoder.\n\n% STGNN依赖一个图\n\\noindent\\textbf{Graph structure learning.} Many STGNNs~\\cite{2017DCRNN, GWNet,2020GMAN} depend on a pre-defined graph to indicate the relationship between nodes (\\ie time series). \nHowever, such a graph is not available or is incomplete in many cases.\n% 直观的想法\nAn intuitive idea is to train a matrix $\\mathbf{A}\\in\\mathbb{R}^{N\\times N}$, where $\\mathbf{A}_{ij}\\in[0,1]$ indicates the dependency between time series $i$ and $j$.\n% 难点 learn the spatial dependencies in a supervised manner.\nHowever, since the learning of graph structure and STGNNs are coupled compactly, and there is no supervised loss information for graph structure learning~\\cite{2021REST}, optimizing such a contiguous matrix usually leads to a complex bilevel optimization problem~\\cite{2019LDS}.\nIn addition, the dependency $\\mathbf{A}_{ij}$ is usually measured by the similarity between time series, which is also a challenging task.\n\nFortunately, we can alleviate these problems based on the pre-trained TSFormer.\nMotivated by recent works~\\cite{2018NRI, 2019LDS, 2021GTS}, we aim to learn a discrete sparse graph, where $\\mathbf{\\Theta}_{ij}$ parameterizes the Bernoulli distribution from which the discrete dependency graph $\\mathbf{A}$ is sampled.\n% 做法\nFirst, we introduce graph regularization to provide supervised information for graph optimization based on the representations of TSFormer.\nSpecifically, we denote $\\mathbf{H}^i=\\mathbf{H}^i_1\\parallel\\mathbf{H}^i_2...\\mathbf{H}^i_{P-1}\\parallel\\mathbf{H}^i_{P}\\in\\mathbb{R}^{Pd}$ as the feature of time series $i$, where $\\parallel$ means the concatenation operation.\nThen we calculate a $k$NN graph $\\mathbf{A}^a$ among all the nodes. We can control the sparsity of the learned graph by setting different $k$.\nBenefiting from the ability of TSFormer, $\\mathbf{A}^a$ can reflect the dependencies between nodes, which is helpful to guide the training of the graph structure.\nThen, we compute $\\mathbf{\\Theta}_{ij}$ as follows:\n\\begin{equation}\n    \\begin{aligned}\n    \\mathbf{\\Theta}_{ij}&=\\text{FC}(\\text{relu}(\\text{FC}(\\mathbf{Z}^i\n    \\parallel \\mathbf{Z}^j)))\\\\\n    \\mathbf{Z}^i&= \\text{relu}(\\text{FC}(\\mathbf{H}^i)) + \\mathbf{G}^i,\n    \\end{aligned}\n\\end{equation}\nwhere $\\mathbf{\\Theta}_{ij}\\in \\mathbb{R}^{2}$ is the unnormalized probability. The first dimension indicates the probability of positive, and the second dimension indicates  the probability of negative.\n{\\color{black}$\\mathbf{G}^i$ is the global feature of time series $i$, which is obtained by a convolutional network $\\mathbf{G}^i=\\text{FC}(\\text{vec}(\\text{Conv}(\\mathbf{S}^i_{train})))$, where\n$\\mathbf{S}^i_{train}\\in \\mathbb{R}^{L_{train}}$ is the entire sequence $i$ over training dataset, and $L_{train}$ is the length of the training dataset.}\n$\\mathbf{S}^i_{train}$ is static for all samples during training, helping to make the training process more robust and accurate.\nThe feature $\\mathbf{H}^i$ is dynamic for different training samples to reflect the dynamics of dependency graphs~\\cite{2021DGCRN}.\nAs such, we use the cross-entropy between $\\mathbf{\\Theta}$ and the $k$NN graph $A^a$ as graph structure regularization:\n\\begin{equation}\n    \\mathcal{L}_{graph} = \\sum_{ij}-\\mathbf{A}_{ij}^a\\log\\mathbf{\\Theta}^{'}_{ij}-(1-\\mathbf{A}_{ij}^{a})\\log(1-\\mathbf{\\Theta}^{'}_{ij}),\n\\end{equation}\nwhere $\\mathbf{\\Theta}^{'}_{ij}=\\text{softmax}(\\mathbf{\\Theta}_{ij})\\in\\mathbb{R}$ is the normalized probability.\n\nThe last problem of discrete graph structure learning is that the sampling operation from $\\mathbf{\\Theta}$ to adjacent matrix $\\mathbf{A}$ is not differentiable.\nHence, we apply the Gumbel-Softmax reparametrization trick proposed by \\cite{Gumbel1, Gumbel2}:\n\\begin{equation}\n    \\mathbf{A}_{ij}=\\text{softmax}((\\mathbf{\\Theta}_{ij}+\\mathbf{g})/\\tau),\n\\end{equation}\nwhere $\\mathbf{g}\\in \\mathbb{R}^2$ is a vector of i.i.d. samples drawn from a $\\text{Gumbel(0,1)}$ distribution.\n$\\tau$ is the softmax temperature parameter.\nThe Gumbel-Softmax converges to one-hot samples~(\\ie discrete) when $\\tau \\rightarrow 0$.\n\n\\noindent\\textbf{Downstream spatial-temporal graph neural network.}\nA normal downstream STGNN takes the last patch and the dependency graph as input, while the enhanced STGNN also considers the input patch's representation.\nSince the TSFormer has strong power at extracting very long-term dependencies, the representation $\\mathbf{H}^i_P$ contains rich context information.\nSTEP framework can extend to almost any STGNN, and we choose a representative method as our backend, the Graph WaveNet~\\cite{GWNet}.\nGraph WaveNet captures spatial-temporal dependencies efficiently and effectively by combining graph convolution with dilated casual convolution. \nIt makes predictions based on its output latent hidden representations $\\mathbf{H}_{gw}\\in\\mathbb{R}^{N\\times d'}$ by a regression layer, which is a Multi-Layer Perception~(MLP).\n{\\color{black}For brevity, we omit its details, and interested readers can refer to the paper~\\cite{GWNet}}.\nDenoting the representations $\\mathbf{H}^i_P$ of TSFormer for all node $i$ as $\\mathbf{H}_P\\in\\mathbb{R}^{N\\times d}$, we fuse the representations of Graph WaveNet and TSFormer by:\n\\begin{equation}\n    \\mathbf{H}_{final}=\\text{SP}(\\mathbf{H}_P) + \\mathbf{H}_{gw},\n    \\label{fuse}\n\\end{equation}\nwhere $\\text{SP}(\\cdot)$ is the semantic projector to transform the $\\mathbf{H}^i_P$ to the semantic space of $\\mathbf{H}_{gw}$. We implement it with a MLP.\nFinally, we make predictions by the regression layer:  \n$\\hat{\\mathcal{Y}}\\in\\bb{R}^{T_f\\times N \\times C}$. \nGiven the ground truth $\\cal{Y}\\in\\bb{R}^{T_f\\times N \\times C}$, we use mean absolute error as the regression loss:\n\\begin{equation}\n    \\cal{L}_{regression} = \\cal{L}(\\hat{\\cal{Y}}, \\cal{Y})=\\frac{1}{T_fNC}\n    \\sum_{j=1}^{T_f}\\sum_{i=1}^{N}\\sum_{k=1}^{C}|\\hat{\\cal{Y}}_{ijk} - \\cal{Y}_{ijk}|, \n    \\label{loss}\n\\end{equation}\nwhere $N$ is the number of nodes, $T_f$ is the number of forecasting steps, and $C$ is the dimensionality of the output.\nThe downstream STGNN and the graph structure is trained in an end-to-end manner:\n\\begin{equation}\n    \\mathcal{L} = \\mathcal{L}_{regression} + \\lambda \\mathcal{L}_{graph}.\n    \\label{full_loss}\n\\end{equation}\nWe set the graph regularization term $\\lambda$ gradually decay during the training process to go beyond the $k$NN graph.\nNotably, the pre-trained TSFormer encoder is fixed in the forecasting stage to reduce computational and memory overhead.\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\n% In this section, we present experiments on three commonly used real-world datasets to demonstrate the effectiveness of STEP, including the proposed TSFormer and the enhanced STGNN.\n% 我们首先介绍实验设置，包括数据集、Baseline和参数设定。\n% 之后，我们对比D2STGNN和Baselines在两个数据集的上的性能。\n% 然后，我们设计实验验证了Decoupled Framework的优越性。\n% 最后，我们做了详尽的消融实验和参数实验，来验证我们方法的每个部件的作用。\n% We first introduce the experimental settings, including datasets, baselines, metrics, and parameter settings.\n% Then, we conduct experiments to compare the performance of STEP with other baselines.\n% Furthermore, we design more experiments to demonstrate the ability of the TSFormer in learning temporal patterns.\n% Finally, we design comprehensive ablation studies to evaluate the impact of important hyper-parameters and components.\n\n% {\\color{blue}Following previous works~\\cite{2021GTS, 2020MTGNN}}, \nIn this section, we present experiments on three real-world datasets to demonstrate the effectiveness of the proposed STEP and TSFormer.\nFurthermore, we conduct comprehensive experiments to evaluate the impact of important hyper-parameters and components.\nMore experimental details, such as optimization settings and efficiency study, can be found in Appendix \\ref{appendix_experiments}, \\ref{appendix_efficiency}, and \\ref{appendix_visualization}.\nIt is notable that we conduct pre-training for each dataset since these datasets are heterogeneous in terms of length of time series, physical nature, and temporal patterns. \nOur code can be found in this repository\\footnote{\\url{https://github.com/zezhishao/STEP}}.\n\n",
                "subsection 4.1": {
                    "name": "Experimental Setup",
                    "content": "\n\\noindent\\textbf{Datasets.}\nFollowing previous works~\\cite{2021GTS, 2020MTGNN, GWNet}, we conduct experiments on three commonly used multivariate time series datasets:\n\\begin{itemize}\n    \\item \\textbf{METR-LA} is a traffic speed dataset collected from loop-detectors located on the LA County road network~\\cite{METR-LA}. \n    It contains data of 207 selected sensors over a period of 4 months from Mar to Jun in 2012~\\cite{2017DCRNN}. \n    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 34,272.\n    \\item \\textbf{PEMS-BAY} is a traffic speed dataset collected from California Transportation Agencies (CalTrans) Performance Measurement System (PeMS)~\\cite{PEMS-BAY}.\n    It contains data of 325 sensors in the Bay Area over a period of 6 months from Jan 1st 2017 to May 31th 2017~\\cite{2017DCRNN}.\n    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 52,116.\n    \\item \\textbf{PEMS04} is a traffic flow dataset also collected from CalTrans PeMS~\\cite{PEMS-BAY}.\n    It contains data of 307 sensors in the Bay Area over a period of 2 months from Jan 1st 2018 to Feb 28th 2018~\\cite{2019ASTGCN}.\n    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 16,992.\n    \n\\end{itemize}\nThe statistical information is summarized in Table \\ref{tab:datasets}.\nFor a fair comparison, we follow the dataset division in previous works.\nFor METR-LA and PEMS-BAY, we use about 70\\% of data for training, 20\\% of data for testing, and the remaining 10\\% for validation~\\cite{2017DCRNN, GWNet}.\nFor PEMS04, we use about 60\\% of data for training, 20\\% of data for testing, and the remaining 20\\% for validation~\\cite{2021ASTGNN, 2019ASTGCN}.\n\n\n\\noindent\\textbf{Baselines.} We select a wealth of baselines that have official public code.\n% including the traditional methods and the typical deep learning methods, as well as  the very recent state-of-the-art works.\nHistorical Average~(HA), VAR~\\cite{VAR}, and SVR~\\cite{SVR} are traditional methods.\nFC-LSTM~\\cite{2014Seq2Seq}, DCRNN~\\cite{2017DCRNN}, Graph WaveNet~\\cite{GWNet}, ASTGCN~\\cite{2019ASTGCN}, and STSGCN~\\cite{2020STSGCN} are typical deep learning methods. GMAN~\\cite{2020GMAN}, MTGNN~\\cite{2020MTGNN}, and GTS~\\cite{2021GTS} are recent state-of-the-art works.\nMore details of baselines can be found in Appendix \\ref{appendix_basline}.\n\n\\noindent\\textbf{Metrics.}\nWe evaluate the performances of all baselines by three commonly used metrics in multivariate time series forecasting, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and Mean Absolute Percentage Error (MAPE).\n\n\\noindent\\textbf{Implementation.}\n% The proposed model is implemented by Pytorch 1.9.1 on two NVIDIA Tesla V100 GPUs.\nWe set patch size $L$ to 12.\nWe set the number of patches $P$ to 168 for METR-LA and PEMS-BAY, and 336 for PEMS04, \\ie we use historical information for a week for METR-LA and PEMS-BAY, and two weeks for PEMS04.\nWe aim at forecasting the next 12 time steps.\nThe masking ratio $r$ is set to 75\\%.\nThe hidden dimension of the latent representations of TSFormer $d$ is set to 96.\nThe TSFormer encoder uses 4 layer of Transformer blocks, and the decoder uses 1 layer.\nThe number of attention heads in Transformer blocks is set to 4.\nThe hyper-parameter of Graph WaveNet is set to default in their papers~\\cite{GWNet}.\nFor the $k$NN graph $\\mathbf{A}^a$, we set $k$ to 10.\nWe perform significance tests~(t-test with p-value < 0.05) over all the experimental results.\n\n\n% \\vspace{-0.3cm}\n"
                },
                "subsection 4.2": {
                    "name": "Main Results",
                    "content": "\n% 我们的模型在所有的数据集上一致地好于其他的模型\n% GTS、MTGNN联合建模了图结构和下游图神经网络，\n% % MTGNN replaces the GNN and Gated TCN in Graph WaveNet with mix-hop propagation layer [1] and dilated inception layer, and proposes the learning of latent adjacency matrix to seek further improvement.\n% % GTS introduces a neighborhood graph as a regularization that improves graph quality and reformulate the problem as a unilevel optimization problem.\n% % However, 他们无法consistently超越他们baseline。\n% % Kindly note that the results of GTS may have some gaps with the original paper. This is mainly due to GTS calculated the evaluation metrics in a slightly different manner, some details can be found in the appendix in the GTS paper~\\cite{2021GTS} and similar issues in its official repository\\footnote{https://github.com/chaoshangcs/GTS/issues} We unify the evaluation calculation process and run GTS five times, and report its best performance.\n% Benefiting from the powerful ability of the attention mechanism in capturing long-term dependency, GMAN performs better in long-term prediction.\n% DCRNN and Graph WaveNet are two typical spatial-temporal graph neural networks. Even if compared with many of the newer works such as ASTGCN and STSGCN, their performance is still very promising. This may be due to their refined data assumptions and reasonable model architecture. \n% FC-LSTM, a classic recurrent neural network for sequential data, can not perform well since it only considers temporal features, but ignores the dependencies among time series.\n% Other non-deep learning methods HA, VAR, and SVR perform worst since they have strong assumption about the data, \\eg stationary or linear. Thus, they can not capture the strong nonlinear and dynamic spatial and temporal correlations in real world datasets.\nAs shown in Table \\ref{tab:main}, our STEP framework consistently achieves the best performance in almost all horizons in all datasets, indicating the effectiveness of our framework.\nGTS and MTGNN jointly learn the graph structure among multiple time series and the spatial-temporal graph neural networks.\nGTS extends DCRNN by introducing a neighborhood graph as a regularization to improve graph quality and reformulates the problem as a unilevel optimization problem. \nMTGNN replaces the GNN and Gated TCN in Graph WaveNet with mix-hop propagation layer~\\cite{2019MixHop} and dilated inception layer, and proposes to learn latent adjacency matrix to seek further improvement.\nHowever, they can not consistently outperform other baselines.\nKindly note that the results of GTS may have some gaps with the original paper because it calculates the evaluation metrics in a slightly different manner.\nSome details can be found in the appendix in the original paper~\\cite{2021GTS} and similar issues in its official code repository\\footnote{\\url{https://github.com/chaoshangcs/GTS/issues}}.\nWe unify the evaluation process with other baselines, run GTS five times, and report its best performance.\nGMAN performs better in long-term prediction benefiting from the powerful ability of the attention mechanism in capturing long-term dependency.\nDCRNN and Graph WaveNet are two typical spatial-temporal graph neural networks. \nEven compared with many newer works such as ASTGCN and STSGCN, their performance is still very promising. \nThis may be due to their refined and reasonable model architecture. \nFC-LSTM, a classic recurrent neural network, can not perform well since it only considers temporal features, ignoring the dependencies between time series.\nOther non-deep learning methods HA, VAR, and SVR perform worst since they have strong assumptions about the data, \\eg stationary or linear. \nThus, they can not capture the strong nonlinear and dynamic spatial and temporal correlations in real-world datasets.\n\nIn a nutshell, STEP provides stable performance gains for Graph WaveNet by fully exploiting representations extracted by TSFormer from very long-term historical time series.\nHowever, despite the significant performance improvements, it is difficult for us to intuitively understand what TSFormer has learned and how it can help STGNNs. \n% In the next subsection, we will verify that TSFormer has learned multiple periodicities in the time dimension.\nIn the next subsection, we will inspect the TSFormer and demonstrate the learned multiple periodicities temporal pattern.\n\n% Traditional methods such as HA, SVR perform worst because of their strong assumption about the data, \\eg stationary or linear. \n% Therefore, they cannot capture the complex spatial and temporal dependencies in real-world data.\n% FC-LSTM, a classic recurrent neural network for sequential data, can not perform well since it only considers temporal features, but ignores the spatial impact in traffic data and, which is crucial in traffic forecasting.\n% VAR takes both spatial and temporal information into consideration, thus it achieves better performance.\n% However, VAR cannot capture strong nonlinear and dynamic spatial-temporal correlations. \n% {\\color{black} Recently proposed spatial-temporal models overcome these shortcomings and make considerable progress. DCRNN and Graph WaveNet are two typical spatial-temporal coupling models among them.}\n% Graph WaveNet combines GNN and Gated TCN to form a spatial-temporal layer while DCRNN replaces the fully connected layer in GRU by diffusion convolution to get a diffusion convolutional GRU.\n% Even if compared with many of the latest works such as ASTGCN and STSGCN, their performance is still very promising. \n% This may be due to their refined data assumptions and reasonable model architecture.\n% MTGNN replaces the GNN and Gated TCN in Graph WaveNet with mix-hop propagation layer~\\cite{2019MixHop} and dilated inception layer, and proposes the learning of latent adjacency matrix to seek further improvement.\n% ASTGCN and STSGCN perform not well on all four datasets, which is possibly because their model is complex and based on the coupling framework, making them more difficult to converge. \n% Benefiting from the powerful ability of the attention mechanism in capturing long-term dependency, GMAN performs better in long-term prediction.\n% {\\color{black} Based on the DCRNN architecture, DGCRN captures the dynamic characteristics of the spatial topology and achieves better performance than other baselines.\n% Our model still outperforms DGCRN. We conjecture the key reason lies in the decoupled ST framework.\n% }\n\n% In a nutshell, the results in Table~\\ref{tab:main} validate the superiority of D$^2$STGNN over various baselines.\n\n% Note that the final performance is affected by many aspects: the modeling of temporal and spatial dependencies and spatial topology.\n% Therefore, although Table \\ref{tab:main} has shown the superiority of the D$^2$STGNN model, it is not enough to evaluate the effectiveness of the decoupled spatial-temporal framework. \n\n% 先按照常规操作进行说明\n% 再说明结论：TSFormer极大的增强了时空图神经网络，但是我们难以从这个结果中直观地理解TSFormer到底学到了什么。下一小节中，我们将验证TSFormer学到了时间维度上的多周期性。\n\n"
                },
                "subsection 4.3": {
                    "name": "Inspecting The TSFormer",
                    "content": "\n\\label{sec_inspecting}\n\n\n\n% 目标：在这一部分，我们想要直观的理解TSFormer学到了什么。\n% 设置：因此，我们基于PEMS04数据集，随机选取一个时间序列并随机选取一个验证样本，来分析TSFormer及其输出。在我们的设置中，PEMS04一个样本具有336个Patches，即覆盖两个周的时间长度。\nIn this subsection, we would like to intuitively explore what TSFormer has learned.\nWe conduct experiments on the PEMS04 dataset.\nSpecifically, we randomly select a time series in the PEMS04 and then randomly choose a sample of the test dataset to analyze TSFormer.\nNote that each input sample in PEMS04 has 336 patches of length 12, which means it covers data of the past two weeks.\n\n% \\subsubsection{Learned Temporal Pattern}\n\\noindent\\textbf{Learned temporal pattern.}\n% 第一个我们想要知道的问题是TSFormer是否学到时间维度的模式。\n% 我们期望它输出有意义的表征，并且能够解决图1(b)中的问题。\n% 因此，给定输入的Patch，我们随机选取其中一个，然后计算输出的表征和其他Patches的相似度，选取最相似的几个。\n% 我们使用Cos Similarity作为相似度度量。\n% 实验结果如图()所示。所有selected paches都用红色的线画出。我们选择了第36个Patches作为匹配模板，我们选取3个相似的其他Patches，which被红色圈出。显然模型正确的归类Patches。\n% 为了获取bigger picture，我们还计算了所有Patches两两之间的相似度，得到了一个\n% 其结果如图()所示，呈现了明显的周期性。\n% Patches之间的相似性由daily模式和Weekly模式决定。\n% 每一个Patch会一天中同一时刻的Patch相似，和一周中同一天的同一时刻的Patches更相似。\n% 全蓝的Column或者Row，代表这个时刻传感器出现了宕机或者存在很大的噪声波动，造成它和其他的Patches都不相似。\n% 由于TSFormer学到了正确的Patches之间的关系，它能够显著地增强下游时空图神经网络也就不奇怪了。\nFirstly, we would like to explore whether TSFormer learned temporal patterns.\nWe expect it to generate meaningful representations and be able to solve the problem in Figure \\ref{Intro}(b).\nTherefore, we randomly select a patch, compute the cosine similarity with the representations of all the other patches, and select the most similar 3 patches. The result is shown in Figure \\ref{inspecting}(a), where the original patch is in the black circle, and the selected most similar patches are in the red circle. \n% Apparently, TSFormer extract has learned temporal patterns.\nApparently, TSFormer has a strong ability to identify similar patches.\nFurthermore, in order to get the bigger picture, we also calculate the pairwise similarity between all patches and get a $336\\times 336$ heat map, where element in $i$-th column and $j$-th row indicates the cosine similarity between patch $i$ and patch $j$.\nThe result shown in Figure \\ref{inspecting}(c) presents clear daily and weekly periodicities.\n% Each patch is similar to a patch at the same time of a day, and more similar to a patch at the same time of the week on the same day.\nFor each patch, it is similar to the patch at the same time of a day, and the most similar patch usually falls on the same time of the week on the same day. \nThe observation is in line with human intuition.\nThe blue columns or rows mean that the sensor is down or has a large noise fluctuation at this moment, which makes it different from other patches.\nSince TSFormer has learned the correct relationship between patches, it is reasonable that it can significantly enhance the downstream STGNNs.\n\n% \\subsubsection{Reconstruction Visualization}\n\\noindent\\textbf{Reconstruction visualization.}\n% 另外，我们还可视化了TSFormer重建的结果。\n% 结果显示TSFormer能够基于少量的Unmasked Patches准确的重建出Masked Patches。\nAdditionally, we also visualized the results of the TSFormer reconstruction, which is shown in Figure \\ref{inspecting}(b), where the grey line presents masked patches and the red line demonstrates the reconstruction.\nThe results show that TSFormer can effectively reconstruct masked patches based on a small number of unmasked patches~(blue line).\n\n% \\subsubsection{Positional embeddings}\n\\noindent\\textbf{Positional embeddings.}\n% TSFormer和MAE、经典的Transformer之间的一个较大的不同就是可学习的Positional Embedding。\n% 我们想要知道它是否学习到了合理的位置编码。\n% 因此，我们计算336个Patches的位置编码两两之间的Cosine相似度，其热力图如图(a)所示。\n% 我们惊讶的发现，TSFormer的位置表征非常好的反映了时间序列中的多周期性。\n% 主要是因为不同于输出的表征需要依赖于输入，位置编码是完全自由训练的，它更少地受输入数据的噪声的影响。\n% TSFormer能够学习到的符合时间模式的位置表征是TSFormer能够成功的关键因素。\n% 将可学习的PE替换成sin PE时，我们发现TSFormer无法得到有意义的输出。\nAnother important difference between TSFormer and MAE~\\cite{2021MAE} and the original Transformer~\\cite{2017Transformer} is the learnable positional embedding. \nTherefore, we would like to explore whether TSFormer has learned reasonable positional embeddings.\nWe compute the cosine similarity between the positional embeddings of 336 patches and get a $336\\times 336$ heat map, shown in Figure \\ref{inspecting}(d).\nWe find that the positional embedding of TSFormer better reflects the multi-periodicity in time series.\nThis is because, unlike the representation of the encoder, which needs to depend on the input patches, the positional embeddings are completely free to optimize and are less affected by the noise of the input data.\nWe conjecture that such positional embedding is the key factor for the success of TSFormer since we found that TSFormer could not get meaningful representations if we replace the learnable positional embeddings with the deterministic, sinusoidal ones.\n\n\\vspace{-0.2cm}\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\nIn this part, we conduct experiment to verify the impact of some key components.\nFirst, we set \\textit{STEP w/o GSL} to test the performance without the graph structure learning model.\nSecond, we set \\textit{STEP w/o reg} to replace the $k$NN graph computed by the representations of TSFormer with the $k$NN graph in GTS~\\cite{2021GTS}, which is computed based on the cosine similarity of raw time series $\\mathbf{S}^{i}_{train}$, to test the superiority of the long sequence representations of TSFormer.\nFinally, we also test more downstream STGNNs to verify the generality of STEP.\nWe choose DCRNN as another backend, \\ie \\textit{STEP-DCRNN}.\nAdditionally, we also present the performance of DCRNN for comparison.\nThe results are shown in Figure \\ref{ablation}(a).\n\n% STEP w/o GSL若于STEP，我们的GSL模型始终扮演一个正向角色。\n% 但STEP w/o的性能依旧很好，这意味着TSFormer产生的segment-level的表征非常非常重要。\n% STEP和STEP w/o reg比，意味着TSFormer的表达能力能进一步提升学习到的图的质量。\n% 正如Intro中提到的，DCRNN和GWNet是两类最具代表性的方法。DCRNN代表了一大类基于Seq2Seq架构的STGNN[123]。\n% 我们按照公式(0)，将TSFormer的表征加到Seq2Seq架构Encoder端的lantent 表征上。\n% 可以看到STEP模型显著地增强了DCRNN的性能，这验证了我们的框架的通用性。\nAs can be seen from the figure, STEP outperforms \\textit{STEP w/o GSL}, which shows that our graph structure learning module consistently plays a positive role.\nMeanwhile, \\textit{STEP w/o GSL} still achieves satisfactory performance, demonstrating that segment-level representation plays a vital role.\nSTEP also outperforms \\textit{STEP w/o reg}, showing that the long sequence representations of TSFormer is superior in improving the graph quality.\nIn addition, as mentioned in Section \\ref{sec_intro}, DCRNN represents a large class of STGNNs~\\cite{2019STMetaNet, 2021GTS, 2021REST, 2020GMAN} that are based on the seq2seq~\\cite{2014Seq2Seq} architecture.\nWe fuse the representation of TSFormer to the latent representations of the seq2seq encoder according to Eq.(\\ref{fuse}).\nWe can see that STEP significantly enhances the performance of DCRNN, which verifies the generality of STEP.\n\n\n\n"
                },
                "subsection 4.5": {
                    "name": "Hyper-parameter Study",
                    "content": "\n{\\color{black}\nWe conduct experiments to analyze the impacts of two hyper-parameters: the masking ratio $r$, the $k$ of the $k$NN graph in graph structure learning. We present the results on METR-LA dataset.\n\nThe effect of $r$ and $k$ are shown in Figure \\ref{ablation}(b) and Figure \\ref{ablation}(c), respectively. We find there exist optimal values for both $r$ and $k$. \nFor masking ratio $r$, when $r$ is small, masked values in time series can be predicted by simple average or interpolation. Thus it creates a trivial self-supervised learning task and can not get useful representations. \nWhen $r$ is large, the model would lose too much information and fail to learn temporal patterns. % In the experiment, we find the value of 75\\% results in the optimal model performance.\nFor $k$ of the $k$NN graph in graph structure learning, a small value of $k$ would make the learned graph incomplete and lose dependency information, thus the performance is worse. A large value of $k$ would introduce redundancies, which may hurt the information aggregation of graph neural networks, leading to unsatisfactory performance. }"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n",
                "subsection 5.1": {
                    "name": "Spatial-Temporal Graph Neural Networks",
                    "content": "\nThe accuracy of multivariate time series forecasting has been largely improved by artificial intelligence~\\cite{Innovation}, especially deep learning techniques.\nAmong these techniques, Spatial-Temporal Graph Neural Networks~(STGNNs) are the most promising methods, \nwhich combine Graph Neural Networks (GNNs)~\\cite{2017GCN, 2016ChebNet} and sequential models~\\cite{2014GRU, 2014Seq2Seq} to model the spatial and temporal dependency jointly.\nGraph WaveNet~\\cite{GWNet}, MTGNN~\\cite{2020MTGNN}, STGCN~\\cite{2018STGCN}, and StemGNN~\\cite{2020StemGNN} combine graph convolutional networks and gated temporal convolutional networks with their variants. These methods are based on convolution operation, which facilitates parallel computation.\nDCRNN~\\cite{2017DCRNN}, ST-MetaNet~\\cite{2019STMetaNet}, AGCRN~\\cite{2020AdaptiveGCRN}, and TGCN~\\cite{2019TGCN} combine diffusion convolutional networks and recurrent neural networks~\\cite{2014GRU, 2014Seq2Seq} with their variants. \nThey follow the seq2seq~\\cite{2014Seq2Seq} architecture to predict step by step.\nMoreover, attention mechanism is widely used in many methods, such as GMAN~\\cite{2020GMAN} and ASTGCN~\\cite{2019ASTGCN}. \nAlthough STGNNs have made significant progress, the complexity of STGNNs is high because it needs to deal with both temporal and spatial dependency at every step.\nTherefore, STGNNs can only take short-term historical time series as input, such as the past 1 hour~(twelve time steps in many datasets).\n\nMore recently, an increasing number of works~\\cite{2018NRI, 2019LDS, 2021GTS} have focused on joint learning of graph structures and graph neural networks to model the dependencies between nodes.\n% NRI~\\cite{2018NRI} takes the form of a variational auto-encoder, in which the latent code represents the underlying dependency graph and the reconstruction is based on graph neural networks.\nLDS~\\cite{2019LDS} models the edges as random variables whose parameters are treated as hyperparameters in a bilevel learning framework.\nThe random variables parameterize the element-wise Bernoulli distribution from which the adjacency matrix $\\mathbf{A}$ is sampled.\nGTS~\\cite{2021GTS} introduces a neighborhood graph as a regularization that improves graph quality and reformulates the problem as a unilevel optimization problem.\nNotably, We follow the framework of GTS but enhance it by the pre-training model since TSFormer gives better latent representations of time series for calculating their correlations.\n% We will conduct the ablation studies in Section {\\color{red}X}.\n\n% GWNet MTGNN STGCN 2020StemGNN\n% DCRNN STMetaNet 2020AdaptiveGCRN 2019TGCN\n% GMAN \n% FCGAGA 2021HGCN\n\n% Spatial Temporal Graph Neural Networks: DCRNN, GWNet, MTGNN, ASTGCN, STGCN, GMAN, 2020StemGNN. FCGAGA, 2020AdaptiveGCRN, 2021HGCN, 2019STMetaNet, 2019TGCN, Innovation, GCN\n% Graph Stucture Learning: NRI, LDS, GTS, 2021REST\n% 结论：取得了很大的进展，但是由于复杂度的限制，只能应用在短期序列上；\n"
                },
                "subsection 5.2": {
                    "name": "Pre-training Model",
                    "content": "\n% NLP\nThe pre-training model is used to learn a good representation from massive unlabeled data and then use these representations for other downstream tasks.\nRecent studies have demonstrated significant performance gains on many natural language processing tasks with the help of the representation extracted from pre-training models~\\cite{2020PTM}.\nProminent examples are the BERT~\\cite{2019BERT} and GPT~\\cite{2020GPT}, which are based on the Transformer encoder and decoder, respectively.\nThe Transformer architecture is more powerful and more efficient than LSTM architecture~\\cite{2014Seq2Seq, 2018ELMO} and has become the mainstream approach for designing pre-training models.\n% CV\nMore recently, Transformer for images has attracted increasing attention because of its powerful performance.\nViT~\\cite{2020ViT} proposes to split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer, showing impressive performance.\nHowever, ViT needs supervised training, which requires massive labeled data.\nOn the contrary, MAE~\\cite{2021MAE} uses self-supervised learning based on the masked autoencoding strategy. \nMAE enables us to train large models efficiently and effectively and outperforms supervised pre-training.\nAlthough the pre-training model has made significant progress in natural language processing and computer vision, progress in time series lags behind them. In this paper, we propose a pre-training model~(named TSFormer) for time series based on Transformer blocks and improve the performance of the downstream forecasting task.\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nIn this paper, we propose a novel STEP framework for multivariate time series forecasting to address the inability of STGNNs to learn long-term information.\nThe downstream STGNN is enhanced by a scalable time series pre-training model TSFormer.\nTSFormer is capable of efficiently learning the temporal pattern from very long-term historical time series and generating segment-level representations, which provide rich contextual information for short-term input of STGNNs and facilitate modeling dependencies between time series.\nExtensive experiments on three real-world datasets show the superiority of the STEP framework and the proposed TSFormer.\n\\begin{acks}\nThis work is partially supported by NSFC No. 61902376 and No. 61902382.\nIn addition, Zhao Zhang is supported by the China Postdoctoral Science Foundation under Grant No. 2021M703273.\n\\end{acks}\n\\bibliographystyle{ACM-Reference-Format}\n\\normalem\n\\bibliography{references_dblp}\n\\appendix\n% \\section{More TSFormer Details}\n% \\label{appendix_tsformer}\n% % 数据采样\n% We obain the training samples over the raw time series through a sliding window of length $T_p=P*C$, where $P$ is the number of patches and $L$ is the patch length.\n% We $L=12$ for all datasets since most of previous STGNNs takes such input length.\n% We set $P=168$ for METR-LA and PEMS-BAY, $P=336$ for PEMS04 datasets.\n% Therefore, the input sequence of node $i$ can be denoted as $\\mathbf{S}^i\\in\\mathbb{R}^{P\\times LC}$, where $C$ is the input channel.\n\n% \\noindent \\textbf{Masking.} \n% First, the original input time series $\\mathbf{S}^i$ is divided into $P$ patches~(\\ie segments).\n% The $j$th patch can be denoted as $\\mathbf{S}^i_j\\in\\mathbb{R}^{LC}$.\n% Then we uniform randomly mask a portion of patches with mask ratio $r=0.75$.\n\n% \\noindent \\textbf{Encoder.} \n% The input embedding layer is a linear projection to transform the input patch into latent space:\n% \\begin{equation}\n%     \\mathbf{U}_j^i = \\mathbf{S}^i_j \\cdot \\mathbf{W} + \\mathbf{b},\n% \\end{equation}\n% where $\\mathbf{W}\\in\\mathbb{R}^{(L*C)\\times d}$ and $\\mathbf{b}\\in\\mathbb{R}^d$ is learnable parameters, and $\\mathbf{U}^i_j\\in\\mathbb{R}^{d}$ are the model input vector.\n% The positional encoding layer adds learnable positional embeddings to all patches, including the masked tokens.\n% Then we apply a series of Transformer Blocks on the \\textit{unmasked} patches, and obtain the latent representations $\\mathbf{H}_j^i\\in\\mathbf{R}^d$, where $j$ is the visible patch index.\n\n% $\\mathbf{H}^i_j$, for all unmasked tokens\n\n% $\\hat{\\mathbf{S}^i_j}\\in\\mathbb{R}^{d\\times(L*C)}$, for all masked tokens\n\n% \\begin{equation}\n%     \\mathcal{L}_{pre-training}=\\mathcal{L}(\\mathbf{S}^i_j, \\hat{\\mathbf{S}^i_j})=\\frac{1}{LCP*r}\\sum_{j}\\sum_{l=1}^{L}\\sum_{k=1}^{C}(|(\\mathbf{S}^i_j)_{lk} - (\\hat{\\mathbf{S}^i_{j}})_{lk}|)\n% \\end{equation}\n\n% These operations are computed in parallel for all time series.\n"
            },
            "section 7": {
                "name": "More Experiments Details",
                "content": "\n\\label{appendix_experiments}\n",
                "subsection 7.1": {
                    "name": "Baseline Details",
                    "content": "\n\\label{appendix_basline}\n\\begin{itemize}\n    \\item \\textbf{HA}: \n    Historical Average model, which models time series as a periodic process and uses weighted averages from previous periods as predictions for future periods.\n    \\item \\textbf{VAR}: \n    Vector Auto-Regression~\\cite{VAR, lutkepohl2005new} assumes that the past time series is stationary and estimates the relationship between the time series and their lag value.~\\cite{2020STGNN}\n    \\item \\textbf{SVR}: Support Vector Regression (SVR) uses linear support vector machine for classical time series regression task.\n    \\item \\textbf{FC-LSTM}~\\cite{2014Seq2Seq}: Long Short-Term Memory network with fully connected hidden units is a well-known network architecture that is powerful in capturing sequential dependency.\n    \\item \\textbf{DCRNN}~\\cite{2017DCRNN}: Diffusion Convolutional Recurrent Neural Network~\\cite{2017DCRNN} replaces the fully connected layer in GRU~\\cite{2014GRU} by diffusion convolutional layer to form a new Diffusion Convolutional Gated Recurrent Unit~(DCGRU).\n    \\item \\textbf{Graph WaveNet}~\\cite{GWNet}: Graph WaveNet stacks gated temporal convolutional layer and GCN layer by layer to jointly capture the spatial and temporal dependencies.\n    \\item \\textbf{ASTGCN}~\\cite{2019ASTGCN}: ASTGCN combines the spatial-temporal attention mechanism to capture the dynamic spatial-temporal characteristics simultaneously.\n    \\item \\textbf{STSGCN}~\\cite{2020STSGCN}: STSGCN is proposed to effectively capture the localized spatial-temporal correlations and consider the heterogeneity in spatial-temporal data.\n    \\item \\textbf{MTGNN}~\\cite{2020MTGNN}: MTGNN extends Graph WaveNet through the mix-hop propagation layer in the spatial module, the dilated inception layer in the temporal module, and a more delicate graph learning layer.\n    \\item \\textbf{GMAN}~\\cite{2020GMAN}: GMAN is an attention-based model which stacks spatial, temporal and transform attentions.\n    \\item \\textbf{GTS}~\\cite{2021GTS}: GTS learns a graph structure among multiple time series and forecasts them simultaneously with DCRNN. \n\\end{itemize}\n\n"
                },
                "subsection 7.2": {
                    "name": "Optimization Settings",
                    "content": "\n\n\n\n\\noindent \\textbf{Pre-training stage.} \nThe default setting is shown in Table \\ref{tab_pretrain}. \nWe use uniform distribution to initialize the positional embeddings, and we use truncated normal distribution with $\\mu=0$ and $\\sigma=0.02$ to initialize the mask token, similar to MAE~\\cite{2021MAE}. \nWe use PyTorch official implementation to implement the Transformer blocks.\nWe use the linear scaling rule for learning rate and batch size: lr = base\\_lr $\\times$ (batch\\_size/8) for all datasets in the pre-training stage.\n\n\n\n\\noindent \\textbf{Forecasting stage.} \nAll the settings are shown in Table \\ref{tab_forecast}.\nFollowing many recent works, such as MTGNN~\\cite{2020MTGNN} and GTS~\\cite{2021GTS}, we use the curriculum learning strategy for the forecasting task.\nThe strategy gradually increases the prediction length of the model with the increase in iteration number.\nWe increase the prediction length by one per \\text{cl\\_num} epochs.\nMoreover, we additionally perform a warm-up of \\text{warm\\_num} epochs to better initialize the model for curriculum learning.\nIn addition, $\\lambda$ in Equation (\\ref{full_loss}) decays by $\\lambda=1/(\\lceil epoch/6 \\rceil)$, where $\\lceil \\cdot \\rceil$ means ceiling function and \\textit{epoch} is the epoch number.\n\n% Mask\n% \n% 输入输出\n% 维度参数\n% 优化器参数\n% 优化步骤参数\n% Number of Head \n\n\n\n\n\n"
                }
            },
            "section 8": {
                "name": "Efficiency",
                "content": "\n\\label{appendix_efficiency}\n% 分为预训练和预测两部分\n% 预训练我们对比不同Mask Ratio下的训练时间\n% 测试阶段我们对比几个Baseline的训练时间。\n% % 测试阶段模型有两个结果，一个是with preprocess，另一个是w/o preprocess\nIn this part, we compare the efficiency of STEP with other models and their own variants based on the METR-LA dataset.\nFor a more intuitive and effective comparison, we compare the average training time required for each epoch.\nAll the experiments are running on an Intel(R) Xeon(R) Gold 5217 CPU @ 3.00GHz, 128G RAM computing server, equipped with RTX 3090 graphics cards.\nFirst, we compare the efficiency of TSFormer in the pre-training stage under different masking ratios.\nThe result is shown in Figure \\ref{speed}.\nAs the masking ratio increases, the TSFormer will be more efficient.\nIn summary, thanks to the high masking ratio and fewer Transformer blocks in the encoder and decoder, the TSFormer is lightweight and can be trained efficiently on a single NVIDIA 3090 GPU.\n\n\nSecond, we compare the efficiency of STEP framework in the forecasting stage with its variants.\nRecalling that the parameter of TSFormer is fixed during the forecasting stage, we can use TSFormer to provide off-the-shelf representations by preprocessing the whole dataset to reduce redundant calculations in the training process.\nWe also test the efficiency of STEP without preprocessing, denoting the variant as \\textit{STEP w/o pre}.\nIn addition, we test the efficiency of STEP without graph structure learning, \\ie \\textit{STEP w/o GSL}.\n% Recalling that the pre-trained TSFormer is not trainable in the forecasting stage, we can preprocess all the samples and generate the latent representations in advance. \n% We also test the efficiency of STEP without preprocessing, denoting the variant as \\textit{STEP w/o pre}.\nThe result is shown in Figure \\ref{speed2}.\n% 预训练确实大大地减少了重复计算\n% 但我们发现IO的消耗可能是另一个瓶颈：因为我们需要频繁地从磁盘上读取包含着XXX和XXX的文件并将他们送入到GPU。在我们的实验中，数据读取时间大概占到1/3左右，这个数值随着硬盘缓存的增大会有所改善。\nWe have the following findings: \n(i) the graph structure learning module accounts for about 55s per epoch on average.\n(ii) preprocessing does significantly reduce repetitive computations.\n% But we found that disk I/O may be another bottleneck because we need to frequently read files containing $\\mathbf{H}^i$ from disk and send them to GPU. In our experiment, the data fetching time accounts for about 1/3 of the total time, and this value will decrease with the increase of the SSD cache and speed.\n\n\n% While STEP doesn't add much complexity after preprocessing, disk I/O can become the bottleneck because we need to read large files storing $\\mathbf{H}^i$from disk and send them to GPU frequently. \n% The yellow part shows the time to fetch data, which is nearly half of the total time. \n% Therefore, how to further avoid or alleviate the disk I/O bottleneck is an important issue that need to be addressed in the further.\n\n% Although STEP does not add much more complexity after preprocessing, the disk IO may become the bottleneck since we need to frequently read large files that store $\\mathbf{H}^i$ from disk.\n% The yellow part demonstrates the data fetching time, which accounts for nearly half of the total time.\n% Therefore, how to further break the disk bottleneck is a important issue.\n\n \n"
            },
            "section 9": {
                "name": "Visualization",
                "content": "\n\\label{appendix_visualization}\nIn order to further intuitively understand and evaluate our model, in this section, we give more visualizations.\nFirst, we provide more visualizations about reconstructions of the TSFormer on PEMS04 dataset like Figure \\ref{inspecting}(b).\nThe results are shown in Figure \\ref{vis_rec}. \nNote that due to space limitation, we only visualize time series in a small window rather than the whole input time series $\\mathbf{S}^i$.\n% We are surprisingly find that even given very limited information surrounding the unmasked patches, TSFormer reconstructs the masked patches very accurately.\nSurprisingly, we find that even given very limited information surrounding the unmasked patches, TSFormer reconstructs the masked patches accurately.\nThese results again indicate that our model has a strong ability to learn rich temporal patterns from very long-term time series.\nThen, we visualize the prediction of our model and the groundtruth data based on METR-LA dataset.\nWe randomly selected six time series and displayed their data from June 13th 2012 to June 16th 2012~(located the test dataset).\nThe forecasting results on six randomly selected time series are shown in Figure \\ref{vis_for}.\nWe can see that our model can accurately make predictions for different time series.\nFurthermore, we find that the model has the ability to resist noise. \nFor example, in the right top figure, the traffic sensor apparently failed in the afternoon of June 13th, 2012. \nHowever, the model does not overfit the noise.\n\n% gives more visualization about the reconstruction shown in \\ref{inspecting}(b) and prediction of our STEP.\n\n\n% visualize the prediction of our STEP and the real data. \n% Furthermore, we demonstrate more reconstruction results of TSFormer.\n% In order to further intuitively understand and evaluate our model, in this section, we visualize the prediction of our STEP and the real data. \n% Furthermore, we demonstrate more reconstruction results of TSFormer.\n% In order to further intuitively understand and evaluate our model, in this section, we visualize the prediction of our STEP and the real data. \n% Furthermore, we demonstrate more reconstruction results of TSFormer.\n% In order to further intuitively understand and evaluate our model, in this section, we visualize the prediction of our STEP and the real data. \n% Furthermore, we demonstrate more reconstruction results of TSFormer.\n% In order to further intuitively understand and evaluate our model, in this section, we visualize the prediction of our STEP and the real data. \n% Furthermore, we demonstrate more reconstruction results of TSFormer.\n% In order to further intuitively understand and evaluate our model, in this section, we visualize the prediction of our STEP and the real data. \n\n\n\n"
            }
        },
        "tables": {
            "tab_pretrain": "\\begin{table}[h]\n    % \\setlength{\\abovecaptionskip}{0.2cm}\n    % \\setlength{\\belowcaptionskip}{-0.1cm}\n\\caption{Pre-training setting.}\n\\label{tab_pretrain}\n\\centering  \n\\begin{tabular}{p{3cm}|p{4cm}}\nconfig  & value \\\\\n\\toprule\noptimizer & AdamW~\\cite{AdamW}\\\\\nbase learning rate & 5.0e-4\\\\\nweight decay & 0\\\\\nepsilon & 1.0e-8\\\\\noptimizer momentum & $\\beta_1, \\beta_2=0.9, 0.95$\\\\\nlearning rate schedule & MultiStepLR\\\\\nmilestones & 50\\\\\ngamma & 0.5\\\\\ngradient clip & 5\\\\ \n\\end{tabular}\n\\end{table}",
            "tab_forecast": "\\begin{table}[h]\n    % \\setlength{\\abovecaptionskip}{0.2cm}\n    % \\setlength{\\belowcaptionskip}{-0.1cm}\n\\caption{Forecasting setting.}\n\\label{tab_forecast}\n\\centering  \n\\begin{tabular}{p{3cm}|p{4cm}}\nconfig  & value \\\\\n\\toprule\noptimizer & Adam~\\cite{Adam}\\\\\nlearning rate & 0.001/0.005/0.002\\\\\n&{\\small(PEMS-BAY/METR-LA/PEMS04)}\\\\\nbatch size & 64/64/32\\\\\n&{\\small(PEMS-BAY/METR-LA/PEMS04)}\\\\\nweight decay & 1.0e-5\\\\\nepsilon & 1.0e-8\\\\\nlearning rate schedule & MultiStepLR\\\\\nmilestones & [1, 18, 36, 54, 72]\\\\\ngamma & 0.5\\\\\ngradient clip & 5\\\\\n\\text{cl\\_num}&3\\\\\n\\text{warm\\_num}&30\\\\\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "Intro": "\\begin{figure}[t]\n  \\centering\n  \\setlength{\\abovecaptionskip}{0.2cm}\n  \\setlength{\\belowcaptionskip}{-0.4cm}\n  \\includegraphics[width=1\\linewidth]{Figures/Intro_narrow.pdf}\n  \\caption{\n%   \\small\n{\n\\fontsize{8.7pt}{\\baselineskip}\\selectfont \n  Examples of traffic flow multivariate time series data.\n  (a) The two time series exhibit complex temporal patterns and strong spatial correlations.\n  (b) Similar traffic trends within small windows in different contexts.\n  (c) Different traffic trends within a small window between two similar series.\n  }}\n  \\label{Intro}\n\\end{figure}",
            "model": "\\begin{figure*}[t]\n  \\centering\n  \\setlength{\\abovecaptionskip}{0.2cm}\n  \\setlength{\\belowcaptionskip}{-0.2cm}\n  \\includegraphics[width=0.985\\linewidth]{Figures/STEP.pdf}\n  \\caption{\n  {\\color{black}\n  The overview of the proposed STEP framework.\n  \\underline{Left}: the pre-training stage. We split very long-term time series into segments and feed them into TSFormer, which is trained via the masked autoencoding strategy.\n  \\underline{Right}: the forecasting stage. We enhance the downstream STGNN based on the segment-level representations of the pre-trained TSFormer.}\n  }\n  \\label{model}\n\\end{figure*}",
            "inspecting": "\\begin{figure}[t]\n    \\setlength{\\abovecaptionskip}{0.2cm}\n    \\setlength{\\belowcaptionskip}{-0.4cm}\n  \\centering\n  \\includegraphics[width=0.97\\linewidth]{Figures/inspecting_small.pdf}\n  \\caption{Inspecting the TSFormer. (a) Learned temporal periodicity. (b) Reconstruction. (c) Similarity of  latent representations among different patches. (d) Similarity of positional embeddings among different patches.}\n  \\label{inspecting}\n\\end{figure}",
            "ablation": "\\begin{figure}\n    \\setlength{\\abovecaptionskip}{0.0cm}\n    \\setlength{\\belowcaptionskip}{-0.4cm}\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{Figures/Ablation_new.pdf}\n  \\caption{Ablation study and hyper-parameter study.}\n  \\label{ablation}\n\\end{figure}",
            "speed": "\\begin{figure}[ht]\n    \\setlength{\\abovecaptionskip}{0.2cm}\n    % \\setlength{\\belowcaptionskip}{-0.1cm}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{Figures/Speed_TSFormer.pdf}\n  \\caption{Training speed of different masking ratio $r$.}\n  \\label{speed}\n\\end{figure}",
            "speed2": "\\begin{figure}[ht]\n    \\setlength{\\abovecaptionskip}{0.2cm}\n    % \\setlength{\\belowcaptionskip}{-0.1cm}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{Figures/Speed2.pdf}\n  \\caption{Training speed of different methods.}\n  \\label{speed2}\n\\end{figure}",
            "vis_rec": "\\begin{figure}[ht]\n    % \\setlength{\\abovecaptionskip}{0.cm}\n    % \\setlength{\\belowcaptionskip}{-0.1cm}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{Figures/Vis_Reconstruction.pdf}\n  \\caption{Reconstruction visualizations.}\n  \\label{vis_rec}\n\\end{figure}",
            "vis_for": "\\begin{figure}[ht]\n  \\centering\n  \\includegraphics[width=1\\linewidth]{Figures/Vis_Forecasting.pdf}\n  \\caption{Forecasting visualizations.}\n  \\label{vis_for}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\setlength\\abovedisplayskip{0.2cm}\n\\setlength\\belowdisplayskip{0.2cm}\n    \\mathbf{U}_j^i = \\mathbf{W}\\cdot\\mathbf{S}^i_j + \\mathbf{b},\n\\end{equation}",
            "eq:2": "\\begin{equation}\n    \\begin{aligned}\n    \\mathbf{\\Theta}_{ij}&=\\text{FC}(\\text{relu}(\\text{FC}(\\mathbf{Z}^i\n    \\parallel \\mathbf{Z}^j)))\\\\\n    \\mathbf{Z}^i&= \\text{relu}(\\text{FC}(\\mathbf{H}^i)) + \\mathbf{G}^i,\n    \\end{aligned}\n\\end{equation}",
            "eq:3": "\\begin{equation}\n    \\mathcal{L}_{graph} = \\sum_{ij}-\\mathbf{A}_{ij}^a\\log\\mathbf{\\Theta}^{'}_{ij}-(1-\\mathbf{A}_{ij}^{a})\\log(1-\\mathbf{\\Theta}^{'}_{ij}),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\mathbf{A}_{ij}=\\text{softmax}((\\mathbf{\\Theta}_{ij}+\\mathbf{g})/\\tau),\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\mathbf{H}_{final}=\\text{SP}(\\mathbf{H}_P) + \\mathbf{H}_{gw},\n    \\label{fuse}\n\\end{equation}",
            "eq:6": "\\begin{equation}\n    \\cal{L}_{regression} = \\cal{L}(\\hat{\\cal{Y}}, \\cal{Y})=\\frac{1}{T_fNC}\n    \\sum_{j=1}^{T_f}\\sum_{i=1}^{N}\\sum_{k=1}^{C}|\\hat{\\cal{Y}}_{ijk} - \\cal{Y}_{ijk}|, \n    \\label{loss}\n\\end{equation}",
            "eq:7": "\\begin{equation}\n    \\mathcal{L} = \\mathcal{L}_{regression} + \\lambda \\mathcal{L}_{graph}.\n    \\label{full_loss}\n\\end{equation}"
        },
        "git_link": "https://github.com/zezhishao/STEP"
    }
}