{
    "meta_info": {
        "title": "Simple and Efficient Heterogeneous Graph Neural Network",
        "abstract": "Heterogeneous graph neural networks (HGNNs) have powerful capability to embed\nrich structural and semantic information of a heterogeneous graph into node\nrepresentations. Existing HGNNs inherit many mechanisms from graph neural\nnetworks (GNNs) over homogeneous graphs, especially the attention mechanism and\nthe multi-layer structure. These mechanisms bring excessive complexity, but\nseldom work studies whether they are really effective on heterogeneous graphs.\nThis paper conducts an in-depth and detailed study of these mechanisms and\nproposes Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To\neasily capture structural information, SeHGNN pre-computes the neighbor\naggregation using a light-weight mean aggregator, which reduces complexity by\nremoving overused neighbor attention and avoiding repeated neighbor aggregation\nin every training epoch. To better utilize semantic information, SeHGNN adopts\nthe single-layer structure with long metapaths to extend the receptive field,\nas well as a transformer-based semantic fusion module to fuse features from\ndifferent metapaths. As a result, SeHGNN exhibits the characteristics of simple\nnetwork structure, high prediction accuracy, and fast training speed. Extensive\nexperiments on five real-world heterogeneous graphs demonstrate the superiority\nof SeHGNN over the state-of-the-arts on both accuracy and training speed.",
        "author": "Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, Dongrui Fan",
        "link": "http://arxiv.org/abs/2207.02547v3",
        "category": [
            "cs.LG"
        ],
        "additionl_info": "Accepted by AAAI 2023"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\n%The complex systems in real world are commonly associated with various types of entities and relations, widely known as heterogeneous graph. \n\n% \\todo{it is the original word 'unnecessary complexity and redundant computation', 'excess complexity'. Try to re-write according our special points} % NO, just reuse it. As node-level attention->unnecessary complexity, repeated neighbor aggregation ->redundant computation \u642d\u914d\u7684\u5f88\u597d\n\nRecent years witness explosive growth in graph neural networks (GNNs) in pursuit of performance improvement of graph representation learning~\\cite{comprehensive_gnn_survey,ijcai2022p772,lin2022comprehensive}. GNNs are primarily designed for homogeneous graphs associated with a single type of nodes and edges, following a neighborhood aggregation scheme to capture structural information of a graph, where the representation of each node is computed by recursively aggregating the features of neighbor nodes~\\cite{kipf2016semi}.\n\nHowever, GNNs are insufficient to deal with the heterogeneous graph which possesses rich semantic information in addition to structural information~\\cite{HG_survey}. Many real-world data in complex systems are naturally represented as heterogeneous graphs, where multiple types of entities and relations among them are embodied by various types of nodes and edges, respectively. For example, as shown in Figure \\ref{tab:hgnns_categories}, the citation network ACM includes several types of nodes: Paper (P), Author (A), and Subject (S), as well as many relations with different semantic meanings, such as Author$\\xrightarrow{\\rm writes}$Paper, Paper$\\xrightarrow{\\rm cites}$Paper, Paper$\\xrightarrow{\\rm belongs~to}$Subject.\n%The metapath, a composite format of multi-types, are used to reflect high-level relation of two nodes.\nThese relations can be composited with each other to form high-level semantic relations, which are represented as metapaths~\\cite{sun2011pathsim,sun2012mining}.\n% Metapaths~\\cite{sun2011pathsim,sun2012mining} have been widely used for mining and learning with heterogeneous graphs. \nFor example, the 2-hop metapath Author-Paper-Author (APA) represents the co-author relationship, \n% that the two authors have cooperated on a paper \nwhile the 4-hop metapath Author-Paper-Subject-Paper-Author (APSPA) describes that the two authors have been engaged in the research of the same subject.\n%It can be seen, depending on the metapaths, the relation between nodes in the heterogeneous graph can expose different semantics. \n% It can be seen that heterogeneous graphs contain not only structural information but also rich semantic information, which GNNs %is primarily designed for homogeneous graphs so that they \n% fail to exploit.\nHeterogeneous graphs contain more comprehensive information and rich semantics and require specifically designed models.\n\nVarious heterogeneous graph neural networks (HGNNs) have been proposed to capture semantic information, achieving great performance in heterogeneous graph representation learning~\\cite{hgnn_survey_tangjie,hgnn_survey_hanjiawei,hgnn_survey_shichuan,hgnn_survey_shiruipan,9855397}.\n% Therefore, HGNNs are at the heart of a broad range of applications such as social network analysis~\\cite{hamilton2017inductive,yasunaga2019scisummnet}, recommendation~\\cite{zhao2017meta,hu2018leveraging}, and knowledge graph inference~\\cite{chen2017task,oh2018knowledge,zhang2019iteratively}.\nTherefore, HGNNs are at the heart of a broad range of applications such as social network analysis~\\cite{liu2018heterogeneous}, recommendation~\\cite{fan2019metapath,niu2020dual}, and knowledge graph inference~\\cite{bansal2019a2n,vashishth2020compositionbased,wang2021mixed}.\n% \u8fd9\u91cc metapath-based \u4ecb\u7ecd\u7684\u7a0d\u5fae\u7b80\u7565\u4e00\u4e9b\n\nFigure \\ref{tab:hgnns_categories} depicts the two main categories of HGNNs.\n% There are two main categories of HGNNs, as shown in Figure \\ref{tab:hgnns_categories}.\n\\textit{Metapath-based methods}~\\cite{RGCN2018, HetGNN2019, HAN2019, MAGNN2020} capture the structural information of the same semantic first and then fuse different semantic information. These models first aggregate neighbor features at the scope of each metapath to generate semantic vectors, and then fuse these semantic vectors to generate the final embedding vector.\n% execute neighbor aggregation to aggregate metapath-based neighbors' features for each node for each metapath, and then perform semantic aggregation to aggregate intermediate results across different methpaths.\n%, providing best-in-class performance in heterogeneous GraphRL.\n% \u60f3GNN\u4e00\u6837\u7684\u805a\u5408\u4fe1\u606f\n\\textit{Metapath-free methods}~\\cite{RSHN2019, HetSANN, HGT, HGB} capture structural and semantic information simultaneously. These models aggregate messages from a node\u2019s local neighborhood like traditional GNNs, but use extra modules (e.g. attentions) to embed semantic information such as node types and edge types into propagated messages.\n% The outline of metapath-based methods and metapath-free methods are shown in Figure \\ref{tab:hgnns_categories}(a).\n% attention mechanism or mean aggregator to directly aggregate features of different types of nodes and edges to capture structural and semantic information simultaneously, also significantly improving the performance of heterogeneous GraphRL. \n% Not surprisingly, since HGNNs hold the state-of-the-art performance in \n% In addition, hierarchy attention mechanism is widely used in HGNNs, with node-level attention that learns importance among neighbor nodes and semantic-level attention that learns importance between different semantics~\\cite{HAN2019}.\n\n\n\n\n\nExisting HGNNs inherit many mechanisms from GNNs over homogeneous graphs, especially the attention mechanism and the multi-layer structure, as illustrated in Figure \\ref{tab:hgnns_categories}, but seldom work studies whether these mechanisms are really effective on heterogeneous graphs.\n% or how the maximize the use of them on HGNNs\nMoreover, the hierarchy attention calculation in the multi-layer network and repeated neighbor aggregation in every epoch bring excessive complexity and computation. For example, the neighbor aggregation process with attention modules takes more than 85\\% of total time in the metapath-based model HAN~\\cite{HAN2019} and the metapath-free model HGB~\\cite{HGB}, which has become the speed bottleneck for the application of HGNNs on larger-scale heterogeneous graphs.\n\nThis paper provides an in-depth and detailed study of these mechanisms and yields two findings: (1) \\textit{semantic attention is essential while neighbor attention is not necessary}, (2) \\textit{models with a single-layer structure and long metapaths outperform those with multi-layers and short metapaths}. These findings imply that neighbor attention and multi-layer structure not only introduce unnecessary complexity, but also hinder models to from achieving better performance. % Yan \u8bf4 that \u540e\u9762\u8ddf\u7684\u662f\u8d2c\u4e49\uff0c\u6240\u4ee5\u7528imply\u4e0d\u7528suggest\n% \\todo{add} % give opportunity to improve effect & speed\n% or \u4e0d\u4ec5\u7ed9\u73b0\u6709\u6a21\u578b\u5e26\u6765\u4e86\u989d\u5916\u7684\u65f6\u95f4\u6210\u672c\uff0c\u8fd8\u963b\u788d\u4e86\u4ed6\u4eec\u83b7\u53d6\u66f4\u597d\u7684\u6548\u679c \uff08\u611f\u89c9\u540e\u8005\u66f4\u597d\uff0c\u4e0e\u4e0b\u4e00\u6bb5\u7684to this end \u66f4\u914d\uff09\n\n% Here single layer in HGNNs means the fusion of different semantics only processes once. It is much more than a one-layer GNN as the size of the reception field is exponential with the maximum hop of metapaths.\n\nTo this end, we propose a novel metapath-based method named SeHGNN. SeHGNN employs the mean aggregator~\\cite{sage233} to simplify neighbor aggregation, adopts a single-layer structure with long metapaths to extend the receptive field, and utilizes a transformer-based semantic fusion module to learn mutual attentions between semantic pairs. Additionally, as the simplified neighbor aggregation in SeHGNN is parameter-free and only involves linear operations, it earns the opportunity to execute the neighbor aggregation in the pre-processing step only once. As a result, SeHGNN not only demonstrates better performance but also avoids the need of repeated neighbor aggregation in every training epoch, leading to significant improvements in training speed.\n\nWe conduct experiments on four widely-used datasets from HGB benchmark~\\cite{HGB} and a large-scale dataset from OGB challenge~\\cite{OGB}. Results show that SeHGNN achieves superior performance over the state-of-the-arts for node classification on heterogeneous graphs.\n\n% \\begin{table}\n% \\caption{Notations used in this paper.}\n% \\label{tab:notation}\n% \\resizebox{\\linewidth}{!}{\\begin{tabular}{cl}\n% \\hline\n% Notation & Explanation \\\\\n% \\hline\n% $G$ & Heterogeneous graph, $G=(V,E,\\mathcal{T}^v,\\mathcal{T}^e)$. \\\\\n% $v_i$ & A node $i$, $\\,v_i\\in V$. \\\\\n% $e_{ij}$ & An edge from $j$ to $i$ (short of $e_{i\\leftarrow j}$), $\\,e_{ij}\\in E$. \\\\\n% $\\mathcal{N}_i$ & The set of neighbor nodes of node $i$. \\\\\n% $c$ & A node type $c\\in \\mathcal{T}^v$. \\\\\n% $r_{c_1c_2}$ & \\begin{tabular}[c]{@{}l@{}}An edge type with target node type $c_2$ and \\\\ $\\quad$ source node type $c_1$ (short of $r_{c_1\\leftarrow c_2}$), $\\,r_{c_1c_2}\\in\\mathcal{T}^e$.\\end{tabular} \\\\\n% $V^c$ & The set of all nodes with type $c$. \\\\\n% $\\mathcal{A}$ & The adjacency matrix of graph. \\\\\n% $\\mathcal{P}$ & A metapath, $\\,\\mathcal{P}\\in\\Phi$. \\\\\n% $p$ & A metapath instance. \\\\\n% $\\textbf{x}_i$ & The raw (content) feature vector of node $i$. \\\\\n% $\\textbf{h}'_i$ & The projected feature vector of node $i$. \\\\\n% $\\textbf{z}_i^r$ & The semantic vector of relation $r$ of node $i$. \\\\\n% $\\textbf{h}_i$ & The output embedding of node $i$. \\\\\n% $W^c$ & A projection weight matrix of type $c$. \\\\\n% \\hline\n% \\end{tabular}}\n% \\end{table}\n\nThe contributions of this work are summarized as follows:\n\n\\begin{itemize}\n\\item We conduct an in-depth study about the attention mechanism and the network structure in HGNNs and obtain two important findings, which reveal the needlessness of neighbor attention and the superiority of utilizing the single-layer structure and long metapaths.\n\\item Motivated by the findings above, we propose a simple and effective HGNN architecture SeHGNN. \nTo easily capture structural information, SeHGNN pre-computes the neighbor aggregation in the pre-processing step using a light-weight mean aggregator, which removes the overused neighbor attention and avoids repeated neighbor aggregation in every training epoch.\nTo better utilize semantic information, SeHGNN adopts the single-layer structure with long metapaths to extend the receptive field, as well as a transformer-based semantic fusion module to fuse features from different metapaths.\n\\item Experiments on five widely-used datasets demonstrate the superiority of SeHGNN over the state-of-the-arts, i.e., high prediction accuracy and fast training speed.\n\\end{itemize}\n\n\\begin{comment}\nExisting HGNNs usually learn to embed information using hierarchical attention mechanism and repeated neighbor aggregation, suffering from unnecessary complexity and redundant computation. \nOn the one hand, the role of node-level attention which learns importance among neighbor nodes, is overestimated, and preliminary experiments demonstrate that attention within the same type of relation is unnecessary or leads to overfitting.\n% TODO: \u662f\u5426\u5728\u6587\u7ae0\u7684\u5176\u4ed6\u67d0\u4e9b\u4f4d\u7f6e\u201c\u5177\u4f53\u7684\u201d\u8bf4\u660eattention\u5728HGNN\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff1f\uff1f\n% \u6bd4\u5982 related networks\u7684section\u5f00\u5934\u7b2c\u4e00\u6bb5\u91cc\uff1f\n% the hierarchy attention mechanism is widely used in HGNNs, involving each metapath scope of neighbor aggregation, semantic aggregation, and each GNN layer of metapath-free methods. Attention mechanism with extra trainable parameters and computation is naturally expensive due to its high degree of complexity.\n%\n% On the one hand, hierarchy attention mechanism is widely used in heterogeneous GraphRL~\\cite{hgnn_survey_tangjie,hgnn_survey_hanjiawei}, to first aggregate neighbor information in the same semantic and then aggregate semantic information across different semantic, based on trainable attention parameters. However, attention mechanism is naturally much expensive than mean aggregator~\\cite{GraphSAGE} due to its high-degree of complexity.\nOn the other hand, HGNNs collect structural information through aggregating messages from neighbors in every training epoch, resulting in repeated neighbor aggregation during training. Furthermore, the neighbor aggregation\n% dominants the total time in each training epoch.\nis the most time-consuming step in each training epoch.\nTake a metapath-based model HAN~\\cite{HAN2019} as an example, the time consumption of the neighbor aggregation is usually dozens of times compared with that of the semantic aggregation.\n% , as the neighbor aggregation involves irregular data access and the attention modules bring extra computation\nTherefore, the hierarchy attention mechanism and repeated neighbor aggregation bring extremely high compute costs to HGNNs.\n\n% generally follow a recursive neighborhood aggregation scheme~\\cite{hgnn_survey_shiruipan,hgnn_survey_tangjie}, where within a single layer of a HGNN each node aggregates its neighbors\u2019 features, resulting in repeated neighbor aggregation for each epoch. \n% As a result of the two points above, \n\n% Therefore, massive number of redundant neighbor aggregation is introduced in training, which is proportionate to the number of epoch, leading to extreme high compute cost.\n\n%\u5f15\u7528\u591a\u4e00\u4e2aGNN\u548cHGNN\u7684\u76f8\u5173\u7efc\u8ff0\u8bba\u6587\uff0c\u6709\u5229\u4e8e\u78b0\u5230\u76f8\u5173Reviewer\u7684\u65f6\u5019\uff0c\u4e0d\u81f3\u4e8e\u6709\u8ba8\u538c\u7684\u611f\u89c9\n\n% stop here\n\n% To reduce the excess complexity, \nTo this end,\nthis paper proposes \\textit{Simple and Efficient Heterogeneous Graph Neural Network} (SeHGNN), a novel metapath-based method, which avoids overused node-level attention at the scope of each metapath and pre-computes the neighbor aggregation only once in the pre-processing stage.\n% Specifically, \\textit{first}, through preliminary experiments on the state-of-the-art HGNNs (e.g., HAN~\\cite{HAN2019} and HGB~\\cite{HGB}), we observe that the attention mechanism within the same metapath or relation  tends to be unnecessary or lead to overfitting. \n%\n% \\textit{Second}, inspired by this observation, \nSpecifically, \\textit{firstly}, SeHGNN utilizes a light-weight mean aggregator rather than the attention mechanism in the neighbor aggregation step, which is simple but efficient and enables parameter-free neighbor aggregation. \n\\textit{Secondly}, the parameter-free neighbor aggregation step is pre-computed only once in the pre-processing stage so that the results of neighbor aggregation can be reused in every training epoch, which greatly reduces the amount of computation.\n% compared with computing neighbor aggregation in training.\n%\n\\textit{Finally}, a transformer-based semantic aggregator and a new label propagation method are integrated into SeHGNN for further accuracy improvement.\n% Unlike previous work, SeHGNN pre-computes the parameter-free neighbor aggregation for each metapath only once in the pre-processing step, and uses a transformer-like~\\cite{transformer} learner to combine semantic vectors across metapaths to generate the final embedding of each node. \nExtensive experiments demonstrate that SeHGNN offers the simple network structure, high prediction accuracy, and fast training speed. \n\nThe contributions in this work are summarized as follows:\n% \\todo{concise description}\n\n% \\squishlist\n\\begin{itemize}\n \\item Through preliminary experiments, we observe the unnecessity of node-level attention within the same relation and the intensive computation of the neighbor aggregation step executed in every training epoch. These endow an opportunity to reduce the excess complexity and to remove neighbor aggregation from training epoch for HGNNs.\n \n%  that the node-level attention within the same relation is unnecessary and that for metapath-based methods, the repeated neighbor aggregation in every training epoch causes high-cost redundant computation. \\todo{endows an opportunity xxx}\n\n \\item Motivated by the observations above, we propose a light-weight neighbor aggregator which avoids the overused node-level attention at the scope of each metapath. Furthermore, the parameter-free neighbor aggregation step is executed only once in the pre-processing stage. As a result, the neighbor aggregation results can be reused during training. These designs remove the unnecessary complexity and redundant computation.\n\n \\item We propose \\textit{Simple and Efficient Heterogeneous Graph Neural Network} (SeHGNN) which utilizes the light-weight neighbor aggregator\n%  \\todo{be consistent with the second contribution}\n to learn structural information for each metapath, and a novel transformer-based semantic aggregator to combine semantic information across metapaths for the final embedding of each node. Extensive experiments are conducted on five wide-used real-world datasets to evaluate SeHGNN. The results demonstrate the superiority of SeHGNN over the state-of-the-arts, i.e., \n%  simple yet \n high prediction accuracy and fast training speed. \n%SeHGNN greatly reduces the complexity and computation and gains \\todo{} improvement of accuracy compared with \\todo{xxx}. \n \n\\end{itemize}\n\\end{comment}\n\n\n% \\begin{figure}[ht]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{sehgnn_intro_816}\n%   \\caption{The general architectures of metapath-based methods and metapath-free methods on heterogeneous graphs.}\n%   \\label{tab:hgnns_categories}\n% \\end{figure}\n\n\n\\begin{comment}\nThe widely existed complex systems such as social networks, citation networks, recommendation systems, knowledge graphs, are commonly abstracted as graph structures, where the nodes and edges represent entities and relations separately. Most real-world graphs consist of various types of entities and relations, widely known as heterogeneous information network (HIN)~\\cite{HG_survey}, or heterogeneous graph(HG). For example, the ACM citation dataset[23] contains five types of nodes: papers, authors, institutions, venues, as well as different types of relations between them.\n\n\n\nA large number of new algorithms on heterogeneous graphs have been proposed, with a variety of new mechanisms for aggregating features with different semantics. Some works~\\cite{yun2019graph, hong2020attention, hu2020heterogeneous, HGB} embed the type information into the propagated messages, while others~\\cite{zhu2019relation, zhang2019heterogeneous, wang2019heterogeneous, fu2020magnn} utilize different sub-networks to tackle different semantic branches. These two kinds of methods show competitive results on different datasets.\n\nAttention mechanism is widely used in heterogeneous graphs, for nodes or messages in the same semantic branch or between different semantic branches, for the purpose of let the network pay more attention on more important neighbors or semantics. However, through experiments on typical heterogeneous networks like HAN~\\cite{wang2019heterogeneous} and HGB~\\cite{HGB}, we find that attention within the same relation or metapath may be unnecessary or lead to overfit.\n\n\\todo{See comments here}\n% HAN\u662f\u8fd9\u6837\uff0c\u90a3\u4e48MAGNN\u5462\uff1f\n\nBased on this discovery, we adjust the framework commonly used in heterogeneous graphs and propose a novel network architecture, Metapath-propagated Simple Graph Neural Network(MSGNN). It separates the intra-metapath propagation part and pre-compute it with average aggregation before training, which is the key point of out model, accompanied with a new label propagation method and transformer-based inter-metapath aggregation. We test our model on 4 widely used datasets and one large ogbn-mag dataset, and experiments prove the efficiency of our model.\n\n\\todo{See comments here}\n%\u8fd9\u91cc\u53ea\u63d0\u53ca\u4e86accuracy\u7684\u90e8\u5206\uff0c\u8fd8\u8981\u63d0\u4e00\u4e0bsimple\u7684\u90e8\u5206\uff0c\u90a3\u91cc\u4f53\u73b0\u7b80\u6d01\uff1f\n\nOur contribution is reflected in the following four aspects:\n\n1. review two types of heterogeneous graph methods, metapath-based methods and metapath-free methods, and indicates the redundant part in typical models.\n\n2. We proposes a novel metapath-based algorithm, which pre-compute\nthe intra-metapath aggregation (for simplicity/avoid overfit), accompanied with\nnew label propagation method (for better utilization of labels) and\ntransformer-based semantic aggregation (for better attention and interpretation)\n\n3. Experiments on 4 widely used datasets and one big ogbn-mag dataset\nprove the efficiency of our model.\n\n4. Compared with the existing heterogeneous graph algorithms, where the metapaths are determined by experts in advance, we propose a metapath pruning method, which not only maintains the accuracy of the existing algorithms, but also further reduces the amount of calculation.\n\\end{comment}\n\n\n\n\n\n\n\n"
            },
            "section 2": {
                "name": "Preliminaries",
                "content": "\n\n% This section gives related preliminaries and brief introductions of graph convolutional networks (GCNs) as well as graph attention networks (GATs). GCNs and GATs are widely used as basic layers in HGNNs. Notations used in this paper are listed in Table \\ref{tab:notation}. \n\n% \u6216\u8bb8\u5c06\u660e\u767d\u4e3a\u4ec0\u4e48\u8981\u52a0\u5165\u5bf9\u67d0\u4e8b\u7269\u7684\u4ecb\u7ecd\uff0c\u4e5f\u662f\u4e00\u4e2a\u597d\u4e60\u60ef\n\n% , as well as three common conceptions, metapaths, metapath-based neighbors and metapath neighbor graphs, to describe the composite relationship in heterogeneous graphs.\n\n% \\subsection{Heterogeneous Graph}\n\n\n% \u8fd9\u5f20\u8868\u53c2\u8003HAN \u8fdb\u884c\u4fee\u6539\n\n\\textbf{Definition 1 \\,\\,\\textit{Heterogeneous graphs}.\\,}\n\\textit{A heterogeneous graph is defined as $G=\\{V,E,\\mathcal{T}^v,\\mathcal{T}^e\\}$, where $V$ is the set of nodes with a node type mapping function $\\phi:V\\rightarrow\\mathcal{T}^v$, and $E$ is the set of edges with an edge type mapping function $\\psi:E\\rightarrow\\mathcal{T}^e$.}\nEach node $v_i{\\in}V$ is attached with a node type $c_i{=}\\phi(v_i){\\in}\\mathcal{T}^v$. Each edge $e_{t\\leftarrow s}{\\in}E\\,$($e_{ts}$ for short) is attached with a relation $r_{c_t\\leftarrow c_s}{=}\\psi(e_{ts}){\\in}\\mathcal{T}^e$ ($r_{c_tc_s}$ for short), pointing from the source node $s$ to the target node $t$.\nWhen $|\\mathcal{T}^v|{=}|\\mathcal{T}^e|{=}1$, the graph degenerates into homogeneous. \n\nThe graph structure of $G$ can be represented by a series of adjacency matrices $\\{A_r: r\\in\\mathcal{T}^e\\}$. For each relation $r_{c_tc_s}{\\in}\\mathcal{T}^e$,\\, $A_{c_tc_s}{\\in}\\mathbb{R}^{|V^{c_t}|\\times |V^{c_s}|}$ is the corresponding adjacency matrix where the nonzero values indicate positions of edges $E^{c_tc_s}$ of the current relation.\n% means an edge of type $r_{c_tc_s}$ exists.\n% \\todo{shrink the distance in equations here}\n\n\\noindent \\textbf{Definition 2 \\,\\,\\textit{Metapaths}.\\,}\n\\textit{A metapath defines a composite relation of several edge types, represented as $\\mathcal{P}\\triangleq c_1\\leftarrow c_2\\leftarrow \\ldots\\leftarrow c_{l}$\\, ($\\mathcal{P}= c_1c_2\\ldots c_{l}$ for short).}\n\nGiven the metapath $\\mathcal{P}$, a \\textbf{metapath instance} $p$ is a node sequence following the schema defined by $\\mathcal{P}$, represented as $p(v_1,v_l)=\\{v_1,\\,e_{12},\\,v_2,\\,\\ldots,\\,v_{l-1},\\,e_{(l-1)l},\\,v_l:v_i\\in V^{c_i},\\,e_{i(i+1)}\\in E^{c_ic_{i+1}}\\}$. In particular, $p(v_1,v_l)$ indicates the relationship of $(l{-}1)$-hop neighborhood where $v_1$ is the target node and $v_l$ is one of $v_1$'s \\textbf{metapath-based neighbors}. \n% MAGNN\u7684\u5b9a\u4e49\u5c31\u662ftgt type\u5728\u524d\uff0csrc\u5728\u540e \n\n% \u4ee5\u4e0b\u90e8\u5206\u5728 simplified neighbor aggregation \u5b9a\u4e49\uff0c\u4f5c\u4e3a\u6211\u4eec\u63d0\u51fa\u7684\u4e00\u79cd\u7b80\u5316\u5f62\u5f0f\n% The adjacency matrix $A_\\mathcal{P}$ obtained by the multiplications of adjacency matrices as\n% \\begin{equation}\n% A_\\mathcal{P}=A_{r_1}A_{r_2}\\ldots A_{r_l}\n% \\end{equation}\n\n% \\subsection{Metapath Neighbor Graph}\nGiven a metapath $\\mathcal{P}$ with the node types of its two ends as $c_1, c_l$, a \\textbf{metapath neighbor graph} $G^\\mathcal{P}=\\{V^{c_1}\\bigcup V^{c_l}, E^\\mathcal{P}\\}$ can be constructed out of $G$, where an edge $e_{ij}\\in E^\\mathcal{P},\\phi(i)=c_1,\\phi(j)=c_l$ exists in $G^\\mathcal{P}$ if and only if there is an metapath instance $p(v_i,v_j)$ of $\\mathcal{P}$ in $G$.\n\n% Given a metapath $\\mathcal{P}$, we can re-connect the nodes in $\\mathcal{P}$ to get a metapath graph $G_\\mathcal{P}$ . Edge $u\\rightarrow v$ exists in $G_\\mathcal{P}$ if and only if there is at least one path between $u$ and $v$ following the metapath $\\mathcal{P}$ in the original graph $G$.\n% % For each metapath, we can extract a metapath graph from the original graph. The metapath graph is a homogeneous graph and an edge in the metapath graph corresponding to a metapath in the original graph.\n\n\n% Definition of HG and metagraph & general GCN\n\n% \\begin{table*}[!t]\n% \\caption{The unified framework of metapath-based HGNNs.}\n% \\label{tab:stages}\n% \\begin{tabular}{|c|cccc|}\n% \\hline\n%                                                                                 & \\multicolumn{1}{c|}{RGCN}                & \\multicolumn{1}{c|}{HetGNN}                                             & \\multicolumn{1}{c|}{HAN} & MAGNN                                          \\\\ \\hline\n% \\begin{tabular}[c]{@{}c@{}}Feature\\\\ projection\\end{tabular}                    & \\multicolumn{4}{c|}{$\\textbf{h}'_v=W^{\\phi(v)} \\textbf{x}_v$}                                                                                                                                                                        \\\\ \\hline\n% \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Neighbor\\\\ aggregation\\end{tabular}} & \\multicolumn{1}{c|}{\\multirow{2}{*}{$\\textbf{z}_v^r=\\frac{1}{|\\mathcal{N}_v^r|}\\sum_{u\\in\\mathcal{N}_v^r}\\textbf{h}'_u$}} & \\multicolumn{1}{c|}{\\multirow{2}{*}{$\\textbf{z}_v^t={\\rm Bi\\raisebox{0mm}{-}LSTM}(\\{\\textbf{h}'_u: u\\in \\mathcal{N}_v^t\\})$}}                             & \\multicolumn{1}{c|}{$\\gamma^\\mathcal{P}_{v,u}=\\sigma(\\textbf{a}^T_{\\mathcal{P}}\\cdot [\\textbf{h}'_v || \\textbf{h}'_u])$}    & \\begin{tabular}[c]{@{}c@{}}$\\textbf{h}'_{p(v,u)}={\\rm Encoder}(p(v,u))$\\\\ $\\gamma^\\mathcal{P}_{v,u}=\\sigma(\\textbf{a}^T_{\\mathcal{P}}\\cdot [\\textbf{h}'_v || \\textbf{h}'_{p(v,u)}])$\\end{tabular} \\\\ \\cline{4-5} \n%                                                                                 & \\multicolumn{1}{c|}{}                    & \\multicolumn{1}{c|}{}                                                & \\multicolumn{2}{c|}{$\\alpha^\\mathcal{P}_{v,u}=\\frac{\\exp(\\gamma^\\mathcal{P}_{v,u})}{\\sum_{t\\in\\mathcal{N}^\\mathcal{P}_v}\\exp(\\gamma^\\mathcal{P}_{v,t})},\\,\\,\\textbf{z}^\\mathcal{P}_v=\\sigma(\\sum_{t\\in\\mathcal{N}^\\mathcal{P}_v}\\alpha^\\mathcal{P}_{v,u}\\textbf{h}'_{p(v,u)})$}                                                     \\\\ \\hline\n% \\begin{tabular}[c]{@{}c@{}}Semantic\\\\ aggregation\\end{tabular}                  & \\multicolumn{1}{c|}{$\\textbf{h}_v=\\sum_r \\textbf{z}_v^r+W_0\\textbf{x}_v$}                  & \\multicolumn{1}{c|}{\\begin{tabular}[c]{@{}c@{}}$\\alpha_v^t=\\frac{\\exp{({\\rm LeakyReLU}(\\textbf{a}^T[\\textbf{z}_v^t || \\textbf{h}'_v]))}}{\\sum_{k} \\exp{({\\rm LeakyReLU}(\\textbf{a}^T[\\textbf{z}_v^k || \\textbf{h}'_v]))}}$\\\\ $\\textbf{h}_v=\\alpha_v\\textbf{h}'_v+\\sum_{t}\\alpha_v^t\\textbf{z}^t_v$\\end{tabular}} & \\multicolumn{2}{c|}{\\begin{tabular}[c]{@{}c@{}}$w_{\\mathcal{P}}=\\frac{1}{||V^{\\phi(v)}||}\\sum_{k\\in V^{\\phi(v)}}\\textbf{q}^T\\cdot\\tanh(\\textbf{W}\\textbf{z}_k^\\mathcal{P}+\\textbf{b})$\\\\ $\\beta_{\\mathcal{P}_i}=\\frac{\\exp(w_{P_i})}{\\sum_{\\mathcal{P}_j}\\exp(w_{P_j})},\\,\\,\\textbf{h}_v = \\sum_{\\mathcal{P}_i}\\beta_{\\mathcal{P}_i}\\textbf{z}^\\mathcal{P}_v$\\end{tabular}}       \\\\ \\hline\n% \\end{tabular}\n% \\end{table*}\n\n\\begin{comment}\n\\subsection{Graph Neural Network}\n% \u8981\u4e48\u7528representaion\uff0c\u8981\u4e48\u7528embedding vector\nThe target of GNNs is to learn representation $h$ of each node based on the graph structure and the initial raw node features $x$. For each node $v\\in V$, GNNs learn a embedding vector $h_v$ by aggregating features $\\{x_u: u\\in \\mathcal{N}_v\\}$ from local neighbor set $\\mathcal{N}_v$ of $v$. The embedding vectors are used in the downstream task such as node classification,\n\n\\textbf{Graph Convolutional Network} (GCN)~\\cite{kipf2016semi} proposes a multi-layer network using the following layer-wise propagation rule:\n$$\n\\setlength{\\abovedisplayskip}{2pt}\n\\setlength{\\belowdisplayskip}{2pt}\nH^{(l+1)}=\\sigma (\\hat{A}H^{(l)}W^{(l)}),\n$$\nwhere $H^{(l)}$ is the representation of all nodes after the $l^{th}$ layer ($H^{(0)}=X$), $W^{(l)}$ is the trainable weight matrix, $\\sigma$ is the nonlinear activation function, and $\\hat{A}$ is the normalized adjacency matrix with ``re-normalization tricks\u201d. Furthermore, GraphSAGE~\\cite{sage233} proposes scalable GCNs for large graphs by introducing mini-batch training and neighbor sampling. GraphSAGE experiments on several aggregation methods, and recently the \\textbf{mean aggregator} is the most popular one which averages feature vectors \nof neighbor nodes.\n\n\\textbf{Graph Attention Network} (GAT)~\\cite{velivckovic2017graph} leverages weighted aggregation by introducing attention mechanism, for the purpose of encouraging the model to focus on the most important parts of the neighbor nodes. The calculation of attention values follows\n$$\n\\setlength{\\abovedisplayskip}{2pt}\n\\setlength{\\belowdisplayskip}{2pt}\n\\alpha_{ij}=\\frac{\\exp{({\\rm LeakyReLU}(\\textbf{a}^T[\\textbf{Wh}_i || \\textbf{Wh}_j]))}}{\\sum_{k\\in \\mathcal{N}_i} \\exp{({\\rm LeakyReLU}(\\textbf{a}^T[\\textbf{Wh}_i || \\textbf{Wh}_k]))}},\n$$\nwhere $\\textbf{a}$ and $\\textbf{W}$ are learnable weights, $||$ means the concatenation of two vectors. The aggregation result is in the weighted sum format, that is, $h_i^{(l+1)}=\\sum_{j\\in{\\mathcal{N}_i}}\\alpha_{ij}^{(l)}h_j^{(l)}$.\n\\end{comment}\n\n% Definition of HG and metagraph & general GCN\n\n\n"
            },
            "section 3": {
                "name": "Related Work",
                "content": " \\label{sec:related_work}\n\n\\begin{comment}\n% This section describes two main types of HGNNs, i.e., metapath-based HGNNs and metapath-free HGNNs, according to how they aggregate structural and semantic information in heterogeneous graphs. In addition, this section presents the relation between SeHGNN and SGC\\cite{SGC}-based models.\n\n% RGCN\n% \u4e0d\u6d89\u53ca softmax\uff0c\u76f4\u63a5add\n% RGCN \u53ef\u4ee5\u770b\u4f5c\u662ftwo-stage aggregation\uff0c\u56e0\u4e3a\u6bcf\u79cdstage \u5185\u90e8\u7684aggr=mean\n% file:///Users/yangxc/Files/Code/dgl-0.6.1/docs/build/html/api/python/nn.pytorch.html?highlight=relgraphconv#dgl.nn.pytorch.conv.RelGraphConv \u53ef\u4ee5\u89c1\u5230\u4e00\u4e2a $c_{i,r}$ is the normalizer equal to |\\mathcal{N}^r(i)|.\n\n% HPN(\u4f18\u5148\u7ea7\u4e0d\u9ad8)\n\n% GTN(\u53ef\u4ee5\u63d0\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3abaseline\u6bd4\u8f83\uff0c\u4f46\u4e0d\u5bb9\u6613\u52a0\u5165\u8868\u4e2d)\n\\end{comment}\n\n% \u8981\u4e48\u7528representaion\uff0c\u8981\u4e48\u7528embedding vector\nFor homogeneous graphs, GNNs are widely used to learn node representation from the graph structure. % and the raw node features. \nThe pioneer GCN~\\cite{kipf2016semi} proposes a multi-layer network following a layer-wise propagation rule, where the $l^{th}$ layer learns an embedding vector $h_v^{(l+1)}$ by aggregating features $\\{h_u^{(l)}: u\\in \\mathcal{N}_v\\}$ from the local 1-hop neighbor set $\\mathcal{N}_v$ for each node $v\\in V$.\nGraphSAGE~\\cite{sage233} improves the scalability for large graphs by introducing mini-batch training and neighbor sampling~\\cite{9601152}.\nGAT~\\cite{velivckovic2017graph} introduces the attention mechanism to encourage the model to focus on the most important part of neighbors.\nSGC~\\cite{SGC} removes nonlinearities between consecutive graph convolutional layers,\nwhich brings great acceleration and does not impact model effects.\n% SGC~\\cite{wu2019simplifying} decouples the feature propagation and the nonlinear feature projection of a multi-layer GNN.\n% and pre-computes the feature propagation in the pre-processing stage.\n\n\\begin{comment}\n% \\subsection{Metapath-based HGNNs} % \u8fd9\u91cc\u52a0s\u662f\u4e3a\u4e86\u5f3a\u8c03\u6bcf\u4e00\u5f20\u6709\u56db\u79cd\u65b9\u6cd5\n\n% introduction\u91cc\u9762\u8bf4\u4e86HGNNs\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b\uff0c\u8fd9\u91cc\u8981\u8fdb\u4e00\u6b65\u8bf4\u660e\u4e3a\u4ec0\u4e48\u80fd & \u600e\u4e48\u6837\u6765 \u5206\u4e3a\u4e24\u7c7b \n % metapath-based HGNNs\u7684\u4e09\u4e2a\u6b65\u9aa4\u653e\u5230 motivation \u7684\u540e\u534a\u90e8\u5206\uff08\u4e24\u4e2a\u91cd\u8981\u5b9e\u9a8c\u540e\uff09\u8fdb\u884c\uff0c\u5f15\u51fa\u662f\u5982\u4f55\u4e00\u6b65\u6b65\u6539\u8fdb\u5230SeHGNN\u7684\nLayers in HGNNs not only aggregate features from neighbors but also fuse features of different semantics. According to whether to deal with structural and semantic information separately or together, HGNNs can be divided into metapath-based ones and metapath-free ones. One typical layer in metapath-based HGNNs composes of three steps: feature projection, neighbor aggregation, and semantic fusion. It starts with the feature projection step which projects raw feature vectors of different types of nodes to the same latent data space. Then at the scope of each metapath, the neighbor aggregation step aggregates feature vectors of metapath-based neighbors\n% in current metapath neighbor graph \\mzou{what do you mean by current metapath neighbor graph?},\nand generates a semantic feature vector for each node. At last, the semantic aggregation step fuses those semantic vectors across different metapaths for each node, and outputs the final embeddings of nodes. In addition, metapath-based HGNNs usually build subgraphs such as metapath neighbor graphs out of the original heterogeneous graph in the pre-processing stage.\n% Table \\ref{tab:stages} shows the unified framework of metapath-based HGNNs.\n\n% generates several representatives for each node from different semantic branches, each branch corresponding to one type of metapath. For each given metapath, the representation vector is generated by aggregating messages from this metapath-based neighbors of the target node. The second step fuses those semantic branches for each node, usually with an attention mechanism to learn the importance of different metapath branches and assign different attention values to them. The first step is named with intra-metapath aggregation or neighbor aggregation, and the second step called inter-metapath aggregation or semantic-level aggregation. Besides, it is common for heterogenous graph methods to start with a feature projection step, which transforms features of different type of nodes into the same feature spaces. Table 2 shows the framework of the following metapath-based methods. \n\n\n%Metapath-based HGNNs does two-step feature propagation. The first step generates several representatives for each node from different semantic branches, each branch corresponding to one type of metapath. For each given metapath, the representation vector is generated by aggregating messages from this metapath-based neighbors of the target node. The second step fuses those semantic branches for each node, usually with an attention mechanism to learn the importance of different metapath branches and assign different attention values to them. The first step is named with intra-metapath aggregation or neighboraggregation, and the second step called inter-metapath aggregation or semantic-level aggregation. Besides, it is common for heterogenous graph methods to start with a feature projection step, which transforms features of different type of nodes into the same feature spaces. Table 2 shows the framework of the following metapath-based methods. \n\n\n% \\todo{see comments}\n% \u53bb\u6389GCN pioneer\u7684\u63cf\u8ff0\n% \u6682\u65f6\u4e0d\u63d0 multi-layer\n\\end{comment}\n\nHeterogeneous graphs contain rich semantics besides structural information, revealed by multiple types of nodes and edges. According to the way to deal with different semantics, HGNNs are categorized into metapath-based and metapath-free methods.\n\nMetapath-based HGNNs first aggregate neighbor features of the same semantic and then fuse different semantics.\n% \u5730\u4f4d\u5982\u4f55\uff0c\u505a\u4e86\u4ec0\u4e48\nRGCN~\\cite{RGCN2018} is the first to separate 1-hop neighbor aggregation according to edge types.\nHetGNN~\\cite{HetGNN2019} takes use of neighbors of different hops. It uses random walks to collect neighbors of different distances and then aggregates neighbors of the same node type.\n% \u7a81\u51faGNN\u53ea\u5904\u7406 one-hop neighbors\nHAN~\\cite{HAN2019} utilizes metapaths to distinguish different semantics. It aggregates structural information with neighbor attention in each metapath neighbor graph in the \\textit{neighbor aggregation} step, and then fuses outputs from different subgraphs with semantic attention for each node in the \\textit{semantic fusion} step.\nMAGNN~\\cite{MAGNN2020} further leverages all nodes in a metapath instance rather than only the nodes of the two endpoints.\n\nMetapath-free HGNNs aggregate messages from neighbors of all node types simultaneously within the local 1-hop neighborhood like GNNs, but using additional modules such as attentions to embed semantic information such as node types and edge types into propagated messages.\nRSHN~\\cite{RSHN2019} builds the coarsened line graph to obtain the global embedding representation of different edge types, and then it uses the combination of neighbor features and edge-type embeddings for feature aggregation in each layer.\n% the propagated messages at the aggregation step of each GNN layer are the combination of neighbor features and edge type embeddings.\nHetSANN~\\cite{HetSANN} uses a multi-layer GAT network with type-specific score functions to generate attentions for different relations.\nHGT~\\cite{HGT} proposes a novel heterogeneous mutual attention mechanism based on Transformer~\\cite{transformer}, using type-specific trainable parameters for different types of nodes and edges.\n% \u6211\u4eec\u7684transformer\u5bf9\u6240\u6709\u7c7b\u578b\u4f7f\u7528\u7684\u662f\u540c\u6837\u7684 parameters\uff08\u4e0d\u8003\u8651 feature projection \u7684\u8bdd\uff09\nHGB~\\cite{HGB} takes the multi-layer GAT network as the backbone and incorporates both node features and learnable edge-type embeddings to generate attention values.\n% It also constructs a benchmark for HGNNs, which unifies the network settings and datasets of previous work.\n\n\n\n% HAN\u548cHGB\u5206\u522b\u662f\u5728\u5bf9\u5e94\u7684source code\u4e0a\u8dd1\u7684\n% HAN_ACMRaw\u548cHGB_ACM\u662f\u4e0d\u540c\u7684\u6570\u636e\u96c6\n\n\n\n\nExcept for the above two main categories of HGNNs, SGC-based work such as NARS~\\cite{NARS2020}, SAGN~\\cite{SAGN2021}, and GAMLP~\\cite{GAMLP2021} also show impressive results on heterogeneous graphs, but they aggregate features of all types of nodes together without explicitly distinguishing different semantics.\n\n\\begin{comment}\n% (This embedding pacitipates in propagate messages.)\n\n\n% Metapath-free HGNNs execute one-step aggregation and capture structural and semantic information simultaneously. These methods are based on multi-layer GNN networks, and usually use hierarchy attention mechanism with attention modules at each layer to learn importance among edges. Each layer aggregates propagated messages from direct (first-order) neighbors in the original heterogeneous graph to learn the structure information, while the semantic information like node and edge types are embedded inside the propagated messages or attention values in attention mechanism. Metapath-free methods do not build subgraphs, but extra modules are required to embed the semantic information.\n% \\todo{add: metapath-based needs to add metapath neighbor graph while metapath-free does not}\n\n% Metapath-free Methods \u6ca1\u6709\u663e\u5f0f\u7684\u533a\u5206\u4e0d\u540crelation\u7684aggregation\u8fc7\u7a0b\uff0c\u800c\u662f\u901a\u8fc7\u989d\u5916\u7684\u4fe1\u606f\u6697\u793amessages\u662f\u5426\u6765\u81ea\u540c\u4e00relation\u7684\u7ed3\u70b9\n\n% \\mzou{double check}\\mzou{The most obvious difference between A and B is that A performs message propagation alongside the original heteroheneous graph only once. Such an implementation does not partition subgraphs but requires extra information like edge attributes that implies different type of nodes or edges during the propagation.}\n\n\n% Metapath-free methods process each neighbor node with the same network.\n\n% Metapath-free methods propagates messages alongside the original heterogeneous graph, with extra information such as edge attributes that implies different type of nodes or edges during propagation. The propagation is one stage.\n% \u518d\u753b\u4e00\u5f20\u56fe\uff1f\uff1f\n\n% RSHN\n% RSHN \u4e2d\u63d0\u5230\u4e86\u4e0d\u9700\u8981 prior knowledge like metapath\n% \u4e0d\u6d89\u53ca softmax\uff0c\u76f4\u63a5add\n% \u770b\u4f5cone stage\uff0c\u6240\u6709\u7c7b\u578b\u7684\u90bb\u5c45\u8282\u70b9\u7684message\u76f4\u63a5\u52a0\u5230\u4e00\u8d77\n% \u4e0d\u8fc7\u6765\u81ea\u4e0d\u540c\u7c7b\u578b\u8282\u70b9\u7684\u4fe1\u606f\u5f15\u5165\u4e86\u4e0d\u540c\u7684 edge_attr\n\\textbf{RSHN}~\\cite{RSHN2019} firstly builds coarsened line graph to obtain embeddings of different edge types, and then combines feature vectors of first-order neighbors and their corresponding edge embeddings to generate the propagated messages for aggregation.\n\n% edge features first, then uses a novel Message Passing Neural Network (MPNN) [13] to propagate node and edge features.\n\n% \\todo{see raw tex here}\n% R-GSN/HGConv \u4e2dmetapath-based \u5206\u9636\u6bb5\u7684\u63cf\u8ff0\u53ef\u4ee5\u53c2\u8003\u4e00\u4e0b\n\n% HetSANN\n% \u6240\u6709 relation \u4e00\u8d77\u505a softmax\n% one stage\n% \u6211\u89c9\u5f97 HGB \u5e94\u8be5\u53ef\u4ee5\u4ee3\u8868\u8fd9\u79cd\u7b97\u6cd5\u4e86\uff0c\u4e3b\u8981\u662f tensorflow \u4fee\u6539 softmax \u7684\u503c\u5bf9\u6211\u6765\u8bf4\u4e0a\u624b\u8fd8\u8981\u5f88\u591a\u65f6\u95f4\n\\textbf{HetSANN}~\\cite{HetSANN} uses a multi-layer GNN with type-specific GAT layers for the aggregation of local information. The type-specific GAT layer utilizes unique trainable attention parameter $\\textbf{a}^r$ for each edge type $r$.\n\n\\textbf{HGT}~\\cite{HGT} proposes a novel heterogeneous mutual attention mechanism based on Transformer~\\cite{transformer}, using type-specific parameters to characterize the heterogeneous attention over each edge. It also uses a type-aware sampling method to tackle large heterogeneous graphs. \n\n% for handling large academic heterogeneous graphs with heterogeneous subgraph sampling.\n% \\todo{tell SeHGNN's advantage oevr sampling at somewhere else}\n% As HGT mainly focuses on handling web-scale graphs via graph sam- pling strategy [14, 42], the datasets used in its paper (> 10,000,000 nodes) are unaffordable for most HGNNs, unless adapting them by subgraph sampling. To eliminate the impact of subgraph sampling techniques on the performance, we apply HGT with its official code on the relatively small datasets that are not used in its paper, producing mixed results when compared to GAT (See Table 3).\n\n% HGB\uff08HeterGCN\uff09\n% \u6240\u6709 relation \u4e00\u8d77\u505a softmax\n% one stage\n\\textbf{HGB}~\\cite{HGB} constructs a benchmark for HGNNs, which unifies the network settings and datasets of previous work. It also proposes a simple but strong baseline model which takes a multi-layer GAT network as backbone and uses learnable edge type embeddings. For one edge, both the edge type embedding and node embeddings of the source node and the target node are used to calculate the attention value at each layer.\n\n% introduces edge weights to seperate different type of relations. Each type of relation shares the unique edge embedding and the edge embeding engages in the attention weight calculation in the GAT layer in aggregation stage.\n% \\todo{See raw text here}\n% \u7b2c\u4e00\u53e5\u8bdd\u5e94\u8be5 \u4ecb\u7ecd\u6bcf\u79cd\u7b97\u6cd5\u5404\u81ea\u7684\u7279\u70b9\uff0c\u7b2c\u4e8c\u53e5\u8bdd\u518d\u4ecb\u7ecd\u8fd9\u79cd\u7b97\u6cd5\u5728\u6211\u4eec\u7684\u4f53\u7cfb\u4e2d\u7684\u5730\u4f4d\n\n% HGT\n% \u6240\u6709 relation \u4e00\u8d77\u505a softmax\n% one stage\n% \u4e3b\u8981\u662f HGB \u91cc\u9762\u6ca1\u6709\u5b9e\u73b0\uff08\u867d\u7136\u8bf4OpenHGNN\u5b9e\u73b0\u4e86\uff09\uff0c\u5b9e\u73b0\u7684\u4f18\u5148\u7ea7\u4e0d\u9ad8\n\n% \u4e0d\u8003\u8651 R-HGNN\uff0c\u56e0\u4e3ahttps://zhuanlan.zhihu.com/p/375476415 \u5bf9\u5176\u8bc4\u4ef7\u4e0d\u9ad8\n% \u4e0d\u8003\u8651 HeCo\uff0c\u539f\u56e0\uff1a\u81ea\u76d1\u7763\u6a21\u578b\n% \u4e0d\u8003\u8651 HGNN-AC\uff0c\u56e0\u4e3a\u6709\u4e00\u4e2a\u989d\u5916\u7684 attribute completion loss\n% \u4e0d\u8003\u8651 HGSL\uff0c\u539f\u56e0\uff1a\u81ea\u6210\u4f53\u7cfb\uff0c\u5f15\u5165\u4e86\u591a\u79cd\u4e0d\u540c\u7684subgraph\u7684\u6982\u5ff5\uff0c\u4e0d\u6613\u5f52\u5165metapath-free or metapath-based\n% \u4e0d\u8003\u8651 HDE\uff0c\u539f\u56e0 \u8fd9\u662f\u4e00\u4e2a link prediction \u4efb\u52a1\n\n% \\subsection{Label Propagation Methods}\n% %Labels of training nodes are conventionally only used as supervision signals in loss functions in most graph learning methods.\n% Many graph networks have utilized labels as part of network input during training period instead of only regarding it as the supervision loss signals.~\\cite{wang2020unifying} and~\\cite{wang2021bag} has already utilized one-hot format of labels as inputs. UniMP~\\cite{shi2020masked} use label propagation prior in computing edge weights.\n\n% % \u4e2a\u4eba\u4e0d\u592a\u60f3\u5f15\u7528bags of tricks\u90a3\u7bc7\u8bba\u6587\u3002\u4e5f\u4e0d\u592a\u60f3\u5f15\u7528 C&S \u90a3\u7bc7\u8bba\u6587\n% % C&S \u662f\u786e\u5b9a\u6027\u7684\uff0cAPPNP\u662f\u7eb3\u5165\u8bad\u7ec3\u7684\uff0c\u8fd9\u4e24\u4e2a\u540e\u671f\u53ef\u4ee5\u5bf9\u6bd4\u5b9e\u9a8c\u4e00\u4e0b\n% % UniMP serves as regularization to assist the GCN in learning proper edge weights. \u4e0d\u518d\u4f7f\u75280-1\u7684\u90bb\u63a5\u77e9\u9635\uff0c\u800c\u662f\u628a\u90bb\u63a5\u77e9\u9635\u5185\u90e8\u7684\u6743\u91cd\u505a\u4e3alearnable papameters\n\n%~\\cite{zhu\u04532002learning} works with graphs with no raw features, and directly simply regards the partially observed label matrix $Y\\in\\mathbb{R}^{N\\times C}$ as input features for propagation.\n% % UniMP [30] proposes to map the partially observed label matrix Y to the dimension of the node feature matrix X and add these two matrices together as the new input feature. To fight against the label leakage problem, UniMP further randomly masks the training nodes during every training epoch.\n\n% % \u6211\u4eec\u63d0\u51fa\u7684\u4e24\u4e2a\u65b0\u65b9\u6cd5\n% % 1. \u53bb\u9664labels\u81ea\u5df1\u5bf9\u81ea\u5df1\u7684\u5f71\u54cd\n% % 2. balance train \u548c val \u7684\u5206\u5e03\n\n% % \u5176\u5b9e\u8fd9\u91cc\u53ef\u4ee5\u7b80\u5355\u7684\u8bb2\uff0c\u5c31\u76f4\u63a5\u8bf4\uff0c\u8fd9\u51e0\u7bc7\u8bba\u6587have utilized utilized one-hot format of labels as inputs\uff0c\u53e6\u5916To fight against the label leakage problem, UniMP further randomly masks the training nodes during every training epoch. \n% In UniMP: Since we have taken the node label as input, using it for supervised training will cause the label leakage problem. The model will overfit in the self-loop input label while performing poor in inference. To address this issue, we propose a masked label prediction strategy, which randomly masks some training in- stances\u2019 label and then predicts them to overcome label leak- age. This simple and effective training method is drawn the lesson from masked word prediction in BERT [Devlin et al., 2018], and simulates the procedure of transducing labels in- formation from labeled to unlabeled examples in the graph.\n\n% <Put in \\textbf{Methods} Chapter>\n\n% In~\\cite{zhu2002learning, wang2020unifying,shi2020masked, wang2021bag}, one-hot format labels have already been taken as inputs and participate propagation. To fight against the label leakage problem, UniMP~\\cite{shi2020masked} further randomly masks the training nodes during every training epoch.\n\\end{comment}\n\n% \u6709\u4e24\u79cd\u65b9\u6cd5\u5f15\u51fa SeHGNN\n% \u4e00\u662f\u901a\u8fc7\u5bf9\u73b0\u6709 metapath-based\u65b9\u6cd5\u7684framework\u7684\u5206\u6790\n% \u4e8c\u662f\u901a\u8fc7 SGC\n% \u65e2\u7136\u5df2\u7ecf\u9009\u62e9\u4e86\u7b2c\u4e00\u6761\uff0c\u90a3\u4e48\u7b2c\u4e8c\u6761\u6ca1\u6709related works\u7684\u5fc5\u8981\n\\begin{comment}\n\\subsection{SGC-based methods}\n\\mzou{What is the necessarity of SGC-based model here? }\n\n\\textbf{Simple Graph Convolution Networks} SGC~\\cite{wu2019simplifying} decouples the feature propagation and the non-linear transformation process, and the former is executed during pre-processing.\n\\begin{equation}\n\\hat{Y}={\\rm softmax}(\\hat{A}^KXW)\n\\end{equation}\nwhere $Y$ is prediction results, $K$ is pre-defined number of layers for feature propagation.\n\nSIGN~\\cite{rossi2020sign} proposes to concatenate the different\niterations of propagated features with linear transformation, in the form of $[X^{(0)}W^0,X^{(1)}W^1,\\ldots,X^{(K)}W^K]$.\n% Following SGC~\\cite{wu2019simplifying}, some recent methods adopt layer-wise propagation to combine the features with different propagation layers. SIGN~\\cite{rossi2020sign} proposes to concatenate the propagated features at different propagation depth after simple linear transformation: $[X^{(0)}W_0,X^{(1)}W_1,...,X^{(K)}W_K]$. ${\\rm S^2GC}$~\\cite{zhu2020simple} proposes the simple spectral graph convolution to average the propagated features in different iterations as $X^{(K)}=\\sum_{l=0}^K\\hat{A}^lX^{(0)}$. In addition, GBP~\\cite{chen2020scalable} further improves the combination process by weighted averaging as $X^{(K)}=\\sum_{l=0}^K\\omega_l\\hat{A}^lX^{(0)}$ with the layer weight $\\omega_l=\\beta(1-\\beta)^l$. SAGN~\\cite{sun2021scalable} and GAMLP~\\cite{zhang2021graph} ...\nFurther, NARS~\\cite{na} and GAMLP~\\cite{GAMLP} have tried to incorporate SGC into heterogeneous graphs. These models sample subgraphs from the original heterogeneous graphs according to relation types, regard the subgraph as a homogeneous graph although it may have different kinds of nodes and edges, generate propagated features of different steps on each subgraph, and process those features with 1-d convolution.\n%  The propagated features of the same propagation step across different subgraphs are aggregated using 1-d convolution. After that, aggregated features of different steps are fed into our GAMLP to get the final results. \n\\end{comment}\n\n% \\todo{see raw tex here}\n% \u8fd9\u91cc\u63d0\u4e00\u4e0b\u6211\u4eec\u7684\u65b9\u6cd5\u4e5f\u53d7\u5230\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u542f\u53d1\n% \u4f46\u4ed6\u4eec\u7684\u65b9\u6cd5\u4e5f\u6709\u4e0d\u5408\u7406\u4e4b\u5904\uff0c\u8003\u8651\u67d0\u79cdrelation\uff0crelation\u7684\u4e24\u7aef\u8fde\u63a5\u7684\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u7ed3\u70b9\u3002\u5728SGC\u8fd9\u4e00\u4e2apre-process\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5c06\u4ed6\u4eec\u76f8\u52a0\u662f\u4e0d\u5408\u7406\u7684\uff0c\u56e0\u4e3a\u4e24\u79cd\u7ed3\u70b9\u7684\u7279\u5f81\u5411\u91cf\u5f88\u6709\u53ef\u80fd\u96b6\u5c5e\u4e8e\u4e0d\u540c\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u6211\u4eec\u7684\u7ed9\u4e88metapath\u7684\u4f20\u64ad\u65b9\u6cd5\u66f4\u52a0\u4f18\u96c5\n% \u8fd9\u4e2a\u8fd8\u662f\u653e\u5728methods\u4e2d\u5427\n\n% \\todo{see raw tex here}\n% \u5176\u5b9e\u6211\u89c9\u5f97 S2GC \u548c GBP \u4e0e\u672c\u7bc7\u8bba\u6587\u7684\u5173\u7cfb\u4e0d\u5927\uff0c\u6ca1\u6709\u5fc5\u8981\u5f15\u7528\n\n\\begin{comment}\n\\subsection{Differences with SGC-based models}\nSeHGNN is also inspired by the Simplified Graph Convolutional Network (SGC)~\\cite{SGC}, which is designed for homogeneous graphs. SGC decouples the feature propagation and the nonlinear feature projection of a multi-layer GNN and pre-computes the feature propagation in the pre-processing stage. SeHGNN can also be seen as an extension of SGC on heterogeneous graphs.\n\nIn addition, several SGC-based models like NARS~\\cite{NARS2020} and GAMLP \\cite{GAMLP2021} have attempted to apply SGC to heterogeneous graphs, but they simply and inherently treat the processing of a heterogeneous graph as the processing of multiple homogeneous graphs. These models split one heterogeneous graph into several relation subgraphs which contain different subsets of edge types. Each subgraph is treated as homogeneous and processed using SGC. There are two inefficiencies of these models. Firstly, they directly aggregate the raw features of different\ntypes of nodes in each subgraph although these features may have different feature dimensions or lie in different latent spaces. \nSecondly, a subgraph may contain several metapaths with different semantics, but the feature aggregation in these models is semantic-agnostic and even confuses the semantics.\n% Secondly, they do not separate different high-level semantics, i.e., metapaths during feature propagation.\n% And they do not utilize the numerous intrinsic high-level semantics in heterogeneous graphs, i.e., metapaths.\n%  and apply SGC on each subgraph. A relation subgraph contains a  in the original heterogeneous graph, and the subgraph can be homogeneous or heterogeneous. There are two shortcomings of these models. Firstly, they aggregates raw features straight on each subgraph. This is not proper in a heterogeneous subgraph\n% as features of different node types may have different feature dimension or lie in different data space. Secondly, a subgraph may contain several metapaths with different semantics, but the feature aggregation in these models confuses the semantics. \n\nHowever, SeHGNN avoids the two inefficiencies above by proposing a well-designed framework that combines the advantages of both metapath-based methods and SGC. SeHGNN is not only able to learn structural information on each metapath neighbor graph separately without confusing semantics like metapath-based methods, but also inherits the characteristic of fast training speed from SGC by pre-computing neighbor aggregation for each metapath in the pre-processing stage.\n% performing neighbor aggregation on every metapath neighbor graph. \n% \\todo{highlight difference}\n%Experiment results also prove the superiority of SeHGNN.\n\\end{comment}\n"
            },
            "section 4": {
                "name": "Motivation",
                "content": " \\label{sec:motivation}\n\n% This section clarifies how the SeHGNN is proposed. \n% This section presents the motivations for SeHGNN. On one hand, we observe that the attention values within the same type of relation are similar. Further experiments imply that attention mechanism within the same metapath or relation tends to be unnecessary or lead to overfitting.\n% On the other hand, we find the calculation of neighbor aggregation is time-consuming.\n% Finally, the experiment results above motivate us to simplify the network structure.\n\n\n\n\n\\begin{comment}\nExisting HGNNs usually learn to embed information using hierarchy attention mechanism and repeated neighbor aggregation, suffering from unnecessary complexity and redundant computation. These inefficiencies are represented in the following two aspects. \n\n% Preliminary experiments on typical HGNNs imply that the attention within the same type of relation is unnecessary, which shows in two aspects:\n\n% On one hand, \n% for the typical metapath-free method HGB, which aggregates messages from different relations simultaneously, \n\\textbf{Overused node-level attention.}\nWe observe that for each target node in HGB, the node-level attention values of the same type of relations are similar, while those among different types of relations differ a lot. Figure \\ref{tab:stdev} (a) gives a diagram of this phenomenon. \nIn particular, for each type of relation, we calculate the standard deviation of attention values for each node on ACM dataset.\n% if the number of edges of this relation is not less than 2. \nFigure \\ref{tab:stdev} (b) shows the distributions of these standard deviations, where \u201call\u201d refers to the standard deviations across all relations. As can be seen, the difference of attention values within the same type of relation is much smaller than that across different types of relations. \n\n% \\begin{figure}[!htbp]\n%   \\centering\n%   \\includegraphics[width=\\linewidth]{localstdev0412_4.pdf}\n%   \\caption{(a) An illustration of that same type of relations has similar attention values in HGB. (b) The box plot of standard deviations of attention values for different relations in HGB.}\n%   \\label{tab:stdev}\n% \\end{figure}\n\\end{comment}\n\nExisting HGNNs inherit many mechanisms from GNNs without analysis of their effects.\nIn this section, we conduct an in-depth and detailed study of widely-used mechanisms, i.e. the attention mechanism and the multi-layer structure for HGNNs. Through experiments, we obtain two important findings that guide us in designing the architecture of SeHGNN. All results presented in this section are the average of 20 runs with different data partitions to mitigate the influence of random noise.\n\n\\noindent \\textbf{Study on attentions.}\nHGNNs use multiple attentions, as shown in Figure~\\ref{tab:hgnns_categories}, which are calculated using distinct modules or parameters. These attentions can be classified into two types: \\textit{neighbor attention} within neighbors of the same relation and \\textit{semantic attention} among different relations. Metapath-based methods like HAN incorporate two attentions in the neighbor aggregation step and the semantic fusion step, respectively. Metapath-free methods like HGB compute attentions of 1-hop neighbors with relation-specific embeddings. While distinguishing between the two types of attention in metapath-free methods can be challenging, we can perform additional calculations to eliminate the influence of either attention. Specifically, for the attentions values of each node's neighbors, we can average them within each relation which equals removing neighbor attention, or normalize them within each relation so that each relation contributes equally to final results to remove semantic attention.\n\n% to remove neighbor attention, \n% we can average attention values within each relation of each node's neighbors; to remove semantic attention, we can normalize attention values within each relation\n% or normalize attention values within each relation to remove semantic attention.\n\nRe-implement experiments on HGB reveal that a well-trained HGB model tends to assign similar attention values within each relation, leading us to investigate the necessity of different attentions. We conduct experiments on HAN and HGB, where * means removing neighbor attention and ${^\\dagger}$ means removing semantic attention. Results in Table \\ref{tab:observation} indicate that models without semantic attention exhibit a decrease of model effects, while models without neighbor attention do not, from which we obtain the first finding.\n\n\\textit{Finding 1: Semantic attention is essential, while neighbor attention is not necessary.} This finding is reasonable, as semantic attention is able to weigh the importance of different semantics. And for neighbor attention, various SGC-based work~\\cite{SGC,rossi2020sign,GAMLP2021} has demonstrated that simple mean aggregation can be just as effective as aggregation with attention modules.\n% The results about neighbor attention are not surprising as various SGC-based work~\\cite{SGC,rossi2020sign,GAMLP2021} has demonstrated that\n% \u53e6\u5916\u4e00\u4e2a\u91cd\u8981\u7684\u539f\u56e0\u662fneighbor attention \u4e0d\u4e00\u5b9a\u51c6\u786e\n% simple mean aggregation can be as effective as that with attention modules. In addition, semantic attention is necessary to weigh the importance of different semantics, as short metapaths naturally represent a closer relationship than long metapaths and some metapaths are missing for certain nodes.\n\n\\noindent \\textbf{Study on multi-layer structure.}\nWithout neighbor attention, metapath-free methods have an equivalent form that first averages features of neighbors within each relation and then fuses outputs of different relations. Therefore, they can be converted to metapath-based methods with a multi-layer structure and only 1-hop metapaths in each layer. So, in the following experiments, we focus on the influence of number of layers and metapaths in metapath-based methods.\n\nWe conduct experiments on HAN using a list of numbers to represent the structure of each variant. For instance, on the ACM dataset, the structure (1,1,1,1) represents a four-layer network with 1-hop metapaths PA, PS in each layer, and (4) represent a single-layer network with all metapaths no more than 4-hop, such as PA, PS, PAP, PSP, PAPAP, PSPSP and so on. These lists also exhibit the sizes of receptive field. For example, structures (1,1,1,1), (2,2), (4) have the same receptive field size, which involves 4-hop neighbors. \nBased on the results shown in Table \\ref{tab:observation2}, we conclude the second finding.\n\n\\textit{Finding 2: Models with a single-layer structure and long metapaths outperform those with multi-layers and short metapaths.} As shown in Table \\ref{tab:observation2}, models with a single-layer and long metapaths achieve better performance under the same size of the receptive field. \n% We attribute this phenomenon to that multi-layer networks perform semantic fusion in each layer, making high-level semantics indistinguishable. For example, we can easily distinguish high-level semantics such as being written by the same author (PAP) or familiar authors (PAPAP) through metapaths in models with structure (4), which is unavailable for a four-layer network (1,1,1,1) as all intermediate vectors between two consecutive layers are mixture of different semantics.\nWe attribute this to the fact that multi-layer networks fuse semantics per layer, making it difficult to distinguish high-level semantics. For instance, in a model with the structure (4), the utilization of multi-hop metapaths allow us to distinguish between high-level semantics, such as being written by the same author (PAP) or familiar authors (PAPAP), whereas these distinctions are unavailable in a four-layer network (1,1,1,1) as all intermediate vectors between two consecutive layers represent mixtures of different semantics.\n% For example, models with structure (4) may contain the metapath APSPA describing the same research field of two authors, which cannot be distinguished out of a four-layer network (1,1,1,1). \n% In addition, increasing the maximum metapath length could enhance model effects as it brings more metapaths with different semantics.\nMoreover, increasing the maximum metapath length enhances the model's performance by introducing more metapaths with different semantics.\n\n\\begin{comment}\nFurther experiments prove that for the node-level attention, using same attention values for the same type of relations achieves equivalent or even better performance on typical metapath-based or metapath-free methods. In the experiments, for the metapath-based method HAN, we replace the GAT layer in each metapath neighbor graph with the mean aggregator. And for the metapath-free method HGB, we add additional functions to average the attention values of the same type of relations for each target node. As Table \\ref{tab:observation} shows, HGB maintains the same accuracy while HAN gets better effect after the modifications. We reason the accuracy improvement is derived from that ${\\rm {HAN}^\\dagger}$ is free of the complexity brought by the attention mechanism and therefore suffering less from overfitting.\n\n% we test the effect of models if we use average attention values for the same type of relation or metapath on both metapath-free and metapath-based methods. On the typical metapath-free method HGB, after the calculation of attention, we add extra functions to average the attention values of the same type of relations for each target node. \n% The line HGB* in Table \\ref{tab:observation} shows the results if the modification is performed on a well-trained HGB model with parameters unchanged, and line ${\\rm {HGB}^\\dagger}$ is after re-training the model. \n\n% On the typical metapath-based method HAN, we replace the GAT layer in each metapath neighbor graph with the mean aggregator and re-train the model, and the result shows in line ${\\rm {HAN}^\\dagger}$. We reason the performance boost on ${\\rm {HAN}^\\dagger}$ is from decreased overfitting, as ${\\rm {HAN}^\\dagger}$ has fewer parameters. Those results demonstrates that using average attention values for the same type of relations or metapaths can achieve equivalent or even better performance.\n\n% for a well-trained HGB model, if we keep the parameters unchanged, and add extra functions after the calculation of attention to average the attention values of the same type of relations for each target node, the model performance makes almost no difference (HGB*).\n\n% On the other hand, using average attention values for the same type of relations can achieve equivalent or even better performance. For the typical metapath-based method HAN, we replace the GAT layer in each metapath neighbor graph with the mean aggregator in the neighbor aggregation step; for the typical metapath-free method HGB, we adopt the modifications in HGB*. Table \\ref{tab:observation} shows the performance after re-training the two models (${\\rm {HAN}^\\dagger}$ and ${\\rm {HGB}^\\dagger}$). We reason the performance boost on ${\\rm {HAN}^\\dagger}$ is from decreased overfitting, as ${\\rm {HAN}^\\dagger}$ has fewer parameters without attention modules in the neighbor aggregation step.\n\n\n\n\\textbf{Time-consuming neighbor aggregation.}\nPrevious metapath-based methods execute neighbor aggregation in every training epoch, which brings excess computation. The neighbor aggregation step captures structural information on each metapath neighbor graph, which brings intensive computation and usually dominates the total time used for each epoch. Experiments show that in HAN, the time consumption of neighbor aggregation is $92\\times$ and $84\\times$ that of semantic aggregation on DBLP and ACM respectively.\n\n% ACM hidden=8 head=8 83.64\n% DBLP 92.068\n\n% DBLP hidden=64 head=8 101.63\n% ACM 103.73\n\n% As the attention within the same type of relation is unnecessary and even lead to overfitting, and attention modules bring extra excessive computation in neighbor aggregation, it \n\\end{comment}\n\n\\noindent \\textbf{Proposal of SeHGNN.}\nMotivated by the two findings, on one hand, we can avoid redundant neighbor attention by employing mean aggregation at the scope of each metapath without sacrificing model effects; on the other hand, we can simplify the network structure using a single layer, but use more and longer metapaths to expand the receptive field and achieve better performance. Furthermore, as the neighbor aggregation part without the attention module involves only linear operations and no trainable parameters, it endows an opportunity to execute neighbor aggregation in the pre-processing step only once rather than in every training epoch, which significantly reduces the training time. Overall, these optimizations simplify the network structure and make it more efficient, which are the key points of SeHGNN.\n\n\\begin{comment}\ndifferent with previous HGNNs which performs feature projection \n\nMotivated by the observation above, on one hand, we can simplify the network structure and make it more efficient, by using the mean aggregator for each metapath neighbor graph without sacrificing the performance of the model. On the other hand, it endows an opportunity to execute neighbor aggregation in the pro-processing stage based on the following reasons: \n% Based on metapath-based methods, we take following steps to simplify the model. Firstly, the mean aggregation is used for each metapath neighbor graph in neighbor aggregation. \n% Furthermore, % Secondly, \na) as both the feature projection step and the neighbor aggregation step contain no nonlinear functions, the order of the two steps can be exchanged; b) as the mean neighbor aggregator is parameter-free, it can be pre-computed in the pre-processing step. \n\nAfter those designs, not only the redundant attention computation is avoided for each metapath neighbor graph by using the mean aggregator, but also the neighbor aggregation is executed only once in the pre-processing step rather than in every training epoch, which significantly reduces the training time. This is the key point of SeHGNN.\n% TODO \u4e0e\u4e0b\u9762\u6ce8\u91ca\u4e2d\u7684\u8bdd\u8fdb\u884c\u6574\u5408\n% Thus, each epoch in training can reuse the same compute result, which greatly reduces the number of computation compared with computing neighbor aggregation in training.\n% So we propose a novel SeHGNN, which removes the redundant node-level attention module and avoids repeated neighbor aggregation calculation. \n% It is not only SeHGNN significantly reduces the training time, but later experiments proves the great prediction power of SeHGNN.\n\\end{comment}\n\n\n"
            },
            "section 5": {
                "name": "Methodology",
                "content": "\n\nThis section formally proposes \\textit{Simple and Efficient Heterogeneous Neural Network} (SeHGNN).\n% , a metapath-based method for heterogeneous graphs. \nSeHGNN's architecture is illustrated in Figure \\ref{tab:framework} and includes three primary components: simplified neighbor aggregation, multi-layer feature projection, and transformer-based semantic fusion. \nFigure \\ref{tab:framework} also highlights the distinction between SeHGNN and other metapath-based HGNNs, i.e., SeHGNN pre-computes the neighbor aggregation in the pre-processing step, thereby avoiding the excessive complexity of repeated neighbor aggregation in every training epoch.\n% In addition, SeHGNN utilizes the multi-stage training strategy and a novel label propagation method for better performance.\nAlgorithm \\ref{alg:algorithm} outlines the overall training process.\n\n% \u4e3a\u4ec0\u4e48\u8981\u7528simplified\u8fd9\u4e2a\u9898\u76ee\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u81f4\u656c Simplified graph convolutional network\n",
                "subsection 5.1": {
                    "name": "Simplified Neighbor Aggregation",
                    "content": " \\label{sec:simplified_neighbor_aggregation}\n\n% A metapath is the high-level semantic relationship in heterogeneous graphs. For each node, the neighbor aggregation generates semantic feature vectors for different metapaths.\n% neighbor aggregation \u662f\u4e3a\u4e86\u4ea7\u751f\u4e0d\u540c\u8bed\u4e49\u7684\u4fe1\u606f\uff0c\u6bd4\u5982APA\u548cAPTPA\uff0c\u4f46\u8fd9\u79cd\u4e1c\u897f\u5e94\u8be5\u5728introduction\u548crelated works\u91cc\u9762\u5df2\u7ecf\u8bb2\u660e\u767d\u4e86\u3002\u8bba\u6587\u7684\u7a7a\u95f4\u975e\u5e38\u73cd\u8d35\uff0c\u6ca1\u6709\u5fc5\u8981\u91cd\u590d\u3002\n\nThe simplified neighbor aggregation is executed only once in the pre-processing step and generates a list of feature matrices $M=\\{X^\\mathcal{P}:\\mathcal{P}\\in\\Phi_X\\}$ of different semantics for the set $\\Phi_X$ of all given metapaths. \n% These matrices are taken as inputs of the network in later training. \nGenerally, for each node $v_i$, it employs the mean aggregation to aggregate features from metapath-based neighbors for each given metapath, and outputs a list of semantic feature vectors, denoted as\n% \\begin{equation}\n$$\nm_i=\\{\\mathbf{z}^\\mathcal{P}_i=\\frac{1}{||S^\\mathcal{P}||}\\sum_{p(i,j)\\in S_\\mathcal{P}}\\mathbf{x}_j:\\mathcal{P}\\in\\Phi_X\\}, \\label{intra-aggr-node}\n$$\n% \\end{equation}\nwhere $S^\\mathcal{P}$ is the set of all metapath instances corresponding to metapath $\\mathcal{P}$ and $p(i,j)$ represents one metapath instance with the target node $i$ and the source node $j$.\n\n% \u73b0\u6709\u7684\u5f02\u6784\u56fe\u7b97\u6cd5\u4f7f\u7528\u4e86\u5f88\u590d\u6742\u7684\u65b9\u5f0f\n% However, the collection of metapath-based neighbors introduces extra complexity, and we use the multiplication of adjacency matrices for simplicity.\n% introduces extra complexity -> \u53ea\u5bf9 avoid attention \u4f7f\u7528\uff0c\u8fd9\u91cc\u53ea\u7528costing\nWe propose a new method to simplify the collection of metapath-based neighbors.\nExisting metapath-based methods like HAN build metapath neighbor graphs that enumerate all metapath-based neighbors for each metapath, which brings a high overhead as the number of metapath instances grows exponentially with the length of the metapath. Inspired by the layer-wise propagation of GCN, we calculate the final contribution weight of each node to targets using the multiplication of adjacency matrices. Specifically, let $X^c=\\{{x_0^c}^T;{x_1^c}^T;$ $\\ldots;{x_{||V^c||-1}^c}^T\\}\\in\\mathbb{R}^{||V^c||\\times d^c}$ be the raw feature matrix of all nodes belonging to type $c$, where\n% each row in this matrix is the raw feature vector of the corresponding node and \n$d^c$ is the feature dimension, and then the simplified neighbor aggregation process can be expressed as\n% \\begin{equation}\n$$\nX^\\mathcal{P}=\\hat{A}_{c,c_1}\\hat{A}_{c_1,c_2}\\ldots\\hat{A}_{c_{l-1},c_l}X^{c_l}, \\label{intra-aggr-matrix}\n$$\n% \\end{equation}\nwhere $\\mathcal{P}=cc_1c_2\\ldots c_l$ is a $l$-hop metapath, and $\\hat{A}_{c_i,c_{i+1}}$ is the row-normalized form of adjacency matrix $A_{c_i,c_{i+1}}$ between node type $c_i$ and $c_{i+1}$.\nPlease note that, the aggregation results of short metapaths can be used as intermediate values for long metapaths. \n% Given a short metapath $\\mathcal{P}'=c_1c_2\\ldots c_l$ and its neighbor aggregation result $X^{\\mathcal{P}'}$, we can calculate $X^\\mathcal{P}=\\hat{A}_{c,c_1}X^{\\mathcal{P}'}$.\nFor example, given two metapaths PAP and PPAP for the ACM dataset, we can calculate $X^{PAP}$ first and then calculate $X^{PPAP}=\\hat{A}_{PP}X^{PAP}$. \n\n\n\\begin{algorithm}[tb]\n\\caption{The overall training process of SeHGNN}\n\\label{alg:algorithm}\n\\textbf{Input}: Raw feature matrices $\\{X^{c_i}: c_i\\in \\mathcal{T}^v\\}$; the raw label matrix $Y$; metapath sets $\\Phi_X$ for features and $\\Phi_Y$ for labels\\\\\n\\textbf{Parameter}: ${\\rm MLP}_{\\mathcal{P}_i}$ for feature projection; $W_Q$, $W_K$, $W_V$, $\\beta$ for semantic fusion; ${\\rm MLP}$ for downstream tasks \\\\\n\\textbf{Output}: Node classification results ${\\rm Pred}$ for target type $c$\n\n\\begin{algorithmic}[1] %[1] enables line numbers\n\\STATE \\textbf{\\% Neighbor aggregation}\n\\STATE Calculate aggregation of raw features for each $\\mathcal{P}\\in\\Phi_X$ \\\\ $X^\\mathcal{P}{=}\\hat{A}_{c,c_1}\\ldots \\hat{A}_{c_{l{-}1}c_l}X^{c_l},\\,\\,\\,\\,\\mathcal{P}{=}cc_1\\ldots c_{l{-}1}c_l$\n% \\FOR{${\\rm each\\,stage\\,k}$}\n% \\STATE \\textbf{\\% Label propagation}\n\\STATE Calculate aggregation of labels for each $\\mathcal{P}\\in\\Phi_Y$ \\\\\n$Y^\\mathcal{P}{=}{\\rm rm\\_diag}(\\hat{A}_{c,c_1}\\ldots \\hat{A}_{c_{l{-}1}c})Y,\\,\\,\\,\\,\\mathcal{P}{=}cc_1\\ldots c_{l{-}1}c$\n\\STATE Collect all semantic matrices \\\\ $M=\\{X^\\mathcal{P}:\\mathcal{P}\\in\\Phi_X\\}\\bigcup\\{Y^\\mathcal{P}:\\mathcal{P}\\in\\Phi_Y\\}$\n\\FOR{${\\rm each\\,epoch}$}\n\\STATE \\textbf{\\% Feature projection}\n\\STATE ${H'}^{\\mathcal{P}_i}={\\rm MLP}_{\\mathcal{P}_i} (M^{\\mathcal{P}_i}),\\,\\,M^{\\mathcal{P}_i}\\in M$\n\\STATE \\textbf{\\% Semantic fusion}\n\\STATE $Q^{\\mathcal{P}_i}{=}W_Q{H'}^{\\mathcal{P}_i},\\,K^{\\mathcal{P}_i}{=}W_K{H'}^{\\mathcal{P}_i},\\,V^{\\mathcal{P}_i}{=}W_V{H'}^{\\mathcal{P}_i}$\n\\STATE $\\alpha_{ij}=\\frac{\\exp{(Q^{\\mathcal{P}_i}\\cdot {K^{\\mathcal{P}_j}}^T})}{\\sum_{t}\\exp{(Q^{\\mathcal{P}_i}\\cdot {K^{\\mathcal{P}_t}}^T)}}$\n\\STATE $H^{\\mathcal{P}_i}=\\beta \\sum_j\\alpha_{ij}V^{\\mathcal{P}_i} + {H'}^{\\mathcal{P}_i}$\n\\STATE $H^c={\\rm concatenate}([H^{\\mathcal{P}_1} || H^{\\mathcal{P}_2} || \\ldots])$\n\\STATE \\textbf{\\% Downstream tasks}\n\\STATE ${\\rm Pred}={\\rm MLP}(H^c)$\n\\STATE Calculate loss function $\\mathcal{L}=-\\sum_i y_i\\ln pred_i$, do back-propagation and update network parameters\n\\ENDFOR\n% \\STATE \\textbf{\\% Enlarging training set}\n% \\STATE $L_{k+1}=L_0\\bigcup \\{i:\\max({\\rm Pred}_i)>\\delta^{(k+1)}\\}$\n% \\STATE Update $Y^{(k+1)}$ according to $L_{k+1}$ and ${\\rm Pred}$\n% \\ENDFOR\n\\STATE \\textbf{return} ${\\rm Pred}$\n\\end{algorithmic}\n\\end{algorithm}\n\n% Besides, previous research~\\cite{wang2020unifying, wang2021bag, shi2020masked} has demonstrated that using labels as extra inputs provides enhancements in model effects, so we consider the aggregation of labels as well. Similar to the aggregation of raw features, the one-hot format labels can be propagated along various metapaths, generating\nBesides, previous~\\cite{wang2020unifying, wang2021bag, shi2020masked} research has demonstrated that incorporating labels as additional inputs can improve model performance. To capitalize on this, similar to the aggregation of raw features, we represent labels in the one-hot format and propagate them across various metapaths.\nThis process generates a series of matrices $\\{Y^\\mathcal{P}:\\mathcal{P}\\in\\Phi_Y\\}$ that reflect the label distribution of corresponding metapath neighbor graphs.\n% The metapaths $\\Phi$ for feature propagation and $\\Phi'$ for label propagation are different, as many type nodes has raw features but only nodes of the target type of current classification task has labels.\nPlease note that the two endpoints of any metapath $\\mathcal{P}\\in\\Phi_Y$ should be the target node type $c$ in the node classification task.\nGiven a metapath $\\mathcal{P}=cc_1c_2\\ldots c_{l-1}c\\in\\Phi_Y$,\n% where $c$ is the target node type in the node classification task,\nthe label propagation process can be represented as\n$$\nY^\\mathcal{P}={\\rm rm\\_diag}(\\hat{A}^\\mathcal{P})Y^c,\\, \\hat{A}^\\mathcal{P}=\\hat{A}_{c,c_1}\\hat{A}_{c_1,c_2}\\ldots\\hat{A}_{c_{l-1},c},\n$$\nwhere $Y^c$ is the raw label matrix. In the matrix $Y^c$, rows corresponding to nodes in the training set take the values of one-hot format labels, while other rows are filled with 0. To avoid label leakage, we prevent each node from receiving the ground truth label information of itself, by removing the diagonal values in the results of multiplication of adjacency matrices. The label propagation also executes in the neighbor aggregation step and produces semantic matrices as extra inputs for later training.\n\n\n\n\n\n% \\begin{table*}[!htbp]\n% \\resizebox{\\linewidth}{!}{\n% % \\centering\\small\n% \\begin{tabular}{cccccccccc}\n% \\hline\n%                      &           & \\multicolumn{2}{c}{DBLP}                  & \\multicolumn{2}{c}{IMDB}                  & \\multicolumn{2}{c}{ACM}                   & \\multicolumn{2}{c}{Freebase}              \\\\ \\hline\n%                      &           & macro-f1            & micro-f1            & macro-f1            & micro-f1            & macro-f1            & micro-f1            & macro-f1            & micro-f1            \\\\ \\hline\n% \\multirow{4}{*}{1st} & RGCN      & 91.52\u00b10.50          & 92.07\u00b10.50          & 58.85\u00b10.26          & 62.05\u00b10.15          & 91.55\u00b10.74          & 91.41\u00b10.75          & 46.78\u00b10.77          & 58.33\u00b11.57          \\\\\n%                      & HetGNN    & 91.76\u00b10.43          & 92.33\u00b10.41          & 48.25\u00b10.67          & 51.16\u00b10.65          & 85.91\u00b10.25          & 86.05\u00b10.25          & -                   & -                   \\\\\n%                      & HAN       & 91.67\u00b10.49          & 92.05\u00b10.62          & 57.74\u00b10.96          & 64.63\u00b10.58          & 90.89\u00b10.43          & 90.79\u00b10.43          & 21.31\u00b11.68          & 54.77\u00b11.40          \\\\\n%                      & MAGNN     & 93.28\u00b10.51          & 93.76\u00b10.45          & 56.49\u00b13.20          & 64.67\u00b11.67          & 90.88\u00b10.64          & 90.77\u00b10.65          & -                   & -                   \\\\ \\hline\n% \\multirow{4}{*}{2nd} & RSHN      & 93.34\u00b10.58          & 93.81\u00b10.55          & 59.85\u00b13.21          & 64.22\u00b11.03          & 90.50\u00b11.51          & 90.32\u00b11.54          & -                   & -                   \\\\\n%                      & HetSANN   & 78.55\u00b12.42          & 80.56\u00b11.50          & 49.47\u00b11.21          & 57.68\u00b10.44          & 90.02\u00b10.35          & 89.91\u00b10.37          & -                   & -                   \\\\\n%                      & HGT       & 93.01\u00b10.23          & 93.49\u00b10.25          & 63.00\u00b11.19          & 67.20\u00b10.57          & 91.12\u00b10.76          & 91.00\u00b10.76          & 29.28\u00b12.52          & 60.51\u00b11.16          \\\\\n%                      & HGB       & 94.01\u00b10.24          & 94.46\u00b10.22          & 63.53\u00b11.36          & 67.36\u00b10.57          & 93.42\u00b10.44          & 93.35\u00b10.45          & 47.72\u00b11.48          & \\textbf{66.29\u00b10.45} \\\\ \\hline\n% \\multirow{2}{*}{3rd} & SeHGNN    & \\textbf{95.06\u00b10.17} & \\textbf{95.42\u00b10.17} & 67.11\u00b10.25 & 69.17\u00b10.43 & \\textbf{94.05\u00b10.35} & \\textbf{93.98\u00b10.36} & 51.87\u00b10.86 & 65.08\u00b10.66          \\\\\n%                      & SeHGNN+ms & $\\times$ & $\\times$ & \\textbf{67.65\u00b10.46} & \\textbf{69.76\u00b10.52} & $\\times$ & $\\times$ & \\textbf{52.26\u00b10.84} & 65.22\u00b10.40          \\\\\n%                      \\hline\n% \\multirow{4}{*}{4th} & Variant\\#1 & 93.61\u00b10.51 & 94.08\u00b10.48 & 64.48\u00b10.45                   & 66.58\u00b10.42                   & 93.06\u00b10.18                   & 92.98\u00b10.18                   & 33.23\u00b11.39                   & 57.60\u00b11.17                   \\\\\n%                      & Variant\\#2 & 94.66\u00b10.27                   & 95.01\u00b10.24                   & 65.27\u00b10.60                   & 66.68\u00b10.52      & 93.46\u00b10.43                   & 93.38\u00b10.44                                & 46.82\u00b11.12                   & 64.08\u00b11.43                   \\\\\n%                      & Variant\\#3 & 94.86\u00b10.14          & 95.24\u00b10.13          & 66.63\u00b10.34          & 68.21\u00b10.32          & 93.95\u00b10.48          & 93.87\u00b10.50          & 50.71\u00b10.44          & 63.41\u00b10.47          \\\\\n%                      & Variant\\#4 & 94.52\u00b10.05          & 94.93\u00b10.06          & 64.99\u00b10.54          & 66.65\u00b10.50          & 93.88\u00b10.63          & 93.80\u00b10.64          & 35.48\u00b11.36          & 60.03\u00b11.13          \\\\ \\hline\n% \\end{tabular}}\n% \\caption{Experiment results on the four datasets from HGB benchmark, where ``-'' means that the models run out of memory and ``$\\times$'' means that multi-stage training does not bring improvements and we do not exhibit corresponding results.} \\label{tab:result_on_midlle_dataset_with_ms}\n% \\end{table*}\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Multi-layer Feature Projection",
                    "content": " \\label{sec:multi_layer_feature_projection}\n% \u5fc5\u987b\u589e\u52a0\u8fd9\u6837\u4e00\u4e2a\u7ae0\u8282\uff0c\u89e3\u91ca X_P->H'_P\u7684\u8fc7\u7a0b\n\n% \u987a\u5e8f\uff1a\u505a\u4e86\u4ec0\u4e48\uff0c\u5728\u6574\u4f53\u7ed3\u6784\u91cc\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\n% \u5fc5\u987b\u662fmap to same data space\uff0c\u56e0\u4e3a\u540e\u9762\u7684semantic aggregation\u662f \n% The feature projection step projects semantic vectors into the same feature space. This is because the output semantic vectors of the prior neighbor aggregation step have different dimensions or lie in different data spaces, while the next semantic aggregation step uses shared parameters across different metapaths and requires the input semantic vectors lying in the same latent space.\nThe feature projection step projects semantic vectors into the same data space, as semantic vectors of different metapaths may have different dimensions or lie in various data spaces. Generally, it defines a semantic-specific transformation matrix $W^\\mathcal{P}$ for each metapath $\\mathcal{P}$ and calculates ${H'}^\\mathcal{P}=W^\\mathcal{P}X^\\mathcal{P}$.\nFor better representation power, we use a multi-layer perception block $\\rm{MLP}_\\mathcal{P}$ for each metapath $\\mathcal{P}$ with a normalization layer, a nonlinear layer, and a dropout layer between two consecutive linear layers. Then, this process can be expressed as\n$${H'}^\\mathcal{P}=\\rm{MLP}_\\mathcal{P}(X^\\mathcal{P}).$$\n\n\n"
                },
                "subsection 5.3": {
                    "name": "Transformer-based Semantic Fusion",
                    "content": " \\label{sec:semantic_aggregation}\n\nThe semantic fusion step fuses semantic feature vectors and generates the final embedding vector for each node. Rather than using a simple weighted sum format, we propose a transformer~\\cite{transformer}\\,-based semantic fusion module to further explore the mutual relationship between each pair of semantics.\n\n% A metapath is the high-level semantic relationship in heterogeneous graphs. For each node, the neighbor aggregation generates semantic feature vectors for different metapaths. Semantic aggregation aggregates those semantic feature vectors together to generate final node representation embedding for each node.\n\n% For previous metapath-based methods, semantic attention assigns weights for different semantic vectors. Those weights are generated from each semantic vector and the corresponding attention parameters, without perception of other semantic vectors. Inspired by the architecture design of Transformer~\\cite{transformer}, we propose a transformer-like semantic aggregator. \n\nThe transformer-based semantic fusion module is designed to learn the mutual attention between pairs of semantic vectors, given the pre-defined metapath list $\\Phi=\\{\\mathcal{P}_1,\\ldots,\\mathcal{P}_K\\}$ and projected semantic vectors $\\{{h'}^{\\mathcal{P}_1},\\ldots,{h'}^{\\mathcal{P}_K}\\}$ for each node.\n% Specifically, with the pre-defined metapath list $\\Phi=\\{\\mathcal{P}_1,\\ldots,\\mathcal{P}_K\\}$ and projected semantic vectors $\\{{h'}^{\\mathcal{P}_1},\\ldots,{h'}^{\\mathcal{P}_K}\\}$ for each node, the transformer-based semantic fusion module learns the mutual attention for each pair of semantic vectors. \nFor each semantic vector ${h'}^{\\mathcal{P}_i}$, the module maps this vector into a query vector $q^{\\mathcal{P}_i}$, a key vector $k^{\\mathcal{P}_i}$, and a value vector $v^{\\mathcal{P}_i}$. The mutual attention weight $\\alpha_{(\\mathcal{P}_i,\\mathcal{P}_j)}$ is the dot product result of the query vector $q^{\\mathcal{P}_i}$ and the key vector $k^{\\mathcal{P}_j}$ after a softmax normalization. The output vector $h^{\\mathcal{P}_i}$ of current semantic $\\mathcal{P}_i$ is the weighted sum of all value vectors $v^{\\mathcal{P}_j}$ plus a residual connection. The process of semantic fusion can be presented as\n%\n$$\nq^{\\mathcal{P}_i}=W_Q{h'}^{\\mathcal{P}_i},\\,\nk^{\\mathcal{P}_i}=W_K{h'}^{\\mathcal{P}_i},\\,\nv^{\\mathcal{P}_i}=W_V{h'}^{\\mathcal{P}_i},\\,\\,\\mathcal{P}_i\\in\\Phi,\n$$\n%\n$$\n\\alpha_{(\\mathcal{P}_i,\\mathcal{P}_j)}=\\frac{\\exp{(q^{\\mathcal{P}_i}\\cdot {k^{\\mathcal{P}_j}}^T)}}{\\sum_{\\mathcal{P}_t\\in\\Phi}\\exp{(q^{\\mathcal{P}_i}\\cdot {k^{\\mathcal{P}_t}}^T)}},\n$$\n%\n$$\nh^{\\mathcal{P}_i}=\\beta\\sum_{\\mathcal{P}_j\\in\\Phi}\\alpha_{(\\mathcal{P}_i,\\mathcal{P}_j)}\\,v^{\\mathcal{P}_j}+{h'}^{\\mathcal{P}_i},\n$$\nwhere $W_Q,W_K,W_V,\\beta$ are trainable parameters shared across all metapaths.\n\nThe final embedding vector of each node is the concatenation of all those output vectors. For downstream tasks like the node classification, another MLP is used to generate prediction results, which can be expressed as\n$$\n{\\rm Pred}={\\rm MLP}([h^{\\mathcal{P}_1} || h^{\\mathcal{P}_2} || \\ldots || h^{\\mathcal{P}_{|\\Phi|}}]).\n$$\n\n% \\subsection{Additional Training Techniques}\n\n% \\textbf{Metapath-based label propagation.}\n% % Labels in the form of one-hot vectors\n% % Labels could be also be utilized to improve the performance.\n% Previous work~\\cite{wang2020unifying, wang2021bag, shi2020masked} proves that using labels as extra inputs can improve the model effect. We propose a novel method adapted to the architecture of SeHGNN to utilize labels. Like the neighbor aggregation of raw features, the one-hot format labels can be propagated along various metapaths. Such process generates a series of matrices of propagated labels $\\{Y^\\mathcal{P}:\\mathcal{P}\\in\\Phi_Y\\}$ which reflect the label distribution of corresponding metapath neighbor graphs.\n% % The metapaths $\\Phi$ for feature propagation and $\\Phi'$ for label propagation are different, as many type nodes has raw features but only nodes of the target type of current classification task has labels.\n% Please note that the two ends of any metapath $\\mathcal{P}\\in\\Phi_Y$ should be the target node type $c$ in the node classification task.\n% Given a metapath $\\mathcal{P}=cc_1c_2\\ldots c_{l-1}c\\in\\Phi_Y$,\n% % where $c$ is the target node type in the node classification task,\n% the label propagation process can be represented as\n% $$\n% Y^\\mathcal{P}={\\rm remove\\_diag}(\\hat{A}^\\mathcal{P})Y^c,\\, \\hat{A}^\\mathcal{P}=\\hat{A}_{c,c_1}\\hat{A}_{c_1,c_2}\\ldots\\hat{A}_{c_{l-1},c}\n% $$\n% where $Y_c$ is the raw label matrix. In matrix $Y_c$, rows of nodes in the initial training set take the values of one-hot format labels, and other rows are filled with 0. For the aim of avoiding label leakage, we set to 0 the final contribution weight from the label of any node to itself, by removing the diagonal values of the results of multiplication of adjacency matrices. The label propagation also executes in the neighbor aggregation step and outputs semantic matrices as extra inputs for later training.\n\n% \\textbf{Multi-stage training.} Recent work~\\cite{li2018deeper, sun2020multi, yang2021self} introduces multi-stage training to utilize label information beyond the training set. At the end of each training stage, it selects test nodes with confident predictions and adds these nodes to the training set. Then the model is reset and the training process for the next stage starts. Specifically, let $L_0$ be the initial training set, then after the training in stage $k>1$, nodes whose maximum prediction scores are above the threshold $\\delta_k$ are regard as ``confident'' and added to the training set for the next stage $k+1$. The process is presented as $L_{k+1}=L_0\\cup \\{i:\\max {\\rm Pred}_i>\\delta_k\\}$.\n\n% In addition, if the above two technologies are simultaneously used during training, the raw label matrix $Y^{(k)}$ should be updated at the beginning of each stage $k$. For nodes in $L_k$ but not in the initial training set $L_0$, we use their prediction scores to fill the corresponding rows in $Y^{(k)}$.\n"
                }
            },
            "section 6": {
                "name": "Experiment",
                "content": "\n\n%In this section, we present experiments to demonstrate the efficacy of SeHGNN with aims to address the following questions:\n\n% In this section, we present extensive experiments with the aim of answering the following research questions:\n\n% \\squishlist\n\n%  \\item \\textbf{RQ1:} How does SeHGNN perform for node classification tasks? \\todo{make it specific.}\n%  \\item \\textbf{RQ2:} What is the effect of three components in SeHGNN including the simplified neighbor aggregation, the transformer-like semantic aggregation, and the label propagation module?\n%  \\item \\textbf{RQ3:} How does SeHGNN show superiority in training speed?\n% %  \\item \\textbf{RQ4:} How does SeHGNN select metapaths?\n\n% \\squishend\n\n% \\subsection{Experiment Setup}\n\n% \\textbf{Datasets.}\n% In the evaluation, we use five widely-used datasets in the semi-supervised node classification task: four middle-scale datasets including DBLP, ACM, IMDB, as well as Freebase, and a large-scale dataset Ogbn-mag from OGB Challenge~\\cite{hu2021ogb}. The simple statistics of these heterogeneous graphs are summarized in Table~\\ref{tab:dataset}. \n% In addition, one-hot format index vectors are assigned to nodes with no attributes as their dummy input features. More experiments details are clarified in the appendix.\nExperiments are conducted on four widely-used heterogeneous graphs including DBLP, ACM, IMDB, and Freebase from HGB benchmark~\\cite{HGB}, as well as a large-scale dataset ogbn-mag from OGB challenge~\\cite{hu2021ogb}. The details about all experiment settings and the network configurations are recorded in Appendix\\footnote{Appendix can be found at \\url{https://arxiv.org/abs/2207.02547}. Codes are available at \\url{https://github.com/ICT-GIMLab/SeHGNN}.}.\n\n",
                "subsection 6.1": {
                    "name": "Results on HGB Benchmark",
                    "content": "\n\nTable~\\ref{tab:result_on_midlle_dataset} presents the performance of SeHGNN on four datasets compared to several baselines in HGB benchmark, including four metapath-based methods (1st block) and four metapath-free methods (2nd block). \n% , where ``ms'' is the abbreviation of multi-stage training.\nResults demonstrate the effectiveness of SeHGNN as it achieves the best performance over all these baselines but the second best for micro-f1 accuracy on the Freebase dataset.\n\nAdditionally, we conduct comprehensive ablation studies to validate the two findings in the Motivation section and to determine the importance of other modules. The 4th block of Table~\\ref{tab:result_on_midlle_dataset} shows the results of four variants of SeHGNN.\n\nVariant\\#1 utilizes GAT for each metapath in the neighbor aggregation step like HAN. Variant\\#2 uses a two-layer structure, where each layer has independent neighbor aggregation and semantic fusion steps, but the maximum hop of metapaths in each layer is half of that in SeHGNN to ensure that SeHGNN and its Variant\\#2 have the same size of receptive field. The performance gap between SeHGNN and its two variants proves that the two findings also apply for SeHGNN. \n% Variant\\#1 utilizes GAT for each metapath in the neighbor aggregation step like HAN. Variant\\#2 uses the two-layer structure, where each layer has independent neighbor aggregation and semantic fusion steps, but the maximum hop of metapaths in each layer is half of that in SeHGNN to ensure that SeHGNN and its Variant\\#2 have the same size of receptive field. The performance gap between SeHGNN and two variants proves these two findings also hold for SeHGNN. \n\nVariant\\#3 does not include labels as extra inputs, and Variant\\#4 replaces the transformer-based semantic fusion with the weighted sum fusion like HAN. Notably, although inferior to SeHGNN, Variant\\#3 has already outperformed most baselines except the micro-f1 on the Freebase dataset. These results show that the utilization of label propagation and transformer-based fusion improves model performance.\n\n\n\n"
                },
                "subsection 6.2": {
                    "name": "Results on Ogbn-mag",
                    "content": "\n\n The ogbn-mag dataset presents two extra challenges: (1) some types of nodes lack raw features, (2) target type nodes are split according to years, causing training nodes and test nodes to have different data distribution. Existing methods usually address these challenges by (1) generating extra embeddings (abbreviated as \\textit{emb}) using unsupervised representation learning algorithms like ComplEx~\\cite{ComplEx2016} and (2) utilizing multi-stage learning (abbreviated as \\textit{ms}), which selects test nodes with confident predictions in last training stage, adds these nodes to the training set and re-trains the model in the new stage~\\cite{li2018deeper, sun2020multi, yang2021self}. To provide a comprehensive comparison, we compare results with or without these tricks. For methods without \\textit{emb}, we use randomly initialized raw feature vectors.\n\nTable~\\ref{tab:result_on_large_dataset} displays the results on the large-scale dataset ogbn-mag compared with baselines on the OGB leaderboard. Results show that SeHGNN outperforms other methods under the same condition. It is worth noting that SeHGNN with randomly initialized features even outperforms others with well-trained embeddings from additional representation learning algorithms, which reflects that SeHGNN learns more information from the graph structure.\n\n\n% \\subsection{Results on middle-scale datasets}\n% We compare SeHGNN against several well-known state-of-arts including four metapath-based HGNNs and four metapath-free HGNNs on middle-size datasets. These HGNNs have been introduced in Section~\\ref{sec:related_work}. Furthermore, to dissect the efficiency of SeHGNN and extra gains from multi-stage training, we evaluate SeHGNN with and without the multi-stage technique. For the baselines on the four middle-scale datasets, we adopt results published in HGB~\\cite{HGB}.\n\n% Table~\\ref{tab:result_on_midlle_dataset} shows that SeHGNN achieves the best performance over all baselines except micro-f1 accuracy in Freebase dataset.\n\n\n% \\todo{Explain why better than other HGNNs}\n% The performance gains are attributed to three aspects: a) more semantic messages, the light-weight neighbor aggregation brings the opportunity to use more metapaths; b) better semantic aggregation, a transformer-based semantic aggregator is used to learn the attention values between each pair of semantic vectors, rather than just getting a single weighted sum result with attention mechanism; c) less overfitting, the preliminary experiments in Section~\\ref{sec:motivation} have implied that the node-level attention within the same relation may lead to overfitting, but SeHGNN avoids using node-level attention in the neighbor aggregation step.\n\n% SeHGNN's simplified neighbor aggregation module in the pre-processing stage avoids repeated computation during training in previous work, which enable us to adopt as many as possible metapaths\n\n% In addition, the multi-stage training technique only slightly improves the accuracy on DBLP and ACM datasets but gains great improvement on IMDB and Freebase datasets. This is because this technique utilizes extra information from validation and test sets with pseudo labels, so that datasets with the relative low prediction accuracy may benefit more from the expanded training data.\n\n% gives partial data pseudo labels to the verification set and test set, and includes them in the training.\n% \u8fd9\u662f\u56e0\u4e3amulti-stage training \u8d4b\u4e88\u4e86\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u90e8\u5206\u6570\u636epeseodu labels\uff0c\u5e76\u4e14\u5c06\u4ed6\u4eec\u7eb3\u5165\u8bad\u7ec3\u4e2d\u3002\n\n%Table~\\ref{tab:result_on_large_dataset} shows the superior performance of SeHGNN on large-scale dataset. Note that SeHGNN achieves better performance with the help of extra features and the multi-stage training technique, and that SeHGNN has comparable performance even without extra features.\n\n% It shows that SeHGNN has comparable performance even without extra features. And SeHGNN achieves better performance with the help of extra features and the multi-stage training technique. These results demonstrate the superior performance of SeHGNN on large-scale dataset. \n\n%When the extra embeddings is not used, SeHGNN suffers less performance loss, which proves the robustness of SeHGNN relative to GAMLP.\n\n% The result on these OGB lists is the ability of the refined and try best to explore model. NARS and gamlp divide MAG into 8 subgraphs. Each subgraph contains one or more edge types. Each subgraph is regarded as isomorphic graph perform SGC on it ...\n\n% \\todo{see comments}\n% \u63d0\u4ea4\u4e00\u4e0b for IMDB & Freebase\u7684\u7ed3\u679c\u3002\u5199\u4e0d\u4e0b \u7684\u8bdd\u5206\u6210\u4e0a\u4e0b\u4e24\u4e2a\u8868\n\n% \\todo{\u6211\u8bb0\u5f97\u4e0d\u7528\u63d0\u53casmall\uff0c\u4e5f\u6bd4dblp\u5feb\u3002\u5982\u679c\u662f\u8fd9\u6837\u7684\u8bdd\uff0c\u5c31\u5c3d\u91cf\u4e0d\u8981\u5f15\u5165\u5bf9how to select really usedful metapaths\u7684\u8ba8\u8bba\u4e86\uff0c\u9664\u4e86\u53ef\u4ee5\u5728\u5c55\u671b\u91cc\u7a0d\u5fae\u63d0\u51e0\u53e5}\n\n% \\begin{table}[!htbp]\n% \\caption{Experiment results of the comparison with the state-of-the-arts on large-scale dataset.} \\label{tab:result_on_large_dataset2}\n\n% \\begin{tabular}{lcc}\n% \\hline\n% Methods               & Validation accuracy        & Test accuracy       \\\\ \\hline\n% SeHGNN                  & 55.95\u00b10.11          & 53.99\u00b10.18          \\\\\n% ComplEx+SeHGNN              & 56.56\u00b10.07          & 54.78\u00b10.17          \\\\\n% SeHGNN (4 stages)      & 58.70\u00b10.08          & 56.71\u00b10.14          \\\\\n% ComplEx+SeHGNN (4 stages)  & \\textbf{59.17\u00b10.09} & \\textbf{57.19\u00b10.12} \\\\ \\hline\n% \\end{tabular}\n% \\end{table}\n\n\n% First, both NARS and gamlp are based on sign, that is, collecting $[x, ax, a^2x,...]$, Then a list of features is processed by neural network. NARS and gamlp extend such methods on heterogeneous graphs. Firstly, the original graph is divided into multiple subgraphs, each subgraph contains one or more edge types, and then each subgraph is regarded as isomorphic graph perform SGC on it. We believe that not proper, because a single subgraph may contain many types of nodes, it is inappropriate to aggregate them directly without any mapping. Our method is based on the extension of the existing heterogeneous graph algorithm. We find that we can change the order, and finally introduce our method. Our method is interpretable because information is passed along Metapath and contains semantic meaning. Finally, our method is indeed inspired by it\n\n% \uff08\u52a0\u4e00\u4e2a\u7ae0\u8282\uff0c\u7a81\u51fa\u6211\u4eec\u7684\u65b9\u6cd5\u548cSGC-based\u65b9\u6cd5\u7684\u533a\u522b\uff0c\u653e\u5728methods\u4e2d\u3002\u9996\u5148\uff0cNARS\u548cGAMLP\u90fd\u662f\u57fa\u4e8eSIGN\u7684\uff0c\u5373\u641c\u96c6[X, AX, A^2X, ...]\uff0c\u7136\u540e\u7528\u795e\u7ecf\u7f51\u7edc\u5904\u7406a list of features\u3002NARS\u548cGAMLP\u5728\u5f02\u6784\u56fe\u4e0a\u8fdb\u884c\u4e86\u8fd9\u7c7b\u65b9\u6cd5\u7684\u6269\u5c55\uff0c\u9996\u5148\u5c06\u539f\u59cb\u56fe\u5206\u4e3a\u591a\u4e2a\u5b50\u56fe\uff0c\u6bcf\u4e2a\u5b50\u56fe\u542b\u6709\u4e00\u79cd\u6216\u591a\u79cdedge type\uff0c\u7136\u540e\u628a\u6bcf\u4e2a\u5b50\u56fe\u89c6\u4e3a\u540c\u6784\u56feperform SGC on it\u3002\u6211\u4eec\u8ba4\u4e3anot proper\uff0c\u56e0\u4e3a\u5355\u4e2a\u5b50\u56fe\u53ef\u80fd\u542b\u6709\u591a\u79cd\u7c7b\u578b\u7684\u8282\u70b9\uff0c\u76f4\u63a5\u5c06\u5b83\u4eec\u805a\u5408\u800c\u4e0d\u8fdb\u884c\u4efb\u4f55\u6620\u5c04\u5904\u7406\u662f\u4e0d\u6070\u5f53\u7684\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u57fa\u4e8e\u5df2\u6709\u5f02\u6784\u56fe\u7b97\u6cd5\u7684\u6269\u5c55\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u8c03\u6362\u987a\u5e8f\uff0c\u6700\u540e\u63a8\u51fa\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u53ef\u89e3\u91ca\u7684\uff0c\u56e0\u4e3a\u4fe1\u606f\u662f\u6cbf\u7740metapath\u4f20\u9012\uff0c\u5305\u542b\u8bed\u4e49\u610f\u4e49\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u786e\u5b9e\u6536\u5230\u4e86\u5176\u542f\u53d1\uff09\n\n\n"
                },
                "subsection 6.3": {
                    "name": "Time Analysis",
                    "content": "\nFirstly, we theoretically analyze the time complexity of SeHGNN compared to HAN and HGB as Table~\\ref{tab:time_complexity} shows. We assume a one-layer structure with $k$ metapaths for SeHGNN and HAN, and $l$-layer structure for HGB. The maximum hop of metapaths is also $l$ to ensure the same size of receptive field. The number of target type nodes\\footnote{For concise comparison, we only consider the complexity of linear projection for target type nodes on methods HAN and HGB. Please refer to Appendix for details.} is $n$ and the dimension of input and hidden vectors is $d$. The average number of neighbors in metapath neighbor graphs on HAN and involved neighbors during multi-layer aggregation on HGB are $e_1, e_2$, respectively. Please note that both $e_1$ and $e_2$ grow exponentially with the length of metapaths and layer number $l$. For the above five datasets we use tens of metapaths at most, but each node averagely aggregates information from thousands of neighbors for $l\\ge 3$. Generally, we have $e_1\\gg k^2, e_2 \\gg k^2$, so the theoretical complexity of SeHGNN is much lower than that of HAN and HGB.\n\nTo validate our theoretical analysis, we conduct experiments to compare the time consumption of SeHGNN with previous HGNNs. Figure~\\ref{fig:time_usage} shows achieving micro-f1 scores relative to the average time consumption of each training epoch for these models, which reflects the superiority of SeHGNN on both the training speed and the model effect.\n\n\n% \\begin{table}[!t]\n% \\resizebox{\\linewidth}{!}{\n% % \\centering\\small\n% \\tabcolsep=2pt\n% \\begin{tabular}{ccccc}\n% \\hline\n%       & \\begin{tabular}[c]{@{}c@{}}Feature\\\\ projection\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Neighbor\\\\ aggregation\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Semantic\\\\ fusion\\end{tabular} & Total \\\\ \\hline\n% SeHGNN & $O(NlKD^2)$ & - & $O(NK^2D^2)$ & $O(NK^2D^2)$ \\\\\n% HAN    & $O(NK\\mathcal{E}_1D^2)$ & $O(NK\\mathcal{E}_1D)$ & $O(NKD^2)$ & $O(NK\\mathcal{E}_1D^2)$ \\\\\n% HGB    & $O(N\\mathcal{E}_2D^2)$ & \\multicolumn{2}{c}{$O(N\\mathcal{E}_2D)$}   & $O(N\\mathcal{E}_2D^2)$ \\\\ \\hline\n% \\end{tabular}\n% }\n% \\caption{Theoretical complexity of SeHGNN, HAN and HGB in every training epoch. (It we think the complexity of neighbor's feature projection is part of Feature projection)}\n% \\label{tab:time_complexity}\n% \\end{table}\n\n\n\n\n\n\n% Figure~\\ref{fig:time_usage} shows the average time consumption of the state-of-the-art HGNNs and SeHGNN for each training epoch on DBLP dataset. SeHGNN (small) utilizes five metapaths which is the same as these metapath-based methods, while SeHGNN uses nine metapaths as Table~\\ref{tab:metapaths} shows.\n% Figure~\\ref{fig:time_usage} shows that both these two versions of SeHGNN run faster and achieve better accuracy than these state-of-the-arts. \n% Furthermore, SeHGNN achieves better accuracy when uses more number of metapaths, but more training time is consumed. \n\n\n\n%These results imply that SeHGNN takes comparable pre-processing time relative to metapath-based methods, and it takes less time than previous HGNNs but get superior performance.\n\n\n% In this section, we clarify why our model is more efficient than previous work. Both SeHGNN and previous metapath-based methods have pre-processing stage, the difference is that metapath-based methods like HAN builds metapath neighbor graph for each metapath, while SeHGNN pre-computes the neighbor aggregation by continuous matrix multiplication. Figure ?(a) shows that the pre-processing time is comparable. For training stage, SeHGNN is free of neighbor aggregation and it saves lots of time than metapath-based methods. Besides, relative to metapath-free methods, SeHGNN only aggregates semantic messages while metapath-free methods aggregates information from all neighbor nodes, it shows the superior performace of SeHGNN. The performance of the state-of-the-arts HGNNs over their training time and accuracy relative SeHGNN is ploted in Figure ?(b). Although the calculation of transformer costs extra time for SeHGNN, especially when we are using more metapaths than metapath-based methods, our model still takes less time than previous HGNNs in each training epoch.\n\n\n% \u548cHAN/HGB\u8fdb\u884c\u6bd4\u8f83\n% \u901f\u5ea6\u66f4\u5feb\uff0c\u4e0d\u540chidden dim\n% \n% \u51b3\u5b9a\u4e0d\u4f7f\u7528accurracy&time\u968fhidden dim\u53d8\u5316\u7684\u7c7b\u4f3c\u8fd9\u79cd\u7684\u6298\u7ebf\u56fe\n% \u5c31\u6563\u70b9\u56fe\u5427\uff0c\u6bcf\u79cd\u7b97\u6cd5\u4e00\u4e2a\u70b9\uff08\u53ef\u4ee5\u591a\u4e2a\u6570\u636e\u96c6\u591a\u4e2a\u56fe\uff09\n% \u51c6\u786e\u7387\u4e0d\u7528\u8dd1\u4e86\uff0c\u8dd1\u65f6\u95f4\u5c31\u884c\n\n\n% \\todo{see comments for the reason of not adding HAN's result for mag}\n% HAN\u548cMAGNN\u90fd\u9700\u8981global\u7684\u8ba1\u7b97inter-metapath attention\uff0c\u56e0\u4e3a\u6570\u636e\u96c6\u8fc7\u5927\uff0c\u9700\u8981\u5206batch\uff0c\u8ba1\u7b97\u8fd9\u4e2a\u4e1c\u897f\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u6240\u4ee5\u6ca1\u6709\u5217\u51fa\u7ed3\u679c\n% \u662f\u5426\u5c06\u5177\u4f53\u6570\u636e\u5199\u5230introduction\u4e2d\uff1f\u5982\u63d0\u5230efficience\u65f6\uff0c\u5177\u4f53\u5230\u63d0\u5347\u4e86\u591a\u5c11\n\n\n\n"
                }
            },
            "section 7": {
                "name": "Conclusion",
                "content": "\n\n% \\begin{tikzpicture}\n%     \\pie{10/A, 20/B, 30/C, 40/D}\n% \\end{tikzpicture}\n\nThis paper proposes a novel approach called SeHGNN for heterogeneous graph representation learning, which is based on two key findings about attention utilization and network structure.\nSeHGNN adopts a light-weight mean aggregator to pre-compute neighbor aggregation, which effectively captures structural information while avoiding overused neighbor attention and repeated neighbor aggregation.\nMoreover, SeHGNN utilizes a single-layer structure with long metapaths to extend the receptive field and a transformer-based semantic fusion module to better utilize semantic information, resulting in significant improvements in model effectiveness. \nExperiments on five commonly used datasets demonstrate that SeHGNN outperforms state-of-the-art methods in terms of both accuracy and training speed.\n\n\n"
            },
            "section 8": {
                "name": "Acknowledgments",
                "content": "\nThis work was supported by the National Natural Science Foundation of China (Grant No. 61732018, 61872335, and 62202451), Austrian-Chinese Cooperative R\\&D Project (FFG and CAS) (Grant No. 171111KYSB20200002), CAS Project for Young Scientists in Basic Research (Grant No. YSBR-029), and CAS Project for Youth Innovation Promotion Association.\n\n\\bibliography{aaai23}\n\n\\appendix\n"
            },
            "section 9": {
                "name": "Appendix",
                "content": "\n\n",
                "subsection 9.1": {
                    "name": "Observation in HGB models",
                    "content": "\n\nThe metapath-free method HGB~\\cite{HGB} calculates the attention value over each edge using the concatenation of node embeddings of the two endpoints and the edge-type embedding, presented as\n{\\small\n$$\\alpha_{ij}=\\frac{{\\rm exp}({\\rm LeakyReLU}(\\mathbf{a}^T[\\mathbf{Wh}_i||\\mathbf{Wh}_j||\\mathbf{W}_r\\mathbf{r}_{ij}]))}{\\sum_{k\\in\\mathcal{N}_i}{\\rm exp}({\\rm LeakyReLU}(\\mathbf{a}^T[\\mathbf{Wh}_i||\\mathbf{Wh}_k||\\mathbf{W}_r\\mathbf{r}_{ik}]))}.$$\n}\n\n\n\n\n\n\n\nIn our re-implemention experiments of HGB, we find that the attention values are mainly dominated by the edge-type embeddings. This is revealed from the observation that attention values within each relation are similar, while those among different relations ary significantly. Figure~\\ref{tab:stdev} (a) depicts this observation and Figure~\\ref{tab:stdev} (b) shows the statistics of standard deviations of attention values within each relation and among all relations for each target node on the ACM dataset.\n\nThis observation motivates us to investigate the necessity of neighbor attention within each relation and semantic attention among different relations in HGNNs.\n\n"
                },
                "subsection 9.2": {
                    "name": "Framework of existing metapath-based methods",
                    "content": "\nFor each layer of existing metapath-based methods, the calculation can be divided into three primary steps, \\textit{feature projection}, \\textit{neighbor aggregation}, and \\textit{semantic fusion}, as presented in Table~\\ref{tab:stages}. The feature projection step aims to map raw feature vectors of different types of nodes into the same data space, usually presented as one linear projection layer. Then the neighbor aggregation step aggregates feature vectors of neighbors for each semantic scope, e.g., for each relation (can be viewed as 1-hop metapath) in RGCN, for each node type in HetGNN, or for each metapath in HAN and MAGNN. Finally, the semantic aggregation step fuses those semantic vectors across all semantics and outputs the final embedding for each node.\n\nAfter removing neighbor attention, since both the one-layer feature projection and neighbor aggregation contain no nonlinear functions, the order of the two steps can be exchanged. Further, as the neighbor aggregation step does not involve any trainable parameters, it can be put ahead in the pre-processing step, and its results are shared across all training epochs.\n\nIn later experiments, we find a multi-layer block for feature projection can further enhance the performance. Therefore, SeHGNN employs an MLP block rather than a single linear layer for each metapath in the feature projection step.\n\n"
                },
                "subsection 9.3": {
                    "name": "Experiment settings",
                    "content": "\n\nThe evaluation of SeHGNN involves four medium-scale datasets from HGB benchmark~\\cite{HGB} and a large-scale dataset ogbn-mag\\footnote{https://ogb.stanford.edu/docs/nodeprop/\\#ogbn-mag} from the OGB challenge~\\cite{hu2021ogb}. The statistics of these heterogeneous graphs are summarized in Table~\\ref{tab:dataset}.\nFor the medium-scale datasets, we follow the dataset configuration requirements of the HGB benchmark. Specifically, the target type nodes are split with 30\\% for local training and 70\\% for online test, where labels of test nodes are not made public and researchers have to submit their predictions to the website of HGB benchmark for online evaluation. For local training, we randomly split the 30\\% nodes into 24\\% for training for 6\\% for validation. We compare our results with the baseline scores reported in the HGB paper, and all scores are the average of 5 different local data partitions. For the ogbn-mag dataset, we follows the official data partition where papers published before 2018, in 2018, and since 2019 are nodes for training, validation, and test, respectively. We compare our results with the scores on the OGB leaderboard, and all scores are the average of 10 separate trainings.\n\n\n\n\n\n\nWe adopt a simple metapath selection method where we preset the maximum hop and use all available metapaths no more than this maximum hop. We test different combinations of maximum hops for raw feature propagation and label propagation and the final choices are listed in Figure~\\ref{tab:metapaths}.\n\nFor all experiments, SeHGNN adopts a two-layer MLP for each metapath in the feature projection step, where the dimension of hidden vectors is 512. In the transformer-based fusion module, the dimension of query and key vectors are 1/4 of hidden vectors, the dimension of value vectors equals that of hidden vectors, and the number of heads is 1.\n\nSeHGNN is optimized with Adam \\cite{adam} during training. The learning rate is 0.0003 and the weight decay is 0.0001 for the Freebase dataset, and the learning rate is 0.001 and the weight decay is 0 for others.\n\n"
                },
                "subsection 9.4": {
                    "name": "Further analysis of time complexity",
                    "content": "\n\n\n\nThe computation complexity of HAN and HGB methods is challenging to estimate as it encompasses various node types and the reuse of projected neighbor node features in the neighbor aggregation step. To make a straightforward comparison, Table 5 includes the complexity of linear projection for target type nodes in HAN and HGB but omits that of nodes of other types. As illustrated in Table 9, we add the computation complexity of nodes of other types assuming the number of these nodes involved in one training batch to be $m$. These additions include the linear projection in the feature projection step and the calculation of neighbor attentions in the neighbor aggregation step.\n\nWhen training in full-batch mode, $n$ and $m$ represent the total number of target type nodes and other type nodes in the heterogeneous graph, respectively. If $n$ and $m$ are comparable, i.e., $O(n)=O(m)$, then results in Table 9 can be reduced to those in Table 5.\n\nHowever, for mini-batch training, $m$ can be significantly larger than $n$ for each training batch. In the case of a small batch size $n$ and almost no reusable projected neighbor node features, $m$ increases exponentially with the metapath length or number of layers (i.e., $O(m)=O(nke_1)$ for HAN and $O(m)=O(ne_2)$ for HGB). In this scenario, the computation complexity results on HAN and HGB in Table 5 show a significant under-estimation.\n\nIn contrast, estimating the computation complexity of SeHGNN is much simpler. Because the neighbor aggregation is calculated in the pre-processing step, eliminating the involvement of nodes of other types in each training batch.\n\n\\begin{comment}\nEstimating computation complexity of methods HAN and HGB is difficult, as it involves multiple types of nodes and the projected features of neighbors nodes are reused in the neighbor aggregation step. For concise comparison, in Table~\\ref{tab:time_complexity}, we only reserve the complexity of linear projection for target type nodes on HAN and HGB, but omit that for nodes of other types. As shown in Table~\\ref{tab:time_complexity2}, assuming the number of involved nodes of other types is $M$, we can add the computation complexity for these nodes, including the processes of the linear projection in the feature projection step and the computation of neighbor attentions in the neighbor aggregation step.\n\nFor full-batch training, $N,M$ represents the total number of nodes of the target type and other types in the heterogeneous graph, respectively. If $N$ and $M$ are in the same order of magnitude, i.e., $O(M)=O(N)$, then results in Table~\\ref{tab:time_complexity2} can be simplified as those in Table~\\ref{tab:time_complexity}.\n\nFor mini-batch training, however, $M$ can be much larger than $N$ for each training batch. An extreme case is that, when the batch-size is very small and no projected features of neighbors nodes can be reused, then $M$ grows exponentially with the metapath length or the number of layers (represented as $L$ in Table~\\ref{tab:time_complexity2}). In this situation, results in Table~\\ref{tab:time_complexity2} exhibit a huge under-estimation.\n\nRelatively, estimating computation complexity of another method SeHGNN is much easy. As the neighbor aggregation is calculated in the pre-processing step, it no longer involves nodes of other types during each training epoch.\n\\end{comment}\n\n% is the total number of target type nodes and $M$ is \n\n% feature projection and the computation of neighbor attentions for these nodes. For most cases in real heterogeneous graphs, $N$ and $M$ are in the same order of magnitude, which means the space complexity of nodes of other types can also be expressed as $O(N)$. Therefore this omission is reasonable.\n% Assume the number of nodes of other types is $M$, and we add the complexity of linear projection for these nodes for feature projection or the computation of attentions. The final complexity of these steps is shown in Table~\\ref{tab:time_complexity2}.\n\n\n\\begin{comment}\nFor the network backbone, a three-layer MLPs is used for each metapath in feature projection, and another three-layer MLP for the task-specific module to classify nodes. The dimension of hidden vectors is 512. \nFor the transformer-based aggregator in semantic aggregation, the dimension of query and key vectors are 1/4 of hidden vectors and the dimension of value vectors is the same as hidden vectors. The number of attention heads of the transformer-based aggregator is 1.\nIn addition, Table \\ref{tab:metapaths} shows the maximum length and number of metapaths used for neighbor aggregation and label propagation in each dataset. The total number of metapaths corresponds to the number of input semantic feature matrices for the network during training.\n\n\n% \\begin{comment}\nFor the four medium-scale datasets, the dataset configuration and \\todo{division} stay the same as HGB~\\cite{HGB}, where the \\todo{target types nodes} are randomly split with 24\\% for training, 6\\% for validation and 70\\% for test, and the evaluation metrics are the Macro-F1 score and the Micro-F1 score (accuracy). For Ogbn-mag dataset, the official data \\todo{division} is used, where the training, validation, and test sets contain papers published in different years, and the evaluation metric is also the accuracy. Like previous experiments~\\cite{HGB,fu2020magnn}, each presented score are average from 5 different data splits.\n\n%The main statistics of four datasets are summarized in Table~\\ref{tab:dataset}. Note that HetGNN is flexible to be applied to other heterogeneous graphs.\n% \\end{comment}\n\n\n% \\begin{comment}\n\n\\textbf{DBLP}: a citation heterogenous graph extracted from a computer science bibliography website \\footnote{https://dblp.uni-trier.de} which contains 14328 papers (P), 4057 authors (A), 20 venues (V), 7723 terms (T). The target is to predict research area of each author.\n\n\\textbf{ACM}: a citation heterogenous graph extracted from \\footnote{http://dl.acm.org/} which contains 3025 papers (P), 5959 authors (A) and 56 subjects (S). The target is to predict the conference each paper published.\n% conference \u5927\u7c7b\n\n\\textbf{IMDB}: a film heterogenous graph extracted from \\footnote{https://www.kaggle.com/karrrimba/movie-metadatacsv} which contains 4932 movies (M), 6124 actors (A) and 2393 directors (D). The target is to predict the genres of each movie. It is a multi-label classification task, and a movie belongs to a list of genres.\n\n\\textbf{Freebase}: a huge knowledge graph extracted from \\footnote{ https://www.yelp.com/dataset/challenge} which contains 8 type of entities including books, films, music, sports, people, locations, organizations, and businesses. The target is to predict the genre of each book.\n\n\\textbf{Oghb-mag}: a subset of the Microsoft Academic Graph from \\footnote{ https://ogb.stanford.edu/docs/nodeprop/\\#ogbn-mag} which contains 736389 papers, 1134649 authors, 8740 institutions, and 59965 fields. The target is to predict the venue of each paper.\n\n% \\end{comment}\n\n\n% Note that we evaluate the validation accuracy of the propagated\n% remove part of metapaths by for label propagation that the validation accuracy of the corresponding propagated label results is low.\n\n% \\textbf{Metapath Selection} Previous metapath-based methods relies on pre-defined metapaths by experts. Unlike them, we adopt a more general method to generate metapaths. We determine the max hop of metapaths, where a $l-hop$ metapath means a $(l+1)$-length metapath, which defines the radius of reception field of each node. We select all metapaths that is no more than this max hop. As an example, for ACM dataset with metapaths no more than 2-hop, we generate 0-hop features $X_P$ (itself), 1-hop features $X_{PA},X_{PS}, X_{PP}$, and 2-hop features $X_{PAP}, X_{PSP}, X_{PPA}, X_{PPS}$. Similar procedure is used for the propagation of one-hot label features. --\u300b to methods\n\n% \\todo{see comments here}\n% \u8fd9\u91cc\u63d0\u4e00\u4e0b\uff0c\u7531\u4e8e\u6ca1\u6709\u7ed9\u51fatrain\u548cval\u7684\u5212\u5206\uff0c\u6211\u4eec\u505a\u4e86\u4ec0\u4e48\n\n% It is criticized (by metapath-free methods) that metapath-based methods rely on the predefined metapaths by experts in advance for each dataset. Here, we adopt a more general method to determine metapaths. Firstly, we define the maximum length of metapaths $\\mathcal{L}$, which can be regarded as the radius of reception field during feature propagation, equivalent to the number of graph convolution layers in metapath-free methods. Then, we generate all metapaths from small to large. As an example, we define $\\mathcal{L}=3$ in ACM dataset, and we generate following propagated features from small hop to large hop: 0-hop features $X_P$ (itself), 1-hop features $X_{PA},X_{PS}$, and 2-hop features $X_{PAP}, X_{PSP}$.\n\n% Specifically, \n\n% It should be noticed for the medium size datasets, the train/val/test sets are randomly divided and belongs to the same data distribution. But for Ogbn-mag, the train/val/test sets are papers published in different year periods, so the graph structure is naturally biased. Similar to Unimp, we randomly mask part of train labels when generating the propagated label features for the train set until the top-1 accuracy is equivalent between train/val sets.\n\n% We use tricks of GAMLP as Ogbn-mag is unbalanced, by adding label residuals.\n\n\n% \\todo{see comments}\n% % \u5728\u540e\u9762\u7684\u5b9e\u9a8c\u4e2d\uff0c\u90fd\u91c7\u7528\u4e86512 hidden size\uff0cif with attention\uff0c 8 heads and 64 \n% \\todo{add a table for number of metapaths for each dataset}\n\n\n\\textbf{Network Configuration.}\nThe simple statistics of these heterogeneous graphs are summarized in Table~\\ref{tab:dataset}. \nIn addition, one-hot format index vectors are assigned to nodes with no attributes as their dummy input features.\n\nFor the network backbone, a three-layer MLPs is used for each metapath in feature projection, and another three-layer MLP for the task-specific module to classify nodes. The dimension of hidden vectors is 512. \nFor the transformer-based aggregator in semantic aggregation, the dimension of query and key vectors are 1/8 of hidden vectors and the dimension of value vectors is the same as hidden vectors. The number of attention heads of the transformer-based aggregator is 1.\nIn addition, Table \\ref{tab:metapaths} shows the maximum length and number of metapaths used for neighbor aggregation and label propagation in each dataset. The total number of metapaths corresponds to the number of input semantic feature matrices for the network during training.\n\n\\textbf{Training Parameter Setup.} \nThe model is optimized with Adam \\cite{adam} in training. The learning rate is 0.0003 and the weight decay is 0.0001 for the Freebase dataset, and the learning rate is 0.001 and the weight decay is 0 for others. In the following experiments, each presented score is the average from 5 times of training.\n\\end{comment}\n\n\n\n\n\n% \\textbf{Multi-stage training}\n% We predefined a threshold for each dataset, and nodes in the validation set and the test set whose highest prediction score are above the threshold will be included for next stage training with the prediction results as the pseudo labels.\n% % methods \u4e2d\u63d0\uff0c\u6bcf\u4e2astage label features \u9700\u8981\u91cd\u65b0\u751f\u6210\u3002\n\n% the point where the predicted maximum probability value exceeds the threshold will be included in the next step\n\n% \\textbf{Model selection} -> Appendix\n\n% With the progress of training, the accuracy of the validation set and test set gradually converges with fluctuates. For stable results, we use a moving window of size 10, which means the final outputs are the average of current epoch and previous 9 epochs, and then we check the accuracy and loss of the final outputs in validation set. For ACM, DBLP, IMDB, we choose the outputs with lowest validation loss, while for Freebase and Ogbn-mag, we choose the outputs with highest validation accuracy. This is an empirical result.\n\n% For the four medium datasets, HGB does not reveal the test labels, and one have to submit to their website for evaluation. \n\n\n\n% \u73b0\u5728\u770b\u6765\uff0c\u4ee5\u4e0a\u8868\u683c\u91ccACM-HAN\u7684\u7ed3\u679c\u5e94\u8be5\u662fHAN\u5728ACMraw\u4e0a\u7684\u8868\u73b0\uff0c\u800c\u4e0d\u662f\u5728ACM-HGB\u4e0a\u7684\u8868\u73b0\u3002\u8fd9\u91cc\u5e94\u8be5\u662f\u6570\u636e\u62ff\u9519\u4e86\n"
                }
            }
        },
        "tables": {
            "tab:observation": "\\begin{table}[!htbp]\n\\centering\n% \\small\n\\begin{tabular}{lcccc}\n\\hline\n% {\\fontsize{4pt}{0pt}\n                      & \\multicolumn{2}{c}{DBLP}      & \\multicolumn{2}{c}{ACM}       \\\\ \\hline\n                      & macro-f1      & micro-f1      & macro-f1      & micro-f1      \\\\ \\hline\nHAN                   & 92.59         & 93.06         & 90.30         & 90.15         \\\\\nHAN*                  & 92.75         & 93.23         & 90.61         & 90.48         \\\\\nHAN${^\\dagger}$       & 92.19         & 92.66         & 89.78         & 89.67         \\\\ \\hline\nHGB                   & 94.15         & 94.53         & 93.09         & 93.03         \\\\\nHGB*                  & 94.20         & 94.58         & 93.11         & 93.05         \\\\\nHGB${^\\dagger}$       & 93.77         & 94.15         & 92.32         & 92.27         \\\\ \\hline\n\\end{tabular}\n\\caption{Experiments to analyze the effects of two kinds of attentions. * means removing neighbor attention and $\\dagger$ means removing semantic attention.} \\label{tab:observation}\n\\end{table}",
            "tab:observation2": "\\begin{table}[ht]\n\\centering\n% \\small\n\\begin{tabular}{lcccc}\n\\hline\n\\multicolumn{1}{c}{}        & \\multicolumn{2}{c}{DBLP} & \\multicolumn{2}{c}{ACM} \\\\ \\hline\n\\multicolumn{1}{c}{network} & macro-f1    & micro-f1   & macro-f1   & micro-f1   \\\\ \\hline\n(1,)                        & 79.43       & 80.16      & 89.81      & 90.03      \\\\ \\hline\n(1,1)                       & 85.06       & 86.69      & 90.79      & 90.87      \\\\\n(2,)                        & 88.18       & 88.83      & 91.64      & 91.67      \\\\ \\hline\n(1,1,1)                     & 88.38       & 89.37      & 87.95      & 88.84      \\\\\n(3,)                        & 93.33       & 93.72      & 92.67      & 92.64      \\\\ \\hline\n(1,1,1,1)                   & 89.55       & 90.44      & 88.62      & 88.93      \\\\\n(2,2)                       & 91.88       & 92.35      & 92.57      & 92.53      \\\\\n(4)                         & \\textbf{93.60}       & \\textbf{94.02}      & \\textbf{92.82}      & \\textbf{92.79}      \\\\ \\hline\n\\end{tabular}\n\\caption{Experiments to analyze the effects of different combinations of the number of layers and the maximum metapath hop. e.g., the structure (1,1,1) means a three-layer network with all metapaths no more than 1 hop in each layer.}\n\\label{tab:observation2}\n\\end{table}",
            "tab:result_on_midlle_dataset": "\\begin{table*}[!htbp]\n% \\resizebox{\\linewidth}{!}{\n\\centering\\small\n\\begin{tabular}{cccccccccc}\n\\hline\n                     &           & \\multicolumn{2}{c}{DBLP}                  & \\multicolumn{2}{c}{IMDB}                  & \\multicolumn{2}{c}{ACM}                   & \\multicolumn{2}{c}{Freebase}              \\\\ \\hline\n                     &           & macro-f1            & micro-f1            & macro-f1            & micro-f1            & macro-f1            & micro-f1            & macro-f1            & micro-f1            \\\\ \\hline\n\\multirow{4}{*}{1st} & RGCN      & 91.52\u00b10.50          & 92.07\u00b10.50          & 58.85\u00b10.26          & 62.05\u00b10.15          & 91.55\u00b10.74          & 91.41\u00b10.75          & 46.78\u00b10.77          & 58.33\u00b11.57          \\\\\n                     & HetGNN    & 91.76\u00b10.43          & 92.33\u00b10.41          & 48.25\u00b10.67          & 51.16\u00b10.65          & 85.91\u00b10.25          & 86.05\u00b10.25          & -                   & -                   \\\\\n                     & HAN       & 91.67\u00b10.49          & 92.05\u00b10.62          & 57.74\u00b10.96          & 64.63\u00b10.58          & 90.89\u00b10.43          & 90.79\u00b10.43          & 21.31\u00b11.68          & 54.77\u00b11.40          \\\\\n                     & MAGNN     & 93.28\u00b10.51          & 93.76\u00b10.45          & 56.49\u00b13.20          & 64.67\u00b11.67          & 90.88\u00b10.64          & 90.77\u00b10.65          & -                   & -                   \\\\ \\hline\n\\multirow{4}{*}{2nd} & RSHN      & 93.34\u00b10.58          & 93.81\u00b10.55          & 59.85\u00b13.21          & 64.22\u00b11.03          & 90.50\u00b11.51          & 90.32\u00b11.54          & -                   & -                   \\\\\n                     & HetSANN   & 78.55\u00b12.42          & 80.56\u00b11.50          & 49.47\u00b11.21          & 57.68\u00b10.44          & 90.02\u00b10.35          & 89.91\u00b10.37          & -                   & -                   \\\\\n                     & HGT       & 93.01\u00b10.23          & 93.49\u00b10.25          & 63.00\u00b11.19          & 67.20\u00b10.57          & 91.12\u00b10.76          & 91.00\u00b10.76          & 29.28\u00b12.52          & 60.51\u00b11.16          \\\\\n                     & HGB       & 94.01\u00b10.24          & 94.46\u00b10.22          & 63.53\u00b11.36          & 67.36\u00b10.57          & 93.42\u00b10.44          & 93.35\u00b10.45          & 47.72\u00b11.48          & \\textbf{66.29\u00b10.45} \\\\ \\hline\n3rd                  & SeHGNN    & \\textbf{95.06\u00b10.17} & \\textbf{95.42\u00b10.17} & \\textbf{67.11\u00b10.25} & \\textbf{69.17\u00b10.43} & \\textbf{94.05\u00b10.35} & \\textbf{93.98\u00b10.36} & \\textbf{51.87\u00b10.86} & 65.08\u00b10.66          \\\\ \\hline\n\\multirow{4}{*}{4th} & Variant\\#1 & 93.61\u00b10.51 & 94.08\u00b10.48 & 64.48\u00b10.45                   & 66.58\u00b10.42                   & 93.06\u00b10.18                   & 92.98\u00b10.18                   & 33.23\u00b11.39                   & 57.60\u00b11.17                   \\\\\n                     & Variant\\#2 & 94.66\u00b10.27                   & 95.01\u00b10.24                   & 65.27\u00b10.60                   & 66.68\u00b10.52      & 93.46\u00b10.43                   & 93.38\u00b10.44                                & 46.82\u00b11.12                   & 64.08\u00b11.43                   \\\\\n                     & Variant\\#3 & 94.86\u00b10.14          & 95.24\u00b10.13          & 66.63\u00b10.34          & 68.21\u00b10.32          & 93.95\u00b10.48          & 93.87\u00b10.50          & 50.71\u00b10.44          & 63.41\u00b10.47          \\\\\n                     & Variant\\#4 & 94.52\u00b10.05          & 94.93\u00b10.06          & 64.99\u00b10.54          & 66.65\u00b10.50          & 93.88\u00b10.63          & 93.80\u00b10.64          & 50.30\u00b10.23          & 64.53\u00b10.38          \\\\ \\hline\n\\end{tabular} % }\n\\caption{Experiment results on the four datasets from HGB benchmark, where ``-'' means that the models run out of memory.}\n% \\todo{Freebase variant#4\u7684\u7ed3\u679c\u6709\u95ee\u9898\uff0clearning rate\u8fc7\u4f4e\u5bfc\u81f4\u6b20\u62df\u5408}\n\\label{tab:result_on_midlle_dataset}\n\\end{table*}",
            "tab:result_on_large_dataset": "\\begin{table}[!t]\n% \\resizebox{\\linewidth}{!}{\n\\centering\\small\n\\begin{tabular}{lcc}\n\\hline\nMethods               & Validation accuracy & Test accuracy       \\\\ \\hline\nRGCN                  & 48.35\u00b10.36          & 47.37\u00b10.48          \\\\\nHGT                   & 49.89\u00b10.47          & 49.27\u00b10.61          \\\\\nNARS                  & 51.85\u00b10.08          & 50.88\u00b10.12          \\\\\nSAGN                  & 52.25\u00b10.30          & 51.17\u00b10.32          \\\\\nGAMLP                 & 53.23\u00b10.23          & 51.63\u00b10.22          \\\\ \\hline\nHGT+emb               & 51.24\u00b10.46          & 49.82\u00b10.13          \\\\\nNARS+emb              & 53.72\u00b10.09          & 52.40\u00b10.16          \\\\\nGAMLP+emb             & 55.48\u00b10.08          & 53.96\u00b10.18          \\\\\nSAGN+emb+ms           & 55.91\u00b10.17          & 54.40\u00b10.15          \\\\\nGAMLP+emb+ms          & 57.02\u00b10.41          & 55.90\u00b10.27          \\\\ \\hline\nSeHGNN                & 55.95\u00b10.11          & 53.99\u00b10.18          \\\\\nSeHGNN+emb            & 56.56\u00b10.07          & 54.78\u00b10.17          \\\\\nSeHGNN+ms             & 58.70\u00b10.08          & 56.71\u00b10.14          \\\\\nSeHGNN+emb+ms         & \\textbf{59.17\u00b10.09} & \\textbf{57.19\u00b10.12} \\\\ \\hline\n\\end{tabular} % }\n\\caption{Experiment results on ogbn-mag compared with methods on the OGB leaderboard, where ``emb'' means using extra embeddings and ``ms'' means multi-stage training.}\n% \\caption{Experiment results on the large-scale dataset ogbn-mag compared with methods on the OGB leaderboard, where ``emb'' means using extra embeddings and ``ms'' means using multi-stage training.}\n\\label{tab:result_on_large_dataset}\n\\end{table}",
            "tab:time_complexity": "\\begin{table}[!t]\n% \\resizebox{\\linewidth}{!}{\n% \\centering\n\\small\n\\tabcolsep=-1pt\n\\begin{tabular}{ccccc}\n\\hline\n       & \\begin{tabular}[c]{@{}c@{}}Feature\\\\ projection\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Neighbor\\\\ \\,\\,\\,aggregation\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Semantic\\\\ fusion\\end{tabular} & Total \\\\ \\hline\n       % & FP & NA & SF & Total \\\\ \\hline\n\\,SeHGNN & $\\,\\,O(nkd^2)$ & -- & $O(n(kd^2{+}k^2d))$ & $\\,\\,O(nd(k^2{+}kd))$ \\\\\nHAN    & $O(nd^2)$ & $O(nke_1d)$ & $O(nkd^2)$ & $O(nd(ke_1{+}kd))$ \\\\\nHGB    & $O(nld^2)$ & \\multicolumn{2}{c}{$O(ne_2d)$\\,\\,\\,\\,\\,\\,\\,\\,} & $O(nd(e_2{+}ld))$ \\\\ \\hline\n\\end{tabular} % }\n% \\caption{Theoretical complexity of SeHGNN, HAN and HGB in every training mini-batch. Please note that we only consider the computation of target type nodes for HAN and HGB in the feature projection step.}\n\\caption{Time complexity of SeHGNN, HAN and HGB.}\n\\label{tab:time_complexity}\n\\end{table}",
            "tab:stages": "\\begin{table*}[!ht]\n% \\resizebox{\\linewidth}{!}{\n\\tabcolsep=3.5pt\n\\small\\centering\n\\begin{tabular}{|c|c|c|cc|}\n\\hline\n                                                                                & RGCN & HetGNN & \\multicolumn{1}{c|}{HAN} & MAGNN \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Feature\\\\ projection\\end{tabular}                     & $\\textbf{h}_v^r=W^{\\psi(r)} \\textbf{x}_v$ & $\\textbf{h}'_v={\\rm Bi\\raisebox{0mm}{-}LSTM}^{\\phi(v)} (\\textbf{x}_v)$ & \\multicolumn{2}{c|}{$\\textbf{h}'_v=W^{\\phi(v)} \\textbf{x}_v$}                                                                                                                                                                        \\\\ \\hline\n\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Neighbor\\\\ aggregation\\end{tabular}} & {\\multirow{3}{*}{$\\textbf{z}_v^r=\\frac{1}{|\\mathcal{N}_v^r|}\\sum_{u\\in\\mathcal{N}_v^r}\\textbf{h}^r_u$}} & {\\multirow{3}{*}{$\\textbf{z}_v^t={\\rm Bi\\raisebox{0mm}{-}LSTM}(\\{\\textbf{h}'_u: u\\in \\mathcal{N}_v^t\\})$}}                             & \\multicolumn{1}{c|}{$\\gamma^\\mathcal{P}_{v,u}=\\sigma(\\textbf{a}^T_{\\mathcal{P}}\\cdot [\\textbf{h}'_v || \\textbf{h}'_u])$}    & \\begin{tabular}[c]{@{}c@{}}$\\textbf{h}'_{p(v,u)}={\\rm Encoder}(p(v,u))$\\\\ $\\gamma^\\mathcal{P}_{v,u}=\\sigma(\\textbf{a}^T_{\\mathcal{P}}\\cdot [\\textbf{h}'_v || \\textbf{h}'_{p(v,u)}])$\\end{tabular} \\\\ \\cline{4-5} \n                                                                                &                     &                                                 & \\multicolumn{2}{c|}{$\\alpha^\\mathcal{P}_{v,u}=\\frac{\\exp(\\gamma^\\mathcal{P}_{v,u})}{\\sum_{k\\in\\mathcal{N}^\\mathcal{P}_v}\\exp(\\gamma^\\mathcal{P}_{v,k})},\\,\\,\\textbf{z}^\\mathcal{P}_v=\\sigma(\\sum_{k\\in\\mathcal{N}^\\mathcal{P}_v}\\alpha^\\mathcal{P}_{v,k}\\textbf{h}'_{p(v,k)})$}                                                     \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Semantic\\\\ fusion\\end{tabular}                  & {$\\textbf{h}_v=\\sum_r \\textbf{z}_v^r+W_0\\textbf{x}_v$}                  & {\\begin{tabular}[c]{@{}c@{}}$\\alpha_v^t=\\frac{\\exp{({\\rm LeakyReLU}(\\textbf{a}^T[\\textbf{z}_v^t || \\textbf{h}'_v]))}}{\\sum_{k} \\exp{({\\rm LeakyReLU}(\\textbf{a}^T[\\textbf{z}_v^k || \\textbf{h}'_v]))}}$\\\\ $\\textbf{h}_v=\\alpha_v\\textbf{h}'_v+\\sum_{t}\\alpha_v^t\\textbf{z}^t_v$\\end{tabular}} & \\multicolumn{2}{c|}{\\begin{tabular}[c]{@{}c@{}}$w_{\\mathcal{P}}=\\frac{1}{||V^{\\phi(v)}||}\\sum_{k\\in V^{\\phi(v)}}\\textbf{q}^T\\cdot\\tanh(\\textbf{W}\\textbf{z}_k^\\mathcal{P}+\\textbf{b})$\\\\ $\\beta_{\\mathcal{P}_i}=\\frac{\\exp(w_{P_i})}{\\sum_{\\mathcal{P}_j}\\exp(w_{P_j})},\\,\\,\\textbf{h}_v = \\sum_{\\mathcal{P}_i}\\beta_{\\mathcal{P}_i}\\textbf{z}^{\\mathcal{P}_i}_v$\\end{tabular}}       \\\\ \\hline\n\\end{tabular}\n% }\n\\caption{The unified framework of existing metapath-based HGNNs.}\n\\label{tab:stages}\n\\end{table*}",
            "tab:dataset": "\\begin{table}[!t]\n\\tabcolsep=4pt\n\\centering\n% \\resizebox{\\linewidth}{!}{\n\\begin{tabular}{ccccc}\n\\hline\nDataset  & \\#Nodes   & \\begin{tabular}[c]{@{}c@{}}\\#Node\\\\ types\\end{tabular} & \\#Edges    & \\#Classes \\\\ \\hline\nDBLP     & 26,128    & 4 & 239,566    & 4   \\\\\nACM      & 10,942    & 4 & 547,872    & 3   \\\\\nIMDB     & 21,420    & 4 & 86,642     & 5   \\\\\nFreebase & 180,098   & 8 & 1,057,688  & 7   \\\\\nOgbn-mag & 1,939,743 & 4 & 21,111,007 & 349 \\\\ \\hline\n\\end{tabular}\n% }\n\\caption{The statistics of datasets used in this paper.} \\label{tab:dataset}\n\\end{table}",
            "tab:metapaths": "\\begin{table}[!t]\n\\tabcolsep=2pt\n% \\small\n\\centering\n% \\resizebox{\\linewidth}{!}{\n\\begin{tabular}{ccccc}\n\\hline\n                     & \\multicolumn{2}{c}{Feature propagation} & \\multicolumn{2}{c}{Label Propagation} \\\\ \\hline\n\\multicolumn{1}{l}{} & Max Hop & \\#Metapaths         & Max Hop & \\#Metapaths       \\\\ \\hline\nDBLP                 & 2                  & 5                   & 4                 & 4                 \\\\\nIMDB                 & 4                  & 25                  & 4                 & 12                \\\\\nACM                  & 4                  & 41                  & 3                 & 9                 \\\\\nFreebase             & 2                  & 73                  & 3                 & 9                 \\\\\nOgbn-mag             & 2                  & 10                  & 2                 & 5                 \\\\ \\hline\n\\end{tabular}\n% }\n\\caption{The maximum hops and numbers of metapaths for raw feature propagation and label propagation.} \\label{tab:metapaths}\n\\end{table}",
            "tab:time_complexity2": "\\begin{table}[!t]\n% \\resizebox{\\linewidth}{!}{\n% \\small\n\\centering\n\\tabcolsep=3.5pt\n\\begin{tabular}{ccccc}\n\\hline\n       & \\begin{tabular}[c]{@{}c@{}}Feature\\\\ projection\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Neighbor\\\\ aggregation\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Semantic\\\\ fusion\\end{tabular} \\\\ \\hline\n% SeHGNN & $O(NKD^2)$ & -- & $O(N(KD^2{+}K^2D))$ & $O(ND(K^2{+}KD))$ \\\\\nHAN    & $O((n{+}m)d^2)$ & $O(nke_1d{+}(n{+}m)kd)$ & $O(nkd^2)$  \\\\\nHGB    & $O((n{+}m)ld^2)$ & \\multicolumn{2}{c}{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,$O(ne_2d{+}(n{+}m)ld)$} \\\\ \\hline\n\\end{tabular}\n% }\n\\caption{Revised time complexity on HAN and HGB with consideration of nodes of all types.}\n\\label{tab:time_complexity2}\n\\end{table}"
        },
        "figures": {
            "tab:hgnns_categories": "\\begin{figure}[ht]\n  \\centering\n  \\includegraphics[width=\\linewidth]{sehgnn_intro_1201.pdf}\n  \\caption{The general architectures of metapath-based methods and metapath-free methods on heterogeneous graphs.}\n  \\label{tab:hgnns_categories}\n\\end{figure}",
            "tab:framework": "\\begin{figure*}[ht]\n  \\centering\n  \\includegraphics[width=\\linewidth]{framework1201.pdf}\n  \\caption{The architecture of SeHGNN compared to previous metapath-based methods. The example is based on ACM dataset with node types author (A), paper (P), and subject (S). This figure exhibits aggregation of 0-hop metapath P (the target node itself), 1-hop metapaths PA, PS, and 2-hop metapaths PAP, PSP.}\n  \\label{tab:framework}\n\\end{figure*}",
            "fig:time_usage": "\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{speed1130.pdf}\n  \\caption{Micro-f1 scores and time consumption of different HGNNs on DBLP dataset. Numbers below model names exhibit the ratio of time consumption relative to SeHGNN. e.g., ``6x'' below RGCN means RGCN costs 6 times of time.} \\label{fig:time_usage}\n\\end{figure}",
            "tab:stdev": "\\begin{figure}[!b]\n  %\\vspace{-5pt} \n  \\centering\n  \\includegraphics[width=\\linewidth]{localstdev0412_4.pdf}\n%   \\vspace{-30pt} %\u62c9\u8fd1\u56fe\u8868\u6807\u9898\u548c\u56fe\u7684\u8ddd\u79bb\n  \\caption{(a) An illustration of the observation that HGB tends to assign similar attention values within each relation for each target node. (b) The box plot of standard deviations of attention values for different relations in HGB.}\n  \\label{tab:stdev}\n%   \\Description{A woman and a girl in white dresses sit in an open car.}\n%   \\vspace{-10pt} \n\\end{figure}"
        },
        "git_link": "https://github.com/ICT-GIMLab/SeHGNN"
    }
}