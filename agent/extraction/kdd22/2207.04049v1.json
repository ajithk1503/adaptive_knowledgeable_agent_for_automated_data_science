{
    "meta_info": {
        "title": "Learning Causal Effects on Hypergraphs",
        "abstract": "Hypergraphs provide an effective abstraction for modeling multi-way group\ninteractions among nodes, where each hyperedge can connect any number of nodes.\nDifferent from most existing studies which leverage statistical dependencies,\nwe study hypergraphs from the perspective of causality. Specifically, in this\npaper, we focus on the problem of individual treatment effect (ITE) estimation\non hypergraphs, aiming to estimate how much an intervention (e.g., wearing face\ncovering) would causally affect an outcome (e.g., COVID-19 infection) of each\nindividual node. Existing works on ITE estimation either assume that the\noutcome on one individual should not be influenced by the treatment assignments\non other individuals (i.e., no interference), or assume the interference only\nexists between pairs of connected individuals in an ordinary graph. We argue\nthat these assumptions can be unrealistic on real-world hypergraphs, where\nhigher-order interference can affect the ultimate ITE estimations due to the\npresence of group interactions. In this work, we investigate high-order\ninterference modeling, and propose a new causality learning framework powered\nby hypergraph neural networks. Extensive experiments on real-world hypergraphs\nverify the superiority of our framework over existing baselines.",
        "author": "Jing Ma, Mengting Wan, Longqi Yang, Jundong Li, Brent Hecht, Jaime Teevan",
        "link": "http://arxiv.org/abs/2207.04049v1",
        "category": [
            "cs.LG",
            "cs.AI"
        ]
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\\label{sec:intro}\n% % \\begin{figure*}[t]\n% % \\centering\n% %      \\includegraphics[width=.8\\textwidth,height=1.2in]{figs/Interference.pdf}\n% %      \\vspace{-0.15in}\n% %       \\caption{High-order interference in hypergraphs.}\n% %       \\vspace{-0.1in}\n% %       \\label{fig:interference}\n% % \\end{figure*}\n% \\begin{figure}\n% \\begin{subfigure}[b]{0.2\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.2in]{figs/interference-hyper.pdf}\n%         \\caption{Ordinary graph}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.26\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.2in]{figs/hypergraph_cropped.pdf}\n%         \\caption{Hypergraph}\n%     \\end{subfigure}\n%     \\vspace{-3mm}\n%   \\caption{Examples of interference on an ordinary graph and a hypergraph. Consider five individuals $u_1,...,u_5$ with physical contact relations. Each hyperedge in (b) denotes a social gathering event (represented with a circle), while (a) describes the ordinary graph projected from the hypergraph in (b). Each black line in (a) denotes an edge. The arrows denote interference from different individuals to $u_1$. Interference in (a) is pairwise, while high-order interference exists inside each hyperedge in (b).}\n%   \\label{fig:interference}\n% \\vspace{-3mm}\n% \\end{figure}\n\n\n\n\n% Hypergraphs\n{Group interactions among individuals exist in a wide range of scenarios, %web-based platforms,\ne.g., massive gathering events, day-to-day group chats on WhatsApp or WeChat, and workplace interactions on Microsoft Teams or Slack channels. }%group communications on online forums. \nAlthough the conventional pairwise graph definition covers a vast number of applications (e.g., person-to-person physical contact networks or social networks \\cite{brandes2013social}), it fails to capture the complete information of these group interactions (where each interaction may involve more than two individuals) \\cite{bai2021hypergraph,feng2019hypergraph,yadati2018hypergcn}. The notion of the  \\textit{hypergraph} can thus be introduced to address this limitation. \nConsider a hypergraph example that individuals are connected via in-person social events, each gathering event can be represented as a \\textit{hyperedge} (Fig.~\\ref{fig:hypergraph}). Each hyperedge can connect an arbitrary number of individuals, in contrast to an ordinary edge which connects exactly two nodes  (Fig.~\\ref{fig:ordinary-graph}). \n%Consider a hypergraph example in which people are connected via group chats on a digital platform, where each group can be represented as a \\textit{hyperedge} (Fig.~\\ref{fig:hypergraph}). Each hyperedge connects an arbitrary number of individuals, in contrast to an ordinary edge which can only connect exactly two nodes (Fig.~\\ref{fig:ordinary-graph}). \n\n% Graphs exist in a wide range of scenarios, such as social networks \\cite{brandes2013social}, road networks \\cite{coclite2005traffic}, and biological networks \\cite{alm2003biological}, to name a few. Recently, machine learning on graphs is progressing at an astounding rate, especially powered by effective graph neural networks \\cite{kipf2016semi,velivckovic2017graph}. In these studies, machine learning can benefit from the additional relational information in graphs. \n% However, in many scenarios, the relations among different individuals can not be fully represented by ordinary graphs \\cite{bai2021hypergraph,feng2019hypergraph,yadati2018hypergcn}. \n% For example, consider a government investigating pandemic infection through physical contact. Many graph based methods model the contact network with an ordinary graph, where each node represents an individual, and each edge between two individuals stands for the physical contact between them. \n% Nevertheless, such relational information can often go beyond pairwise and exist in a high-order structure, e.g., individuals often participate in different social gathering events. \n% %For example, consider a company investigating employees with social connections among them, many graph based machine learning methods model the social connections with an ordinary graph, where each node represents an employee, and each edge between two employees stands for the social connection between them.\n% %Nevertheless,  social connections can often go beyond pairwise and exist in a high-order structure, e.g., employees often participate in different groups such as project teams or certain departments. \n% These high-order relations can be naturally represented with \\textit{hyperedges}, which can connect arbitrary number of nodes, while ordinary edges can only connect exactly two nodes. A \\textit{hypergraph} is a generalization of a graph which contains hyperedges. Hypergraphs are ubiquitous in real world, such as mass gathering in pandemics, co-authors of publications in research communities, and project groups in companies.\n\n% Prediction on hypergraph -> causal\nWhile many studies have been devoted to utilizing such a generalized hypergraph structure to facilitate machine learning tasks \\cite{bai2021hypergraph,feng2019hypergraph,yadati2018hypergcn,zhang2019hyper}, the majority were still executed at the statistical correlation level, e.g., \n{ predicting the COVID-19 infection risk on each individual (node) by capturing the \\textit{correlations} between one's demographic information (node features), in-person group gathering history (hypergraph structure) and the infection outcomes (node labels).}\n%predicting a user' product purchase interests by capturing the correlations between the user's profile (node features), group chat memberships (hypergraph structure), and historical purchase behavior (node labels).\n%node classification by capturing the correlations between node features, hypergraph structure, and node labels.\nA critical limitation here is the lack of \\textit{causality}, which is particularly important for understanding the impact of a policy intervention { (e.g., wearing face covering) on an outcome of interest (e.g., COVID-19 infection).} %(e.g., advertisement placement) or a product feature change on web applications. \n{ For individuals connected as in Fig.~\\ref{fig:hypergraph}, one may ask ``how would each individual's face covering practice (treatment) \\textit{causally} influence their infection risk (outcome)?''} \n%For individuals connected as in Fig.~\\ref{fig:hypergraph}, a marketer may ask, for example, how would showing an advertisement to each individual (treatment) \\textit{causally} influence their conversion outcome (e.g., product purchase)?\nSuch a causal inference task requires constructing the counterfactual state of the same individual by holding all other possible factors constant except the treatment variable of interest. This is a particularly hard problem on hypergraph data, since the outcome of each individual is not only affected by their own confounding factors \n(e.g., one's health conditions and vaccine status) \nbut also interfered by other individuals on the hypergraph\n{ (e.g., face covering practice of other individuals who may physically contact the target individual through a gathering event).}\n%(e.g., ad placements on other individuals who may pass the promotion information through group chats to the target individual). \n\nIn this paper, we focus on learning causal effects on hypergraphs. We are specifically interested in estimating the individual treatment effect (ITE) under hypergraph interference from observational data. Our study is motivated by the following gaps:  \n(i) \\textit{Empirical constraints of randomized experiments.} One of the most reliable approaches for treatment effect estimation is randomized controlled trials (RCTs). Nevertheless, running RCTs is often expensive, impractical, even unethical \\cite{goldstein2018ethical}, and they are especially difficult on graphs due to the dependencies among connected nodes \\cite{ugander2013graph}.\n%These empirical constraints hence motivate us to study causality from observational data.\n(ii)  \\textit{High-order interference on hypergraphs.} Our work focuses on the problem of ITE estimation, which aims to estimate the causal effect of a certain treatment (e.g., face covering practice) on an outcome (e.g., COVID-19 infection) for each individual. The classic ITE estimation is based on the Stable Unit Treatment Value (SUTVA) assumption \\cite{fisher1936design,splawa1990application} that there is no interference \\cite{tchetgen2012causal,hudgens2008toward} (i.e., spillover effect) among instances {(also referred to as \\textit{units} in causal inference literature)}. That means the outcomes for any instance are not influenced by the treatment assignment of other instances. This assumption can be impractical in the real-world, thus resulting in flawed causal effect estimations, especially on graphs  where the interference among instances are ubiquitous \\cite{ahluwalia2001moderating,yilmaz2002geographic,zeng2019exploration}. There have been many efforts addressing this problem \\cite{aronow2017estimating,basse2018analyzing,imai2020causal,kohavi2013online,tchetgen2012causal,ugander2013graph,yuan2021causal,ma2021causal}, but most assume the interference only exists in a pairwise way on ordinary graphs (as shown in Fig.~\\ref{fig:ordinary-graph}). This pairwise interference notion is insufficient to characterize the high-order interference that exists on hypergraphs. \n%As the example shown in Fig.~\\ref{fig:interference}, in a social gathering event, an individual's ($u_1$) infection risk can be affected by the \\textit{first-order} interference from other individuals ($u_2\\rightarrow u_1$ and $u_3\\rightarrow u_1$) as well as the \\textit{high-order} interference from the interactions among other individuals (the interaction between $u_2$ and $u_3$ may also act on their possibilities of extracting and spreading the virus; consequently, $u_1$'s infection risk can be affected by this second-order interaction effect, i.e., $u_2\\times u_3 \\rightarrow u_1$). \nAs shown in Fig.~\\ref{fig:high-order-interference}, within a {gathering event} (hyperedge) between $u_1$, $u_2$ and $u_3$, an individual's ($u_1$) {infection outcome} can be affected by the \\textit{first-order} interference from other individuals ($u_2\\rightarrow u_1$ and $u_3\\rightarrow u_1$) as well as the \\textit{high-order} interference from the interactions among other individuals (the interaction between $u_2$ and $u_3$ may also act on influencing the exposure of the virus to $u_1$; consequently, $u_1$'s {infection risk} can be affected by this second-order interaction effect, i.e., $u_2\\times u_3 \\rightarrow u_1$). \nNotice that the number of such high-order interference items grows combinatorially as the size of a hyperedge increases, leading to a significant information gap between the original hypergraph and the projected pairwise ordinary graph (which accounts for the first-order interference only). This demands techniques capable of modeling high-order interference, but to the best of our knowledge, very little work has been done in this area.\n    \n%\\end{itemize}\n% Due to the ever-increasing availability of hypergraph data in various domains, a family of recent studies \\cite{bai2021hypergraph,feng2019hypergraph,yadati2018hypergcn,zhang2019hyper} have explored to effectively utilize hypergraph structure to facilitate machine learning tasks. However, most of these tasks are in a statistical level, e.g., node classification by capturing the correlations between node features, hypergraph structure and node labels. But these studies lack any perspective of \\textit{causality}. \n%For example, a company may investigate how much each employee's social media usage would causally influence (rather than be correlated with) their collaboration frequency. \n%For example, a government may investigate how much each individual's travel history would causally influence (rather than be correlated with) their infection risk. \n%To tackle such tasks, causal inference approaches should be used to determine causality. Individual treatment effect (ITE) estimation is one of the most important task in causal inference, which aims to estimate the causal effect of a certain treatment (e.g., travel history) on an outcome (e.g., infection risk) for each individual/instance. In this paper, we focus on ITE estimation on hypergraphs. The gold standard of treatment effect estimation is randomized controlled trials (RCTs). However, conducting RCTs is often impractical, expensive or even unethical \\cite{goldstein2018ethical} in real world, and it is especially difficult in graph data due to the complicated interactions among connected nodes  \\cite{ugander2013graph}. Considering this, many works (including this work) conduct ITE estimation directly from observational data.\n\n% interference (vs simple graph)\n\n\n% ITE estimation and interference\n%Individual treatment effect (ITE) estimation has recently attracted significant attention in various research communities, including recommender system \\cite{sun2015causal}, pandemics \\cite{chernozhukov2021causal}, and economics \\cite{varian2016causal}. ITE estimation aims to estimate the causal effect of a certain treatment (e.g., work social media usage) on an outcome (e.g., number of work collaborations) for each individual/instance (e.g., a user). \n\n% Most of the existing works of ITE estimation \\cite{CFR,CEVAE,netDeconf} are based on the Stable Unit Treatment Value (SUTVA) assumption \\cite{fisher1936design,splawa1990application} that there is no interference (i.e., spillover effect) among instances, i.e., the potential outcomes for any instance are not influenced by the treatment assignment of other instances. \n% However, the interference among instances are ubiquitous in real world, especially in graph data \\cite{ahluwalia2001moderating,yilmaz2002geographic,zeng2019exploration}. \n% % For example, in a social network, the post of an employee may influence their colleagues' collaboration frequencies.\n% For example, in a contact network, the travel history of an individual may influence the infection risk of others who contacted with them. \n% Ignoring the interference may result in biased causal effect estimation results.\n% % Existing works on pairwise/networks\n% There have been many efforts \\cite{aronow2017estimating,basse2018analyzing,imai2020causal,kohavi2013online,tchetgen2012causal,ugander2013graph,yuan2021causal,ma2021causal} to handle the existence of interference. However, most of them \\cite{rakesh2018linked,ma2021causal} assume that interference only exists in a pairwise way in ordinary graphs, as shown in Fig.~\\ref{fig:interference} (a). \n% %Traditional methods would model the relations among individuals as pairs of individuals \\cite{rakesh2018linked} or edges between a pair of nodes (individuals) in an ordinary graph \\cite{ma2021causal}. %In the previous example, when estimating the causal effect of social media usage on the number of collaborations for each user with social connections among them, many methods model the social network as a simple graph, where each node represents a user, and an edge between two users exists when there is social connection between them. These methods consider the spillover effect as the casual influence from an individuals’ treatment to the outcomes of its direct neighbors in the network. \n% Nevertheless, in myriads of real-world scenarios, interference could also go beyond pairwise. %For example, % in social media, employees often participate in different groups (e.g., a project team or a certain department).\n% %As the example shown in Fig.~\\ref{fig:interference}, in different project groups of employees, the post of an employee on social media may elicit aggregated effects (e.g., group discussion) inside the group, and the aggregated effects can causally affect the collaboration frequencies of other employees inside the group. \n% As the example shown in Fig.~\\ref{fig:interference}, in a social gathering event, the behavior of an individual may elicit complicated effects (e.g., decrease people's vigilance) inside the whole event, and in consequence, causally affect the infection risk of other individuals participated in the event. \n% Such interference can be referred as high-order interference. \n% High-order interference may exist in many real-world scenarios, such as pandemic viral infections through mass gathering. \n% To the best of our knowledge, however, little work has been done in this area.\n\n% Challenges: confounders, how to capture high-order interference\n%Considering the aforementioned scenarios, it would be more reasonable to model the spillover effect here as the global casual influence from an individual’s treatment to the outcomes of all the other individuals who are in a same group. In the previous example, each group can be naturally modeled as a hypergraph containing each participated user as a node. In this work, we address the problem of individual treatment effect estimation based on the existence of high-order interference in a given hypergraph. \n\n% In this work, we address the problem of ITE estimation under high-order interference in hypergraphs. This task suffers from several fundamental challenges: 1) there may exist confounders --- confounders are variables which causally affect both the treatment and outcome. Without effective ways to control for the confounders, the causal effect estimation results would be biased; 2) the interference in high-order interactions is much more complicated than pairwise interactions, it is challenging to utilize the high-order relational knowledge to model the high-order interference.\n\nIn this paper, we propose a novel framework--- \\textbf{C}ausal \\textbf{I}nference under \\textbf{S}pillover Effects in \\textbf{Hyper}graphs (\\textit{\\mymodel})---to model high-order interference. At a high-level, \n% Our framework\n% To address the above challenges, we propose a novel framework --- \\textit{\\mymodel} to conduct causal effect estimation under spillover effect in hypergraph. Generally, \nthis framework controls for the confounders and models high-order interference based on representation learning, then estimates the outcomes based on the learned representations. More specifically: (i) \\emph{Controlling for Confounders.} Our framework is based on the widely accepted unconfoundedness assumption \\cite{rubin1980randomization}, i.e., the confounders are contained in the observed features. With this assumption, we leverage representation learning techniques to capture and control for confounders from the features of each individual. Note as shown in previous works \\cite{CFR}, the discrepancy between confounder distributions in the treatment group \n%(the group with individuals which are treated) \nand the control group \n%(the group with individuals which are controlled, i.e., not treated) \ncan lead to biases in causal effect estimations. Therefore, we also propose to use a representation balancing technique to mitigate the discrepancy between these two distributions.\n(ii) \\emph{Modeling High-order Interference.} Modeling high-order relationships can be challenging due to the complexity of enumerating multi-way interactions among nodes within each hyperedge. Historically, one may need to simplify the original hypergraph and approximate it through a series of projected ordinary graphs \\cite{yoon2020much}. This obstacle is fortunately unblocked by the recent advances of hypergraph neural networks \\cite{bai2021hypergraph,yadati2018hypergcn}. We extend this line of techniques to model interference by learning interference representations for each node. To learn the interference representations, the learned confounder representations and the treatment assignment are propagated via hypergraph convolution and attention operations. \n(iii) \\emph{Outcome Prediction.} Based on the learned representations of confounders and interference, we predict the potential outcomes corresponding to different treatment assignments for each individual.\n% More specifically, this framework is based on the unconfoundedness assumption \\cite{rubin1980randomization}, i.e., the confounders are contained in the observed features. To control for the confounders, \\mymodel~ learns the representation from the features of each individual. To model the high-order interference among different individuals, we first model the interactions among individuals with a hypergraph. In this hypergraph, each relation among individuals is modeled as a hyperedge, which can encode more high-order relational information to reflect the interaction among individuals than pairwise edges in ordinary graphs. We then model the high-order interference by utilizing the relational information in the hypergraph. Specifically, we use an interference modelling component in the framework, and encode the learned confouner representations, treatment assignment, and hypergraph structure into a latent representation space to model the high-order interference. Based on the above learned representations of confounders and interference, we predict the potential outcomes corresponding to different treatment assignment for each individual.  The ITE estimation is conducted for each individual based on the potential outcome predictions. Besides, previous works \\cite{CFR} have shown that the discrepancy between distributions of confounders in the treatment group (the group with individuals which are treated) and the control group (the group with individuals which are controlled, i.e., not treated) can lead to biases in causal effect estimation, therefore, we adopt a representation balancing technique to minimize the discrepancy between these two distributions and thus mitigate the estimation biases. \nOverall, the main contributions of this work can be summarized as follows:\n\n\\begin{itemize}\n    \\item %\\textbf{Problem formulation.} \n    We formalize the problem of ITE estimation under high-order interference on hypergraphs. To the best of our knowledge, it is the first work for this problem.%to formally model high-order interference on observational data.\n    \\item %\\textbf{Framework design.} \n    We propose a novel framework \\mymodel~ for the studied problem. \\mymodel~ models confounders and high-order interference via representation learning and hypergraph neural networks.\n    %the problem of ITE esitimation under high-order interference in hypergraph. This framework captures the confounders and model the interference through representation learning. \n    \\item %\\textbf{Experimental evaluation.} \n    We validate the effectiveness of the proposed framework through extensive experiments and provide in-depth analysis on how it acts on different nodes and hyperedges.\n    %We conduct extensive experiments to evaluate the proposed framework. The experimental results show that \\mymodel~ outperforms the state-of-the-art baselines under different scenarios of high-order interference.\n\\end{itemize}\n\n\n\n%in the social gathering events under COVID-19 epidemic, consider the vaccination of each individual as the treatment. The outcome here can be regarded as if those health individuals will be infected after such events.\n\n\n% example: go beyond pairwise\n\n% unconvincing to model as a simple graph\n%Traditional approach tends to model a physical contact network between the participants, and consider the spillover effect as the casual influence from an individuals’ treatment to the outcomes of its direct neighbors in the network. Nevertheless, in such social gathering event, if one or some participants carry COVID-19 virus, it would be unconvincing to model the pairwise physical contact as the potential way of virus spreading for at least two reasons. First, in such social events, people tend to gather together in groups rather than in a pairwise manner; second, even if people join pairwise gathering, virus can spread in different ways, covering a wider area and even affecting the health of all participants. \n% so we model as a hypergraph\n%Considering both of the mentioned perspectives, it would be more reasonable to model the spillover effect here as the global casual influence from an individual’s treatment to the outcomes of all the other individuals who are in a same gathering event. At the same time, the confounder representations of each individual also bear similar global influence from different gathering events. Such gathering events between participants can be naturally modeled as a hypergraph. In such a hypergraph, nodes still represent individuals, while hyperedges can be regarded as different gathering events, where different participants could be involved. \n% corresponding approach\n\n\n%\\noindent{\\textbf{Challenges.}} 1) Confounder learning from in high-order relational knowledge; 2) Interference in high-order interaction is more complicated than pairwise interaction; %3) How to model the \n\n% \\section{Problem Definition}\n% We provide the formal problem definition in this section.\n% \\begin{definition}\n% Suppose a set of individuals $\\mathcal{V}=\\{v_i\\}_{i=1}^n$ are connected via hyperedges $\\mathcal{E}=\\{\\mathbf{e}_k\\}_{k=1}^m$, together these form a \\textbf{hypergraph} $\\mathcal{H}=\\{\\mathcal{V},\\mathcal{E}\\}$ with $n$ nodes and $m$ hyperedges, where each hyperedge can associate any number of nodes. \n% \\end{definition}\n% The observational data is denoted as $\\{\\mathbf{X}, \\mathcal{H}, \\mathbf{T}, \\mathbf{Y}\\}$. Let $\\mathbf{X}$ be the instance attributes (features), such that $\\mathbf{X} =\\{\\mathbf{x}_i\\}_{i=1}^n$, where $\\mathbf{x}_i $ represents the features of the $i$-th instance (e.g., features of each employee), $n$ denotes the number of instances. $\\mathbf{T}=\\{t_i\\}_{i=1}^n$ denotes the set of treatment assignment for all the nodes, here $t_i\\in\\{0,1\\}, \\forall i\\in[n]$. $\\mathbf{Y}=\\{y_i\\}_{i=1}^n$ denotes the set of observed outcomes of all the nodes. \n% %We have the hypergraph $\\mathcal{H}=\\{\\mathcal{V},\\mathcal{E}\\}$, where $\\mathcal{V}$ is the set of nodes, and each node here corresponds to an instance, $|\\mathcal{V}|=n$.  $\\mathcal{E}=\\{e_i| i=1,...,m\\}$ is the set of all hyperedges. Each $e_i$ is a hyperedge that connects multiple nodes, and $m$ is the total number of hyperedges. \n% % We denote the \\textit{potential outcome} of instance $i$ under treatment $t_i$ and exposure variable $\\mathbf{s}_i$ by $y_i(t_i, \\mathbf{s}_i)$. \n% We denote the \\textit{potential outcomes} (i.e., the outcomes which would be realized under a certain treatment) \\cite{rubin1980randomization} of instance $i$ under treatment $t_i=1$ and $t_i=0$  by $y_i^1$ and $y_i^0$ in this setting, respectively. \n% Notice that for each node $i$, only one of the two potential outcomes (the one corresponds to the treatment assignment $t_i$) can be observed in the observational data. \n% Specifically, $y_i^{t_i}=\\Phi_Y(t_i, \\mathbf{x}_i, \\mathbf{T}_{-i}, \\mathbf{X}_{-i}, \\mathbf{H})$, where the subscript ${-i}$ denotes all of other nodes except $i$. $\\mathbf{H}\\in\\mathbb{R}^{n\\times m}$ is an incidence matrix which describes the hypergraph structure of $\\mathcal{H}$. $\\mathbf{H}_{i,e}=1$ if node $i$ is in hyperedge $e$, otherwise $\\mathbf{H}_{i,e}=0$. \n% $\\Phi_Y(\\cdot)$ is an unknown transformation which maps each node's treatment assignment, features and the other nodes' information on the hypergraph to the ground-truth potential outcomes of node $i$.  \n% Although $\\Phi_Y(\\cdot)$ here takes all the nodes on the hypergraph as input, the outcome of each node $i$ is usually be influenced only by its neighboring nodes (nodes which share same hypergraphs with $i$). \n% For example, in Fig.~\\ref{fig:interference}(b), the neighboring nodes of node $u_1$ include $\\{u_2,u_3\\}$ and $\\{u_4,u_5\\}$.\n% %Here $\\mathbf{s}_i$ represents a summary (e.g., mean) of neighboring treatments $\\{t_j| j \\in \\mathcal{N}_e, \\forall~e \\in \\mathcal{E}_i\\}$), \n% In this paper, $\\mathcal{E}_i$ denotes the set of hyperedges which contains node $i$, and $\\mathcal{N}_e$ denotes the nodes in hypergraph $e$. \n% %For example, in Fig.~\\ref{fig:interference}(b), the exposure variable for the node $u_1$ summarizes the treatments of two sets of neighboring nodes $\\{u_2,u_3\\}$ and $\\{u_4,u_5\\}$. \n\n% Given above preliminaries, %additional observed features of each individual $\\mathbf{X}=\\{\\mathbf{x}_i\\}$, \n% we are ready to provide the formal definition of the individual treatment effect.\n\n% \\begin{definition}\n% For each node $i$, the \\textbf{individual treatment effect} (ITE) %under exposure $\\mathbf{s}_i$ \n% is defined by the difference between potential outcomes corresponding to  $t_i=1$ and $t_i=0$:\n% \\begin{equation}\n%     \\tau_i=y_i^1-y_i^0 = \\mathbb{E}[\\Phi_Y(1,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}) - \\Phi_Y(0,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}].\n%     \\label{eq:ite}\n% \\end{equation}\n% % \\begin{equation}\n% %     \\tau_i=\\tau(\\mathbf{x}_i, \\mathbf{s}_i) = \\mathbb{E}[y_i(t_i=1, \\mathbf{s}_i) - y_i(t_i=0, \\mathbf{s}_i)| \\mathbf{x}_i].\n% %     \\label{eq:ite}\n% % \\end{equation}\n% \\end{definition}\n\n% Meanwhile, we introduce the notion of spillover effect to assess the interference on hypergraphs.\n% \\begin{definition}\n% The \\textbf{spillover effect} of node $i$ under its treatment  $t_i$ and other nodes' treatment assignment $\\mathbf{T}_{-i}$ is defined as:\n% % \\begin{equation}\n% %     \\delta(\\mathbf{x}_i, t_i, \\mathbf{s}_i=\\mathbf{s}) = \\mathbb{E}[y_i(t_i, \\mathbf{s}_i=\\mathbf{s}) - y_i(t_i, \\mathbf{s}_i=0)| \\mathbf{x}_i].\n% %     \\label{eq:spillover}\n% % \\end{equation}\n% \\begin{equation}\n%     \\delta_i = \\mathbb{E}[\\Phi_Y(t_i,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}) - \\Phi_Y(t_i,\\mathbf{x}_i,\\mathbf{0},\\mathbf{X}_{-i},\\mathbf{H})].\n%     \\label{eq:spillover}\n% \\end{equation}\n% \\end{definition}\n\n% In this paper, given the observed data $\\{\\mathbf{X}, \\mathcal{H}, \\mathbf{T}, \\mathbf{Y}\\}$, we aim to estimate the ITE defined in Eq.~\\ref{eq:ite} for each node in $\\mathcal{H}$ with the existence of high-order interference defined in Eq.~\\ref{eq:spillover}.\n\\vspace{-2mm}\n"
            },
            "section 2": {
                "name": "Problem Definition and Analysis",
                "content": "\nWe provide the formal problem definition and a brief theoretical analysis of our studied problem in this section. A notation table is provided in Appendix A.\n\\begin{definition}\nSuppose a set of individuals $\\mathcal{V}=\\{v_i\\}_{i=1}^n$ are connected via hyperedges $\\mathcal{E}=\\{\\mathbf{e}_k\\}_{k=1}^m$, together these form a \\textbf{hypergraph} $\\mathcal{H}=\\{\\mathcal{V},\\mathcal{E}\\}$ with $n$ nodes and $m$ hyperedges, where each hyperedge can connect an arbitrary number of nodes. \n\\end{definition}\nThe observational data on this hypergraph can be denoted as $\\{\\mathbf{X}, \\mathcal{H}, \\mathbf{T}, \\mathbf{Y}\\}$, where $\\mathbf{X} =\\{\\mathbf{x}_i\\}_{i=1}^n$, $\\mathbf{T}=\\{t_i\\}_{i=1}^n$ and $\\mathbf{Y}=\\{y_i\\}_{i=1}^n$ represent node features, treatment assignments, and observed outcomes, respectively. $\\mathbf{H}=\\{h_{i,e}\\}\\in\\mathbb{R}^{n\\times m}$ is an incidence matrix which describes the hypergraph structure of $\\mathcal{H}$. $h_{i,e}=1$ if node $i$ is in hyperedge $e$, otherwise $h_{i,e}=0$. For ease of discussion, we consider the treatment assignment for each node as a binary variable in this study (i.e., $t_i\\in\\{0,1\\}$), but our work can be extended to non-binary categorical variables and continuous variables.\n\n{\n%\\vspace{-1mm}\n\\begin{definition}\nThe \\textbf{potential outcome} \\cite{rubin1980randomization} of the instance $i$ (denoted by $y_i^1$ or $y_i^0$) is defined as the realized value of outcome for instance $i$ under the treatment value $t_i=1$ or $t_i=0$. These potential outcomes can be instantiated via a transformation $Y^{T_i}_i=\\Phi_Y(T_i, X_i, T_{-i}, X_{-i}, H)$.  $\\Phi_Y$ can be regarded as a (non-deterministic) function to output potential outcomes, which takes each node's treatment assignment, node features, the information (treatment assignments and node features) of other nodes on the hypergraph, and the hypergraph structure as input\\footnote{In this paper, we use non-bold, italicized, and capitalized letters (e.g., $X_i$) to denote random variables; non-bold lowercase letters (e.g., $t_i$) to denote observed values of a scalar; bold lowercase letters (e.g., $\\mathbf{x}_i$) to denote observed values of a vector; bold capitalized letters (e.g., $\\mathbf{X}$) to denote observed values of a matrix or a set.}, i.e., $y_i^{t_i}=\\Phi_Y(t_i, \\mathbf{x}_i, \\mathbf{T}_{-i}, \\mathbf{X}_{-i}, \\mathbf{H})$, where the subscript ${-i}$ denotes all other nodes on $\\mathcal{H}$ except $i$.\n\\end{definition}\n}\n%\\vspace{-1mm}\n%Let $\\mathbf{X}$ be the instance attributes (features), such that $\\mathbf{X} =\\{\\mathbf{x}_i\\}_{i=1}^n$, where $\\mathbf{x}_i $ represents the features of the $i$-th instance, $n$ denotes the number of instances. $\\mathbf{T}=\\{t_i\\}_{i=1}^n$ denotes the set of treatment assignment for all the nodes, here $t_i\\in\\{0,1\\}, \\forall i\\in[n]$. $\\mathbf{Y}=\\{y_i\\}_{i=1}^n$ denotes the set of observed outcomes of all the nodes. \n%We have the hypergraph $\\mathcal{H}=\\{\\mathcal{V},\\mathcal{E}\\}$, where $\\mathcal{V}$ is the set of nodes, and each node here corresponds to an instance, $|\\mathcal{V}|=n$.  $\\mathcal{E}=\\{e_i| i=1,...,m\\}$ is the set of all hyperedges. Each $e_i$ is a hyperedge that connects multiple nodes, and $m$ is the total number of hyperedges. \n% We denote the \\textit{potential outcome} of instance $i$ under treatment $t_i$ and exposure variable $\\mathbf{s}_i$ by $y_i(t_i, \\mathbf{s}_i)$. \n%We denote the \\textit{potential outcomes} (i.e., the outcomes which would be realized under a certain treatment) \\cite{rubin1980randomization} of instance $i$ under treatment $t_i=1$ and $t_i=0$  by $y_i^1$ and $y_i^0$ in this setting, respectively. \n%Notice that for each node $i$, only one of the two potential outcomes (the one corresponds to the treatment assignment $t_i$) can be observed in the observational data. \n%Specifically, $y_i^{t_i}=\\Phi_Y(t_i, \\mathbf{x}_i, \\mathbf{T}_{-i}, \\mathbf{X}_{-i}, \\mathbf{H})$, where the subscript ${-i}$ denotes all of other nodes except $i$. \n%$\\mathbf{H}\\in\\mathbb{R}^{n\\times m}$ is an incidence matrix which describes the hypergraph structure of $\\mathcal{H}$. $\\mathbf{H}_{i,e}=1$ if node $i$ is in hyperedge $e$, otherwise $\\mathbf{H}_{i,e}=0$. \n%$\\Phi_Y(\\cdot)$ is an unknown transformation which maps each node's treatment assignment, features and the other nodes' information on the hypergraph to the ground-truth potential outcomes of node $i$.  \n%Although $\\Phi_Y(\\cdot)$ here takes all the nodes on the hypergraph as input, the outcome of each node $i$ is usually be influenced only by its neighboring nodes (nodes which share same hypergraphs with $i$). \n%For example, in Fig.~\\ref{fig:interference}(b), the neighboring nodes of node $u_1$ include $\\{u_2,u_3\\}$ and $\\{u_4,u_5\\}$.\n%Here $\\mathbf{s}_i$ represents a summary (e.g., mean) of neighboring treatments $\\{t_j| j \\in \\mathcal{N}_e, \\forall~e \\in \\mathcal{E}_i\\}$), \n%In this paper, $\\mathcal{E}_i$ denotes the set of hyperedges which contains node $i$, and $\\mathcal{N}_e$ denotes the nodes in hypergraph $e$. \n%For example, in Fig.~\\ref{fig:interference}(b), the exposure variable for the node $u_1$ summarizes the treatments of two sets of neighboring nodes $\\{u_2,u_3\\}$ and $\\{u_4,u_5\\}$. \n\nGiven the above preliminaries, %additional observed features of each individual $\\mathbf{X}=\\{\\mathbf{x}_i\\}$, \nwe are ready to provide the formal definition of individual treatment effect on hypergraphs.\n\n{\n\\begin{definition}\nFor each node $i$ on the hypergraph $\\mathcal{H}$, the \\textbf{individual treatment effect} (ITE) %under exposure $\\mathbf{s}_i$ \nis defined by the difference between potential outcomes corresponding to  $t_i=1$ and $t_i=0$:\n\\begin{equation}\n\\begin{split}\n    \\tau(\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}) &=\\mathbb{E}[Y_i^1-Y_i^0|X_i=\\mathbf{x}_i,T_{-i}=\\mathbf{T}_{-i},X_{-i}=\\mathbf{X}_{-i},H=\\mathbf{H}] \\\\\n    &= \\mathbb{E}[\\Phi_Y(1,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}) - \\Phi_Y(0,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H})].\n    \\label{eq:ite}\n\\end{split}\n\\end{equation}\n% \\begin{equation}\n%     \\tau_i=y_i^1-y_i^0 = \\mathbb{E}[\\Phi_Y(1,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}) - \\Phi_Y(0,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H})].\n%     \\label{eq:ite}\n% \\end{equation}\n% \\begin{equation}\n%     \\tau_i=\\tau(\\mathbf{x}_i, \\mathbf{s}_i) = \\mathbb{E}[y_i(t_i=1, \\mathbf{s}_i) - y_i(t_i=0, \\mathbf{s}_i)| \\mathbf{x}_i].\n%     \\label{eq:ite}\n% \\end{equation}\n\\end{definition}\n}\n{We clarify that the ITE in this paper is actually defined in the form of conditional average treatment effect (CATE), similar as \\cite{ma2021causal,guo2020learning}. The expectation is taken over the potential outcome (output of $\\Phi_Y$) of the instances with same node features $\\mathbf{x}_{i}$ and “environmental information” (hypergraph structure $\\mathbf{H}$, other nodes’ features $\\mathbf{X}_{-i}$ and treatments $\\mathbf{T}_{-i}$). \nThe distribution of the output of $\\Phi_Y$ is equivalent to the conditional distribution of the potential outcome conditioned on the parameters in $\\Phi_Y$ with fixed values. For notation simplicity, we also denote $\\tau_i=\\tau(\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H})$ in this paper.\n}\nMeanwhile, we introduce the notion of spillover effect in this work to assess the level of interference on hypergraphs.\n\\begin{definition}\nThe \\textbf{spillover effect} of node $i$ under its treatment  $t_i$ and other nodes' treatment assignment $\\mathbf{T}_{-i}$ on the hypergraph $\\mathcal{H}$ is defined as:\n% \\begin{equation}\n%     \\delta(\\mathbf{x}_i, t_i, \\mathbf{s}_i=\\mathbf{s}) = \\mathbb{E}[y_i(t_i, \\mathbf{s}_i=\\mathbf{s}) - y_i(t_i, \\mathbf{s}_i=0)| \\mathbf{x}_i].\n%     \\label{eq:spillover}\n% \\end{equation}\n\\begin{equation}\n    \\delta_i = \\mathbb{E}[\\Phi_Y(t_i,\\mathbf{x}_i,\\mathbf{T}_{-i},\\mathbf{X}_{-i},\\mathbf{H}) - \\Phi_Y(t_i,\\mathbf{x}_i,\\mathbf{0},\\mathbf{X}_{-i},\\mathbf{H})].\n    \\label{eq:spillover}\n\\end{equation}\n\\end{definition}\n\nIn this paper, given the observed data $\\{\\mathbf{X}, \\mathcal{H}, \\mathbf{T}, \\mathbf{Y}\\}$, we aim to estimate the ITE defined in Eq.~\\ref{eq:ite} for each node in $\\mathcal{H}$ with the existence of high-order interference defined in Eq.~\\ref{eq:spillover}.\n\n",
                "subsection 2.1": {
                    "name": "Theoretical Analysis",
                    "content": "\nWith the above definitions, we show that the ITE can be identifiable from the observational data under the following two assumptions.\n\n% the list may be repeated\nSimilar as the assumptions in other works of causal inference under network interference \\cite{ma2021causal}, for each individual, we assume there exists a summary function capable of characterizing all the ``environmental'' information related to this node on the hypergraph.\n%Formally, we denote the set of all the hyperedges which have interference to node $i$ as $\\tilde{\\mathcal{E}}_i$ and all nodes in these hyperedges as $\\tilde{\\mathcal{V}}_i$. This provides a more generic neighborhood definition for interference but in practice one can restrict the assumption to the first-order neighborhood on the hypergraph (i.e., $\\tilde{\\mathcal{E}}_i=\\mathcal{E}_i$ and $\\tilde{\\mathcal{V}}_i=\\mathcal{V}_i$).\n%(e.g., the hyperedges which contains node $i$, i.e., $\\mathcal{E}_i$) as $\\mathcal{E}^s_i$, and denote the set of nodes contained in at least one of the hyperedges in $\\mathcal{E}^s_i$ as $\\mathcal{V}^s_i$ (e.g., the set of node $i$'s neighboring nodes). \n%We use  $\\tilde{\\mathcal{V}}_i-\\{i\\}$ to denote the set of nodes in $\\tilde{\\mathcal{V}}_i$ except $i$ itself. \n%Suppose there is a summary function $\\mathtt{SMR}(\\cdot)$ which takes $\\tilde{\\mathcal{E}}_i$, the treatment assignment of nodes in $\\tilde{\\mathcal{V}}_i-\\{i\\}$ and the features of these nodes as input, then maps them into a vector $\\mathbf{o}_i$: \nSuppose there is a summary function $\\mathtt{SMR}(\\cdot)$: for each node $i$, $\\mathtt{SMR}(\\cdot)$ takes the hypergraph structure $\\mathbf{H}$, the treatment assignment of other nodes $\\mathbf{T}_{-i}$ and the features of these nodes $\\mathbf{X}_{-i}$ as input, then maps them into a vector $\\mathbf{o}_i$: \n%For simplicity, we denote the list of nodes which influence node $i$ as $\\mathcal{N}_i$ (not including node $i$ itself), and \n%We denote the summary function $\\mathtt{SMR}(\\cdot): \\mathcal{X}^{|\\mathcal{N}_i| \\times d} \\times \\{0,1\\}^{|\\mathcal{N}_i|} \\rightarrow \\mathcal{S}$ over their covariates and treatment assignments is denoted by:\n% \\begin{equation}\n%     \\mathbf{s}_i = \\mathtt{SMR}(\\mathbf{X}_{\\mathcal{N}_i}, \\mathbf{T}_{\\mathcal{N}_i})\n% \\end{equation}\n% \\begin{equation}\n%      \\mathbf{o}_i = \\mathtt{SMR}(\\tilde{\\mathcal{E}}_i, \\mathbf{X}_{\\tilde{\\mathcal{V}}_i-\\{i\\}}, \\mathbf{T}_{\\tilde{\\mathcal{V}}_i-\\{i\\}}).\n%  \\end{equation}\n\\begin{equation}\n      \\mathbf{o}_i = \\mathtt{SMR}(\\mathbf{H}, \\mathbf{T}_{-i}, \\mathbf{X}_{-i}).\n  \\end{equation}\n%Then the potential outcome under treatment $t_i$, $\\mathbf{x}_i$ and summary $\\mathbf{s}_i$ is denoted by $Y_i(t_i,\\mathbf{x}_i,\\mathbf{s}_i)$. \nWe use $H,X,T$ to denote the random variables for the hypergraph structure, features, and treatment assignment for any node. \nThen our first assumption can be formalized as below.\n\\begin{assumption}\n(Expressiveness of summary function) \nFor any node $i$, any values of $H, {X}_{-i}$, and ${T}_{-i}$, if the output of summary function $\\mathbf{o}_i$ is determined, then the value of the potential outcomes $y_i^{1}$ and $y_i^{0}$ with feature $\\mathbf{x}_i$ are also determined. \n%For all the node $\\forall i=1,...,n, \\forall \\mathbf{T}, \\mathbf{T}' \\in \\{0,1\\}^{n}, \\forall \\mathbf{X}, \\mathbf{X}' \\in \\mathcal{X}^{n}$, if the summary function  $\\mathtt{SMR}(\\mathbf{X}_{\\mathcal{N}_i},\\mathbf{T}_{\\mathcal{N}_i}, \\mathcal{E}_i)=\\mathtt{SMR}(\\mathbf{X}'_{\\mathcal{N}_i},\\mathbf{T}'_{\\mathcal{N}_i},\\mathcal{E}_i)$, then the potential outcome $Y_i(t_i, \\mathbf{T}, \\mathbf{X})=Y_i(t_i, \\mathbf{T}', \\mathbf{X}')$.\n\\label{assumption:summary}\n\\end{assumption}\n\nOur second assumption extends the unconfoundedness assumption \\cite{rubin1980randomization} to the hypergraph interference setting. That is we assume conditioned on the above summary function, the observed features can capture all possible confounders.\n\\begin{assumption}\n(Unconfoundedness) For any node $i$, given the node features, the potential outcomes are independent with the treatment assignment and summary of neighbors, i.e., $Y_i^1,Y_i^0 \\bigCI T_i, O_i | {X}_i$. \\label{assumption:unconfoundeness}\n\\end{assumption}\n\nBased on the above assumptions, the identification of the expectation of potential outcomes $Y_i^{1}$ and $Y_i^{0}$ can be proved (here we take $Y_i^{1}$ as an example):\n% \\begin{align}\n% \\mathbb{E}[Y_i|T_i = t_i, \\mathbf{S}_i = \\mathbf{s}_i, \\mathbf{X}_i] &\\overset{Asm. 1}{=} \\mathbb{E}[Y_i(t_i,  \\mathbf{s}_i)|T_i = t_i, \\mathbf{S}_i = \\mathbf{s}_i, \\mathbf{X}_i]\\\\\n% &\\overset{Asm. 2}{=} \\mathbb{E}[Y_i(t_i, \\mathbf{s}_i))|\\mathbf{X}_i]\n% \\end{align}\n{ \n\\begin{align}\n&\\mathbb{E}[Y^{1}_i|T_i=1, X_i=\\mathbf{x}_i, T_{-i}=\\mathbf{T}_{-i},{X}_{-i}=\\mathbf{X}_{-i},H=\\mathbf{H}] \\\\\n\\overset{(a)}{\\longeq}&\\mathbb{E}[\\Phi_Y(T_i=1, X_i=\\mathbf{x}_i, T_{-i}=\\mathbf{T}_{-i},{X}_{-i}=\\mathbf{X}_{-i},H=\\mathbf{H})] \\\\\\overset{(b)}{\\longeq}& \\mathbb{E}[\\Phi_Y(T_i=1, X_i=\\mathbf{x}_i, O_i=\\mathbf{o}_i)] \\\\\n\\overset{(c)}{\\longeq} &\\mathbb{E}[\\Phi_Y(T_i=1, X_i=\\mathbf{x}_i, O_i=\\mathbf{o}_i)|X_i=\\mathbf{x}_i] \\\\\n\\overset{(d)}{\\longeq} &\\mathbb{E}[\\Phi_Y(T_i=1, X_i=\\mathbf{x}_i, O_i=\\mathbf{o}_i)|X_i=\\mathbf{x}_i, T_i=1,O_i=\\mathbf{o}_i] \\\\\n\\overset{(e)}{\\longeq} &\\mathbb{E}[Y_i|X_i=\\mathbf{x}_i, T_i=1,O_i=\\mathbf{o}_i].\n\\end{align}\n}\n\nHere, the equation $(a)$ is based on the definition of potential outcome in this setting; $(b)$ is inferred from Assumption (1); $(c)$ is a straightforward derivation; $(d)$ is based on Assumption (2); and $(e)$ is based on the widely used consistency assumption \\cite{rubin1980randomization}. Based on the above proof for the identification of potential outcomes, the identification of ITE can be straightforwardly derived.\n\n%Although the above ITE definition and analysis include all nodes on the hypergraph as input, in practice one can simplify these by assuming the outcome of node $i$ is interfered by its neighboring nodes only (nodes which share same hyperedges with $i$). \n"
                }
            },
            "section 3": {
                "name": "The Proposed Framework",
                "content": "\n\n\n\n\n\nInspired by the previous theoretical analysis, we propose a novel framework \\mymodel~to address the studied problem. This framework contains three components: confounder representation learning, interference modeling, and outcome prediction.\nHolistically, we aim to learn an expressive transformation to summarize high-order interferences (Assumption~\\ref{assumption:summary}), then take the interference representation, the confounder representation as well as the treatment assignment to estimate the expected potential outcome (Assumption~\\ref{assumption:unconfoundeness}).\nThe illustration of \\mymodel~ is shown in Fig.~\\ref{fig:model}.\n\n",
                "subsection 3.1": {
                    "name": "Confounder Representation Learning",
                    "content": "\nWe first encode the node features $\\textbf{x}_i$ into a latent space via a multilayer perceptron (MLP) module, i.e., $\\mathbf{z}_i=\\text{MLP}(\\mathbf{x}_i)$. This results in a set of representations $\\mathbf{Z}=\\{\\mathbf{z}_i\\}_{i=1}^n$, which is expected to capture all potential confounders, so the model can mitigate the confounding biases by controlling for the learned representation $\\mathbf{z}_i$.\n%with a transformation function $g(\\textbf{x})$. We implement the function $g(\\textbf{x})$ with a multilayer perceptron (MLP) module. For each node with input feature $\\textbf{x}_i$, we learn its confounder representation as $\\mathbf{z}_i=g(\\mathbf{x}_i)$. $\\mathbf{z}_i$ is expected to capture all the confounders, so the model can mitigate the confounding biases by controlling for the learned representation $\\mathbf{z}_i$ for any node $i$. We denote $\\mathbf{Z}=\\{\\mathbf{z}_i\\}_{i=1}^n$.  \n\n\\paragraph{Representation Balancing.} Note a discrepancy may exist between the distributions of confounder representation $\\mathbf{Z}$ in the treatment group and the control group, incurring biases in causal effect estimation, as shown in \\cite{CFR,yao2018representation}. To minimize this discrepancy, we leverage the representation balancing technique by adding a discrepancy penalty to the loss function, where this discrepancy penalty can be calculated with any distribution distance metrics. In our implementation, we use the Wasserstein-1 distance \\cite{CFR} between the representation distributions of treatment group and control group. \n%The distributions of learned representations of confounders in treatment group and control group may have discrepancy. Such discrepancy may incur biases to causal effect estimation results. Existing works \\cite{CFR} have theoretically proved that minimizing the distance between the confounder representation distributions of treatment group and control group can help mitigate the biases in causal effect estimation. In this work, we use a representation balancing technique by adding the Wasserstein-1 distance \\cite{CFR} between the representation distributions of treatment group and control group into the loss function. \n\n%we employ representation balancing techniques \\cite{CFR} at each time period by adding a distribution balancing constraint $\\mathscr{L}_b$, which is the Wasserstein-1 distance \\cite{CFR} of the representation distributions between the treatment group and control group. Minimizing the constraint can encourage the representation distributions of these two groups to be closer, and has been proved to benefit the causal effect estimation~\\cite{CFR}.\n\n\\vspace{-2mm}\n"
                },
                "subsection 3.2": {
                    "name": "Interference Modeling",
                    "content": "\nIn this interference modeling component, we take the \n% To model the high-order interference among nodes in the hypergraph and achieve better ITE estimation performance, we develop an interference modeling module.   \n% In the interference modeling module, we take the \nconfounder representation ($\\textbf{Z}$), the treatment assignment ($\\mathbf{T}$), and the relational information on the hypergraph ($\\textbf{H}=\\{h_{i,e}\\}$) as input, to capture the high-order interference for each individual. \nMore specifically, we learn a transformation function $\\Psi(\\cdot)$ through a hypergraph module to generate the interference representations ($\\mathbf{p}_i$) for each node $i$, i.e.,\n$\\mathbf{p}_i=\\Psi(\\mathbf{Z}, \\mathbf{H}, \\mathbf{T}_{-i}, t_i)$. As shown in Fig.~\\ref{fig:model_hyper}, this module is implemented with a hypergraph convolutional network \\cite{yadati2018hypergcn,bai2021hypergraph} and a hypergraph attention mechanism \\cite{bai2021hypergraph,zhang2019hyper,ding2020more}, where the convolutional operator forms the skeleton of interference from hyperedges, and the attention operator enhances this mechanism by allowing flexible node contributions to each hyperedge. %$\\mathbf{p}_i^{1}=\\Psi(\\mathbf{Z}, \\mathcal{H}, \\mathbf{T}_{-i}, T_i=1)_i$ and $\\mathbf{p}_i^{0}=\\Psi(\\mathbf{Z}, \\mathcal{H}, \\mathbf{T}_{-i}, T_i=0)_i$ corresponding to $T_i=1$ and $T_i=0$, respectively. \n%\\footnote{In this paper, we use non-bold, italicized, and capitalized letters to denote random variables.} \n%We implement the function $\\Psi(\\cdot)$ with a hypergraph module, where this module learns  representations for each node. \n\n\\vspace{-2mm}\n% \\subsubsection{Modeling High-order Interference in Hyperedges}\n\\paragraph{Learning interference representations.}\n%\\paragraph{Convolutions across hyperedges.}\n%In the hypergraph module, %we %use the confounder representation $\\mathbf{z}_i$ for each node $i$ with two treatment assignment: $T_i\\mathbf{z}_i$, where $T_i=1$ and $T_i=0$, respectively, \n%take the input features of each node based on the learned confounder representations. \n%We implement this hypergraph module with different hypergraph neural networks, including hypergraph convolutional network \\cite{yadati2018hypergcn,bai2021hypergraph} and hypergraph attention network \\cite{bai2021hypergraph,zhang2019hyper,ding2020more}.\nTo learn the representations which encode the interference in the hypergraph for each node, we propagate the treatment assignment and confounder representations with a hypergraph convoluntional layer. \nWe first introduce a vanilla Laplacian matrix for the hypergraph $\\mathcal{H}$:\n\\begin{equation}\n\\label{eq:laplacian}\n    \\mathbf{L} = \\mathbf{D}^{-1/2}\\mathbf{HB}^{-1}\\mathbf{H}^\\top\\mathbf{D}^{-1/2}.\n\\end{equation}\nHere $\\mathbf{D}\\in \\mathbb{R}^{n\\times n}$ is a diagonal matrix where each element represents the node degree (i.e., $\\sum_{e=1}^m h_{i,e}$). $\\mathbf{B}\\in \\mathbb{R}^{m\\times m}$ is another diagonal matrix, where each element is the size of each hyperedge ($\\sum_{i=1}^n h_{i,e}$).\nThen we can define the hypergraph convolution operator as:\n\\begin{equation}\n\\label{eq: hgcn}\n    \\mathbf{P}^{(l+1)} = \\text{LeakyReLU}\\left(\\mathbf{L}\\mathbf{P}^{(l)}\\mathbf{W}^{(l+1)}\\right),\n\\end{equation}\nwhere $\\mathbf{P}^{(l)}$ represents the representations from the $l$-th layer in the hypergraph module. %\\footnote{Here we leave out the superscripts of treatment assignment for notation simplicity.} \n%We denote the representation in the last layer as $\\mathbf{P}=\\{p_i\\}_{i=1}^n$. %$\\mathbf{P}^1=\\{p_i^1\\}_{i=1}^n$ and $\\mathbf{P}^0=\\{p_i^0\\}_{i=1}^n$ corresponding to $T_i=1$ and $T_i=0$, respectively. \nWe feed the first layer with the previous confounder representation masked by the treatment assignment, i.e., $\\mathbf{p}_i^{(0)}={t_i}*\\mathbf{z}_i$,\n%as node features in this module%corresponding to $T_i=1$ and $T_i=0$, respectively, \nwhere $*$ denotes element-wise multiplication.  \n%$\\sigma(\\cdot)$ is a non-linear activation function, such as LeakyReLU \\cite{maas2013rectifier}. \n$\\mathbf{W}^{(l+1)}\\in \\mathbb{R}^{d^{(l)}\\times d^{(l+1)}}$ is the parameter matrix in the ($l$+1)-th layer, where $d^{(l)}$ and $d^{(l+1)}$ refer to the dimensionality of the interference representations in the $l$-th and ($l$+1)-th layers, respectively.\n\n%\\subsubsection{Modeling Different Significance of  High-order Interference regarding Different Nodes}\n\\paragraph{Modeling interference with different significance.}\n%\\paragraph{Attentions across nodes.}\n%However, when model the interference, different nodes on a hyperedge may have different importances. \nAlthough the above convolution layer can pass interferences through hyperedges, it does not provide much flexibility to account for the significance of interference for different nodes through different hyperedges. \nIn the aforementioned COVID-19 example, intuitively, those individuals who are active in certain group gathering events are more likely to influence or be influenced by others in these groups. To better capture this intrinsic relationship between nodes and hyperedges on a hypergraph, \n%more flexibly capture the high-order interference, \nwe leverage a hypergraph attention mechanism \\cite{bai2021hypergraph,zhang2019hyper,ding2020more} to learn attention weights for each node and the corresponding hyperedges that contain this node. \n%This attention mechanism can reflect the intrinsic relationship among nodes in the hypergraph. %dynamic incidence matrix, which can better reveal the intrinsic relationship among nodes in the hypergraph. \n\nMore specifically, we compute a representation for each hyperedge ($e$) by aggregating across its associated nodes ($\\mathcal{N}_e$): $\\mathbf{z}_{e}=\\textsc{Agg}(\\{\\mathbf{z}_i~|~i\\in \\mathcal{N}_e\\})$. Here, $\\textsc{Agg}(\\cdot)$ can be any aggregation functions (e.g., the mean aggregation). For each node $i$ and its associated hyperedge $e$, \nthe attention score between a node $i$ and a hyperedge $e$ can be calculated as:\n%$\\alpha_{i,e}$ between them models the importance of node $i$ in hyperedge $e$, and $\\alpha_{i,e}$ can be calculated with different attention score functions such as the widely used bilinear \\cite{luong2015effective} function or the scaled dot product \\cite{vaswani2017attention} function. For example:\n\\begin{equation}\n    \\alpha_{i,e} = \\frac{\\text{exp}(\\sigma(\\text{sim}(\\mathbf{z}_i \\mathbf{W}_a,\\mathbf{z}_e \\mathbf{W}_a)))}{\\sum_{k \\in \\mathcal{E}_i} \\text{exp}(\\sigma(\\text{sim}(\\mathbf{z}_i \\mathbf{W}_a,\\mathbf{z}_k \\mathbf{W}_a)))},\n\\end{equation}\nwhere $\\sigma(\\cdot)$ is a non-linear activation function, $\\mathcal{E}_i$ denotes the set of hyperedges associated with the node $i$. Here we use $\\mathbf{W}_a$ to denote a parameter matrix to compute the node-hyperedge attention. %$\\mathcal{N}_i^\\mathcal{E}$ is the neighborhood set.\n$\\text{sim}(\\cdot)$ is a similarity function, which can be implemented as:\n\\begin{equation}\n    \\text{sim}(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{a}^\\top[\\mathbf{x}_i\\|\\mathbf{x}_j],\n\\end{equation}\nwhere $\\mathbf{a}$ is a weight vector, $[\\cdot\\|\\cdot]$ is a concatenation operation. \n\nThen we use the attention scores to model the interference with different significance. Specifically, we replace the original incidence matrix $\\mathbf{H}$ in Eq.~\\ref{eq:laplacian} with an enhanced matrix $\\tilde{\\mathbf{H}}=\\{\\tilde{h}_{i,e}\\}$, where $\\tilde{h}_{i,e}=\\alpha_{i,e}h_{i,e}$.\n%We incorporate the attention technique into the incidence matrix $\\mathbf{H}$ by replacing $\\mathbf{H}$ with $\\mathbf{H}'_{i,e}=\\alpha_{i,e}\\mathbf{H}_{i,e}$, then we learn the node representations through hypergraph convoluntional layers in a similar way as described in Eq.~\\ref{eq: hgcn}. \nIn this way, the interference from different nodes on the same hyperedge can be assigned with different importance weights, indicating different levels of contribution for interference modeling. We denote the final representations from the last convolution layer as $\\mathbf{P}=\\{\\mathbf{p}_i\\}_{i=1}^n$ and expect it to capture the high-order interference for each node.\n\n\\paragraph{Representation Balancing}\nSimilar to the coufounder representation learning module, we %apply the representation balancing technique on the final interference representations to\ncalculate a discrepancy penalty to reflect the difference between the distributions of interference representations in treatment and control groups. We sum up these two discrepancy penalties together to compute a representation balancing loss, denoted by $\\mathcal{L}_b$.\n\n\n% Here $g(.)$ can be parameterized with an HyperGAT \\cite{ding2020more} based module. \n% % expressions\n% For example, the input feature can be first transformed to lower dimensional space via an MLP with single hidden layer, and the representation of the $e$-th hyperedge $\\textbf{e}_e$ can then be learned via an attention-based weighted sum of the transformed input features of all involved nodes, i.e., $\\textbf{e}_e = \\sum_{i \\in \\mathcal{N}_e} \\alpha_{i,e}\\text{MLP}(\\mathbf{x}_i)$. $\\mathcal{N}_e$ denotes the set of nodes involved in the $e$-th hyperedge. Here the attention can be computed based on the input features (node-hyperedge attention):\n% \\begin{equation}\n% \\label{attension-1}\n%     \\alpha_{ie} = \\frac{\\text{exp}(\\text{ReLU}(\\boldsymbol{a}^T\\mathbf{x}_i))}{\\sum_{k \\in \\mathcal{N}_e } \\text{exp}(\\text{ReLU}(\\boldsymbol{a}^T\\mathbf{x}_i))},\n% \\end{equation}\n% where $\\boldsymbol{a}$ is the weight vector to be learned. Then the confounder representations for the $i$-th node $\\textbf{h}_{i}$ can be attained via aggregating the representations of hyperedges as:\n% \\begin{equation}\n%     \\mathbf{h}_i = \\text{AGGREGATE}(\\{\\mathbf{e}_e: e \\in \\mathcal{E}_i\\}) %\\sum_{e \\in \\mathcal{E}_i} \\alpha_{e,i} \\mathbf{e}_e.\n% \\end{equation}\n% Here the aggregation function AGGREGATE(.) can be formulated in different ways, e.g., $\\sum_{e \\in \\mathcal{E}_i} \\beta_{e,i} \\mathbf{e}_e$  based on the hyperedge-node attention, where $\\beta_{e,i}$ can be computed via a similar way as Eq. (\\ref{attension-1}).\n% In this way, with the help of the two attention mechanisms (i.e., node-hyperedge and hyperedge-node), we can achieve the confounder representation learning based on the high-order relations.\n\n% Next, to achieve the final node representations for potential outcome prediction, our proposed approach also needs to model the spillover effect along the hyperedges. The embedding representing the spillover effect along an hyperedge from all involved individuals can be modeled based on a set transformer \\cite{lee2019set} with all corresponding involved treatments as input to handle different numbers of neighbors and provide permutation-invariant output.\n\n"
                },
                "subsection 3.3": {
                    "name": "Outcome Prediction",
                    "content": "\nWith the confounder representation $\\mathbf{z}_i$ and the interference representation $\\mathbf{p}_i$, we model the potential outcomes as:\n%Considering both the learned confounder representations and the modeled interference, we obtain the final representation  of each node $i$ for potential outcome prediction as:\n% \\begin{align}\n% \\hat{y}^1_i = f_1([\\mathbf{z}_i \\| \\mathbf{p}_i^{1}]),\\,\\hat{y}^0_i = f_0([\\mathbf{z}_i \\| \\mathbf{p}_i^{0}]),\n% \\end{align}\n\\begin{align}\n\\hat{y}^1_i = f_1([\\mathbf{z}_i \\| \\mathbf{p}_i]),\\,\\hat{y}^0_i = f_0([\\mathbf{z}_i \\| \\mathbf{p}_i]),\n\\end{align}\nwhere $f_1(\\cdot)$ and $f_0(\\cdot)$ are learnable functions to predict the potential outcome w.r.t. $t=1$ and $t=0$. We implement $f_1(\\cdot)$ and $f_0(\\cdot)$ with two MLP modules. Then the prediction for the observed outcome is obtained by $\\hat{y}_i=\\hat{y}^{t_i}_i$.\n%when the treatment assignment is $1$ or $0$, respectively. %We take the prediction for the observed outcome as $\\hat{y}_i=f_{t_i}([\\mathbf{z}_i \\| \\mathbf{p}_i])$. \n%We use means square loss (MSE) for the outcome prediction. The final loss function can be formulated as:\nWe optimize the model to minimize the following loss function: \n\\begin{equation}\n    \\mathcal{L}=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2 + \\alpha\\mathcal{L}_b + \\lambda\\|\\mathbf{\\Theta}\\|^2,\n\\end{equation}\nwhere the first term is the standard mean squared error, $\\mathcal{L}_b$ is the representation balancing loss, $\\mathbf{\\Theta}$ represents the parameters in this neural network model. $\\alpha$ and $\\lambda$ are two hyperparameters which control the weights for the representation balancing loss and the parameter regularization term. The ITE for each instance $i$ can be estimated as: $\\hat{\\tau}_i=\\hat{y}^1_i-\\hat{y}^0_i$.\n% the concatenation operation; $\\mathcal{N}_e$ denotes the set of nodes involved in the $e$-th hyperedge; $\\omega_{ie}$ is computed based on the hyperedge representation $\\textbf{e}_e$ and confounder representation for each node $\\textbf{h}_i$; $\\mathcal{E}_i$ denotes the hyperedge set which involves the $i$-th node.\n%It should be noted that $\\gamma_{ei}$ (also computed based on $\\textbf{e}_e$ and $\\textbf{h}_i$ as Eq. (\\ref{attension-1})) measures how much the treatments of other nodes in the $e$-th hyperedge affect the outcome of the $i$-th node, which is a key part in modeling the spillover from hyperedge to every single individual.\n%Finally, we use the output representation $\\textbf{z}_i$ for potential outcome prediction. Here we also use a module $f_{y}(\\mathbf{z}_i)$ to predict the potential outcomes.\n\n"
                },
                "subsection 3.4": {
                    "name": "Discussion",
                    "content": " \nHere we revisit some implicit assumptions in the proposed framework. \nFirst, we assume that the interference for each node $i$ comes from its neighbors through the hypergraph structure. Here, the interference from neighbors that are multiple hops away can also be captured by stacking more hypergraph convoluntional layers.  \nSecond, for simplicity, we assume that the interference for each node only comes from other nodes with non-zero treatment assignment. \nThird, we assume that the representations of nodes in the same hyperedge are similar in the latent space. Besides, following \\cite{bai2021hypergraph}, we assume that the representations of hyperedges are homogeneous with node representations. Nevertheless, we should still mention that this proposed framework is general and extendable, where the above assumptions can be further relaxed by enriching the hypergraph processing module.\n%\\input{theory}\n\n"
                }
            },
            "section 4": {
                "name": "Experiments",
                "content": "\nIt is typically very hard to obtain the ground-truth counterfactual data as only one of the two potential outcomes can be obtained in the observational data. Hence, in this section, we follow a standard practice to evaluate the proposed framework and the alternative approaches on three semi-synthetic datasets. We aim to leverage as much real-world information as possible in the simulated environment. Our datasets are all based on real-world hypergraph data and we retain the treatment allocations as well as node features (covariates) if they are available. We simulate the outcome generation process to assess the true individual treatment effect (ITE), which eventually allows us to evaluate the performance of the ITE estimation from different causal inference approaches. \n% By conducting experiments on these semi-synthemtic datasets, we hope to answer the following questions:\n% \\begin{itemize}\n%     \\item \\textbf{Q1}: What is the overall ITE estimation performance of \\mymodel~ compared with state-of-the-art causal inference methods?\n%     \\item \\textbf{Q2}: How much ITE estimation improvement can be brought from modeling high-order interference on hypergraphs?\n%     \\item \\textbf{Q3}: How does the high-order interference act on different types of individuals?\n%     \\item \\textbf{Q4}: How do different components in the proposed framework contribute to the ITE estimation performance?\n%     \\item \\textbf{Q5}: How sensitive is \\mymodel~ towards different settings of model hyper-parameters?\n% \\end{itemize}\n%We compare the ITE performance of the proposed framework with various state-of-the-art baselines under different settings, including different specifications of ITE and interference in the hypergraph. We also conduct ablation studies to validate the contributions of different components in the proposed framework, and then we use parameter studies to evaluate the robustness of our framework.\n\n% \\begin{table}[t]\n% %\\small\n%   \\caption{Detailed statistics of the datasets.} \\label{tab:stats}\n%  \\vspace{-2mm}\n%   \\label{tab:dataset}\n%   \\begin{tabular}{llll}\n%     \\toprule\n%     Dataset &Contact & GoodReads &  Microsoft \\\\\n%     \\midrule\n%     \\# of nodes &$327$ &$57,031$ &$91,391$  \\\\ \n%     \\# of hyperedges& $7,818$  & $12,709$ & $237,078$\\\\ \n%     \\# of hyperedges (size>2) &$2,320$ & $8,067$ &$183,684$  \\\\\n%     \\# of features & $50$  & $500$ & $200$ \\\\ \n%     Ave hyperedge size & $2.33$& $5.76$& $7.66$  \\\\\n%     Ave node degree & $55.63$& $1.36$& $20.19$  \\\\\n%      Ratio ($\\%$) of treated & $50.2\\%$& $40.0\\%$ & $42.9\\%$ \\\\ \n%      %Avg ATE $\\pm$ STD & &  & \\\\ \n%   \\bottomrule\n% \\end{tabular}\n% %\\vspace{-3mm}\n% \\end{table}\n\n\n\n",
                "subsection 4.1": {
                    "name": "Dataset and Simulation",
                    "content": "\nWe obtain the semi-synthetic data based on two publicly available hypergraph datasets (\\textbf{Contact} \\cite{mastrandrea2015contact,benson2018simplicial}, \\textbf{Goodreads} \\cite{wan2018item,wan2019fine}) and one large-scale proprietary web application dataset (\\textbf{Microsoft Teams}).\n%As it is notoriously hard to obtain the ground-truth counterfactual data, we use three semi-synthetic datasets based on real-world data sources, including high school contact network, GoodReads review, and Microsoft social media. \nWe do not account for the temporal information of each hyperedge in our experiments and leave this as a future research direction instead.\nIn all three datasets, we discard extremely large hyperedges and keep those with no more than $50$ nodes only.\\footnote{Note hyperedges with large size of nodes are usually less meaningful \\cite{benson2018simplicial}.} \n%Although the hyperedges have timestamps, we treat them equally in simulation. % On the contact dataset, we simulate both the treatment and outcome; while on the Amazon and Microsoft datasets, we use real-world treatment, and generate the potential outcomes. \n%Detailed statistics of the processed datasets are included in Table~\\ref{tab:stats}.\n%Source code and the two public datasets will be made available at publication time.\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Outcome Simulation.",
                        "content": " Given the treatment allocations $\\mathbf{T}$, node features $\\mathbf{X}$, and the hypergraph structure $\\mathbf{H}$, the potential outcome of an individual $i$ can be simulated via\n% \\noindent\\textbf{General simulation process.} Generally, in this process, we use the real-world covariates, treatments, and hypergraph structure, then we generate the potential outcomes by simulating the individual treatment effects and interference.\n\n% Specifically, to simulate the potential outcomes, we use the following different scenarios:\n% % \\begin{equation}\n% %     y_i(\\mathbf{T}, \\mathbf{x}_i) = f_y(\\mathbf{h}_i) + f_t(t_i) + f_s([t_{\\mathcal{N}_i}]) + \\epsilon_y,\n% % \\end{equation}\n\\begin{equation}\n    y_i = f_{y,0}(\\mathbf{x}_i) + \\overbrace{\\gamma f_t(t_i, \\mathbf{x}_i)}^{\\mathclap{\\text{individual treatment effect (ITE)}}} + \\underbrace{\\beta f_s(\\mathbf{T}, \\mathbf{X}, \\mathbf{H})}_{\\mathclap{\\text{hypergraph spillover effect}}} +~\\epsilon_{y_i},\n\\end{equation}\nwhere $f_{y,0}(\\mathbf{x}_i)$ describes the outcome of instance $i$ when $t_i=0$ and without network interference, $f_t(\\cdot)$ calculates the ITE of each instance,  $f_s(\\cdot)$ calculates the spillover effect, and $\\epsilon_{y_i}$ denotes the random noise from a Gaussian distribution $\\mathcal{N}(0,1)$. \n%To comprehensively evaluate the performance of the proposed model, we consider different settings of above potential outcome generation function. We use linear/quadratic transformations to implement $f_{y,0(\\cdot)}$, $f_t(\\cdot)$, and $f_s(\\cdot)$, where $f_s$ is a transformation on the summary of neighbors.\nWe specify $f_{y,0}(\\mathbf{x}_i)$ as a linear transformation of $\\mathbf{x}_i$:\n\\begin{equation}\n    f_{y,0}=\\mathbf{w}_0\\mathbf{x}_i,\n\\end{equation}\nwhere $\\mathbf{w}_0\\sim \\mathcal{N}(0,\\mathbf{I}), \\mathbf{w}_0\\in \\mathbb{R}^d$. Then we control the individual treatment effect ($f_t(t_i, \\mathbf{x}_i)$) and the hypergraph spillover effect ($f_s(\\mathbf{T}, \\mathbf{X}, \\mathbf{H})$) under two different settings:\n%\n% \\noindent\\textbf{Microsoft.}~ In Microsoft dataset, we aim to estimate the causal effect of social media usage (treatment) on the collaboration frequency (outcome) for each employee (instance). We collected a hypergraph which describes the Teams membership of $230,171$ employees, where each edge is a Teams group in a snapshot on March 1st. In each group, one employee's behavior on social media may lead to aggregated effect (e.g., group discussion), and elicit complicated causal effect to other group members' collaboration frequencies. We filtered the group size within $50$. We collect the covariates of employees, and divide the covariates into four categories: user identification, location, job, and work experience. We preprocess the treatment assignment into binary values by taking the treatment assignment of an employee as 1 if this employee has composed at least one message during March 1st-7th, otherwise the treatment assignment is 0. We generate the potential outcomes by specifying the different components as follows:\n%\n% where $\\|$ denotes the concatenation operator, and $K$ represents the number of covariates' categories. In each category $k$, we use a different transformation function $\\Phi_k(\\mathbf{x}_i)=\\mathbf{w}_k\\mathbf{x}_i+\\epsilon_1$, where $\\mathbf{w}_k$ is a $d$-dimensional vector, and each element is generated from Gaussian distribution $\\mathcal{N}(0,1)$. $\\epsilon_1$ is a noise term generated from $\\mathcal{N}(0,0.001)$. The second part aggregate the neighbors' information, where $\\alpha_{ei}$ and $\\beta_{je}$ denotes the importance of hyperedge $e$ to node $i$, and the importance of node $j$ to hyperedge $e$, respectively. $\\Psi_k(\\mathbf{x}_i)=\\mathbf{v}_k\\mathbf{x}_i+\\epsilon_2$, where $\\mathbf{v}_k\\sim \\mathcal{N}(0,1)$ and $\\epsilon_2\\sim \\mathcal{N}(0,0.001)$. $\\lambda_1$ and $\\lambda_2$ control the impact of self and neighbors, respectively. \n% We use each employee's number of composed messages on Teams as treatment, and we simulate the number of collaboration social ties as outcome.\n% \\begin{equation}\n%     f_{y,0}=\\mathbf{w}_0\\mathbf{x}_i,\n% \\end{equation}\n% where $\\mathbf{w}_0\\sim \\mathcal{N}(0,\\mathbf{I}), \\mathbf{w}_0\\in \\mathbb{R}^d$. \n% For $f_t$, we have the following different choices:\n\\begin{enumerate}\n    \\item \\textbf{Linear.}\n      \\begin{equation}\n        f_t(t_i, \\mathbf{x}_i) = \\left\\{\n        \\begin{aligned}\n        & \\mathbf{w}_1\\mathbf{x}_i + \\epsilon & \\text{if } t_i=1\\\\\n        & 0 & \\text{if } t_i=0\n        \\end{aligned}\\right.\n    \\end{equation}\nHere $\\mathbf{w}_1 \\in \\mathbb{R}^d$, and each element in $\\mathbf{w}_1$ follows a Gaussian distribution. \nWe generate $f_s$ as:\n% \\begin{equation}\n%     f_s(\\mathbf{T},\\mathbf{x}_i,\\mathcal{H}) = \\frac{1}{|\\mathcal{N}_i|} \\sum_{j\\in \\mathcal{N}_i} f_t(t_j, \\mathbf{x}_j).\n% \\end{equation}\n\\begin{equation}\n    f_s(\\mathbf{T},\\mathbf{X},\\mathbf{H}) = \\frac{1}{|\\mathcal{E}_i|} \\sum_{e\\in\\mathcal{E}_i} \\sigma'(\\frac{1}{|\\mathcal{N}_e|}\\sum_{j\\in \\mathcal{N}_e} t_j \\times f_t(t_j, \\mathbf{x}_j)).\n\\end{equation}\nHere, $\\sigma'(\\cdot)$ is a function on the aggregation over each hyperedge. We implement it with an identity function by default.\n    \\item \\textbf{Quadratic.}\n     \\begin{equation}\n        f_t(t_i, \\mathbf{x}_i) = \\left\\{\n        \\begin{aligned}\n        & \\mathbf{x}_i^\\top\\mathbf{W}_t\\mathbf{x}_i + \\epsilon & \\text{if } t_i=1\\\\\n        & 0 & \\text{if } t_i=0\n        \\end{aligned}\\right.\n    \\end{equation}\nHere $\\mathbf{W}_t \\in \\mathbb{R}^{d\\times d}$, and each element in $\\mathbf{W}_t$ follows a Gaussian distribution. \nWe generate $f_s$ as:\n% \\begin{equation}\n%     f_s(\\mathbf{T},\\mathbf{x}_i,\\mathcal{H}) = \\frac{1}{|\\mathcal{N}_i|} \\sum_{j\\in \\mathcal{N}_i} f_t(t_j, \\mathbf{x}_j).\n% \\end{equation}\n\\begin{equation}\n    f_s(\\mathbf{T},\\mathbf{X},\\mathbf{H}) = \\frac{1}{|\\mathcal{E}_i|} \\sum_{e\\in\\mathcal{E}_i} \\sigma'(\\frac{1}{|\\mathcal{N}_e|^2}(\\mathbf{T}_e*\\mathbf{X}_e)\\mathbf{W}_t(\\mathbf{T}_e*\\mathbf{X}_e)^\\top).\n  \\end{equation}    \n  Here $\\mathbf{X}_e$ and $\\mathbf{T}_e$ are the feature matrix and treatment assignment of nodes contained in hyperedge $e$,  respectively. Here $*$ denotes element-wise multiplication.\n\\end{enumerate}\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "Dataset Details.",
                        "content": " We follow the above process to generate potential outcomes on all three datasets. Additional details about each dataset are provided as the follows.\n\n\\xhdr{Contact.} This dataset collects interactions recorded by wearable sensors among students at a high school \\cite{mastrandrea2015contact,benson2018simplicial}, and includes 327 nodes and 7,818 hyperedges. \n%The average node degree in this dataset is 55.6 and the average hypergraph size is 2.3. \nEach node represents a person, and each hyperedge stands for a group of individuals are in close physical proximity to each other. This contact hypergraph data allows us to simulate a hypothetical question: ``how does one's face covering practice (treatment) causally affect their infection risk of an infectious disease (outcome)?''.\n%The treatment is whether a person practices face covering at each contact, and the outcome is if this person is infected. \nIn each group contact, one may bring the virus to the surrounding environment, and thus affect other people's infection risk. Due to the lack of detailed information about each individual, apart from the potential outcome, we also generate the treatment ($t_i$) and the covariates ($\\mathbf{x}_i$) as the follows:\n%This dataset includes $327$ nodes and $7,818$ hyperedges. In this dataset, we generate the covariates, treatment and potential outcomes. More specifically, we generate them as follows:\n\\begin{equation}\n    \\mathbf{x}_i \\sim \\mathcal{N}(0,\\mathbf{I}), {t}_{i} \\sim Ber(\\text{sigmoid}(\\mathbf{x}_i\\mathbf{v}_t)),\n\\end{equation}\nwhere $\\mathbf{I}$ is an $d\\times d$ identity matrix, here we set $d=50$. $\\mathbf{v}_t$ is a $d$-dimensional vector where each element inside follows a  Gaussian distribution. Eventually about $50\\%\\sim 60\\%$ of the nodes are treated (${t}_{i}=1$) in our experiments.\n\n\n\\xhdr{GoodReads.} This dataset collects book information from the book review website GoodReads\\footnote{https://www.goodreads.com/}, including the book title, authors, descriptions, reviews, and ratings \\cite{wan2018item,wan2019fine}. We take each book in the \\textit{Children} category as an instance. The bag-of-words of the book descriptions are used as the covariates of each book. %We train a LDA model with 50 topics with the entire training corpus. \nEach hyperedge corresponds to each author and all books sharing the same author are in the same hyperedge.\n%We construct the hypergraph by taking each hyperedge corresponding to an author, and all the books which share the same author are in the same hyperedge. \nThe real-world book ratings are considered as treatment assignments: for each node $i$, we define $t_i=1$ if the rating score is larger than 3 and $t_i=0$ otherwise. We aim to study the causal effect of the rating score on the sales of each book. The ratings of each author's books can establish this author's overall reputation, and thus influence the sales of other books from the same author. The final processed dataset includes 57,031 nodes (where $40\\%$ are treated) and 12,709 hyperedges. Note each book may have more than one author, and each author may have published multiple books.\n%(the average node degree is 1.36)(the average hyperedge size is 5.76).\n%We simulate the potential outcomes in the same way as Microsoft dataset. \n\n\\xhdr{Microsoft Teams.} We sampled 91,391 anonymized employees of a multinational technology company and collected their aggregated telemetry data on Microsoft Teams\\footnote{\\href{https://www.microsoft.com/en-us/microsoft-teams}{https://www.microsoft.com/en-us/microsoft-teams}}. Microsoft Teams is a workplace communication platform where users are allowed to create a group space (i.e., ``team'' or ``channel'') to enable public communication within each group. We are interested in how a user's usage of these group spaces causally affects their productivity. We process the treatment assignment into binary values by taking it as 1 if the employee has sent out at least one message in any of these group spaces during the first week of March, 2021; otherwise the treatment is assigned as 0. Each group space can be regarded as a hyperedge, where information can be shared via group discussions thus one's activeness on this platform may affect other individuals' outcomes in the same group. Employee demographics (e.g., office location, job description, work experience) were leveraged as the covariates. \n\n\n\n% \\begin{table}[]\n% \\small\n% \\centering\n% \\setlength{\\tabcolsep}{2.5pt}\n%  \\caption{Comparison of the performance of ITE estimation under linear setting. ``CT\", ``GR\" and ``MS\" stand for Contact, GoodReads and Microsoft Teams datasets, respectively.}\n%  \\vspace{-2mm}\n%  \\label{tab:baseline_linear}\n% \\begin{tabular}{l|l|cc|cc}\n% \\hline\n%               \\multicolumn{1}{c|}{}                          &                           & \\multicolumn{2}{c|}{Linear}                                                                     & \\multicolumn{2}{c}{Quadratic}         \\\\\\cline{3-6}\n% \\multicolumn{1}{c|}{\\multirow{-2}{*}{Data}} & \\multirow{-2}{*}{Method} & $\\sqrt{\\epsilon_{PEHE}}$                                &  $\\epsilon_{ATE}$                                                           &  $\\sqrt{\\epsilon_{PEHE}}$                 & $\\epsilon_{ATE}$       \\\\\n% \\hline\n% CT &  LR & $25.41$~\\scriptsize{$\\pm0.12$}         & $9.11$~\\scriptsize{$\\pm0.28$}   & $38.22$~\\scriptsize{$\\pm2.46$} & $20.28$~\\scriptsize{$\\pm1.21$}                     \\\\\n%  & CEVAE                     & $22.88$~\\scriptsize{$\\pm3.40$}         & $8.29$~\\scriptsize{$\\pm2.18$ } & $35.28$~\\scriptsize{$\\pm 2.38$} & $18.22$~\\scriptsize{$\\pm 2.39$}                  \\\\\n%  & CFR                       & $24.04$~\\scriptsize{$\\pm2.40$}         & $7.17$~\\scriptsize{$\\pm1.37$}  & $32.24$~\\scriptsize{$\\pm 3.21$} & $17.28$~\\scriptsize{$\\pm 2.37$}                \\\\\n%  & Netdeconf            & $10.22$~\\scriptsize{$\\pm1.48$}         & $4.29$~\\scriptsize{$\\pm0.42$} & $21.23$~\\scriptsize{$\\pm 2.28$} & $11.39$~\\scriptsize{$\\pm 2.33$ }      \\\\\n%  & GNN-HSIC               &   $7.42$~\\scriptsize{$\\pm1.24$}          & $2.06$~\\scriptsize{$\\pm 0.11$} & $16.28$~\\scriptsize{$\\pm 0.76$} & $7.28$~\\scriptsize{$\\pm 1.22$}   \\\\\n%  & GCN-HSIC               & $7.28$~\\scriptsize{$\\pm 1.40$}          & $2.08$~\\scriptsize{$\\pm 0.14$}  & $14.23$~\\scriptsize{$\\pm 0.64$} & $6.27$~\\scriptsize{$\\pm 0.47$} \\\\\n%  & \\mymodel          & {\\bm{$3.45$}~\\scriptsize{\\bm{$\\pm 0.87$}}  }                  & {\\bm{$1.39$}~\\scriptsize{\\bm{$\\pm0.08$}}     }                                   & \\bm{$9.20$}~\\scriptsize{\\bm{$\\pm 0.28$}}  & \\bm{$2.24$}~\\scriptsize{\\bm{$\\pm 0.22$}}       \\\\\n% \\hline\n% GR &  LR  & $23.01$~\\scriptsize{$\\pm 0.13$}         & $13.42$~\\scriptsize{$\\pm 0.40$}  & $48.56$~\\scriptsize{$\\pm 3.21$} & $31.19$~\\scriptsize{$\\pm 1.49$}                    \\\\\n%  & CEVAE                     & $22.69 $~\\scriptsize{$\\pm 0.08$}        & $12.49$~\\scriptsize{$\\pm 0.19$ }                 & $45.21$~\\scriptsize{$\\pm 3.10$} & $29.22$~\\scriptsize{$\\pm 1.39$}        \\\\\n%  & CFR                       & $20.30$~\\scriptsize{$\\pm 0.11$}         & $13.21$~\\scriptsize{$\\pm 0.29$ }                 & $41.72$~\\scriptsize{$\\pm 2.28$} & $26.28$~\\scriptsize{$\\pm 1.38$}        \\\\\n%  & Netdeconf                 & $18.39$~\\scriptsize{$\\pm 0.59$}         & $12.20$~\\scriptsize{$\\pm 0.10$}  & $35.18$~\\scriptsize{$\\pm 2.48$} & $21.20$~\\scriptsize{$\\pm 2.41$}            \\\\\n%  & GNN-HSIC                 & $17.20$~\\scriptsize{$\\pm 0.72$}         & $12.18$~\\scriptsize{$\\pm 0.42$}  & $27.22$~\\scriptsize{$\\pm 2.48$} & $16.87$~\\scriptsize{$\\pm 1.48$}             \\\\\n%  & GCN-HSIC                 & $16.01$~\\scriptsize{$\\pm0.62$}          & $12.06$~\\scriptsize{$\\pm 0.46$} & $25.42$~\\scriptsize{$\\pm 2.39$} & $16.28$~\\scriptsize{$\\pm 1.39$}     \\\\\n%  & \\mymodel                      & \\bm{$15.68$}~\\scriptsize\\bm{{$\\pm0.67$}} & \\bm{$11.81$}~\\scriptsize\\bm{{$\\pm0.48$} }           & \\bm{$19.23$}~\\scriptsize\\bm{{$\\pm 1.39$}} & \\bm{$13.33$}~\\scriptsize{\\bm{$\\pm 0.84$}   }           \\\\\n% \\hline\n% MS & LR    & $22.80$~\\scriptsize{$\\pm 2.01$}                   & $21.41 \\pm$~\\scriptsize{$2.34$}                                          & $414.17$~\\scriptsize{$\\pm 39.43$  }                    & $192.80$~\\scriptsize{$\\pm 29.75$}      \\\\\n%  &CEVAE           & $19.36$~\\scriptsize{$\\pm 2.52$}                    & $8.63$~\\scriptsize{$\\pm 2.47$}        & $315.01$~\\scriptsize{$\\pm25.31$} & $188.47$~\\scriptsize{$\\pm42.79$}            \\\\\n%  &CFR             & $25.23$~\\scriptsize{$\\pm  0.02$}                   & $18.28$~\\scriptsize{$\\pm  0.05$}          & $392.56$~\\scriptsize{$\\pm 43.39$} & $189.75$~\\scriptsize{$\\pm48.11$}  \\\\\n%  &Netdeconf       & $11.11$~\\scriptsize{$\\pm 0.02$}                  & $9.22$~\\scriptsize{$\\pm 0.10$ }      & $241.02$~\\scriptsize{$\\pm 23.25$ }                    & $147.29$~\\scriptsize{$\\pm10.39$}                                                 \\\\\n%  &GNN-HSIC    &$9.38$~\\scriptsize{$\\pm 1.41$}          & $6.91$~\\scriptsize{$\\pm  1.19$}             & $114.28$~\\scriptsize{$\\pm 36.29$}                     & $81.21$~\\scriptsize{$\\pm 25.39$}                               \\\\               \n%  &GCN-HSIC  &    $8.27$~\\scriptsize{$\\pm 1.29$} & $6.60$~\\scriptsize{$\\pm 1.53$} &   $109.57$~\\scriptsize{$\\pm38.58$}  & $77.75 $~\\scriptsize{$\\pm39.39$}  \\\\\n%  &\\mymodel          & {\\bm{$5.13$}~\\scriptsize{\\bm{$\\pm 1.78$}}}               & {\\bm{$4.46$}~\\scriptsize{\\bm{$\\pm1.92$}} }    & \\bm{$81.08$}~\\scriptsize\\bm{{$\\pm 3.74$}  }            & \\bm{$74.41$}~\\scriptsize{\\bm{$\\pm4.24$}  }                                   \\\\\n% \\hline\n% \\end{tabular}\n% \\end{table}\n\n\n\n% \\begin{figure*}[t]\n% \\centering\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_pehe_linear.pdf}\n%         \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, Microsoft}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_ate_linear.pdf}\n%         \\caption{Linear, $\\epsilon_{ATE}$, Microsoft}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_pehe_quadratic.pdf}\n%         \\caption{Quadratic, $\\sqrt{\\epsilon_{PEHE}}$, Microsoft}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_ate_quadratic.pdf}\n%         \\caption{Quadratic, $\\epsilon_{ATE}$, Microsoft}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_linear.pdf}\n%         \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, GoodReads}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_linear.pdf}\n%         \\caption{Linear, $\\epsilon_{ATE}$, GoodReads}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_quadratic.pdf}\n%         \\caption{Quadratic,$\\sqrt{\\epsilon_{PEHE}}$,GoodReads}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_quadratic.pdf}\n%         \\caption{Quadratic, $\\epsilon_{ATE}$, GoodReads}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_pehe_linear.pdf}\n%         \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, Contact}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_ate_linear.pdf}\n%         \\caption{Linear, $\\epsilon_{ATE}$, Contact}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_pehe_quadratic.pdf}\n%         \\caption{Quadratic, $\\sqrt{\\epsilon_{PEHE}}$, Contact}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.24\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_ate_quadratic.pdf}\n%         \\caption{Quadratic, $\\epsilon_{ATE}$, Contact}\n%     \\end{subfigure}\n%         \\vspace{-2mm}\n%   \\caption{Comparison of the performance of ITE estimation under different settings.}\n%   \\label{fig:ITE}\n% \\vspace{-3mm}\n% \\end{figure*}\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Experiment Settings",
                    "content": "\n",
                    "subsubsection 4.2.1": {
                        "name": "Metrics",
                        "content": "\nWe evaluate the performance of causal effect estimation through two standard metrics, including Rooted Precision in Estimation of Heterogeneous Effect ($\\sqrt{\\epsilon_{PEHE}}$) \\cite{BART} and Mean Absolute Error ($\\epsilon_{ATE}$) \\cite{willmott2005advantages}. These metrics can be defined as follows:\n\\begin{equation}\n    \\sqrt{\\epsilon_{PEHE}} = \\sqrt{\\frac{1}{n}\\sum_{i\\in[n]}(\\tau_i-\\hat{\\tau}_i)^2}, \\,\\,\\epsilon_{ATE} = |\\frac{1}{n}\\sum_{i\\in [n]}{\\tau_i}-\\frac{1}{n}\\sum_{i\\in [n]}\\hat{\\tau}_i|.\n\\end{equation}\nLower $\\sqrt{\\epsilon_{PEHE}}$ or $\\epsilon_{ATE}$ indicates better causal effect estimations.\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "Baselines",
                        "content": "\nTo investigate the effectiveness of our framework, we compare it with multiple state-of-the-art ITE estimation baselines. These baselines can be divided into the following categories:\n\\begin{itemize}\n    \\item \\xhdr{No graph.} We compare the estimation results with traditional methods which do not consider graph data and spillover effects. These methods include outcome regression which is implemented by linear regression (\\textbf{LR}), counterfactual regression (\\textbf{CFR} \\cite{CFR}), causal effect variational autoencoder (\\textbf{CEVAE} \\cite{CEVAE}). By comparing the proposed framework to these methods, we evaluate the effectiveness of modeling interference for ITE estimation.\n    \\item \\xhdr{No spillover effect in ordinary graphs.} Although assuming no  spillover effect exists, the network deconfounder (\\textbf{Netdeconf}) \\cite{guo2020learning} captures latent confounders for ITE estimation by utilizing the network structure among instances. \n    \\item \\xhdr{Spillover effect in ordinary graphs.} We compare our framework with other ITE estimation baselines which can handle the pairwise spillover effect on ordinary graphs: %Linked Causal Variational Autoencoder (\\textbf{LVAE}) \\cite{rakesh2018linked}, \n    a node representation learning based method \\cite{ma2021causal} estimates ITE under network interference, including two variants: (a) \\textbf{GNN + HSIC}, which is based on graph neural network \\cite{morris2019weisfeiler} and Hilbert Schmidt independence criterion (HSIC) \\cite{gretton2005measuring}, and (b) \\textbf{GCN + HSIC}, which is based on GCN \\cite{kipf2016semi}. \n    \n    To utilize the baselines which handle ordinary graphs, \n    %These methods are designed to handle interference in ordinary graphs with pairwise edges, so \n    we project the original hypergraph $\\mathcal{H}$ to an ordinary graph $\\mathcal{G}=\\{\\mathcal{V}, \\mathcal{E}^p\\}$ by setting $(v_i,v_j)\\in \\mathcal{E}^p$ if $v_i$ and $v_j$ are contained in at least one common hyperedge in $\\mathcal{H}$. By comparing \\mymodel~to the above baselines, we are able to evaluate the benefits of modeling high-order interferences on the original hypergraph.\n\\end{itemize}\n\n"
                    },
                    "subsubsection 4.2.3": {
                        "name": "Setup",
                        "content": " We randomly partition all datasets into 60\\%-20\\%-20\\% training/validation/test splits. All the results are averaged over ten repeated executions. Unless otherwise specified, we set the hyperparameters as $\\alpha=0.001$, $\\beta=1.0$, $\\gamma=1.0$, $\\lambda=0.01$, the dimension for confounder representation and interference representation both as $64$. We use ReLU as the  activation function, and use an Adam optimizer. By default, the interference modeling component contains one hypergraph convolutional layer.\n\n%\\subsection{Q1: ITE Estimation Performance}\n"
                    }
                },
                "subsection 4.3": {
                    "name": "ITE Estimation Performance",
                    "content": "\nWe include the results for the ITE estimation task in Table~\\ref{tab:baseline_linear}. From this table we \n%To evaluate the performance of ITE estimation of the proposed framework under the existence of spillover effect in hypergraph, we compare our framework with other baselines in different settings of interference simulation. Fig~\\ref{fig:ITE} shows the ITE estimation performance of different methods with different values of $\\beta$ in linear and quadratic settings. We \nobserve that the proposed framework outperforms all the baselines under both linear and quadratic outcome simulation settings. We attribute these results to the fact that \\mymodel~ utilizes the  relational information in hypergraph to model the high-order interference, and thus mitigates the influence of the spillover effect on ITE estimation performance. \n%the information loss in high-order interference. \nCompared with other baselines, the methods which incorporate the pairwise network interference (\\textbf{GCN-HSIC} and \\textbf{GNN-HSIC}), as well as \\textbf{Netdeconf} which utilizes the network structure for ITE estimation, perform better than those baselines which do not take advantage of the relational information (\\textbf{LR}, \\textbf{CEVAE}, \\textbf{CFR}). \n\nWe also vary the hyperparameter ($\\beta$) which controls the significance of hypergraph spillover effect in the outcome simulation and report the ITE estimation results in Fig.~\\ref{fig:ITE}.\nAs $\\beta$ increases, i.e., the outcome is more heavily influenced by interference, larger performance gains can be observed from the proposed framework (\\mymodel) against baselines. \nThis observation further validates the effectiveness of our framework in modeling the interference for enhancing the performance of ITE estimation.\n\n\n\n\n\n\n"
                },
                "subsection 4.4": {
                    "name": "Ablation Study",
                    "content": "\n\n\n\n\n\n\n% \\begin{figure}\n% \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/ablation_GoodReads.pdf}\n%         \\caption{GoodReads}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/ablation_contact.pdf}\n%         \\caption{Contact}\n%     \\end{subfigure}\n%     \\vspace{-1mm}\n%   \\caption{Ablation studies of different variants of the proposed framework \\mymodel. Results are reported under the linear setting but similar patterns can be found under the quadratic setting and on all datasets. }\n%   \\label{fig:ablation}\n% %\\vspace{-3mm}\n% \\end{figure}\n\n% \\begin{figure}\n% \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\sqrt{\\mathcal{E}_{PEHE}}$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\mathcal{E}_{ATE}$}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\sqrt{\\mathcal{E}_{PEHE}}$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\sqrt{\\mathcal{E}_{PEHE}}$}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\sqrt{\\mathcal{E}_{PEHE}}$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\mathcal{E}_{ATE}$}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\sqrt{\\mathcal{E}_{PEHE}}$}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.11\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.6in]{figs/parameter_GoodReads_balancing_pehe.pdf}\n%         \\caption{$\\alpha$,$\\sqrt{\\mathcal{E}_{PEHE}}$}\n%     \\end{subfigure}\n%     \\vspace{-1mm}\n%   \\caption{Comparison of ITE estimation performance of the proposed framework \\mymodel~ under different parameters or model structures on GoodReads dataset.}\n%   \\label{fig:param}\n% %\\vspace{-3mm}\n% \\end{figure}\n\n\n\nTo investigate the effectiveness of different components in the proposed framework, we conduct ablation studies by considering the following variants: 1) we apply the proposed model \\mymodel~ on the projected graph (in a hypergraph structure) (denoted as \\textbf{\\mymodel-P}); 2) we replace the hypergraph neural network module with a graph neural network module with the same number of layers, and then apply it on the projected graph (in an original graph structure) (\\textbf{\\mymodel-G}). { Notice that although both evaluated on the projected graph, \\textbf{\\mymodel-G} handles ordinary graphs with its graph neural network module, while \\textbf{\\mymodel-P} handles hypergraphs with its hypergraph neural network module}; 3) we remove the balancing techniques in the framework (\\textbf{\\mymodel-NB}). The ITE estimation results are reported in Fig.~\\ref{fig:ablation}, where we notice significant performance gaps between \\textbf{\\mymodel-P}/\\textbf{\\mymodel-G} and \\mymodel, which imply the effectiveness of modeling the high-order relationships on hypergraphs. We also observe the ITE estimation performance degrades after removing the representation balancing modules, which indicates the effectiveness of the representation balancing techniques on mitigating the biases of ITE estimations.\n\n% High-order interferences\n%\\subsection{Q2: High-order Interference for ITE Estimation}\n\\vspace{-2mm}\n"
                },
                "subsection 4.5": {
                    "name": "A Closer Look at High-Order Interference",
                    "content": "\nIn addition to the overall ITE estimation performance, we take a closer look at high-order interference. %through both quantitative and qualitative analysis.\nWe investigate how the proposed framework responds to hyperedges with difference sizes. \n%Intuitively, high-order interference becomes more prominent among a large group of nodes.\n%In order to investigate how the high-order interference influence the causal estimation results, we further conduct in-depth experiments with finer granularity of the high-order interference. \nMore specifically, we remove the hyperedges with size larger than $k$, denote the modified hypergraph as $\\mathcal{H}^{(k)}$, and vary the value of $k$. In Fig~\\ref{fig:hyper_k}, we compare the ITE estimation performance of the proposed framework \\mymodel~ with its variant on the projected ordinary graph \\textbf{\\mymodel-G}. We observe that: { 1) When $k=2$ (hyperedge size $\\le$ 2), the performance of HyperSCI-G is close to HyperSCI. Because when $k=2$, graph convolution can be regarded as a special case of hypergraph convolution with small differences in the graph Laplacian matrix (as illustrated in \\cite{bai2021hypergraph}). Empirically this leads to a minor performance difference between HyperSCI-G and HyperSCI;} 2) When $k$ increases, the performance of ITE estimation from both methods are gradually improved, but such an improvement becomes less significant when $k$ is larger. Besides, we notice \\mymodel~ consistently outperforms \\textbf{\\mymodel-G} and such a difference becomes larger as $k$ increases, indicating its efficacy on modeling high-order interference especially on large hyperedges.\n%, as large-size hyperedges (e.g., over $20$ on GoodReads) are very rare on these hypergraphs. We also show the performance of \\textbf{\\mymodel-G} in the same settings in Fig~\\ref{fig:hyper_k}. \\textbf{\\mymodel-G} is a variant of \\mymodel, where the hypergraph neural network layers are replaced by graph neural network layers, and it can only handle the plain graphs projected from the modified hypergraphs. Generally, the performance of \\textbf{\\mymodel-G} is also improving with higher $k$, but compared with \\mymodel, the improvement is relatively weaker as this variant can not deal with high-order information. \n\n\\vspace{-1mm}\n"
                },
                "subsection 4.6": {
                    "name": "Sensitivity Analysis",
                    "content": "\n\nTo evaluate the robustness of the proposed framework, we present the ITE estimation performance of \\mymodel~under different settings of model hyper-parameters in Fig.~\\ref{fig:param}. More specifically, we vary the value of the balancing weight from $\\{0.0001,0.001,0.01,0.1\\}$, and vary the representation dimension from $\\{16, 32, 64, 128\\}$. We also vary the number of attention head from $\\{1,2,3,4\\}$, then change the parameter of regularization weight from $\\{0.0001, 0.001, 0.01, 0.1\\}$.\n% We implement the hypergraph neural network module with hypergraph convoluntional network (hyperGCN) \\cite{bai2021hypergraph} or hypergraph attention network (hyperGAT) \\cite{bai2021hypergraph}. \nAs can be observed, our framework is generally robust to different hyper-parameter settings, but proper fine-tuning of these hyper-parameters is still beneficial for the ITE estimation performance. %The hyperGAT based framework performs slightly better than hyperGCN.\n\n\n\n\n\n\n\n%To further investigate the influence of high-order interference on the ITE estimation results for different nodes, we divide the nodes into different groups with respect to their positions in hypergraph and ratios of treatment assignment among its neighbors. More specifically, for each node $i$, we quantitatively describe its position on hypergraph by summing up the size of all hyperedges containing it, i.e., $|\\mathcal{N}_i|$, where  $\\mathcal{N}_i=\\bigcup_{e\\in\\mathcal{E}_i}\\{j\\in\\mathcal{N}_e\\}$, here $\\bigcup$ denotes union operation. Intuitively, this value denotes the number of neighbor nodes (not including $i$ itself, and may be counted repeatedly in different hyperedges) of node $i$, and reflects the importance of the whole hypergraph to node $i$ in some degree. For each node $i$, we also calculate the ratio of the number of neighbors with same treatment assignment with $t_i$ to the number of all the neighbors. Specifically, we compute $r(i)=\\frac{\\sum_{j\\in \\mathcal{N}_i}\\mathbf{1}(t_j=t_i)}{|\\mathcal{N}_i|}$. By intuition, the value of $r(i)$ indicates whether the node belongs to the majority w.r.t. treatment assignment inside its neighborhood (the higher value denotes that it is closer to majority). We divide all the nodes in GoodReads dataset w.r.t. their values of $|\\mathcal{N}_i|$ and $r(i)$ into $6\\times 6$ grids. In each group, we make comparison between the ITE estimation results made (i) with/without the hypergraph; and (ii) with the original hypergraph and with the projected graph. Fig.~\\ref{fig:heatmap} shows the difference between ITE estimation results in different grids. \n\n%To provide more intuitive understanding, we further conduct case studies of the difference between ITE estimations with hypergraph and projected graph in Fig.~\\ref{fig:case}, where several well-known books with their $r(i)$ and $|\\mathcal{N}(i)|$ are shown. We can observe that the difference of ITE estimation in both of these two comparisons becomes stronger when $r(i)$ is smaller and $|\\mathcal{N}_i|$ is higher. This observation implicitly indicates that the nodes with more high-order relation connection with others more tend to be interfered by its neighbors. Besides, those nodes with \"minority\" treatment assignment are prone to be influenced by others. For example, the book ``Peter Pan\" and ``Harry Potter\" with consistent treatment assignment among neighbors, and small number of neighbors, are relatively less impacted by the interference, while the book ``Oddhopper Opera\" with minority treatment assignment, and many neighbors sharing the same authors, suffers much more from the interference on hypergraph. \n%To provide more intuitive illustration of this observation, we select some books corresponding to different grids. \n\n%To summarize, we consistently observe the benefits of modeling high-order interference on large hyperedges. Although the interference becomes pronounced as the size of one's neighborhood increases, it can also be affected by the agreement of treatment assignments between the individual node and its neighborhood.\n\n\n% %\\subsection{Q4: Ablation Studies}\n% \\subsection{Ablation Study and Sensitivity Analysis}\n\n% \\begin{figure}\n% \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/ablation_GoodReads.pdf}\n%         \\caption{GoodReads}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/ablation_contact.pdf}\n%         \\caption{Contact}\n%     \\end{subfigure}\n%     \\vspace{-3mm}\n%   \\caption{Ablation studies of different variants of the proposed framework \\mymodel.}\n%   \\label{fig:ablation}\n% %\\vspace{-3mm}\n% \\end{figure}\n\n% \\begin{figure}\n% \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/parameter_GoodReads_balancing.pdf}\n%         \\caption{Balancing weight}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/parameter_GoodReads_dimension.pdf}\n%         \\caption{Representation dimension}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/parameter_GoodReads_attention.pdf}\n%         \\caption{\\# of attention head}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.23\\textwidth}\n%         \\centering\n%         \\includegraphics[height=1.in]{figs/parameter_GoodReads_regularization.pdf}\n%         \\caption{Regularization weight}\n%     \\end{subfigure}\n%     \\vspace{-3mm}\n%   \\caption{Comparison of ITE estimation performance of the proposed framework \\mymodel~ under different parameters or model structures on GoodReads dataset.}\n%   \\label{fig:param}\n% %\\vspace{-3mm}\n% \\end{figure}\n\n% To investigate the effectiveness of different components in the proposed framework, we conduct ablation studies by comparing \\mymodel~ with multiple variants. Specifically, we consider the following variants: 1) we apply the proposed model \\mymodel~ on the projected graph, and denote this variant as \\textbf{\\mymodel-P}; 2) we replace the hypergraph neural network module with a graph neural network module with same number of layers, and then apply it on the projected graph. This variant is denoted by \\textbf{\\mymodel-G}; 3) we remove the balancing techniques, and denote this variant as \\textbf{\\mymodel-NB}. Here we show the comparison of the ITE estimation performance of the proposed framework and the above variants in Fig.~\\ref{fig:ablation}. Although Fig.~\\ref{fig:ablation} only presents the results in the linear setting on GoodReads dataset,  similar observations can also be found in other settings and other datasets. We observe that, both \\textbf{\\mymodel-P} and \\textbf{\\mymodel-G} have worse ITE estimation performance  than \\mymodel, which indicates that the high-order information does contribute to the\n% modeling of interference and enhance the performance of ITE estimation. Besides, the comparison between \\textbf{\\mymodel-NB} and \\mymodel~ suggests that the representation balancing techniques also lead to better ITE estimation performance.\n\n% %\\subsection{Q5: Sensitivity Analysis}\n% To evaluate the robustness of the proposed framework, we present the ITE estimation performance of \\mymodel~under different settings of model structure and parameters in Fig.~\\ref{fig:param}. More specifically, we vary the value of the balancing weight from $\\{0.1, 0.01, 0.001, 0.0001\\}$, and vary the representation dimension size from $\\{16, 32, 64, 128\\}$. We also vary the number of attention head from $\\{1,2,3,4\\}$, then change the parameter regularization weight from $\\{0.0001, 0.001, 0.01, 0.1\\}$.\n% % We implement the hypergraph neural network module with hypergraph convoluntional network (hyperGCN) \\cite{bai2021hypergraph} or hypergraph attention network (hyperGAT) \\cite{bai2021hypergraph}. \n% Generally, our framework is not very sensitive to hyparameter settings, but proper fine-tuning of the hyperparameters is still beneficial for the ITE estimation performance. %The hyperGAT based framework performs slightly better than hyperGCN.\n\\vspace{-2mm}\n"
                }
            },
            "section 5": {
                "name": "Related Work",
                "content": "\n\\vspace{-1mm}\n\\xhdr{Causal studies under network interference.} There have been many causal studies \\cite{aronow2017estimating,basse2018analyzing,imai2020causal,kohavi2013online,tchetgen2012causal,ugander2013graph,yuan2021causal,ma2021causal,bhattacharya2020causal} which address the existence of network interference. These works mainly include the following categories: (i) \\textit{Random assignment strategy under interference} \\cite{aronow2017estimating,basse2018analyzing,imai2020causal,ugander2013graph,fatemi2020minimizing}. These works focus on experimental studies under interference (without SUTVA assumption). %Some of them are based on cluster random assignment [], which treats instances at a group level. \nIn some studies \\cite{karrer2021network}, strong interference is assumed to exist within each group while there is no interference across different groups; (ii) \\textit{Causal effect estimation on observational data with interference} \\cite{rakesh2018linked,ma2021causal,arbour2016inferring,tchetgen2021auto}. Different from the experimental studies which can design assignment strategy, another line of works (and also our work) assume interferences exist across individuals in the observational data. They relax the SUTVA assumption and define the potential outcome with a function that takes the instance covariates and treatment assignment of each individual and other interacted individuals as input. Among them, Rakesh et al. \\cite{rakesh2018linked} propose a Linked\nCausal Variational Autoencoder (LCVA) framework to estimate the causal effect of a treatment on an outcome with the existence of interference between pairs of instances. Different from these works that focus on pairwise spillover effects, Ma et al. \\cite{ma2021causal} consider the spillover effect in network structure, and propose a graph neural network (GNN) \\cite{kipf2016semi} based framework for causal effect estimation under network interference. %GNNs are used as effective tools to capture the dependencies between nodes and links in the given graph. \nHowever, these works are still limited in pairs of individuals or ordinary graphs and lack consideration of high-order interference.\n{\nAnother line of studies is bipartite causal inference  \\cite{zigler2021bipartite,pouget2019variance}. Traditionally, bipartite causal inference involves two types of units: interventional/outcome units. Interventional units are assigned with treatments, and outcomes are observed from outcome units.  \n%In this setting, interference means the outcome of an outcome unit is influenced by multiple interventional units. \nAlthough this setup is different from ours, considering that there is a node-hyperedge bipartite corresponding to each hypergraph (and ordinary graph), thus %bipartite can also fit in some scenarios of hypergraph. Although \nthe two modeling approaches (bipartite and hypergraph) are conceptually similar. Nevertheless, % each has its own advantages in practice. \nwe argue hypergraph is a more appropriate framing in many scenarios since: i) hypergraph does not require instantiating edges as additional nodes, or treating these two kinds of nodes differently, thus more computationally efficient; ii) hypergraph has the potential to be more convenient and efficient when generalizing to new hyperedges, while bipartite needs to generate both new nodes for the new hyperedges and their associated new edges.\n}\n\n\\xhdr{Hypergraph algorithms and neural networks.} To process hypergraph structures for downstream tasks, a line of works simplify the hypergraph structure by taking abstract representations of complicated multi-way interactions \\cite{benson2018simplicial,li2013link,xu2013hyperlink,yadati2018link,zhang2018beyond}. Other works directly tackle the original hypergraph structure \\cite{arya2018exploiting,benson2018sequences,feng2019hypergraph,huang2015scalable,sharma2014predicting,yadati2018hypergcn}.  \nRecently, numerous works have studied on hypergraph neural networks \\cite{bai2021hypergraph,yadati2018hypergcn}. Feng el al. \\cite{feng2019hypergraph} propose hypergraph neural networks (HGNN) framework to encode high-order data correlation in a hypergraph structure. A hyperedge convolution operation is designed for representation learning. Bai et al. \\cite{bai2021hypergraph} introduce two end-to-end trainable operators hypergraph convolution and hypergraph attention to learn node representations in hypergraphs. Yadati et al. \\cite{yadati2018hypergcn} develop a self-attention based hypergraph neural network Hyper-SAGNN, which is applicable to homogeneous or heterogeneous hypergraphs with variable hyperedge sizes. Jiang et al. \\cite{jiang2019dynamic} propose a dynamic hypergraph neural network (DHGNN) which can dynamically update hypergraph structure on each layer. \n\\vspace{-2mm}\n"
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\n\\vspace{-1mm}\nIn this paper, we study an important research problem of individual treatment effect estimation with the existence of high-order interference on hypergraphs. We identify and analyze the influence of high-order interference in causal effect estimation. To address this problem, we propose a novel framework \\mymodel, which estimates the ITEs based on representation learning. More specifically, \\mymodel~ learns the representation of confounders, models the high-order interference with a hypergraph neural network module, then predicts the potential outcomes for each instance with the learned representations. We conduct extensive experiments to evaluate the proposed framework, where the results consistently validate the effectiveness of \\mymodel~ in ITE estimation under different interference scenarios. \n%An interesting future direction is to apply this proposed framework into real-world applications, such as evaluating new features launched on social media without A/B testing.\n\n\n\n\\clearpage\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{ref}\n\n\\clearpage\n\\appendix\n"
            },
            "section 7": {
                "name": "Notation Table",
                "content": "\nWe use Table~ \\ref{tab:notation} to list the most important notations used in this paper.\n\n\n"
            },
            "section 8": {
                "name": "More Experimental Results",
                "content": "\n",
                "subsection 8.1": {
                    "name": "ITE Estimation Performance under Different Settings on All the Datasets",
                    "content": "\nIn this section, we show the ITE estimation performance under different settings (including the linear and quadratic settings with $\\beta$ among $\\{1.0, 3.0, 5.0\\}$) on all the datasets in Fig.~\\ref{fig:ITE_complete}. We can observe that the proposed \\mymodel~ consistently outperforms the baselines under different settings on all the datasets. The superiority of our framework against baselines becomes more obvious when $\\beta$ is larger (i.e., the interference is stronger), because our framework can better handle the interference in the hypergraph.\n\n\n\n\n\n% \\begin{figure}\n% \\centering\n%      \\includegraphics[width=.42\\textwidth,height=1.6in]{figs/case.pdf}\n%      \\vspace{-0.15in}\n%       \\caption{Case studies: the difference between ITE estimations with hypergraph and projected ordinary graph on GoodReads dataset. The difference becomes stronger when $r(i)$ is lower and and $|\\mathcal{N}(i)|$ is higher.}\n% %      \\vspace{-0.1in}\n%       \\label{fig:case}\n% \\end{figure}\n\n%\\subsection{Q3: High-order Interference for Different Individuals: Overview and Case Studies}\n% node: d\n%To further investigate the influence of interference for different nodes, we conduct case studies on the GoodReads dataset. \n\n"
                },
                "subsection 8.2": {
                    "name": "Case Studies",
                    "content": "\nWe further conduct case studies to investigate how the proposed method acts on individuals in responding to their neighboring nodes (i.e., the size of one's neighborhood and the homophily of treatment assignments within one's neighborhood). The neighborhood of $i$ is defined as the set of nodes which are connected with $i$ via any hyperedges, i.e., $\\mathcal{N}_i=\\bigcup_{e\\in\\mathcal{E}_i}\\{j\\in\\mathcal{N}_e\\}$. The homophily of treatment assignment is defined as the ratio of neighboring nodes which share the same treatment assignment as oneself, i.e., $r(i)=\\frac{\\sum_{j\\in \\mathcal{N}_i}\\mathbf{1}(t_j=t_i)}{|\\mathcal{N}_i|}$. In Fig.~\\ref{fig:heatmap}, we show the difference between the ITE estimation results made %(i) with/without the hypergraph; and (ii) \nwith the original hypergraph and with the projected graph, w.r.t. $\\mathcal{N}_i$ and $r(i)$. Overall, we see larger divergences on individuals with a larger neighborhood size but less agreement with their neighbors in terms of treatment assignments. \nIn Fig.~\\ref{fig:case-study}, we further showcase the insights by presenting several representative children books on the GoodReads dataset. For example, the author of ``Peter Pan'' had not published many works but these books all received good rating scores, leading to a ``consistent\" reputation of the author. Therefore, the outcome of the book ``Peter Pan'' is less impacted by the high-order interference among its neighbors. \nOn the other hand, the high rating score of the book ``Oddhopper Opera'' differs from most of its neighbors, leading to a mixed reputation of the author. In this case, the potential outcome is more likely to be affected by the high-order interference on the hypergraph.\n\n\n\n\n"
                }
            },
            "section 9": {
                "name": "Details of Experimental Settings",
                "content": "\nAll the experiments are conducted under the following environment:\n\\begin{itemize}\n    \\item Operating system: Ubuntu 18.04\n    \\item GPU memory: 16GB\n    \\item Pytorch 1.9.0, Cuda ToolKit 11.1, cuDNN 8.0.5\n\\end{itemize}\n\n\\noindent\\textbf{Baseline parameter settings.} For the baselines CEVAE, CFR, Netdeconf, GNN-HSIC, and GCN-HSIC, we set the representation dimension as 32, 32, 100, 64, respectively. The numbers of training epochs for these baselines are set as $500$. The number of samples in CEVAE in training is set as $5$ by default.\n\n\n% \\begin{figure*}\n% \\centering\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_pehe_linear.pdf}\n%         \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, CT}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_ate_linear.pdf}\n%         \\caption{Linear, $\\epsilon_{ATE}$, CT}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_pehe_quadratic.pdf}\n%         \\caption{Quadratic, $\\sqrt{\\epsilon_{PEHE}}$, CT}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_contact_ate_quadratic.pdf}\n%         \\caption{Quadratic, $\\epsilon_{ATE}$, CT}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_linear_2.pdf}\n%         \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, GR}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_linear.pdf}\n%         \\caption{Linear, $\\epsilon_{ATE}$, GR}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_quadratic.pdf}\n%         \\caption{{\\small Quadratic,$\\sqrt{\\epsilon_{PEHE}}$,GR}}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_quadratic.pdf}\n%         \\caption{Quadratic, $\\epsilon_{ATE}$, GR}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_pehe_linear.pdf}\n%         \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, MS}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_ate_linear.pdf}\n%         \\caption{Linear, $\\epsilon_{ATE}$, MS}\n%     \\end{subfigure}\n%   \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_pehe_quadratic.pdf}\n%         \\caption{Quadratic, $\\sqrt{\\epsilon_{PEHE}}$, WC}\n%     \\end{subfigure}\n%     \\begin{subfigure}[b]{0.235\\textwidth}\n%         \\centering\n%         \\includegraphics[height=0.95in]{figs/ITE_Microsoft_ate_quadratic.pdf}\n%         \\caption{Quadratic, $\\epsilon_{ATE}$, WC}\n%     \\end{subfigure}\n%   \\caption{Comparison of the performance of ITE estimation under different settings. ``CT\", ``GR\" and ``WC\" stand for Contact, GoodReads and Workplace Communication datasets, respectively.}\n%   \\label{fig:ITE_complete}\n% \\vspace{-3mm}\n% \\end{figure*}\n\n\n"
            }
        },
        "tables": {
            "tab:baseline_linear": "\\begin{table}[]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{2.5pt}\n \\caption{ITE estimation performance (mean $\\pm$ standard error). ``CT\", ``GR\" and ``MS\" stand for Contact, GoodReads and Microsoft Teams datasets, respectively.}\n %\\vspace{-2mm}\n \\label{tab:baseline_linear}\n\\begin{tabular}{l|l|cc|cc}\n\\hline\n              \\multicolumn{1}{c|}{}                          &                           & \\multicolumn{2}{c|}{Linear}                                                                     & \\multicolumn{2}{c}{Quadratic}         \\\\\\cline{3-6}\n\\multicolumn{1}{c|}{\\multirow{-2}{*}{Data}} & \\multirow{-2}{*}{Method} & $\\sqrt{\\epsilon_{PEHE}}$                                &  $\\epsilon_{ATE}$                                                           &  $\\sqrt{\\epsilon_{PEHE}}$                 & $\\epsilon_{ATE}$       \\\\\n\\hline\nCT &  LR & $25.41$~\\scriptsize{$\\pm0.04$}         & $9.11$~\\scriptsize{$\\pm0.09$}   & $38.22$~\\scriptsize{$\\pm0.77$} & $20.28$~\\scriptsize{$\\pm0.38$}                     \\\\\n & CEVAE                     & $22.88$~\\scriptsize{$\\pm1.07$}         & $8.29$~\\scriptsize{$\\pm0.69$ } & $35.28$~\\scriptsize{$\\pm 0.75$} & $18.22$~\\scriptsize{$\\pm 0.76$}                  \\\\\n & CFR                       & $24.04$~\\scriptsize{$\\pm0.75$}         & $7.17$~\\scriptsize{$\\pm0.43$}  & $32.24$~\\scriptsize{$\\pm 1.01$} & $17.28$~\\scriptsize{$\\pm 0.75$}                \\\\\n & Netdeconf            & $10.22$~\\scriptsize{$\\pm0.47$}         & $4.29$~\\scriptsize{$\\pm0.13$} & $21.23$~\\scriptsize{$\\pm 0.72$} & $11.39$~\\scriptsize{$\\pm 0.74$ }      \\\\\n & GNN-HSIC               &   $7.42$~\\scriptsize{$\\pm0.39$}          & $2.06$~\\scriptsize{$\\pm 0.03$} & $16.28$~\\scriptsize{$\\pm 0.24$} & $7.28$~\\scriptsize{$\\pm 0.39$}   \\\\\n & GCN-HSIC               & $7.28$~\\scriptsize{$\\pm 0.44$}          & $2.08$~\\scriptsize{$\\pm 0.04$}  & $14.23$~\\scriptsize{$\\pm 0.20$} & $6.27$~\\scriptsize{$\\pm 0.15$} \\\\\n & \\mymodel          & {\\bm{$3.45$}~\\scriptsize{\\bm{$\\pm 0.27$}}  }                  & {\\bm{$1.39$}~\\scriptsize{\\bm{$\\pm0.03$}}     }                                   & \\bm{$9.20$}~\\scriptsize{\\bm{$\\pm 0.09$}}  & \\bm{$2.24$}~\\scriptsize{\\bm{$\\pm 0.07$}}       \\\\\n\\hline\nGR &  LR  & $23.01$~\\scriptsize{$\\pm 0.04$}         & $13.42$~\\scriptsize{$\\pm 0.12$}  & $48.56$~\\scriptsize{$\\pm 1.02$} & $31.19$~\\scriptsize{$\\pm 0.47$}                    \\\\\n & CEVAE                     & $22.69 $~\\scriptsize{$\\pm 0.03$}        & $12.49$~\\scriptsize{$\\pm 0.06$ }                 & $45.21$~\\scriptsize{$\\pm 3.10$} & $29.22$~\\scriptsize{$\\pm 0.44$}        \\\\\n & CFR                       & $20.30$~\\scriptsize{$\\pm 0.03$}         & $13.21$~\\scriptsize{$\\pm 0.09$ }                 & $41.72$~\\scriptsize{$\\pm 0.72$} & $26.28$~\\scriptsize{$\\pm 0.43$}        \\\\\n & Netdeconf                 & $18.39$~\\scriptsize{$\\pm 0.19$}         & $12.20$~\\scriptsize{$\\pm 0.03$}  & $35.18$~\\scriptsize{$\\pm 0.78$} & $21.20$~\\scriptsize{$\\pm 0.76$}            \\\\\n & GNN-HSIC                 & $17.20$~\\scriptsize{$\\pm 0.23$}         & $12.18$~\\scriptsize{$\\pm 0.13$}  & $27.22$~\\scriptsize{$\\pm 0.78$} & $16.87$~\\scriptsize{$\\pm 0.47$}             \\\\\n & GCN-HSIC                 & $16.01$~\\scriptsize{$\\pm0.20$}          & $12.06$~\\scriptsize{$\\pm 0.15$} & $25.42$~\\scriptsize{$\\pm 0.76$} & $16.28$~\\scriptsize{$\\pm 0.76$}     \\\\\n & \\mymodel                      & \\bm{$15.68$}~\\scriptsize\\bm{{$\\pm0.21$}} & \\bm{$11.81$}~\\scriptsize\\bm{{$\\pm0.15$} }           & \\bm{$19.23$}~\\scriptsize\\bm{{$\\pm 0.44$}} & \\bm{$13.33$}~\\scriptsize{\\bm{$\\pm 0.27$}   }           \\\\\n\\hline\nMS & LR    & $22.80$~\\scriptsize{$\\pm 0.64$}                   & $21.41 \\pm$~\\scriptsize{$0.74$}                                          & $414.17$~\\scriptsize{$\\pm 3.94$  }                    & $192.80$~\\scriptsize{$\\pm 2.97$}      \\\\\n &CEVAE           & $19.36$~\\scriptsize{$\\pm 0.80$}                    & $8.63$~\\scriptsize{$\\pm 0.78$}        & $315.01$~\\scriptsize{$\\pm2.53$} & $188.47$~\\scriptsize{$\\pm4.27$}            \\\\\n &CFR             & $25.23$~\\scriptsize{$\\pm  0.01$}                   & $18.28$~\\scriptsize{$\\pm  0.02$}          & $392.56$~\\scriptsize{$\\pm 4.33$} & $189.75$~\\scriptsize{$\\pm4.80$}  \\\\\n &Netdeconf       & $11.11$~\\scriptsize{$\\pm 0.01$}                  & $9.22$~\\scriptsize{$\\pm 0.03$ }      & $241.02$~\\scriptsize{$\\pm 2.32$ }                    & $147.29$~\\scriptsize{$\\pm1.04$}                                                 \\\\\n &GNN-HSIC    &$9.38$~\\scriptsize{$\\pm 0.44$}          & $6.91$~\\scriptsize{$\\pm  0.38$}             & $114.28$~\\scriptsize{$\\pm 3.62$}                     & $81.21$~\\scriptsize{$\\pm 2.53$}                               \\\\               \n &GCN-HSIC  &    $8.27$~\\scriptsize{$\\pm 0.41$} & $6.60$~\\scriptsize{$\\pm 0.48$} &   $109.57$~\\scriptsize{$\\pm3.85$}  & $77.75 $~\\scriptsize{$\\pm3.93$}  \\\\\n &\\mymodel          & {\\bm{$5.13$}~\\scriptsize{\\bm{$\\pm 0.56$}}}               & {\\bm{$4.46$}~\\scriptsize{\\bm{$\\pm0.61$}} }    & \\bm{$81.08$}~\\scriptsize\\bm{{$\\pm 0.37$}  }            & \\bm{$74.41$}~\\scriptsize{\\bm{$\\pm0.42$}  }                                   \\\\\n\\hline\n\\end{tabular}\n\\end{table}",
            "tab:notation": "\\begin{table}[H]\n  \\caption{Notation.}\n  \\label{tab:notation}\n  \\vspace{-4mm}\n  \\begin{tabular}{ll}\n    \\toprule\n    Notation & Definition \\\\ \n    \\midrule\n    $\\mathcal{H}$          & hypergraph\\\\\n    $\\mathcal{V},\\mathcal{E}$          & the set of nodes/hyperedges \\\\\n    $\\mathbf{H}$          & hypergraph structure matrix\\\\\n    $n$               & the number of nodes \\\\\n    $m$               & the number of hyperedges\\\\\n    $\\mathcal{N}_i$ & the set of neighboring nodes for the $i$-th node \\\\\n    $\\mathcal{N}_e$ & the set of nodes on the hyperedge $e$ \\\\\n    $\\mathcal{E}_i$ & the set of hyperedges which contains the $i$-th node\\\\\n    $\\mathbf{X},\\mathbf{x}_i$ & features of all nodes/the $i$-th node\\\\\n    $\\mathbf{T},{t}_i$ & treatment assignment of all nodes/the $i$-th node\\\\\n    $\\mathbf{Y},{y}_i$ & observed outcome of all nodes/the $i$-th node\\\\\n    ${y}^1_i,y_i^0$ & potential outcomes of the $i$-th node\\\\\n    $\\Phi_Y(\\cdot)$ & potential outcome function \\\\\n    $\\tau_i,\\hat{\\tau}_i$ & true/predicted ITE for the $i$-th node\\\\\n    $y_i, \\hat{y}_i$ & true/predicted outcome for the $i$-th node\\\\\n    $(\\cdot)_{-i}$ & variables for all the nodes except the $i$-th node\\\\\n    $\\delta_i$ & the spillover effect of the $i$-th node\\\\\n    $\\mathtt{SMR}(\\cdot)$ & summary function\\\\\n    $\\mathbf{O}, \\mathbf{o}_i$ & the environment information of the $i$-th node\\\\\n    $\\mathbf{Z},\\mathbf{z}_i$ & confounder representations of all nodes/the $i$-th node\\\\\n    $\\Psi(\\cdot)$ & interference representation learner\\\\\n    $f_1(\\cdot),f_0(\\cdot)$ & potential outcome prediction functions\\\\\n    $\\mathbf{P},\\mathbf{p}_i$ & interference representations of all nodes/the $i$-th node\\\\\n    $r(i)$ & the ratio of the treatment assignment of the $i$-node \\\\&in its neighborhood\\\\\n    \\bottomrule\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:interference": "\\begin{figure}\n\\begin{subfigure}[b]{0.5\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{figs/illustration-hypergraph.pdf}\n\\caption{Hypergraph}\\label{fig:hypergraph}\n\\end{subfigure}\n\\qquad\n\\begin{subfigure}[b]{0.3\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{figs/illustration-ordinary-graph-no-interference.pdf}\n\\caption{Ordinary graph}\\label{fig:ordinary-graph}\n\\end{subfigure}\n\\\\\n\\begin{subfigure}[b]{\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{figs/illustration-high-order-interference.pdf}\n\\caption{First, second, and third-order interferences with $u_1$.}\n\\label{fig:high-order-interference}\n\\end{subfigure}\n\\vspace{-5mm}\n\\caption{(a) An illustrative example of group interactions on a hypergraph, where each circle represents a hyperedge (group); (b) An ordinary graph projected from this hypergraph; (c) Illustration of interferences with $u_1$ from its neighbors on the hypergraph. Note interference on (b) is pairwise (first-order only) while higher-order interference exists on the original hypergraph (a).\n}\n\\vspace{-2mm}\n\\label{fig:interference}\n\\end{figure}",
            "fig:model": "\\begin{figure*}[t]\n\\centering\n     \\includegraphics[width=.98\\textwidth,height=1.3in]{figs/framework_cropped.pdf}\n     \\vspace{-0.15in}\n      \\caption{An illustration of the proposed framework \\mymodel, which includes three key components:  confounder representation learning, interference modeling, and outcome prediction. %Here we mainly focus on a node $i$ as an example for illustration. %The upper part describes the three main components in \\mymodel, including confounder representation learning, interference modeling, and outcome prediction. The lower part depicts more details of the hypergraph module in interference modeling component.\n      }\n      \\vspace{-0.1in}\n       \\label{fig:model}\n\\end{figure*}",
            "fig:model_hyper": "\\begin{figure}[t]\n\\centering\n     \\includegraphics[width=.48\\textwidth,height=1.3in]{figs/framework_hyper.pdf}\n     \\vspace{-0.2in}\n      \\caption{A detailed illustration of the hypergraph module in the interference modeling component of \\mymodel. Here we use the node $v_1$ (highlighted in yellow) as an example.\n      }\n     \\vspace{-0.05in}\n       \\label{fig:model_hyper}\n\\end{figure}",
            "fig:ITE": "\\begin{figure}[t]\n\\centering\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_linear.pdf}\n        \\caption{$\\sqrt{\\epsilon_{PEHE}}$}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_linear.pdf}\n        \\caption{$\\epsilon_{ATE}$}\n    \\end{subfigure}\n   \\vspace{-7mm}\n  \\caption{Comparison of the performance of ITE estimation under different values of $\\beta$ in linear setting on GoodReads.}\n  \\label{fig:ITE}\n\\vspace{-1mm}\n\\end{figure}",
            "fig:ablation": "\\begin{figure}\n\\begin{subfigure}[b]{0.22\\textwidth}\n        \\centering\n        \\includegraphics[height=.8in]{figs/ablation_GoodReads_pehe.pdf}\n        \\caption{GoodReads, $\\sqrt{\\epsilon_{PEHE}}$}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.22\\textwidth}\n        \\centering\n        \\includegraphics[height=.8in]{figs/ablation_GoodReads_ATE.pdf}\n        \\caption{GoodReads, $\\epsilon_{ATE}$}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.22\\textwidth}\n        \\centering\n        \\includegraphics[height=.8in]{figs/ablation_contact_pehe.pdf}\n        \\caption{Contact, $\\sqrt{\\epsilon_{PEHE}}$}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.22\\textwidth}\n        \\centering\n        \\includegraphics[height=.8in]{figs/ablation_contact_ate.pdf}\n        \\caption{Contact, $\\epsilon_{ATE}$}\n    \\end{subfigure}\n    \\vspace{-1mm}\n  \\caption{Ablation studies of different variants of our framework \\mymodel. Results (mean and standard error) are reported under the linear setting but similar patterns can be found under the quadratic setting and on all datasets. }\n  \\label{fig:ablation}\n\\vspace{-3mm}\n\\end{figure}",
            "fig:hyper_k": "\\begin{figure}\n\\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figs/hypersize_GoodReads.pdf}\n        \\caption{GoodReads, Linear}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.23\\textwidth}\n        \\centering\n        \\includegraphics[height=1.in]{figs/hypersize_contact.pdf}\n        \\caption{GoodReads, Quadratic}\n    \\end{subfigure}\n    \\vspace{-2mm}\n  \\caption{ITE estimation performance of \\mymodel~/ \\mymodel-G on hypergraphs with hyperedge size no more than $k$.}\n  \\label{fig:hyper_k}\n\\vspace{-1mm}\n\\end{figure}",
            "fig:param": "\\begin{figure}\n\\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.9in]{figs/parameter_GoodReads_balancing.pdf}\n        \\caption{Balancing weight}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=.9in]{figs/parameter_GoodReads_dimension.pdf}\n        \\caption{Representation dimension}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=.9in]{figs/parameter_GoodReads_attention.pdf}\n        \\caption{\\# of attention head}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=.9in]{figs/parameter_GoodReads_regularization.pdf}\n        \\caption{Regularization weight}\n    \\end{subfigure}\n  %  \\vspace{-3mm}\n  \\caption{ITE estimation performance (mean and standard error) of the proposed framework \\mymodel~ under different parameters or model structures on GoodReads dataset.}\n  \\label{fig:param}\n%\\vspace{-1mm}\n\\end{figure}",
            "fig:ITE_complete": "\\begin{figure*}\n\\centering\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_contact_pehe_linear.png}\n        \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, CT}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_contact_ate_linear.png}\n        \\caption{Linear, $\\epsilon_{ATE}$, CT}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_contact_pehe_quadratic.png}\n        \\caption{Quadratic, $\\sqrt{\\epsilon_{PEHE}}$, CT}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_contact_ate_quadratic.png}\n        \\caption{Quadratic, $\\epsilon_{ATE}$, CT}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_linear_2.png}\n        \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, GR}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_linear.png}\n        \\caption{Linear, $\\epsilon_{ATE}$, GR}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_GoodReads_pehe_quadratic.png}\n        \\caption{{\\small Quadratic,$\\sqrt{\\epsilon_{PEHE}}$,GR}}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_GoodReads_ate_quadratic.png}\n        \\caption{Quadratic, $\\epsilon_{ATE}$, GR}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_Microsoft_pehe_linear.png}\n        \\caption{Linear, $\\sqrt{\\epsilon_{PEHE}}$, MS}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_Microsoft_ate_linear.png}\n        \\caption{Linear, $\\epsilon_{ATE}$, MS}\n    \\end{subfigure}\n  \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_Microsoft_pehe_quadratic.png}\n        \\caption{Quadratic, $\\sqrt{\\epsilon_{PEHE}}$, MS}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.235\\textwidth}\n        \\centering\n        \\includegraphics[height=0.95in]{figs/ITE_Microsoft_ate_quadratic.png}\n        \\caption{Quadratic, $\\epsilon_{ATE}$, MS}\n    \\end{subfigure}\n  \\caption{Comparison of the performance of ITE estimation under different settings. ``CT\", ``GR\" and ``MS\" stand for Contact, GoodReads and Microsoft Teams datasets, respectively.}\n  \\label{fig:ITE_complete}\n\\vspace{-3mm}\n\\end{figure*}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n      \\mathbf{o}_i = \\mathtt{SMR}(\\mathbf{H}, \\mathbf{T}_{-i}, \\mathbf{X}_{-i}).\n  \\end{equation}",
            "eq:2": "\\begin{equation}\n\\label{eq:laplacian}\n    \\mathbf{L} = \\mathbf{D}^{-1/2}\\mathbf{HB}^{-1}\\mathbf{H}^\\top\\mathbf{D}^{-1/2}.\n\\end{equation}",
            "eq:3": "\\begin{equation}\n\\label{eq: hgcn}\n    \\mathbf{P}^{(l+1)} = \\text{LeakyReLU}\\left(\\mathbf{L}\\mathbf{P}^{(l)}\\mathbf{W}^{(l+1)}\\right),\n\\end{equation}",
            "eq:4": "\\begin{equation}\n    \\alpha_{i,e} = \\frac{\\text{exp}(\\sigma(\\text{sim}(\\mathbf{z}_i \\mathbf{W}_a,\\mathbf{z}_e \\mathbf{W}_a)))}{\\sum_{k \\in \\mathcal{E}_i} \\text{exp}(\\sigma(\\text{sim}(\\mathbf{z}_i \\mathbf{W}_a,\\mathbf{z}_k \\mathbf{W}_a)))},\n\\end{equation}",
            "eq:5": "\\begin{equation}\n    \\text{sim}(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{a}^\\top[\\mathbf{x}_i\\|\\mathbf{x}_j],\n\\end{equation}",
            "eq:6": "\\begin{align}\n\\hat{y}^1_i = f_1([\\mathbf{z}_i \\| \\mathbf{p}_i]),\\,\\hat{y}^0_i = f_0([\\mathbf{z}_i \\| \\mathbf{p}_i]),\n\\end{align}",
            "eq:7": "\\begin{equation}\n    \\mathcal{L}=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2 + \\alpha\\mathcal{L}_b + \\lambda\\|\\mathbf{\\Theta}\\|^2,\n\\end{equation}",
            "eq:8": "\\begin{equation}\n    y_i = f_{y,0}(\\mathbf{x}_i) + \\overbrace{\\gamma f_t(t_i, \\mathbf{x}_i)}^{\\mathclap{\\text{individual treatment effect (ITE)}}} + \\underbrace{\\beta f_s(\\mathbf{T}, \\mathbf{X}, \\mathbf{H})}_{\\mathclap{\\text{hypergraph spillover effect}}} +~\\epsilon_{y_i},\n\\end{equation}",
            "eq:9": "\\begin{equation}\n    f_{y,0}=\\mathbf{w}_0\\mathbf{x}_i,\n\\end{equation}",
            "eq:10": "\\begin{equation}\n    \\mathbf{x}_i \\sim \\mathcal{N}(0,\\mathbf{I}), {t}_{i} \\sim Ber(\\text{sigmoid}(\\mathbf{x}_i\\mathbf{v}_t)),\n\\end{equation}",
            "eq:11": "\\begin{equation}\n    \\sqrt{\\epsilon_{PEHE}} = \\sqrt{\\frac{1}{n}\\sum_{i\\in[n]}(\\tau_i-\\hat{\\tau}_i)^2}, \\,\\,\\epsilon_{ATE} = |\\frac{1}{n}\\sum_{i\\in [n]}{\\tau_i}-\\frac{1}{n}\\sum_{i\\in [n]}\\hat{\\tau}_i|.\n\\end{equation}"
        }
    }
}