{
    "meta_info": {
        "title": "PointPainting: Sequential Fusion for 3D Object Detection",
        "abstract": "Camera and lidar are important sensor modalities for robotics in general and\nself-driving cars in particular. The sensors provide complementary information\noffering an opportunity for tight sensor-fusion. Surprisingly, lidar-only\nmethods outperform fusion methods on the main benchmark datasets, suggesting a\ngap in the literature. In this work, we propose PointPainting: a sequential\nfusion method to fill this gap. PointPainting works by projecting lidar points\ninto the output of an image-only semantic segmentation network and appending\nthe class scores to each point. The appended (painted) point cloud can then be\nfed to any lidar-only method. Experiments show large improvements on three\ndifferent state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on\nthe KITTI and nuScenes datasets. The painted version of PointRCNN represents a\nnew state of the art on the KITTI leaderboard for the bird's-eye view detection\ntask. In ablation, we study how the effects of Painting depends on the quality\nand format of the semantic segmentation output, and demonstrate how latency can\nbe minimized through pipelining.",
        "author": "Sourabh Vora, Alex H. Lang, Bassam Helou, Oscar Beijbom",
        "link": "http://arxiv.org/abs/1911.10150v2",
        "category": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "additionl_info": "11 pages, 6 figures, 8 tables. v1 is initial submission to CVPR 2020.  v2 is final version accepted for publication at CVPR 2020"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": " \\label{sec:intro}\n\\squeeze\n\n\n\n\n\n\nDriven partially by the interest in self-driving vehicles, significant research effort has been devoted to 3D object detection.\nIn this work we consider the problem of fusing a lidar point cloud with an RGB image.\nThe point cloud provides a very accurate range view, but with low resolution and texture information.\nThe image, on the other hand, has an inherent depth ambiguity but offers fine-grained texture and color information.\nThis offers the compelling research opportunity of how to design a detector which utilizes the best of two worlds.\n\nEarly work on KITTI~\\cite{kitti} such as MV3D~\\cite{mv3d} and AVOD~\\cite{avod} proposed multi-view fusion pipelines to exploit these synergies.\nHowever recent detectors such as PointPillars~\\cite{pointpillars}, VoxelNet~\\cite{voxelnet, second} and STD~\\cite{yang2019std} use only \\lidar and still significantly outperform these methods.\nIndeed, despite recent fusion research~\\cite{frustum, sparsepool, liang2019multi, you2019pseudo}, the top methods on the popular KITTI leaderboard~\\cite{kitti} are \\lidar only.\nDoes this mean lidar makes vision redundant for 3D object detection?\n\nThe answer, surely, must be no.\nConsider the example in Fig. \\ref{fig:ped_vs_pole}, where the pedestrian and signpost are clearly visible in the image, yet look more or less identical in the lidar modality.\nSurely vision based semantic information should be useful to improve detection of such objects.\nAlso, by first principle, adding more information should at the minimum yield the \\emph{same} result, not \\emph{worse}.\nSo why has it been so difficult?\nOne reason is due to viewpoint misalignment.\n\n\nWhile both sensors are natively captured in the range-view, most state of the art methods such as PointPillars~\\cite{pointpillars} or STD~\\cite{yang2019std} use convolutions in the \\bev.\nThis view has several advantages including lack of scale ambiguity and minimal occlusions.\nIt also does not suffer from the depth-blurring effect which occurs with applying 2D convolutions to the range view~\\cite{pseudo_lidar}.\nAs a result, \\bev methods outperform top range-view methods, such as LaserNet~\\cite{lasernet, lasernet++}, on the KITTI leaderboard.\nHowever, while a \\lidar point cloud can trivially be converted to \\bev, it is much more difficult to do so with an image.\n\nHence, a core challenge of sensor fusion network design lies in consolidating the lidar \\bev with the camera view.\nPrevious methods can be grouped into four categories: object-centric fusion, continuous feature fusion, explicit transform and detection seeding.\n\nObject-centric fusion, pioneered by MV3D~\\cite{mv3d} and AVOD~\\cite{avod}, is the most obvious choice for a two-stage architecture.\nHere, the modalities have different backbones, one in each view, and fusion happens at the object proposal level by applying roi-pooling in each modality from a shared set of 3D proposals.\nThis allows for end-to-end optimization but tends to be slow and cumbersome.\n\n\nA second family of methods applies ``continuous feature fusion'' to allow feature information to be shared across all strides of the image and lidar backbones~\\cite{contfuse, sparsepool}.\nThese methods can be used with single-state detection designs but require a mapping to be calculated, \\emph{a priori}, for each sample, from the point-cloud to the image.\nOne subtle but important draw-back of this family of methods is ``feature-blurring''.\nThis occurs since each feature vector from the \\bev corresponds to multiple pixels in the image-view, and vice versa.\nContFuse~\\cite{contfuse} proposes a sophisticated method based on kNN, bilinear interpolation and a learned MLP to remedy this, but the core problem persists.\n\nA third family of methods attempts to explicitly transform the image to a \\bev representation~\\cite{oft} and do the fusion there~\\cite{pseudo_lidar, weng2019monocular, you2019pseudo}.\nSome of the most promising \\emph{image-only} methods use this idea of first creating an artificial point cloud from the image and then proceeding in the \\bev~\\cite{pseudo_lidar, weng2019monocular}.\nSubsequent work attempts fusion based on this idea, but the performance falls short of state of the art~\\cite{you2019pseudo}, and requires several expensive steps of processing to build the pseudo-point cloud.\n\n\n\n\n\n\nA fourth family of methods uses detection seeding.\nThere, semantics are extracted from an image \\emph{a priori} and used to seed detection in the point cloud. Frustrum PointNet~\\cite{frustum} and ConvNet~\\cite{fconvnet} use the 2D detections to limit the search space inside the frustum while IPOD~\\cite{ipod} uses semantic segmentation outputs to seed the 3D proposal.\nThis improves precision, but imposes an upper bound on recall.\n\nThe recent work of Liang et. al~\\cite{liang2019multi} tried combining several of these concepts.\nResults are not disclosed for all classes, but the method is outperformed on the car class by the top lidar-only method STD~\\cite{yang2019std} (\\tableref{table:res_bev}).\n\nIn this work we propose PointPainting: a simple yet effective sequential fusion method.\nEach \\lidar point is projected into the output of an image semantic segmentation network and the channel-wise activations are concatenated to the intensity measurement of each \\lidar point.\nThe concatenated (painted) lidar points can then be used in any \\lidar detection method, whether \\bev~\\cite{pointpillars, second, pointrcnn, voxelnet, pixor, yang2019std} or front-view~\\cite{fcl, lasernet}.\nPointPainting addresses the shortcomings of the previous fusion concepts:\nit does not add any restrictions on the 3D detection architecture; it does not suffer from feature or depth blurring; it does not require a pseudo-point cloud to be computed, and it does not limit the maximum recall.\n\nNote that for lidar detection methods that operate directly on the raw point cloud~\\cite{pointpillars, voxelnet, lasernet, pointrcnn}, PointPainting requires minimal network adaptations such as changing the number of channels dedicated to reading the point cloud.\nFor methods using hand-coded features~\\cite{pixor, complexyolo}, some extra work is required to modify the feature encoder.\n\n\n\n\n\n\n\nPointPainting is sequential by design which means that it is not always possible to optimize, end-to-end, for the final task of 3D detection.\nIn theory, this implies sub-optimality in terms of performance.\nEmpirically, however, PointPainting is more effective than all other proposed fusion methods.\nFurther, a sequential approach has other advantages:\n(1) semantic segmentation of an image is often a useful stand-alone intermediate product, and\n(2) in a real-time 3D detection system, latency can be reduced by pipelining the image and lidar networks such that the lidar points are decorated with the semantics from the previous image.\nWe show in ablation that such pipelining does not affect performance.\n\nWe implement PointPainting with three state of the art \\lidar-only methods that have public code: PointPillars~\\cite{pointpillars}, VoxelNet (SECOND)~\\cite{voxelnet, second}, and PointRCNN~\\cite{pointrcnn}. PointPainting consistently improved results (\\figref{fig:point_painting_delta}) and indeed, the painted version of PointRCNN achieves state of the art on the KITTI leaderboard (\\tableref{table:res_bev}).\nWe also show a significant improvement of 6.3 mAP (\\tableref{table:nuscenes_test}) for Painted PointPillars+ on nuScenes~\\cite{nuscenes}.\n\n\n\\mypar{Contributions.}\nOur main contribution is a novel fusion method, PointPainting, that augments the point cloud with image semantics.\nThrough extensive experimentation we show that PointPainting is:\n\\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]\n\\setlength\\itemsep{.1mm}\n\\item \\textbf{general} -- achieving significant improvements when used with 3 top lidar-only methods on the KITTI and nuScenes benchmarks;\n\\item \\textbf{accurate} -- the painted version of PointRCNN achieves state of the art on the KITTI benchmark;\n\\item \\textbf{robust} -- the painted versions of PointRCNN and PointPillars improved performance on \\emph{all classes} on the KITTI and nuScenes \\emph{test} sets, respectively.\n\\item \\textbf{fast} -- low latency fusion can be achieved by pipelining the image and lidar processing steps.\n\\end{itemize}\n\n\n\n\n\n\n% !TEX root = ../pointpainting.tex\n\n"
            },
            "section 2": {
                "name": "PointPainting Architecture",
                "content": " \\label{sec:network}\n\\squeeze\n\nThe PointPainting architecture accepts point clouds and images as input and estimates oriented 3D boxes.\nIt consists of three main stages (Fig.~\\ref{fig:pointpainting}).\n(1) Semantic Segmentation: an image based sem. seg. network which computes the pixel wise segmentation scores.\n(2) Fusion: \\lidar points are painted with sem. seg. scores.\n(3) 3D Object Detection: a lidar based 3D detection network.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
                "subsection 2.1": {
                    "name": "Image Based Semantics Network",
                    "content": "\n\\label{sec:image-segmentation-network}\nThe image sem. seg. network takes in an input image and outputs per pixel class scores.\nThese scores serve as compact summarized features of the image.\nThere are several key advantages of using sem. seg. in a fusion pipeline.\nFirst, sem. seg. is an easier task than 3D object detection since segmentation only requires local, per pixel classification, while object detection requires 3D localization and classification.\nNetworks that perform sem. seg. are easier to train and are also amenable to perform fast inference.\nSecond, rapid advances are being made in sem. seg.~\\cite{cheng2019panopticdeeplab,zhu2019improving}, which allows PointPainting to benefit from advances in both segmentation and 3D object detection.\nFinally, in a robotics or autonomous vehicle system, sem. seg. outputs are useful independent outputs for tasks like free-space estimation.\n\nIn this paper, the segmentation scores for our KITTI experiments are generated from DeepLabv3+~\\cite{deeplabv3plus,zhu2019improving,reda2018sdc}, while for nuScenes experiments we trained a custom, lighter, network.\nHowever, we note that PointPainting is agnostic to the image segmentation network design.\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 2.2": {
                    "name": "PointPainting",
                    "content": "\n\\label{sec:projection}\n\n\\begin{algorithm}\n    \\begin{algorithmic}\n        \\caption{PointPainting(L, S, T, M)} \\label{algorithm:painting}\n        \\State \\textbf{Inputs:}\n        \\State Lidar point cloud $L \\in \\mathbb{R}^{N, D}$ with $N$ points and $D \\geq 3$.\n        \\State Segmentation scores $S \\in \\mathbb{R}^{W, H, C}$ with $C$ classes.\n        \\State Homogenous transformation matrix $T \\in \\mathbb{R}^{4,4}$.\n        \\State Camera matrix $ M \\in \\mathbb{R}^{3,4}$.\n        \\State\n        \\State \\textbf{Output:}\n        \\State Painted lidar points $P \\in \\mathbb{R}^{N, D+C}$\n    \t\\State\n        \\For{$\\vec{l}$ $\\in L$}\n              \\State $\\vec{l}_{\\rm{image}} = \\textrm{PROJECT}(M, T, \\vec{l}_{xyz}$)   \t \\Comment $\\vec{l}_{\\rm{image}} \\in \\mathbb{R}^2$\n              \\State $\\vec{s} = S[\\vec{l}_{\\rm{image}}[0], \\vec{l}_{\\rm{image}}[1], :]$         \t\\Comment $\\vec{s} \\in \\mathbb{R}^C$\n              \\State $\\vec{p} = \\mbox{Concatenate} (\\vec{l}, \\vec{s} )$\t\t\t\t\t\\Comment $\\vec{p} \\in \\mathbb{R}^{D+C}$\n         \\EndFor\n    \\end{algorithmic}\n\\end{algorithm}\n\n\n\nHere we provide details on the painting algorithm.\nEach point in the lidar point cloud is ($x$, $y$, $z$, $r$) or ($x$, $y$, $z$, $r$, $t$) for KITTI and nuScenes respectively, where $x$, $y$, $z$ are the spatial location of each lidar point, $r$ is the reflectance, and $t$ is the relative timestamp of the lidar point (applicable when using multiple lidar sweeps~\\cite{nuscenes}).\nThe lidar points are transformed by a homogenous transformation followed by a projection into the image.\nFor KITTI this transformation is given by $T_{\\rm{camera}\\leftarrow\\rm{lidar}}$.\nThe nuScenes transformation requires extra care since the lidar and cameras operate at different frequencies.\nThe complete transformation is:\n\\begin{equation*}\nT=\nT_{(\\rm{camera}\t\t\\leftarrow\t\t\\rm{ego})}\nT_{\\rm{(ego_{t_c}}\t\\leftarrow\t\t\\rm{ego_{t_l}})}\nT_{(\\rm{ego}\t\\leftarrow\t\t\\rm{lidar})}\n\\end{equation*}\nwith transforms: \\lidar frame to the ego-vehicle frame; ego frame at time of lidar capture, $t_l$, to ego frame at the image capture time, $t_c$; and ego frame to camera frame.\nFinally, the camera matrix, $M$, projects the points into the image.\n\nThe output of the segmentation network is $C$ class scores, where for KITTI $C=4$ (car, pedestrian, cyclist, background) and for nuScenes $C=11$ (10 detection classes plus background).\nOnce the lidar points are projected into the image, the segmentation scores for the relevant pixel, ($h$, $w$), are appended to the lidar point to create the painted lidar point.\nNote, if the field of view of two cameras overlap, there will be some points that will project on two images simultaneously and we randomly choose the segmentation score vector from one of the two images.\nAnother strategy can be to choose the more discriminative score vector by comparing their entropies or the margin between the top two scores. However, we leave that for future studies.\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
                },
                "subsection 2.3": {
                    "name": "Lidar Detection",
                    "content": "\n\\label{sec:pointpillars}\n\nThe decorated point clouds can be consumed by any lidar network that learns an encoder, since PointPainting just changes the input dimension of the lidar points.\nPointPainting can also be utilized by lidar networks with hand-engineered encoder~\\cite{pixor, complexyolo}, but requires specialized feature engineering for each method.\nIn this paper, we demonstrate that PointPainting works with three different lidar detectors: PointPillars~\\cite{pointpillars}, VoxelNet~\\cite{voxelnet,second}, and PointRCNN~\\cite{pointrcnn}.\nThese are all state of the art lidar detectors with distinct network architectures: single stage (PointPillars, VoxelNet) vs two stage (PointRCNN), and pillars (PointPillars) vs voxels (VoxelNet) vs point-wise features (PointRCNN).\nDespite these different design choices, all lidar networks benefit from PointPainting (\\tableref{table:point_delta}).\nNote that we were as inclusive as possible in this selection, and to the best of our knowledge, these represent all of the top KITTI detection leaderboard methods that have public code.\n\n% !TEX root = ../pointpainting.tex\n\n"
                }
            },
            "section 3": {
                "name": "Experimental setup",
                "content": "\n\\squeeze\nIn this section we present details of each dataset and the experimental settings of PointPainting.\n\n",
                "subsection 3.1": {
                    "name": "Datasets",
                    "content": "\n\\label{section:dataset}\n\\squeeze\nWe evaluate our method on the KITTI and nuScenes datasets.\n\n\\mypar{KITTI.}\nThe KITTI dataset~\\cite{kitti} provides synced \\lidar point clouds and front-view camera images.\nIt is relatively small with 7481 samples for training and 7518 samples for testing.\nFor our test submission, we created a minival set of 784 samples from the training set and trained on the remaining 6733 samples.\nThe KITTI object detection benchmark requires detection of cars, pedestrians, and cyclists.\nGround truth objects were only annotated if they are visible in the image, so we follow the standard practice~\\cite{mv3d, voxelnet} of only using \\lidar points that project into the image.\n\n\\mypar{nuScenes.}\nThe nuScenes dataset~\\cite{nuscenes} is larger than the KITTI dataset (7x annotations, 100x images).\nIt it annotated with 3D bounding boxes for 1000 20-second scenes at 2Hz resulting in 28130 samples for training, 6019 samples for validation and 6008 samples for testing.\nnuScenes comprises the full autonomous vehicle data suite: synced \\lidar, cameras and radars with complete 360 coverage;\nin this work, we use the \\lidar point clouds and RGB images from all 6 cameras.\nThe 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.\nFurther, the dataset has an imbalance challenge with cars and pedestrians most frequent, and construction vehicles and bicycles least frequent.\n\n\n\n\n\\squeeze\n"
                },
                "subsection 3.2": {
                    "name": "Semantics Network Details",
                    "content": "\n\\label{sec:seg_net}\n\\squeeze\n\nHere we provide more details on the semantics networks.\n\n\\mypar{KITTI.}\nFor experiments on KITTI~\\cite{kitti}, we used the DeepLabv3+ network\\footnote{https://github.com/NVIDIA/semantic-segmentation}.\nThe network was first pretrained on Mapillary~\\cite{mapillary}, then finetuned on Cityscapes~\\cite{cityscapes}, and finally finetuned again on KITTI pixelwise sem. seg.~\\cite{kitti}.\nNote that the class definition of cyclist differs between KITTI sem. seg. and object detection: in detection a cyclist is defined as rider $+$ bike, while in sem. seg. a cyclist is defined as only the rider with bike a separate class.\nThere was therefore a need to map bikes which had a rider to the cyclist class, while supressing parked bikes to background.\nWe did this after painting by mapping all points painted with the bike class within a $1m$ radius of a rider to the cyclist class; the rest to background.\n\n\\mypar{nuScenes.}\nThere was no public semantic segmentation method available on nuScenes so we trained a custom network using the nuImages dataset.\\footnote{We used an early access version; https://www.nuscenes.org/images.}\nnuImages consists of $100$k images annotated with 2D bounding boxes and segmentation labels for all nuScenes classes.\nThe segmentation network uses a ResNet~\\cite{resnet} backbone to generate features at strides $8$ to $64$ for a FCN~\\cite{fcn} segmentation head that predicts the nuScenes segmentation scores.\n\n\\squeeze\n"
                },
                "subsection 3.3": {
                    "name": "Lidar Network Details",
                    "content": "\n\\squeeze\n\nWe perform experiments using three different lidar networks: PointPillars~\\cite{pointpillars}, VoxelNet~\\cite{voxelnet,second}, and PointRCNN~\\cite{pointrcnn}.\nThe fusion versions of each network that use PointPainting will be referred to as being painted (e.g. Painted PointPillars).\n\n\\mypar{KITTI.}\nWe used the publicly released code for PointPillars\\footnote{https://github.com/nutonomy/second.pytorch}, VoxelNet\\footnote{https://github.com/traveller59/second.pytorch} and PointRCNN\\footnote{https://github.com/sshaoshuai/PointRCNN} and decorate the point cloud with the sem. seg. scores for $4$ classes.\nThis changes the original decorated point cloud dimensions from $9\\rightarrow13$, $7\\rightarrow11$, and $4\\rightarrow8$ for PointPillars, VoxelNet, and PointRCNN respectively.\nFor PointPillars, the new encoder has $(13, 64)$ channels, while for VoxelNet it has $(11, 32), (64, 128)$ channels.\nThe $8$ dimensional painted point cloud for PointRCNN is given as input to both the encoder and the region pooling layer.\nNo other changes were made to the public experimental configurations.\n\n\\mypar{nuScenes.}\nWe use PointPillars for all nuScenes experiments.\nThis requires changing the decorated point cloud from $7\\rightarrow18$, and the encoder has $(18, 64)$ channels now.\n\nIn order to make sure the effect of painting is measured on a state of the art method, we made several improvemnts to the previously published PointPillars setup~\\cite{nuscenes} boosting the mAP by 10\\% on the nuScenes bechmark (\\tableref{table:nuscenes_test}).\nWe refer to this improved baseline as PointPillars+. The changes are inspired by~\\cite{megvii2019nuscenes} and comprise modifying pillar resolution, network architecture, attribute estimation, sample weighting, and data augmentation.\nFirst, the pillar resolution was reduced from 0.25 m to 0.2 m to allow for better localization of small objects.\nSecond, the network architecture was changed to include more layers earlier in the network.\nThird, neither PointPillars nor PointPillars+ predict attributes, instead the attribute estimation heuristic was improved.\nRather than using the most common attribute for each class, the predicted velocities and heights of each box are used to better estimate each attribute.\nFourth, to reduce the class imbalance during training, a sample based weighting method was used where each sample was weighted according to the number of annotations in the sample.\nFifth, the global yaw augmentation was changed from $\\pi$ to $\\pi/6$.\n\n% !TEX root = ../pointpainting.tex\n\n"
                }
            },
            "section 4": {
                "name": "Results",
                "content": " \\label{sec:results}\n\\squeeze\n\nIn this section, we present PointPainting results on the KITTI and nuScenes datasets and compare to the literature.\n\n",
                "subsection 4.1": {
                    "name": "Quantitative Analysis",
                    "content": "\n\n",
                    "subsubsection 4.1.1": {
                        "name": "KITTI",
                        "content": "\nAll detection results are measured using the official KITTI evaluation detection for bird's-eye view (BEV) and 3D.\nThe BEV results are presented here while the 3D results are included in the Supplementary Material.\nThe KITTI dataset is stratified into easy, moderate, and hard difficulties, and the official KITTI leaderboard is ranked by performance on moderate average precision (AP).\n\n\\mypar{Validation Set}\nFirst, we investigate the effect of PointPainting on three leading lidar detectors.\nFig.~\\ref{fig:point_painting_delta} and \\tableref{table:point_delta} demonstrate that PointPainting improves the detection performance for PointPillars~\\cite{pointpillars}, VoxelNet~\\cite{voxelnet,second}, and PointRCNN~\\cite{pointrcnn}.\nThe PointPainting semantic information led to a widespread improvement in detection: 24 of 27 comparisons ($3~\\rm{experiments} \\times 3 ~\\rm{classes} \\times 3 ~\\rm{strata}$) were improved by PointPainting.\nWhile the greatest changes were for the more challenging scenarios of pedestrian and cyclist detection, most networks even saw an improvement on cars.\nThis demonstrates that the utility of PointPainting is independent of the underlying lidar network.\n\n\\mypar{Test Set}\nHere we compare PointPainting with state of the art KITTI test results.\nThe KITTI leaderboard only allows one submission per paper, so we could not submit all Painted methods from \\tableref{table:point_delta}.\nWhile Painted PointPillars performed better than Painted PointRCNN on the val set, of the two only PointPillars has public code for nuScenes.\nTherefore, to establish the generality of PointPainting, we chose to submit Painted PointPillars results to nuScenes test, and use our KITTI submission on Painted PointRCNN.\n\nAs shown in \\tableref{table:res_bev}, PointPainting leads to a robust improvement on the test set for PointRCNN: the average precision increases for every single class across all strata.\nPainted PointRCNN establishes new state of the art performance on mAP and cyclist AP.\n\nBased on the consistency of Painted PointRCNN improvements between val and test ($+2.73$ and $+2.94$ respectively), and the generality of PointPainting (\\tableref{table:point_delta}), it is reasonable to believe that other methods in \\tableref{table:res_bev} would decidedly improve with PointPainting.\nThe strength, generality, robustness, and flexibility of PointPainting suggests that it is the leading method for image-lidar fusion.\n\n\n\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "nuScenes",
                    "content": "\nTo establish the versatility of PointPainting, we examine Painted PointPillars results on nuScenes.\nAs a first step, we strengthened the lidar network baseline to PointPillars+.\nEven with this stronger baseline, PointPainting increases mean average precision (mAP) by $+6.3$ on the test set (\\tableref{table:nuscenes_test}).\nPainted PointPillars+ is only beat by MEGVII's lidar only method on nuScenes.\nHowever, MEGVII's network~\\cite{megvii2019nuscenes} is impractical for a realtime system since it is an extremely large two stage network that requires high resolution inputs and uses multi-scale inputs and ensembles for test evaluation.\nTherefore, Painted PointPillars+ is the leading realtime method on nuScenes.\n\nThe detection performance generalized well across classes with every class receiving a boost in AP from PointPainting (\\tableref{table:nuscenes_per_class}).\nIn general, the worst performing detection classes in PointPillars+ benefited the most from painting, but there were exceptions.\nFirst, traffic cones received the largest increase in AP ($+16.8$) despite already having robust PointPillars+ detections.\nThis is likely because traffic cones often have very few lidar points on them, so the additional information provided by semantic segmentation is extremely valuable.\nSecond, trailer and construction vehicles had lower detection gains, despite starting from a smaller baseline.\nThis was a consequence of the segmentation network having its worst recall on these classes (overall recall of $72\\%$, but only $39\\%$ on trailers and $40\\%$ on construction vehicles; see Supplementary Material for details).\nFinally, despite a baseline of $76$ AP, cars still received a $+1.9$ AP boost, signaling the value of semantic information even for classes well detected by lidar only.\n\n\n\n\n\n"
                },
                "subsection 4.3": {
                    "name": "Qualitative Analysis",
                    "content": "\nHere we give context to the evaluation metrics with some qualitative comparisons in Fig.~\\ref{fig:kitti_visualize} using Painted PointPillars, the best performing network on KITTI val set.\nIn Fig.~\\ref{fig:kitti_visualize} A, original PointPillars correctly detects the cars, but misses a cyclist.\nThe painted point cloud resolves this and the cyclist is detected. It also yields better orientation estimates for the vehicles.\nA common failure mode of lidar based methods is confusion between pedestrians and poles (Fig. ~\\ref{fig:ped_vs_pole}).\nAs expected, PointPainting can help resolve this (Fig.~\\ref{fig:kitti_visualize} B).\nFig.~\\ref{fig:kitti_visualize} C suggests that the lidar detection step can correct incorrect painting.\nThe loose segmentation masks in the image correctly paint nearby pedestrians, but extra paint also gets splattered onto the wall behind them.\nDespite this incorrect semantic information, the network does not predict false positive pedestrians.\nThis leaves unanswered the precise characteristics of sem. seg. (e.g. precision vs recall) to optimize for PointPainting.\nIn Fig.~\\ref{fig:kitti_visualize} D, Painted PointPillars predicts two false positive cyclists on the left because of two compounding mistakes.\nFirst, the sem. seg. network incorrectly predicts pedestrians as riders as they are so close to the parked bikes.\nNext, the heuristic that we used to resolve the discrepancy in the cyclist definition between detection and segmentation annotations (See Section \\ref{sec:seg_net}) exacerbated the problem by painting all bikes with the cyclist class.\nHowever, throughout the rest of the crowded scene, the painted points lead to better oriented pedestrians, fewer false positives, and better detections of far away cars.\n\n% !TEX root = ../pointpainting.tex\n\n\n\n"
                }
            },
            "section 5": {
                "name": "Ablation Studies",
                "content": "\n\nHere we perform ablation studies on the nuScenes dataset.\nAll studies used the Painted PointPillars architecture and were trained for a quarter of the training time as compared to the test submissions.\nUsing the one-cycle optimizer~\\cite{onecycle}, we achieved 33.9 mAP and 46.3 NDS on the nuScenes val set as opposed to 44.85 mAP and 57.34 NDS for full training of Painted PointPillars+.\n\n",
                "subsection 5.1": {
                    "name": "Dependency on Semantics",
                    "content": "\n\n\\mypar{Quality.}\nIn PointPainting, the lidar points are fused with the semantic segmentation of the image.\nWe investigate the impact of the semantic segmentation quality on the final detection performance.\nUsing nuScenes, we generate a series of sem. seg. networks with varying segmentation quality by using multiple intermediate checkpoints from training.\nAs shown in Fig.~\\ref{figure:segmentation_quality}, improved sem. seg. (as measured by mean IOU), leads to improved 3D object detection.\n\nFor an upper bound, we include an ``oracle\" which uses the ground truth 3D boxes to paint the lidar points.\nThis significantly improves the detection performance ($+27$ mAP), which demonstrates that advances in semantic segmentation would radically boost 3D object detection.\n\nUsing the oracle doesn't guarantee a perfect mAP because of several limitations.\nFirst, the ground truth bounding box can contain irrelevant points (e.g. from the ground).\nSecond, nuScenes annotates all objects that contain a single lidar point.\nTurning one lidar point into an accurate, oriented 3D bounding box is difficult.\nThird, we trained it for the same total time as the other ablation studies, but it would probably benefit from longer training.\nFinally, PointPillars' stochastic sampling of the point cloud could significantly filter, or eliminate, the points that contain semantic information if the ground truth object contains only a few points.\n\n\\mypar{Scores vs Labels.}\nWe investigate the effect of the segmentation prediction format on detection performance.\nTo do so we convert the segmentation scores to a one hot encoding, effectively labelling each pixel as the class with the highest score.\nWhen using the labels instead of scores, the NDS was unchanged and the mAP was, surprisingly, $+0.4$ higher.\nHowever, the gains are marginal and within the noise of training.\nWe also hypothesize that for future studies, a combination of calibrated~\\cite{calibration} segmentation scores and a larger PointPillars encoder would perform better.\n\nComparing these results with the segmentation quality ablation suggests that future research focus more on improving segmentation quality and less on representation.\n\n\n\n\n"
                },
                "subsection 5.2": {
                    "name": "Sensitivity to Timing",
                    "content": "\n\\label{timing_sensitivity}\n\nWe investigate the sensitivity of the lidar network to delays in semantic information.\nIn the simplest scenario, which we used in all previous results, each point cloud is matched to the most recent image (Concurrent Matching - Fig.~\\ref{figure:timing_schematic}).\nHowever, this will introduce a latency in a real time system as the fusion step will have to wait for the image based sem. seg. scores.\nTo eliminate the latency, the sem. seg. scores of the previous image can be pipelined into the lidar network (Consecutive Matching - Fig.~\\ref{figure:timing_schematic}).\nThis involves an ego-motion compensation step where the \\lidar pointcloud is first transformed to the coordinate system of the ego-vehicle in the last frame followed by a projection into the image to get the segmentation scores.\nOur experiments suggest that using the previous images does not degrade detection performance (\\tableref{table:time_delay}).\nFurther, we measure that PointPainting only introduces an additional latency of 0.75 ms for the Painted PointPillars architecture (see Supplementary Material for details).\nThis demonstrates that PointPainting can achieve high detection performance in a realtime system with minimal added latency.\n\n% !TEX root = ../pointpainting.tex\n\n\n\\squeeze\n"
                }
            },
            "section 6": {
                "name": "Conclusion",
                "content": " \\label{sec:conclusion}\n\\squeeze\n\nIn this paper, we present PointPainting, a novel sequential fusion method that paints lidar point clouds with image based semantics.\nPointPainting produces state of the art results on the KITTI and nuScenes challenges with multiple different lidar networks.\nThe PointPainting framework is flexible and can combine the outputs of any segmentation network with any lidar network.\nThe strength of these results and the general applicability demonstrate that PointPainting is the leading architecture when fusing image and lidar information for 3D object detection.\n\n\\squeeze\n"
            },
            "section 7": {
                "name": "Acknowledgements",
                "content": "\n\\squeeze\n\nWe thank Donghyeon Won, Varun Bankiti and Venice Liong for help with the semantic segmentation model for nuScenes, and\nHolger Caesar for access to nuImages.\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{../references}\n}\n\n% !TEX root = ../pointpainting.tex\n\\appendix\n\n\\clearpage\n\n\\begin{minipage}[t][2.9cm]{1\\textwidth}\n\\centering\n\\Large \\bf \\title \\par\nPointPainting: Sequential Fusion for 3D Object Detection\\\\\n\\vspace{+4mm}\nSupplementary Material\\\\\n\n\\end{minipage}\n\n"
            },
            "section 8": {
                "name": "PointPainting: 3D results",
                "content": "\n\nIn this section, we present 3D results of PointPainting on the KITTI validation and test sets.\n\n\\mypar{Validation Set}\nSimilar to \\bev results (\\tableref{table:point_delta}), we see that PointPainting substantially improves 3D detection performance on the validation set.\nAs seen in \\tableref{table:point_delta_3d}, 23 out of 27 comparisons (3 experiments x 3 classes x 3 strata) were improved by PointPainting.\n\n\\mypar{Test Set}\nIn the test set (\\tableref{table:res_3d}), we observe that PointPainting consistently improves 3D detection results of PointRCNN on the pedestrians and cyclists classes across all difficulty strata (easy, medium and hard).\nHowever, we see that the 3D results on the car class drops substantially.\nWe think this could be because of overfitting on our minival set which was very small (see Section \\ref{section:dataset}).\n\n"
            },
            "section 9": {
                "name": "PointPainting Latency",
                "content": "\n\nIn Section \\ref{timing_sensitivity}, we concluded that Consecutive matching (see \\figref{figure:timing_schematic}) can minimize the latency introduced by PointPainting without any drop in detection performance.\nHere we provide a detailed breakdown of the latency introduced by PointPainting in the case of Consecutive matching.\n\n\\mypar{Projection}\nThis step involves transforming the pointcloud to the coordinate system of the ego-vehicle in the previous frame followed by a projection into the camera images to get the segmentation scores.\nThis operation only adds a latency of 0.15 ms.\n\n\\mypar{Encoding}\nThe Painted PointPillars encoder operates on an 18 dimensional decorated pointcloud as opposed to the 7 dimensional pointcloud in the original PointPillars architecture.\nWe measure the runtimes for both the encoders in TensorRT and find that PointPainting adds an additional latency of 0.6 ms in the encoding stage.\\\\\n\\\\\nThus, Painted PointPillars only introduces an additional latency of 0.75 ms over PointPillars when Consecutive matching is used.\nThis makes Painted PointPillars a strong candidate for realtime camera-lidar fusion.\n\n"
            },
            "section 10": {
                "name": "nuImages Semantic Segmentation",
                "content": "\n\nHere we present some stats on the semantic segmentation network that we trained on the nuImages dataset.\nThe mean intersection over union (mIoU) on the validation set was 0.65.\nThe class-wise precision and recall on the validation set is shown in \\tableref{table:seg_results}.\nOur model performs the best on the car class and worst on the construction vehicle and trailer classes.\n\n\n\n\n\n\n\n\n"
            }
        },
        "tables": {
            "table:point_delta": "\\begin{table*}[]\n\\small\n\\center\n\\begin{tabular}{| c | c || c | c | c || c | c | c || c | c | c |}\n\\hline\n\\multirow{2}{*}{Method}  & mAP  & \\multicolumn{3}{|c||}{Car} & \\multicolumn{3}{|c||}{Pedestrian} & \\multicolumn{3}{|c|}{Cyclist} \\\\ \\cline{2-11}\n& Mod. & Easy   & Mod.   & Hard   & Easy      & Mod.     & Hard     & Easy     & Mod.    & Hard    \\\\ \\hline \\hline\n% Method               \t\t\t\t& mAP \t\t\t     & Car \t \t\t\t\t\t      & Ped.\t\t\t\t\t\t     & Cyclist \t\t\t\\\\ \\hline \\hline\nPointPillars~\\cite{pointpillars}    \t\t& 73.78    \t\t& 90.09  & 87.57  & 86.03     \t\t& 71.97  & 67.84  & 62.41             \t\t\t& 85.74  & 65.92  & 62.40      \\\\\nPainted PointPillars \t\t\t\t& 76.27    \t\t& 90.01  & 87.65  & 85.56        \t\t& 77.25  & 72.41  & 67.53\t\t        \t\t\t& 81.72  & 68.76  & 63.99\t\t\t\\\\ \\hdashline\nDelta \t\t \t\t\t\t& \\green{+2.50}    \t\t& \\red{-0.08}     & \\green{0.08}\t   & \\red{-0.47}          \t& \\greenbf{+5.28}  & \\greenbf{+4.57}  & \\greenbf{+5.12}\t\t& \\red{-4.02}  & \\green{+2.84}  & \\green{+1.59}\t\t\t\\\\ \\hline \\hline\nVoxelNet~\\cite{voxelnet,second}       & 71.83   \t \t& 89.87  & 87.29  & 86.30    \t\t& 70.08  & 62.44  & 55.02           \t\t\t& 85.48  & 65.77  & 58.97        \t\t\\\\\nPainted VoxelNet     \t\t\t\t& 73.55    \t\t& 90.05  & 87.51  & 86.66        \t\t& 73.16  & 65.05  & 57.33\t\t\t\t\t& 87.46  & 68.08  & 65.59\t\t\t\\\\ \\hdashline\nDelta\t\t\t \t\t\t\t& \\green{+1.71}    \t\t& \\green{+0.18}  & \\green{+0.22}  & \\green{+0.36}     & \\green{+3.08}  & \\green{+2.61}  & \\green{+2.31}\t\t\t\t\t& \\greenbf{+1.98}  & \\green{+2.31}  & \\green{+6.62}\t\t\t\\\\ \\hline \\hline\nPointRCNN~\\cite{pointrcnn}      \t& 72.42    \t\t& 89.78  & 86.19  & 85.02     \t\t& 68.37  & 63.49  & 57.89             \t\t\t& 84.65  & 67.59  & 63.06        \t\t\\\\\nPainted PointRCNN  \t\t\t& 75.80   \t\t& 90.19  & 87.64  & 86.71          \t& 72.65  & 66.06  & 61.24\t\t\t\t\t& 86.33  & 73.69  & 70.17  \t\t\t\\\\ \\hdashline\nDelta \t\t \t\t\t\t& \\greenbf{+3.37} &  \\greenbf{+0.41}  & \\greenbf{+1.45}  & \\greenbf{+1.69}          \t\t& \\green{+4.28}  & \\green{+2.57}  & \\green{+3.35}\t\t\t\t\t& \\green{+1.68}  & \\greenbf{+6.10} & \\greenbf{+7.11}\t\t\\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nPointPainting applied to state of the art \\lidar based object detectors.\nAll lidar methods show an improvement in \\bev (BEV) mean average precision (mAP) of car, pedestrian, and cyclist on KITTI \\emph{val} set, moderate split.\nThe corresponding 3D results are included in \\tableref{table:point_delta_3d} in the Supplementary Material where we observe a similar improvement.\n}\n\\label{table:point_delta}\n\\end{table*}",
            "table:res_bev": "\\begin{table*}[]\n\\small\n\\center\n\\begin{tabular}{| c | c | c || c | c | c || c | c | c || c | c | c |}\n\\hline\n\\multirow{2}{*}{Method} & \\multirow{2}{*}{Modality}  & mAP  & \\multicolumn{3}{|c||}{Car} & \\multicolumn{3}{|c||}{Pedestrian} & \\multicolumn{3}{|c|}{Cyclist} \\\\ \\cline{3-12}\n                        \t\t\t\t&                      & Mod. & Easy   & Mod.   & Hard   & Easy      & Mod.     & Hard     & Easy     & Mod.    & Hard    \\\\ \\hline \\hline\nMV3D\\cite{mv3d}     \t\t\t& L \\& I             & N/A  \t& 86.62  & 78.93  & 69.80  & N/A       & N/A      & N/A      & N/A      & N/A     & N/A     \\\\\nAVOD-FPN\\cite{avod}       \t& L \\& I             & 64.07   & 90.99  & 84.82  & 79.62  & 58.49     & \\textbf{50.32}    & \\textbf{46.98}    & 69.39    & 57.12   & 51.09   \\\\\nIPOD\\cite{ipod}           \t\t& L \\& I             &  64.60  \t& 89.64  & 84.62  & 79.96  & \\textbf{60.88}     & 49.79    & 45.43    & 78.19      & 59.40   & 51.38   \\\\\nF-PointNet\\cite{frustum} \t\t& L \\& I            \t& 65.20   & 91.17  & 84.67  & 74.77  & 57.13     & 49.57    & 45.48    & 77.26    & 61.37   & 53.78   \\\\\nF-ConvNet\\cite{fconvnet}           & L \\& I             & 67.89   & 91.51  & 85.84  & 76.11  & 57.04     & 48.96    & 44.33    & \\textbf{84.16}    & 68.88   & 60.05   \\\\ \\hline\nMMF\\cite{liang2019multi}          & L, I  \\& M      \t& N/A  \t& 93.67  & 88.21  & 81.99  & N/A       & N/A      & N/A      & N/A      & N/A     & N/A     \\\\ \\hline\nLaserNet\\cite{lasernet}     \t\t& L\t\t         & N/A  \t& 79.19. & 74.52  & 68.45   & N/A       & N/A      & N/A      & N/A      & N/A     & N/A     \\\\\nSECOND\\cite{second}      \t\t& L                    &  61.61   & 89.39  & 83.77  & 78.59  & 55.99     & 45.02    & 40.93    & 76.5     & 56.05   & 49.45   \\\\\nPointPillars\\cite{pointpillars}  \t& L                    & 65.98    & 90.07  & 86.56  & 82.81  & 57.60     & 48.64    & 45.78    & 79.90    & 62.73   & 55.58   \\\\\nSTD\\cite{yang2019std}          \t& L                    & 68.38    & \\textbf{94.74}  & \\textbf{89.19}  & \\textbf{86.42}  & 60.02     & 48.72    & 44.55    & 81.36    & 67.23   & 59.35   \\\\\nPointRCNN\\cite{pointrcnn}    \t& L                    & 66.92    & 92.13  & 87.39  & 82.72  & 54.77     & 46.13    & 42.84    & 82.56    & 67.24   & 60.28   \\\\ \\hline\nPainted PointRCNN  \t\t& L \\& I             & \\textbf{69.86}\t& 92.45  & 88.11  & 83.36  & 58.70     & 49.93    & 46.29    & 83.91    & \\textbf{71.54}   & \\textbf{62.97}    \\\\ \\hdashline\nDelta\t \t\t\t\t\t& $\\Delta$ I           & \\greenbf{+2.94} &\t\\greenbf{+0.32} & \\greenbf{+0.72} & \\greenbf{+0.64} & \\greenbf{+3.93} & \\greenbf{+3.80}  & \\greenbf{+3.45} & \\greenbf{+1.35} & \\greenbf{+4.30} &\t\\greenbf{+2.69}     \\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nResults on the KITTI test BEV detection benchmark.\nWe see that Painted PointRCNN sets a new state of the art (\\textbf{69.86 mAP}) in BEV detection performance.\nThe modalities are lidar (L), images (I), and maps (M).\nThe delta is the difference due to Painting, ie Painted PointRCNN minus PointRCNN.\nThe corresponding 3D results are included in \\tableref{table:res_3d} in the Supplementary Material.\n}\n\\label{table:res_bev}\n\\end{table*}",
            "table:nuscenes_per_class": "\\begin{table*}[]\n\\small\n\\center\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\nMethods       \t\t\t\t\t\t& mAP \t& Car      \t& Truck         \t& Bus           \t& Trailer        \t& Ctr. Vhl.  \t& Ped.     \t\t& Motorcycle    \t& Bicycle \t\t& Tr. Cone       \t& Barrier       \t\\\\ \\hline \\hline\nPointPillars~\\cite{pointpillars,nuscenes}     & 30.5\t& 68.4      & 23.0         \t& 28.2          \t& 23.4          \t& 4.1      \t\t& 59.7          \t& 27.4          \t& 1.1             \t& 30.8          \t& 38.9          \t\\\\ \\hline\nPointPillars+   \t\t\t\t\t\t& 40.1\t& 76.0     \t& 31.0    \t\t& 32.1\t\t& 36.6 \t\t& 11.3      \t\t& 64.0 \t\t& 34.2\t\t& 14.0\t\t& 45.6         \t& 56.4          \t\\\\ \\hline\nPainted PointPillars+ \t\t\t\t& 46.4\t& 77.9      & 35.8           \t& 36.1 \t\t& 37.3 \t\t& 15.8          \t& 73.3 \t\t& 41.5\t\t& 24.1\t\t& 62.4         \t& 60.2  \t\t\\\\ \\hdashline\nDelta\t\t\t\t \t\t\t\t& \\greenbf{+6.3} \t& \\greenbf{+1.9} \t& \\greenbf{+4.8}           \t& \\greenbf{+3.9} \t\t& \\greenbf{+0.7} \t\t& \\greenbf{+4.5}          \t& \\greenbf{+9.3} \t\t& \\greenbf{+7.3}\t\t& \\greenbf{+10.1}\t\t& \\greenbf{+16.8}         \t& \\greenbf{+3.8}\t\t\\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nPer class nuScenes performance.\nEvaluation of detections as measured by average precision (AP) or mean AP (mAP) on nuScenes test set.\nAbbreviations: construction vehicle (Ctr. Vhl.), pedestrian (Ped.), and traffic cone (Tr. Cone).\n}\n\\label{table:nuscenes_per_class}\n\\end{table*}",
            "table:nuscenes_test": "\\begin{table}[]\n\\small\n\\center\n\\begin{tabular}{|c|c|c|c|}\n\\hline\nMethod\t\t\t\t\t\t\t\t\t& Modality\t \t\t& NDS\t& mAP   \t\\\\ \\hline \\hline\nMonoDis~\\cite{monodis}\t\t\t\t\t\t& Images\t\t\t& 38.4\t& 30.4\t\\\\ \\hline\nPointPillars~\\cite{pointpillars,nuscenes}  \t\t\t& Lidar\t\t\t& 45.3 \t& 30.5 \t\\\\ \\hline\nPointPillars+    \t\t\t\t\t\t\t\t& Lidar \t\t\t& 55.0 \t& 40.1 \t\\\\ \\hline\nPainted PointPillars+    \t\t\t\t\t\t& Lidar \\& Images \t& 58.1 \t& 46.4\t\\\\ \\hline\nMEGVII~\\cite{megvii2019nuscenes}    \t\t\t& Lidar \t\t\t& 63.3  \t& 52.8 \t\\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nnuScenes test results.\nDetection performance is measured by nuScenes detection score (NDS)~\\cite{nuscenes} and mean average precision (mAP).\n}\n\\label{table:nuscenes_test}\n\\end{table}",
            "table:time_delay": "\\begin{table}[]\n\\small\n\\center\n\\begin{tabular}{|c|c|c|c|}\n\\hline\nMethod\t\t\t\t\t& Matching\t\t& NDS\t& mAP\t\\\\ \\hline \\hline\nPainted PointPillars    \t\t& Concurrent\t\t& 46.3 \t& 33.9 \t\\\\ \\hline\nPainted PointPillars   \t\t& Consecutive \t\t& 46.4  \t& 33.9 \t\\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nTime delay analysis.\nPainted PointPillars results on nuScenes when using concurrent matching (which incurs latency), or consecutive matching (which allows real-time pipelining) as shown in \\figref{figure:timing_schematic}.\nThe use of the previous image minimizes latency without any drop in detection performance.\n}\n\\label{table:time_delay}\n\\end{table}",
            "table:seg_results": "\\begin{table}[]\n\\vspace{+35mm}\n\\begin{tabular}{|c|c|c|}\n\\hline\n\\textbf{Class}       & \\textbf{Recall (\\%)} & \\textbf{Precision (\\%)} \\\\ \\hline\nCar                  & 94                   & 89                      \\\\ \\hline\nBus                  & 71                   & 92                      \\\\ \\hline\nConstruction Vehicle & 40                   & 58                      \\\\ \\hline\nTrailer              & 39                   & 79                      \\\\ \\hline\nTruck                & 69                   & 76                      \\\\ \\hline\nMotorcycle           & 89                   & 87                      \\\\ \\hline\nBicycle              & 58                   & 84                      \\\\ \\hline\nPedestrian           & 80                   & 86                      \\\\ \\hline\nBarrier              & 81                   & 80                      \\\\ \\hline\nTraffic Cone         & 78                   & 84                      \\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nClass-wise Precision and Recall of the semantic segmentation network trained on the nuImages dataset.\n}\n\\label{table:seg_results}\n\\end{table}",
            "table:point_delta_3d": "\\begin{table*}[]\n\\small\n\\center\n\\begin{tabular}{| c | c || c | c | c || c | c | c || c | c | c |}\n\\hline\n\\multirow{2}{*}{Method}  & mAP  & \\multicolumn{3}{|c||}{Car} & \\multicolumn{3}{|c||}{Pedestrian} & \\multicolumn{3}{|c|}{Cyclist} \\\\ \\cline{2-11}\n& Mod. & Easy   & Mod.   & Hard   & Easy      & Mod.     & Hard     & Easy     & Mod.    & Hard    \\\\ \\hline \\hline\n% Method               \t\t\t\t& mAP \t\t\t     & Car \t \t\t\t\t\t      & Ped.\t\t\t\t\t\t     & Cyclist \t\t\t\\\\ \\hline \\hline\nPointPillars~\\cite{pointpillars}    \t\t& 66.96    \t\t& 87.22  & 76.95  & 73.52     \t\t& 65.37  & 60.66  & 56.51             \t\t\t& 82.29  & 63.26  & 59.82      \\\\\nPainted PointPillars \t\t\t\t& 69.03    \t\t& 86.26  & 76.77  & 70.25        \t\t& 71.50  & 66.15  & 61.03\t\t        \t\t\t& 79.12  & 64.18  & 60.79\t\t\t\\\\ \\hdashline\nDelta \t\t \t\t\t\t& \\green{+2.07}    \t\t& \\red{-0.96}     & \\red{-0.18}\t   & \\red{-3.27}          \t& \\greenbf{+6.13}  & \\greenbf{+5.49}  & \\greenbf{+4.52}\t\t& \\red{-3.17}  & \\green{+0.92}  & \\green{+0.97}\t\t\t\\\\ \\hline \\hline\nVoxelNet~\\cite{voxelnet,second}       & 67.12   \t \t& 86.85  & 76.64  & 74.41    \t\t& 67.79  & 59.84  & 52.38           \t\t\t& 84.92  & 64.89  & 58.59        \t\t\\\\\nPainted VoxelNet     \t\t\t\t& 68.01    \t\t& 87.15  & 76.66  & 74.75        \t\t& 68.57  & 60.93  & 54.01\t\t\t\t\t& 85.61  & 66.44  & 64.15\t\t\t\\\\ \\hdashline\nDelta\t\t\t \t\t\t\t& \\green{+0.89}    \t\t& \\green{+0.3}  & \\green{+0.02}  & \\green{+0.34}     & \\green{+0.78}  & \\green{+1.09}  & \\green{+1.63}\t\t\t\t\t& \\green{+0.69}  & \\green{+1.55}  & \\greenbf{+5.56}\t\t\t\\\\ \\hline \\hline\nPointRCNN~\\cite{pointrcnn}      \t& 67.01    \t\t& 86.75  & 76.05  & 74.30     \t\t& 63.29  & 58.32  & 51.59             \t\t\t& 83.68  & 66.67  & 61.92        \t\t\\\\\nPainted PointRCNN  \t\t\t& 70.34   \t\t& 88.38  & 77.74  & 76.76          \t& 69.38  & 61.67  & 54.58\t\t\t\t\t& 85.21  & 71.62  & 66.98  \t\t\t\\\\ \\hdashline\nDelta \t\t \t\t\t\t& \\greenbf{+3.33} &  \\greenbf{+1.63}  & \\greenbf{+1.69}  & \\greenbf{+2.46}          \t\t& \\green{+6.09}  & \\green{+3.35}  & \\green{+2.29}\t\t\t\t\t& \\greenbf{+1.53}  & \\greenbf{+4.95} & \\green{+5.06}\t\t\\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nPointPainting applied to state of the art \\lidar based object detectors.\nAll lidar methods show an improvement in 3D mean average precision (mAP) of car, pedestrian, and cyclist on KITTI \\emph{validation} set, moderate split.\n}\n\\label{table:point_delta_3d}\n\\end{table*}",
            "table:res_3d": "\\begin{table*}[]\n\\small\n\\center\n\\begin{tabular}{| c | c | c || c | c | c || c | c | c || c | c | c |}\n\\hline\n\\multirow{2}{*}{Method} & \\multirow{2}{*}{Modality}  & mAP  & \\multicolumn{3}{|c||}{Car} & \\multicolumn{3}{|c||}{Pedestrian} & \\multicolumn{3}{|c|}{Cyclist} \\\\ \\cline{3-12}\n                        \t\t\t\t&                      & Mod. & Easy   & Mod.   & Hard   & Easy      & Mod.     & Hard     & Easy     & Mod.    & Hard    \\\\ \\hline \\hline\nMV3D\\cite{mv3d}     \t\t\t& L \\& I             & N/A  \t& 74.97  & 63.63  & 54.00  & N/A       & N/A      & N/A      & N/A      & N/A     & N/A     \\\\\nAVOD-FPN\\cite{avod}       \t& L \\& I             & 54.86     & 83.07  & 71.76  & 65.73  & 50.46     & 42.27    & 39.04    & 63.76    & 50.55   & 44.93   \\\\\n%IPOD\\cite{ipod}           \t\t& L \\& I             &  00.00  \t&   &   &   &      &     &     &       &    &    \\\\\nF-PointNet\\cite{frustum} \t\t& L \\& I            \t& 56.02    & 82.19  & 69.79  & 60.59  & 50.53     & 42.15    & 38.08    & 72.27    & 56.12   & 49.01   \\\\\nF-ConvNet\\cite{fconvnet}           & L \\& I             & \\textbf{61.61}    & 87.36  & 76.39  & 66.69  & 52.16     & 43.38    & 38.80    & \\textbf{81.98}    & \\textbf{65.07}   & \\textbf{56.54}   \\\\ \\hline\nMMF\\cite{liang2019multi}          & L, I  \\& M      \t& N/A  \t& \\textbf{88.40}  & 77.43  & 70.22  & N/A       & N/A      & N/A      & N/A      & N/A     & N/A     \\\\ \\hline\n%LaserNet\\cite{lasernet}     \t\t& L\t\t         & N/A  \t&  &   &    & N/A       & N/A      & N/A      & N/A      & N/A     & N/A     \\\\\n%SECOND\\cite{second}      \t\t& L                    & 00.00    &   &   &   &      &     &     &      &    &    \\\\\nPointPillars\\cite{pointpillars}  \t& L                    & 58.29    & 82.58  & 74.31  & 68.99   & 51.45   & 41.92    & \\textbf{38.89}    & 77.10    & 58.65   & 51.92    \\\\\nSTD\\cite{yang2019std}          \t& L                    & 61.25    & 87.95  & \\textbf{79.71}  & 75.09   & \\textbf{53.29}   & \\textbf{42.47}    & 38.35    & 78.69    & 61.59   & 55.30   \\\\\nPointRCNN\\cite{pointrcnn}    \t& L                    & 57.94    & 86.96  & 75.64  & \\textbf{70.70}   & 47.98   & 39.37    & 36.01    & 74.96    & 58.82   & 52.53   \\\\ \\hline\nPainted PointRCNN  \t\t& L \\& I             & 58.82\t& 82.11  & 71.70  & 67.08   & 50.32   & 40.97    & 37.87    & 77.63    & 63.78   & 55.89    \\\\ \\hdashline\nDelta\t \t\t\t\t\t& $\\Delta$ I           & \\greenbf{+0.88} &\t\\red{-4.85} & \\red{-3.94} & \\red{-3.62} & \\greenbf{+2.34} & \\greenbf{+1.6}  & \\greenbf{+1.86} & \\greenbf{+2.67} & \\greenbf{+4.96} &\t\\greenbf{+3.36}     \\\\ \\hline\n\\end{tabular}\n\\vspace{1mm}\n\\caption{\nResults on the KITTI test 3D detection benchmark.\nThe modalities are lidar (L), images (I), and maps (M).\nThe delta is the difference due to Painting, ie Painted PointRCNN minus PointRCNN.\nWe don't include a few entries from \\tableref{table:res_bev} because LaserNet\\cite{lasernet} did not publish 3D results and SECOND\\cite{second}, IPOD\\cite{ipod} no longer have their entries on the public leaderboard since KITTI changed to a 40 point interpolated AP metric instead of 11.\n}\n\\label{table:res_3d}\n\\end{table*}"
        },
        "figures": {
            "fig:point_painting_delta": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width = \\columnwidth]{figures/acc_deltas_grid.pdf}\n\\end{center}\n\\vspace{-2mm}\n\\caption{\nPointPainting is a general fusion method that be can used with any lidar detection network. \nTop left: PointPillars\\cite{pointpillars}, VoxelNet~\\cite{voxelnet, second}, and PointRCNN~\\cite{pointrcnn} on the KITTI~\\cite{kitti} \\bev \\emph{val} set (\\tableref{table:point_delta}). \nThe painted version of PointRCNN is state of the art on the KITTI \\emph{test} set outperforming all published fusion and lidar-only methods (\\tableref{table:res_bev}). \nTop right: improvements are larger for the harder pedestrian (ped.) and cyclist classes. \nError bars indicate std. across methods.\nBottom left: PointPillars+ evaluated on the nuScenes~\\cite{nuscenes} \\emph{test} set. \nThe painted version of PointPillars+ improves all 10 classes for a total boost of 6.3 mAP (\\tableref{table:nuscenes_per_class}).\nBottom right: selected class improvements for Painted PointPillars+ show the challenging bicycle class has the largest gains.\n}\n\\label{fig:point_painting_delta}\n\\end{figure}",
            "fig:network": "\\begin{figure*}\n\\begin{center}\n\\includegraphics[width = 1.0\\textwidth]{./figures/bev_fusion_method_figure.pdf}\n\\end{center}\n\\vspace{-3mm}\n\\caption{PointPainting overview.\nThe PointPainting architecture consists of three main stages: (1) image based semantics network, (2) fusion (painting), and (3) lidar based detector.\nIn the first step, the images are passed through a semantic segmentation network obtaining pixelwise segmentation scores.\nIn the second stage, the \\lidar points are projected into the segmentation mask and decorated with the scores obtained in the earlier step.\nFinally, a lidar based object detector can be used on this decorated (painted) point cloud to obtain 3D detections.\n}\n\\label{fig:network}\n\\label{fig:pointpainting}\n\\end{figure*}",
            "fig:pointpainting": "\\begin{figure*}\n\\begin{center}\n\\includegraphics[width = 1.0\\textwidth]{./figures/bev_fusion_method_figure.pdf}\n\\end{center}\n\\vspace{-3mm}\n\\caption{PointPainting overview.\nThe PointPainting architecture consists of three main stages: (1) image based semantics network, (2) fusion (painting), and (3) lidar based detector.\nIn the first step, the images are passed through a semantic segmentation network obtaining pixelwise segmentation scores.\nIn the second stage, the \\lidar points are projected into the segmentation mask and decorated with the scores obtained in the earlier step.\nFinally, a lidar based object detector can be used on this decorated (painted) point cloud to obtain 3D detections.\n}\n\\label{fig:network}\n\\label{fig:pointpainting}\n\\end{figure*}",
            "fig:ped_vs_pole": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.9\\columnwidth]{./figures/ped_pole_confusion_arrow.png}\n\\end{center}\n\\vspace{-3.3mm}\n\\caption{\nExample scene from the nuScenes~\\cite{nuscenes} dataset.\nThe pedestrian and pole are 25 meters away from the ego vehicle.\nAt this distance the two objects appears very similar in the point cloud.\nThe proposed PointPainting method would add semantics from the image making the lidar detection task easier.\n}\n\\label{fig:ped_vs_pole}\n\\end{figure}",
            "fig:kitti_visualize": "\\begin{figure*}\n\\begin{center}\n\\includegraphics[width = 1.0\\textwidth]{./figures/kitti_qualitative.pdf}\n\\end{center}\n\\vspace{-3.3mm}\n\\caption{Qualitative analysis of KITTI results.\nWe created four different comparison figures.\nFor each comparison, the upper left is the original point cloud, while the upper right is the painted point cloud with the segmentation outputs used to color car (orange), cyclist (red) and pedestrian (blue) points.\nPointPillars / Painted PointPillars predicted 3D bounding boxes are displayed on the both the input point cloud (upper left / right) and projected into the image (lower left / right).\nThe orientation of boxes is shown by a line connecting the bottom center to the front of the box.\n}\n\\label{fig:kitti_visualize}\n\\end{figure*}",
            "figure:segmentation_quality": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width = 8.3cm]{./figures/segmentation_quality.pdf}\n\\end{center}\n\\vspace{-4.9mm}\n\\caption{\nPointPainting dependency on segmentation quality.\nThe Painted PointPillars detection performance, as measured by mean average precision (mAP) on the val split, is compared with respect to the quality of semantic segmentation network used in the painting step, as measured by mean intersection over union (mIoU).\nThe oracle uses the 3D bounding boxes as semantic segmentation.\n}\n\\label{figure:segmentation_quality}\n\\end{figure}",
            "figure:timing_schematic": "\\begin{figure}\n\\begin{center}\n\\includegraphics[width = \\columnwidth]{./figures/timing.pdf}\n\\end{center}\n\\vspace{-3.3mm}\n\\caption{\nReducing latency by pipelining.\nA Painted lidar network requires both point clouds and images.\nUsing the most recent image (Concurrent Matching) adds latency since the lidar network must wait for the image segmentation results.\nThis latency can be minimized using pipelining if the Painted network uses the segmentation mask of previous images (Consecutive Matching).\nUsing consecutive matching, we found that Painted PointPillars only adds a latency of 0.75 ms over the original PointPillars architecture.\nSee Supplementary Material for further details.\n}\n\\squeeze\n\\label{figure:timing_schematic}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation*}\nT=\nT_{(\\rm{camera}\t\t\\leftarrow\t\t\\rm{ego})}\nT_{\\rm{(ego_{t_c}}\t\\leftarrow\t\t\\rm{ego_{t_l}})}\nT_{(\\rm{ego}\t\\leftarrow\t\t\\rm{lidar})}\n\\end{equation*}"
        },
        "git_link": "https://github.com/sshaoshuai/PointRCNN"
    }
}