{
    "meta_info": {
        "title": "Multi-Behavior Hypergraph-Enhanced Transformer for Sequential  Recommendation",
        "abstract": "Learning dynamic user preference has become an increasingly important\ncomponent for many online platforms (e.g., video-sharing sites, e-commerce\nsystems) to make sequential recommendations. Previous works have made many\nefforts to model item-item transitions over user interaction sequences, based\non various architectures, e.g., recurrent neural networks and self-attention\nmechanism. Recently emerged graph neural networks also serve as useful backbone\nmodels to capture item dependencies in sequential recommendation scenarios.\nDespite their effectiveness, existing methods have far focused on item sequence\nrepresentation with singular type of interactions, and thus are limited to\ncapture dynamic heterogeneous relational structures between users and items\n(e.g., page view, add-to-favorite, purchase). To tackle this challenge, we\ndesign a Multi-Behavior Hypergraph-enhanced Transformer framework (MBHT) to\ncapture both short-term and long-term cross-type behavior dependencies.\nSpecifically, a multi-scale Transformer is equipped with low-rank\nself-attention to jointly encode behavior-aware sequential patterns from\nfine-grained and coarse-grained levels. Additionally, we incorporate the global\nmulti-behavior dependency into the hypergraph neural architecture to capture\nthe hierarchical long-range item correlations in a customized manner.\nExperimental results demonstrate the superiority of our MBHT over various\nstate-of-the-art recommendation solutions across different settings. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe new MBHT framework. Our implementation code is released at:\nhttps://github.com/yuh-yang/MBHT-KDD22.",
        "author": "Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, Chenliang Li",
        "link": "http://arxiv.org/abs/2207.05584v2",
        "category": [
            "cs.IR",
            "cs.AI"
        ],
        "additionl_info": "Published as a KDD'22 full paper"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\n% \\begin{figure}\n% \t\\includegraphics[width=0.75\\linewidth]{material/motivation_case}\n% \t\\caption{Attention heatmaps for BERT4Rec dealing with long-term multi-behavior dependencies. Boxes in red denote missing attention.}\n% \t\\label{fig:intro_case_attn}\n% \t\\vspace{-0.15in}\n% \\end{figure}\n\nRecommendation models have emerged as the core components of many online applications~\\cite{zhang2019deep}, such as social media platforms~\\cite{zhou2019online}, video streaming services~\\cite{jiang2020aspect} and online retail systems~\\cite{wu2018turning}. Due to the highly practical value of sequential behavior modeling in various online platforms, sequential recommendation has been widely adopted in online platforms, with the aim of forecasting future users' interacted item based on their past behavior sequences~\\cite{sun2019bert4rec,chang2021surge}.\n\n% With the problem of information overload on online platforms, such as millions of products, videos, news and music, recommendation systems serve as a vital part to connect users with the personalized products they are interested in. An increasing number of service providers, whether on e-commerce, streaming or social platforms, are employing sequential recommendation techniques to model users' interests based on their interaction history. In recent years, a growing number of researches focus on the next-item prediction task, in which sequential interaction histories are used to predict users' next behavior.\n\nGenerally, in the sequential recommendation scenario, the systems rely on the item sequences to model time-evolving user preferences. Upon this learning paradigm, many sequential recommender systems have been proposed to encode the item dependencies based on various neural techniques and provided stronger performance, \\eg recurrent neural recommendation architecture-GRU4Rec~\\cite{hidasi2015session} and convolution-based sequence encoder model-Caser~\\cite{tang2018caser}. Inspired by the Transformer framework, self-attention has been utilized to capture the item-item pairwise correlations in SASRec~\\cite{kang2018sasrec} and BERT4Rec~\\cite{sun2019bert4rec}. Recently, graph neural networks (GNNs) have shown effectiveness in sequential recommender systems. Owing to strength of graph convolutions, GNN-based methods (\\eg SR-GNN~\\cite{wu2019srgnn}, GCSAN~\\cite{xu2019gcsan}) are developed to learn item transitions with message passing over the constructed item graph structures.\n\n% However, an essential and unresolved challenge is how to integrate the user's multi-behavior information in the sequence into the next-item prediction. Typically, user behaviors on online platforms are diverse. For example, on an e-commerce site, a user may click, add to cart, add to favorite and purchase for a product. However, a common drawback of existing approaches is that only a single type of behavior (\\eg purchase or click) is considered in the sequence to model user preferences for the next-item prediction, which may not comprehensively reflect the user preferences distributed over different types of behaviors in many real-world scenarios. For instance, some users prefer to click on some items, add some of them to favorites, and eventually select several items added to their shopping cart and buy one. Some, instead, like to purchase a few items immediately after clicking on them. Obviously, the distribution of user preferences over different behavior types can lead to significant differences in their interaction sequences. This motivates us to elaborately consider different behaviors in sequential recommendation to better model users' fine-grained preferences.\n\n\n\nDespite their effectiveness, most of existing works have thus far focused on sequential behavior pattern encoding for singular type of interactions (\\eg clicks or purchases), without taking multi-typed user-item relationships into consideration. However, in practical online platforms, user behaviors often exhibit both time-dependent and multi-typed, involving different types of user-item interactions, \\eg, page view, add-to-favorite, purchase. As illustrated in Figure~\\ref{fig:intro}, customers may click and add their interested products to their favorite lists before purchasing them. Additionally, we also visualize the learned dependency weights by the baseline BERT4Rec and our \\model\\ method to capture behavior-aware short-term item correlations (among neighboring items \\{[4], [5], [6]\\}) and long-term item-wise dependencies (among \\{[2], [13], [15]\\}). We can observe that the global multi-behavior dependencies can be better distilled by our \\model\\ as compared to BERT4Rec. Hence, effectively enhancing the user preference learning with the exploration of heterogeneous user-item interactions in a dynamic environment, is also the key to making accurate sequential recommendations. Nevertheless, this task is not trivial due to the following challenges:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{Dynamic Behavior-aware Item Transitions}. How to explicitly capture the dynamic behavior-aware item transitions with multi-scale temporal dynamics remains a challenge. There exist different periodic behavior patterns (\\eg daily, weekly, monthly) for different categories of items (\\eg daily necessities, seasonal clothing)~\\cite{ren2019lifelong,2019online}. Therefore, it is necessary to explicitly capture multi-scale sequential effects of behavior-aware item transitional patterns from fine-grained to coarse-grained temporal levels. \\\\\\vspace{-0.12in}\n\n\\item \\textbf{Personalized Global Multi-Behavior Dependencies}. The implicit dependencies across different types of behaviors over time vary from user to user. For example, due to the personalized and diverse user interaction preferences, some people would like to add products to their favorite list if they show interested in the items. Others may prefer to generate their favorite item list with products they are very likely to buy. That is to say, for different users, multi-behaviors have various time-aware dependencies on their interests. Moreover, multi-behavior item-wise dependencies are beyond pairwise relations and may exhibit triadic or event higher-order. Hence, the designed model requires a tailored modeling of diverse users' multi-behavior dependencies with a dynamic multi-order relation learning paradigm.\n\\end{itemize}\n\n\n% Despite the reasonable motivation, it is non-trivial to encode multi-behavior signals in the sequence. To this end, the challenges come from two main perspectives as follow.\n\n% \\paratitle{Modeling Diverse Multi-Behavior Patterns.} As elaborated above, multi-behavioral interaction patterns vary from user to user. The most important manifestation of the diversity is that the distance between behaviors in the sequence is not fixed, and the shifting distance abrogates the strong dependence between behaviors. We present a motivating case collected in a real-world dataset in Figure \\ref{fig:intro_case_attn} to address the challenge in detail. The both users have the same interactions with the product, they browse it first, then add it to favorites, and finally purchase it. However, they have completely different behavior patterns, one completing all three actions consecutively, while the other browsing and then performing some actions on other products, finally favoriting and purchasing after a dozen interactions. Such pattern diversity brings about distribution diversity of distances on the sequence for behaviors to same items, which makes it difficult for existing models to capture such changing distribution. In Figure \\ref{fig:intro_case_attn}, we present attention heatmaps of BERT4Rec when encoding the given two sequences. Although it employs an attentive bidirectional encoder to achieve SOTA performances, it still fails to notice the long-term multi-behavior dependences, \\eg $2\\rightarrow 13$ and $2\\rightarrow 15$. Instead, for short-term multi-behavior dependencies, \\eg $4\\rightarrow 5\\rightarrow 6$, it is rather easier to be noticed. Therefore, in order to learn multi-behavior transition relationships at varying distances, a model that captures the user preferences embodied in the sequences at different time scales is necessary.\n\n% \\begin{figure}[t]\n% \t\\includegraphics[width=0.8\\linewidth]{material/mb_case}\n% \t\\caption{An Illustrative case for multi-order behavior dependencies. The colored dotted lines indicate relevance.}\n% \t\\label{fig:intro_mo}\n% \t\\vspace{-0.25in}\n% \\end{figure}\n\n% \\paratitle{Learning Multi-Order Behavior Dependencies.} For a user interaction sequence with different behavior types, the behavior-wise dependency can be too complex to be modeled by sequential methods. To be specific, the most basic relationship between behaviors is that different behaviors of the same item share similar user interests, and the message passing between these instances should be given higher weight. Further, the behavior-wise high-order relations are dynamic, even triadic and tetradic. We give an example in Figure \\ref{fig:intro_mo} to demonstrate such relations. The user interaction sequence includes game controller, football, PC, TV, phone and laptop. Since game controller is relevant to TV and PC, all three behaviors of game controller should connect with TV and PC. In addition, since PC is closely relevant to phone and laptop, game controller and TV should have multi-order relevance with laptop and phone. The complex multi-order relations are beyond sequential models based on RNN or Transformer.\n\n% For example, a user purchased an iPhone 13 and afterwards she browsed the AirPods and added it to her favorites. Then, she continually browsed headphones of other brands. For this interaction sequence, we not only want the model to learn the relevance between behaviors for AirPods and the purchase of iPhone 13, but also expect the model to capture the second-order similarity between the behaviors for other headphone products and iPhone 13 (\\eg iPhone $\\iff$ AirPods $\\iff$ Other Headphones), in order to recommend headphones that are more suitable for iPhone13 to this user.\n\n\\noindent \\textbf{Contribution}. This work proposes a \\underline{M}ulti-\\underline{B}ehavior \\underline{H}ypergraph-enhanced \\underline{T}ransformer (\\model) to capture dynamic item dependencies with behavior type awareness. Our developed \\model\\ framework consitens of two key learning paradigms to address the aforementioned challenges correspondingly. (1) \\textbf{Behavior-aware Sequential Patterns}. We propose a multi-scale Transformer module to comprehensively encode the multi-grained sequential patterns from fine-grained level to coarse-grained level for behavior-aware item transitions. To improve the efficiency of our sequential pattern encoder, we equip our multi-scale Transformer with the self-attentive projection based on low-rank factorization. To aggregate scale-specific temporal effects, a multi-scale behavior-aware pattern fusion is introduced to integrate multi-grained item transitional signals into a common latent representation space. (2) \\textbf{Global Modeling of Diverse Multi-Behavior Dependencies}. We generalize the modeling of global and time-dependent cross-type behavior dependencies with a multi-behavior hypergraph learning paradigm. We construct the item hypergraph structures by unifying the latent item-wise semantic relateness and item-specific multi-behavior correlations. Technically, to capture the global item semantics, we design the item semantic dependence encoder with metric learning. Upon the hypergraph structures, we design the multi-behavior hyperedge-based message passing schema for refining item embeddings, which encourages the long-range dependency learning of different types of user-item relationships. Empirically, \\model\\ is able to provide better performance than state-of-the-art methods, \\eg BERT4Rec~\\cite{sun2019bert4rec}, HyperRec~\\cite{wang2020next}, SURGE~\\cite{chang2021surge}, MB-GMN~\\cite{xia2021graph}.\n\n% Hence, a framework that can efficiently and accurately capture the multi-order information in a multi-behavior sequence is required.\n\n% To this end, in this paper, we propose a novel dual-view learning framework with \\textbf{m}ulti-scale attention transformer view and \\textbf{h}ypergraph view for modeling complex relations in multi-behavior sequential \\textbf{rec}ommendation, namely \\textbf{MHRec}. The design of MHRec aims at building dual-view representation for next-item prediction from both multi-scale sequential modeling and hypergraph convolution to tackle the two aforementioned challenges. Specifically, we propose a new multi-scale attention mechanism to capture sequential information at different time scales, and embed it into the Transformer architecture to encode user interaction sequences. Additionally, we construct a hypergraph for the sequence to model multi-order message passing. Based on item-item similarities and multi-behavior instances, we construct two types of hyperedges in the hypergraph and utilize a hypergraph convolution function to aggregate information from multi-order perspectives. Finally, we unify the dual-view learning information using graph readout and attention mechanisms, and integrate them into the \\textit{Cloze} task training and evaluation strategy.\n\nThe main contributions are summarized as follows:\n\\begin{itemize}[leftmargin=*]\n\n\\item This work proposes a new framework named \\model\\ for sequential recommendation, which uncovers the underlying dynamic and multi-behavior user-item interaction patterns.\n\n\\item To model multi-grained item transitions with the behavior type awareness, we design a multi-scale Transformer which is empowered with the low-rank and multi-scale self-attention projection, to maintain the evolving relation-aware user interaction patterns.\n\n\\item In addition, to capture the diverse and long-range multi-behavior item dependencies, we propose a multi-behavior hypergraph learning paradigm to distill the item-specific multi-behavior correlations with global and customized sequential context injection.\n\n\\item We perform extensive experiments on three publicly available datasets, to validate the superiority of our proposed \\model\\ over various state-of-the-art recommender systems. Model ablation and case studies further show the benefits of our model.\n\n% We release our codes and implementation details in the uploaded supplementary material for result reproductivity.\n\\end{itemize}\n\n\n% In summary, our main contributions are developed as the following three points:\n% \\begin{itemize}[leftmargin=*]\n% \t\\item To the best of our knowledge, we are the first to study multi-behavior modeling in sequential recommendation. We present two challenges of this task and propose targeted solutions.\n% \t\\item We propose a novel framework, \\baby, which incorporates multi-scale sequential learning with multi-order information aggregation to tackle the challenges. In addition, we implement hypergraph convolution and multi-scale Transformer into the \\textit{Cloze} training-prediction task, which allows better exploitation of the training data by creating more training samples.\n% \t\\item We conduct extensive experiments on three real-world datasets to demonstrate the superiority of our \\baby compared to newest baselines over recommendation performance in various lines and convergence speed, with little extra time costs. We also provide ablation and case studies to offer a better understanding for \\baby.\n% \\end{itemize}\\section{Problem Formulation}\n\nIn this section, we introduce the primary knowledge and formulates the task of multi-behavior sequential recommendation.\\\\\\vspace{-0.13in}\n\n\\noindent \\textbf{Behavior-aware Interaction Sequence}.\nSuppose we have a sequential recommender system with a set of $I$ users $u_i\\in \\mathcal{U}$ where $|\\mathcal{U}|=I$. For an individual user $u_i$, we define the behavior-aware interaction sequence $S_i=[(v_{i,1}, b_{i,1}),..., (v_{i,j}, b_{i,j}), ...,(v_{i,J}, b_{i,J})]$ with the consideration of item-specific interaction type, where $J$ denotes the length of temporally-ordered item sequence. Here, we define $b_{i,j}$ to represent the behavior type of the interaction between user $u_i$ and $j$-th item $v_j$ in $S_i$, such as page view, add-to-favorite, add-to-cart and purchase in e-commerce platforms.\\\\\\vspace{-0.13in}\n\n\\noindent \\textbf{Task Formulation}. In our multi-behavior sequential recommender system, different types of interaction behaviors are partitioned into \\emph{target behaviors} and \\emph{auxiliary behaviors}. Specifically, we regard the interaction with the behavior type we aim to predict as target behaviors. Other types of user behaviors are defined as auxiliary behaviors to provide various behaviour contextual information about users' diverse preference, so as to assist the recommendation task on the target type of user-item interactions. For example, in many online retail platforms, purchase behaviors can be considered as the prediction targets, due to their highly relevance to the Gross Merchandise Volume (GMV) in online retailing to indicate the total sales value for merchandise~\\cite{2019online,wu2018turning}. We formally present our studied sequential recommendation problem as follows:\\vspace{-0.05in}\n\n\\begin{itemize}[leftmargin=*]\n\n\\item \\textbf{Input}: The behavior-aware interaction sequence $S_i=[(v_{i,1}, b_{i,1})\\\\,...,(v_{i,j}, b_{i,j}),...,(v_{i,J}, b_{i,J})]$ of each user $u_i\\in \\mathcal{U}$.\\\\\\vspace{-0.12in}\n\n\\item \\textbf{Output}: The learning function that estimates the probability of user $u_i$ will interact with the item $v_{J+1}$ with the target behavior type at the future $(J+1)$-th time step.\n\n\\end{itemize}\n\n"
            },
            "section 2": {
                "name": "METHODOLOGY",
                "content": "\nFigure~\\ref{fig:framework} presents the overall architecture of our proposed \\model\\ model which consists of three key modules: i) Multi-scale modeling of behavior-aware transitional patterns of user preference; ii) Global learning of multi-behavior dependencies of time-aware user interactions; iii) Cross-view aggregation with the encoded representations of sequential behavior-aware transitional patterns and hypergraph-enhanced multi-behavior dependencies.\n\n\n\n%------------------------------------------------------\n% \\subsection{Problem Formulation}\n% In sequential recommendation, we denote the user set as $\\mathcal{U} = \\{u_1, u_2, \\cdots, u_{|\\mathcal{U}|}\\}$ and the item set as $\\mathcal{V} = \\{v_1, v_2, \\cdots, v_{|\\mathcal{V}|}\\}$. For each user, we denote the history interaction sequence as $S_u = \\{v_1, v_2, \\cdots, v_{t_u}\\}$. Since we introduce multi-behavior signals in the sequence, each item $v_i$ is associated with a certain type of behavior from $\\mathcal{R}=\\{r_1, r_2, \\cdots, r_{|\\mathcal{R}|}\\}$. Besides, $S_u$ may contain duplicated items with different behaviors. For example, a user clicked an item $v_i$ and then add it to cart, and $v_i$ appears twice in $S_u$. To align the sequence length, we pad $S_u$ to a certain length $t$ by $0$ as $H = \\{h_1, h_2, \\cdots, h_t \\}$ to form the input to \\baby, where $h_{t_u+1:t}=0$.\n\n% Given an interaction sequence $S_u$, the task of multi-behavior sequential recommendation, aims to predict the item that user $u$ will purchase at time step $t_u + 1$. It can be formalized as computing the probability for user $u$ at time step $t_u+1$ over the item set $\\mathcal{V}$:\n% \\begin{equation}\n% \tP(v_{t_u+1}=v | S_u), \n% \\end{equation}\n% where $v$ is the purchased item that satisfies $r_v=r_p$.\n%------------------------------------------------------\n\n\n%\\subsection{Multi-Scale Attention Mechanism}\n%\\label{subsec:ms-attn}\n%In sequential recommendation scenarios, different time granularities reflect different aspects of users' interests. It is of great importance to capture users' interests at different time granularities, i.e., time scales to obtain more accurate user representations for next-item prediction. Moreover, as we introduce multi-behavior information in the sequence to aid in the prediction of target behavior, the distance distribution of these multi-behaviors for single items are various. For example, some users may purchase some items directly after they viewed them, while others may firstly view some items, and then add some of them to favorites and eventually purchase one from the favorite items. Therefore, modeling users' interests at different time scales become even more vital at such scenarios.\n%\n%In our \\baby~, we propose to model multi-scale user interests using the devised multi-scale attention mechanism. We borrow the idea of Linformer that the attention matrix can be approximated by a low-rank matrix. Here we obtain the low-rank matrix by two means, \\textit{low-rank projecting} and \\textit{mean pooling}, so as to model different aspects of the sequential information.\n%\n%\\paratitle{Low-rank Projecting Self-Attention.} Given an embedded item sequence $\\boldsymbol{H} \\in \\mathbb{R}^{t \\times d}$, the \\textit{low-rank projecting} self-attention firstly uses two linear projection matrices $\\boldsymbol{E}, \\boldsymbol{F} \\in \\mathbb{R}^{\\frac{t}{l} \\times t}$ to project the original $( t \\times d )$-dimensional key and value layers $\\boldsymbol{HW}^K$ and $\\boldsymbol{HW}^V$. We then compute a weighted mapping matrix for the value layer using scaled dot-product attention.\n%\\begin{align}\n%\t\t\\boldsymbol{H}_l &= \\textsf{L-Attention}(\\boldsymbol{HW}^Q, \\boldsymbol{HW}^K, \\boldsymbol{HW}^V)\\\\\n%\t&=\\text{softmax}(\\frac{\\boldsymbol{HW}^Q(\\boldsymbol{EHW}^K)^\\trans}{\\sqrt{d}})\\cdot \\boldsymbol{FHW}^V.\n%\\end{align}\n%The theoretical support for this approach is that in the context mapping matrix $P=\\frac{QK^\\trans}{\\sqrt{d}}$, more information is concentrated in the largest singular values and the rank of $P$ is lower. Therefore, as we substitute the projected $(t/l \\times d)$-dimensional layers for the original $(t \\times d)$-dimensional full sequence, we force the model to learn important structures in the sequence within an abstract $l$ scale.\n%\n%\\paratitle{Pooling Sequential Self-Attention.} Another view for capturing scaled information in an item sequence is that we apply mean pooling for the embedding of $p$ adjacent items in the sequence. Different from L-Attention that learns abstract important structures from the whole sequence, the idea of P-Attention assumes that $p$ adjacent items shares common user interests within the time session of the sub-sequence. Therefore, we can obtain the important information of the sub-sequence explicitly by applying pooling function to the adjacent items. Here we choose the wide-used mean pooling function.\n%\\begin{equation}\n%\ts_p(\\boldsymbol{H}) = [\\text{mean}(\\boldsymbol{h}_1,\\cdots,\\boldsymbol{h}_p);\\text{mean}(\\boldsymbol{h}_{p+1},\\cdots,\\boldsymbol{h}_{2p});\\cdots].\n%\\end{equation}\n%\n%By applying mean pooling function on every $m$ adjacent items in the sequence, we obtain a sequence with $t/m$ items that keep important information of each session. Then, we compute self-attention for the scaled sequence as:\n%\\begin{align}\n%\t\t\\boldsymbol{H}_p &= \\textsf{P-Attention}(\\boldsymbol{HW}^Q, \\boldsymbol{HW}^K, \\boldsymbol{HW}^V)\\\\\n%\t\t&= \\text{softmax}(\\frac{s_p(\\boldsymbol{H})\\boldsymbol{W}^Q(s_p(\\boldsymbol{H})\\boldsymbol{W}^K)^\\trans}{\\sqrt{d}})\\cdot s_p(\\boldsymbol{H})\\boldsymbol{W}^V.\n%\\end{align}\n%\n%Finally, in order to capture rich multi-scale information in the sequence, we stack several layers of the proposed L-Attention and P-Attention as the final multi-scale attentive representation of $\\boldsymbol{H}$. Here in our \\baby, we stack one LR-SelfAttention layer and two MP-SelfAttention layers at different scales $l, p_1, p_2$. Formally, we have\n%\\begin{equation}\n%\t\\textsf{MS-SelfAttention}(\\boldsymbol{H}) = f_c(\\boldsymbol{H}_{l} \\mathbin\\Vert \\boldsymbol{H}_{p_1} \\mathbin\\Vert \\boldsymbol{H}_{p_2}),\n%\\end{equation}\n%where $f_c(\\cdot)$ is a fully-connected layer that projects the stacked sequence repesentation with shape $((t/l+t/p_1+t/p_2) \\times d)$ to the original size $(t \\times d)$.\n%\n%Furthermore, we can implement multi-headed multi-scale self-attention to make the model capture different information from subspaces. Specifically, multi-headed attention firstly utilizes different linear layers to project $\\boldsymbol{H}$ into $h$ subspaces, and then apply $h$ attention layers separately to produce the final representation which is the concatenation of $h$ heads with another projection. Here the attention for each head is our proposed MS-SelfAttention:\n%\\begin{align}\n%\t\\textsf{MH-MS}(\\boldsymbol{H}) &= (\\text{head}_1 \\mathbin\\Vert \\text{head}_2 \\mathbin\\Vert  \\cdots \\mathbin\\Vert \\text{head}_h)\\boldsymbol{W}^D \\\\\n%\t\\text{head}_i &= \\textsf{MS-SelfAttention}(\\boldsymbol{H}),\n%\\end{align}\n%where the $\\boldsymbol{W}^D \\in \\mathbb{R}^{d\\times d}$ is the output projection matrix. Notice that we have omit the Transformer layer subscript $(l)$ in the above equations for brevity. Virtually, the input, output and trainable parameters are all different across Transformer layers.\n\n",
                "subsection 2.1": {
                    "name": "Multi-Scale Modeling of Behavior-aware Sequential Patterns",
                    "content": "\nIn this section, we present the technical details of our \\model\\ in capturing the behavior-aware user interest with multi-scale dynamics.\n\n% %------------------------------------------------------\n% % \\subsection{Multi-Scale Transformer}\n% For a padded input item sequence of length $t$, we apply the Transformer architecture to calculate hidden states $\\bm{h}_i^l$ for each position $i$ at the Transformer layer $l$. As elaborated in \\cite{vaswani2017attention}, the Transformer layer includes three main components, an \\textit{Embedding layer}, an \\textit{Attention layer} and a \\textit{Position-wise Feed-Forward Network}. Here we provide detailed information for the corresponding three components we devise in our \\baby, separately the embedding layer, the \\textit{multi-scale attention} mechanism and the point-wise feed-forward network.\n% %------------------------------------------------------\n\n",
                    "subsubsection 2.1.1": {
                        "name": "Behavior-aware Context Embedding Layer",
                        "content": "\nTo inject the behavior-aware interaction context into our sequential learning framework, we design the behavior-aware context embedding layer to jointly encode the individual item information and the corresponding interaction behavior contextual signal. Towards this end, given an item $v_j$, we offer its behavior-aware latent representation $\\textbf{h}_j \\in \\mathbb{R}^{d}$ with the following operation:\n\\begin{align}\n\t\\textbf{h}_j = \\textbf{e}_j \\oplus \\textbf{p}_j \\oplus \\textbf{b}_j\n\\end{align}\n\\noindent where $\\textbf{e}_j \\in \\mathbb{R}^{d}$ represents the initialized item embedding. $\\textbf{b}_j \\in \\mathbb{R}^{d}$ is the behavior type embedding corresponding to the interaction type (\\eg page view, add-to-favorite) between user $u_i$ and item $v_j$. Here, $\\textbf{p}_j \\in \\mathbb{R}^{d}$ represents the learnable positional embedding of item $v_j$ which differentiates the temporally-ordered positional information of different interacted items. After this context embedding layer, we can obtain the item representation matrix $\\textbf{H} \\in \\mathbb{R}^{J\\times d}$ for the behavior-aware interacted item sequence $S_i$ of user $u_i$.\n\n% %------------------------------------------------------\n% \\subsubsection{\\rm \\textbf{Embedding Layer.}} As elaborated in, the Transformer layer lacks the capacity to capture temporal order information of the sequence. In order for the model to learn the transition information in the sequence, we refer to the common practice in and inject \\textit{Positional Embedding} into item embeddings as the input to the Transformer layer. In addition, some researchers also inject the category embedding of items here to enhance the expression ability of the model. Here, similar to this attempt, we inject the behavior type embedding additionally, which enables the model to discriminate interest transitions under different behavior semantics to fit the scenario of multi-behavior sequencial recommendation. Given an item $v_i$, we construct its initial representation by summing the corresponding item embedding, positional embedding and behavior type embedding:\n% \\begin{equation}\n% \t\\bm{h}_i^{(0)} = \\bm{v}_i + \\bm{p}_i + \\bm{r}_i,\n% \\end{equation}\n% where $\\bm{v}_i, \\bm{p}_i, \\bm{r}_i$ are $d$-dimensional embeddings for item $v_i$, separately item embedding, positional embedding and behavior type embedding. Note that we adopt the learnable positional embedding instead of the sinusoid embedding to improve performances.\n% %------------------------------------------------------\n\n"
                    },
                    "subsubsection 2.1.2": {
                        "name": "Multi-Scale Transformer Layer",
                        "content": "\nIn practical recommendation scenarios, user-item interaction preference may exhibit multi-scale transitional patterns over time. For instance, users often purchase different categories of products (\\eg daily necessities, clothes, digital devices) with different periodic trends, such as daily or weekly routines~\\cite{jiang2020aspect}. To tackle this challenge, we design a multi-scale sequential preference encoder based on the Transformer architecture to capture the multi-grained behavior dynamics in the behavior-aware interaction sequence $S_i$ of users ($u_i\\in \\mathcal{U}$).\\\\\\vspace{-0.1in}\n\n% %------------------------------------------------------\n% \\subsubsection{\\rm \\textbf{Multi-Scale Attention Layer}}\n% \\label{subsec:ms-attn}\n% In sequential recommendation scenarios, different time granularities reflect different aspects of users' interests. It is of great importance to capture users' interests at different time granularities, i.e., time scales to obtain more accurate user representations for next-item prediction. Moreover, as we introduce multi-behavior information in the sequence to aid in the prediction of target behavior, the distance distribution of these multi-behaviors for single items are various. For example, some users may purchase some items directly after they viewed them, while others may firstly view some items, and then add some of them to favorites and eventually purchase one from the favorite items. Therefore, modeling users' interests at different time scales become even more vital at such scenarios.\n\n% In our \\baby~, we propose to model multi-scale user interests using the devised multi-scale attention mechanism. We borrow the idea of Linformer \\cite{wang2020linformer} that the attention matrix can be approximated by a low-rank matrix. Here we obtain the low-rank matrix by two means, \\textit{low-rank projecting} and \\textit{pooling}, so as to model different aspects of the sequential information.\n% %------------------------------------------------------\n\n\\noindent \\textbf{Low-Rank Self-Attention Module}. Transformer has shown its effectiveness in modeling relational data across various domains (\\eg language~\\cite{yao2020multimodal}, vision~\\cite{liu2021swin}). In Transformer framework, self-attention serves as the key component to perform the relevance-aware information aggregation among attentive data points (\\eg words, pixels, items). However, the high computational cost (\\ie quadratic time complexity) of the self-attention mechanism limits the model scalability in practical settings~\\cite{kitaev2020reformer}. Motivated by the design of Transformer structure in~\\cite{wang2020linformer}, we design a low-rank-based self-attention layer without the quadratic attentive operation, to approximate linear model complexity.\n\nIn particular, different from the original scaled dot-product attention for pairwise relation encoding, we generate multiple smaller attention operations to approximate the original attention with low-rank factorization. We first define two trainable projection matrices $\\boldsymbol{E} \\in \\mathbb{R}^{\\frac{J}{C} \\times J}$ and $\\boldsymbol{F} \\in \\mathbb{R}^{\\frac{J}{C} \\times J}$ to perform the low-rank embedding transformation. Here, $C$ denotes the low-rank scale and $\\frac{J}{C}$ represents the number of low-rank latent representation spaces over the input behavior-aware interaction sequence $S_i$. Formally, we represent our low-rank self-attention as follows:\n\\begin{align}\n\\label{eq:lowrank}\n\t\t\\widehat{\\textbf{H}} =\\textsf{softmax}(\\frac{\\textbf{H}  \\cdot {W}^Q(\\textbf{E} \\cdot \\textbf{H}  \\cdot {W}^K)^\\trans}{\\sqrt{d}})\\cdot \\textbf{F} \\cdot \\textbf{H}  \\cdot {W}^V\n\\end{align}\n\\noindent where ${W}^Q$, ${W}^K$, ${W}^V$ are learnable transformation matrices for embedding projection. In our low-rank self-attention module, $\\boldsymbol{E}$ and $\\boldsymbol{F}$ are utilized to project the ($\\mathbb{R}^{J\\times d}$)-dimensional key and value transformed representations $\\textbf{H}  \\cdot {W}^K$ and $\\textbf{H}  \\cdot {W}^V$ into ($\\mathbb{R}^{\\frac{J}{C}\\times d}$)-dimensional latent low-rank embeddings $\\widehat{\\textbf{H}} \\in \\mathbb{R}^{\\frac{J}{C}\\times d}$. In summary, with the low-rank factor decomposition over the original attention operations, we calculate the context mapping matrix $\\textbf{M}=\\frac{\\textbf{H}  \\cdot {W}^Q (\\textbf{H}  \\cdot {W}^K)^T}{\\sqrt{d}}$ with the dimension of $\\mathbb{R}^{J\\times \\frac{J}{C}}$ as compared to the original dimension $\\mathbb{R}^{J\\times J}$ in the vanilla self-attention mechanism. By doing so, the computational cost of our behavior sequence encoder can be significantly reduced from the $O(J\\times J)$ to $O(J\\times \\frac{J}{C})$ given that the low-rank projected dimension $J/C$ is often much smaller than $J$, \\ie $J/C \\ll\\ J$.\\\\\\vspace{-0.12in}\n\n% %------------------------------------------------------\n% \\paratitle{Low-rank Self-Attention.} Given an embedded item sequence $\\boldsymbol{H} \\in \\mathbb{R}^{t \\times d}$, the \\textit{low-rank projecting} self-attention firstly uses two linear projection matrices $\\boldsymbol{E}, \\boldsymbol{F} \\in \\mathbb{R}^{\\frac{t}{c} \\times t}$ to project the original $( t \\times d )$-dimensional key and value layers $\\boldsymbol{HW}^K$ and $\\boldsymbol{HW}^V$. We then compute a weighted mapping matrix for the value layer using scaled dot-product attention.\n% \\begin{align}\n% \t\t\\boldsymbol{H}_c &= \\textsf{L-SelfAttn}(\\boldsymbol{HW}^Q, \\boldsymbol{HW}^K, \\boldsymbol{HW}^V)\\\\\n% \t&=\\text{softmax}(\\frac{\\boldsymbol{HW}^Q(\\boldsymbol{EHW}^K)^\\trans}{\\sqrt{d}})\\cdot \\boldsymbol{FHW}^V.\n% \\end{align}\n% The theoretical support for this approach is that in the context mapping matrix $P=\\frac{QK^\\trans}{\\sqrt{d}}$, more information is concentrated in the largest singular values and the rank of $P$ is lower. Therefore, as we substitute the projected $(t/c \\times d)$-dimensional layers for the original $(t \\times d)$-dimensional full sequence, we force the model to learn important structures in the sequence within an abstract $c$ scale.\n% %------------------------------------------------------\n\n\\paratitle{Multi-Scale Behavior Dynamics.}\nTo endow our \\model\\ model with the effective learning of multi-scale behaviour transitional patterns, we propose to enhance our low-rank-based transformer with a hierarchical structure, so as to capture granularity-specific behavior dynamics. To be specific, we develop a granularity-aware aggregator to generate  granularity-specific representation $\\textbf{g}_p$ which preserves the short-term behavior dynamic. Here, we define $p$ as the length of sub-sequence for a certain granularity. We formally present our granularity-aware emebdding generation with the aggregated representation $\\boldsymbol{\\Gamma}^p \\in \\mathbb{R}^{\\frac{J}{p} \\times d}$ and $\\boldsymbol{\\gamma} \\in \\mathbb{R}^d$ as follows:\n\\begin{align}\n\\label{eq:attn_pool}\n\\boldsymbol{\\Gamma}^p = \\{\\boldsymbol{\\gamma}_1,...,\\boldsymbol{\\gamma}_{\\frac{J}{p}}\\} = [\\eta(\\textbf{h}_1,...,\\textbf{h}_p);...; \\eta(\\textbf{h}_{J-p+1},...,\\textbf{h}_{J})]\n\\end{align}\n\\noindent where $\\eta(\\cdot)$ represents the aggregator to capture the short-term behavior-aware dynamics. Here, we utilize the mean pooling to perform the embedding aggregation. After that, we feed the granularity-aware behavior representations into a self-attention layer for encoding granularity-specific behavior pattern as shown below:\n\\begin{align}\n\\label{eq:pool_attn}\n\t\t{\\textbf{H}}^p =\\textsf{softmax}(\\frac{\\boldsymbol{\\Gamma}^p \\cdot {W}^Q_p(\\boldsymbol{\\Gamma}^p \\cdot {W}^K_p)^\\trans}{\\sqrt{d}}) \\cdot \\boldsymbol{\\Gamma}^p \\cdot {W}^V_p\n\\end{align}\n\\noindent ${\\textbf{H}}^p \\in \\mathbb{R}^{\\frac{J}{p} \\times d}$ encodes the short-term transitional patterns over different item sub-sequences. In our \\model\\ framework, we design a hierarchical Transformer network with two different scale settings $p_1$ and $p_2$. Accordingly, our multi-scale Transformer can produce three scale-specific sequential behavior embeddings $\\widehat{\\textbf{H}} \\in \\mathbb{R}^{J\\times d}$, ${\\textbf{H}}^{p_1} \\in \\mathbb{R}^{\\frac{J}{p_1} \\times d}$, ${\\textbf{H}}^{p_2} \\in \\mathbb{R}^{\\frac{J}{p_2} \\times d}$.\n\n% %------------------------------------------------------\n% \\paratitle{Pooling Sequential Self-Attention.} Another view for capturing scaled information in an item sequence is that we apply mean pooling for the embedding of $p$ adjacent items in the sequence. Different from L-Attention that learns abstract important structures from the whole sequence, the idea of P-Attention assumes that $p$ adjacent items shares common user interests within the time session of the sub-sequence. Therefore, we can obtain the important information of the sub-sequence explicitly by applying pooling function to the adjacent items. Here we choose the wide-used mean pooling function as\n% \\begin{equation}\n% \ts_p(\\boldsymbol{H}) = [\\text{mean}(\\boldsymbol{h}_1,\\cdots,\\boldsymbol{h}_p);\\text{mean}(\\boldsymbol{h}_{p+1},\\cdots,\\boldsymbol{h}_{2p});\\cdots].\n% \\end{equation}\n% By applying mean pooling function on every $m$ adjacent items in the sequence, we obtain a sequence with $t/m$ items that keep important information of each session. Then, we compute self-attention for the scaled sequence as\n% \\begin{align}\n% \t\t\\boldsymbol{H}_p &= \\textsf{P-SelfAttn}(\\boldsymbol{HW}^Q, \\boldsymbol{HW}^K, \\boldsymbol{HW}^V)\\\\\n% \t\t&= \\text{softmax}(\\frac{s_p(\\boldsymbol{H})\\boldsymbol{W}^Q(s_p(\\boldsymbol{H})\\boldsymbol{W}^K)^\\trans}{\\sqrt{d}})\\cdot s_p(\\boldsymbol{H})\\boldsymbol{W}^V.\n% \\end{align}\n% %------------------------------------------------------\n\n"
                    },
                    "subsubsection 2.1.3": {
                        "name": "\\bf Multi-Scale Behaviour Pattern Fusion",
                        "content": "\nTo integrate the multi-scale dynamic behavior patterns into a common latent representation space, we propose to aggregate the above encoded scale-specific embeddings with a fusion layer presented as follows:\n\\begin{align}\n\\widetilde{\\textbf{H}} = f(\\widehat{\\textbf{H}} \\mathbin\\Vert {\\textbf{H}}^{p_1} \\mathbin\\Vert {\\textbf{H}}^{p_2})\n\\end{align}\n\\noindent Here $f(\\cdot)$ represents the projection function which transforms $\\mathbb{R}^{(\\frac{J}{C} + \\frac{J}{p_1} + \\frac{J}{p_2}) \\times d}$ dimensional embeddings into $\\mathbb{R}^{J\\times d}$ dimensional representations corresponding to different items ($v_j\\in S_i$) in the behavior-aware interaction sequence $S_i$ of user $u_i$. Here, $\\parallel$ denotes the concatenation operation over different embedding vectors. \\\\\\vspace{-0.12in}\n\n% %------------------------------------------------------\n% Finally, in order to capture rich multi-scale information in the sequence, we stack several layers of the proposed L-Attention and P-Attention as the final multi-scale attentive representation of $\\boldsymbol{H}$. Here in our \\baby, we stack one L-SelfAttn layer and two P-SelfAttn layers at different scales $c, p_1, p_2$. Formally, we have\n% \\begin{equation}\n% \t\\textsf{MS-SelfAttn}(\\boldsymbol{H}) = f(\\boldsymbol{H}_{c} \\mathbin\\Vert \\boldsymbol{H}_{p_1} \\mathbin\\Vert \\boldsymbol{H}_{p_2}),\n% \\end{equation}\n% where $f(\\cdot)$ is a fully-connected layer that projects the stacked sequence repesentation with shape $((t/c+t/p_1+t/p_2) \\times d)$ to the original size $(t \\times d)$.\n% %------------------------------------------------------\n\n\\paratitle{Multi-Head-Enhanced Representation Spaces.}\nIn this part, we propose to endow our behavior-aware item sequence encoder with the capability of jointly attending multi-dimensional interaction semantics. In particular, our multi-head sequential pattern encoder projects the $\\textbf{H}$ into $N$ latent representation spaces and performs head-specific attentive operations in parallel.\n\\begin{align}\n\t\\widetilde{\\textbf{H}} &= (\\text{head}_1 \\mathbin\\Vert \\text{head}_2 \\mathbin\\Vert  \\cdots \\mathbin\\Vert \\text{head}_N)\\boldsymbol{W}^D \\\\\\nonumber\n\t\\text{head}_n &= f(\\widehat{\\textbf{H}}_n \\mathbin\\Vert {\\textbf{H}}_n^{p_1} \\mathbin\\Vert {\\textbf{H}}_n^{p_2})\n\\end{align}\n\\noindent where $\\widehat{\\textbf{H}}_n, {\\textbf{H}}_n^{p_1}$ and ${\\textbf{H}}_n^{p_2}$ are computed with head-specific projection matrices $W_n^Q, W_n^K, W_n^V \\in \\mathbb{R}^{d\\times d/N}$, and $\\boldsymbol{W}^D \\in \\mathbb{R}^{d\\times d}$ is the output transformation matrix. The multiple attention heads allows our multi-scale Transformer architecture to encode multi-dimensional dependencies among items in $S_i$.\\\\\\vspace{-0.12in}\n\n%------------------------------------------------------\n% Furthermore, we can implement multi-headed multi-scale self-attention to make the model capture different information from subspaces. Specifically, multi-headed attention firstly utilizes different linear layers to project $\\boldsymbol{H}$ into $h$ subspaces, and then apply $h$ attention layers separately to produce the final representation which is the concatenation of $h$ heads with another projection. Here the attention for each head is our proposed MS-SelfAttention:\n% \\begin{align}\n% \t\\textsf{MH-MS}(\\boldsymbol{H}) &= (\\text{head}_1 \\mathbin\\Vert \\text{head}_2 \\mathbin\\Vert  \\cdots \\mathbin\\Vert \\text{head}_h)\\boldsymbol{W}^D \\\\\n% \t\\text{head}_i &= \\textsf{MS-SelfAttn}(\\boldsymbol{H}),\n% \\end{align}\n% where the $\\boldsymbol{W}^D \\in \\mathbb{R}^{d\\times d}$ is the output projection matrix. Notice that we have omit the Transformer layer subscript $(l)$ in the above equations for brevity. Virtually, the input, output and trainable parameters are all different across Transformer layers.\n% %------------------------------------------------------\n\n\\paratitle{Non-linearity Injection with Feed-forward Module.}\nIn our multi-scale Transformer, we use the point-wise feed-forward network to inject non-linearities into the new generated representations. The non-linear transformation layer is formally represented:\n\\begin{align}\n\t\\textsf{PFFN}(\\widetilde{\\textbf{H}}^{(l)}) &= [\\textsf{FFN}(\\tilde{\\textbf{h}}_1^{(l)})^\\trans, \\cdots, \\textsf{FFN}(\\tilde{\\textbf{h}}_t^{(l)})^\\trans] \\\\\\nonumber\n\t\\textsf{FFN}(\\textbf{x}) &= \\textsf{GELU}(\\textbf{x}\\textbf{W}_1^{(l)} + \\textbf{b}_1^{(l)})\\textbf{W}_2^{(l)}+\\textbf{b}_2^{(l)},\n\\end{align}\n\\noindent In our feed-forward module, we adopt two layers of non-linear transformation with the integration of intermediate non-linear activation $\\textsf{GELU}(\\cdot)$. In addition, $\\textbf{W}_1^{(l)} \\in \\mathbb{R}^{d \\times d_h}, \\textbf{W}_2^{(l)} \\in \\mathbb{R}^{d_h \\times d}, \\textbf{b}_1^{(l)} \\in \\mathbb{R}^{d}, \\textbf{b}_2^{(l)} \\in \\mathbb{R}^{d}$ are learnable parameters of projection matrices and bias terms. Here, $l$ denotes the $l$-th multi-scale Transformer layer.\n\n% %------------------------------------------------------\n% \\subsubsection{\\rm \\textbf{Point-wise Feed-Forward Network.}}\n% The computation of \\textit{Multi-Scale Attention}, as the Vanilla Attention, is mainly built on linear projections. Thus, with the purpose of endowing the model with non-linearity and leveraging the interactions between different dimensions of the attention outputs, we apply a point-wise feed-forward network to further encode the outputs. The whole computation process includes two linear transformations and an intermediate \\textsf{GELU} activation, as\n% \\begin{align}\n% \t\\textsf{PFFN}(\\bm{H}^{(l)}) &= [\\textsf{FFN}(\\bm{h}_1^{(l)})^\\trans, \\cdots, \\textsf{FFN}(\\bm{h}_t^{(l)})^\\trans] \\\\\n% \t\\textsf{FFN}(\\bm{x}) &= \\textsf{GELU}(\\bm{x}\\bm{W}_1^{(l)} + \\bm{b}_1^{(l)})\\bm{W}_2^{(l)}+\\bm{b}_2^{(l)},\n% \\end{align}\n% where $\\bm{W}_1^{(l)} \\in \\mathbb{R}^{d \\times d_h}, \\bm{W}_2^{(l)} \\in \\mathbb{R}^{d_h \\times d}, \\bm{b}_1^{(l)} \\in \\mathbb{R}^{d_h}, \\bm{b}_2^{(l)} \\in \\mathbb{R}^{d}$ are all learnable parameters. $d_h$ is the dimensional size of the hidden states in the \\textsf{PFFN} to strengthen the expression ability of the network, which is usually set as $4d$.\n% %------------------------------------------------------\n\n"
                    }
                },
                "subsection 2.2": {
                    "name": "Customized Hypergraph Learning of Global Multi-Behavior Dependencies",
                    "content": "\n\\label{sec:hg}\nIn our \\model\\ framework, we aim to incorporate long-range multi-behavior dependency into the learning paradigm of evolving user interests. However, it is non-trivial to effectively capture the personalized long-range multi-behavior dependencies. To achieve our goal, we propose to tackle two key challenges in our learning paradigm: \n\n\\begin{itemize}[leftmargin=*]\n\\item i) \\textbf{Multi-Order Behavior-wise Dependency}. The item-wise multi-behavior dependencies are no longer dyadic with the consideration of comprehensive relationships among different types of user behaviors. For example, when deciding to recommend  a specific item to users for their potential purchase preference, it would be useful to explore past multi-behavior interactions (\\eg page view, add-to-favorite) between users and this item. Customers are more likely to add their interested products into their favorite item list before making final purchases.\\\\\\vspace{-0.12in}\n\n\\item ii) \\textbf{Personalized Multi-Behavior Interaction Patterns}. Multi-behavior patterns may vary by users with different correlations across multi-typed user-item interactions. In real-life e-commerce systems, some users like to add many items to their favorite list or cart, if they are interested in, but only a few of them will be purchased later. In contrast, another group of users only tag their interested products as favorite only if they show strong willingness to buy them. Hence, such complex and personalized multi-behavior patterns require our model to preserve the diverse cross-type behaviour dependencies.\n\n\\end{itemize}\n\nTo address the above challenges, we build our global multi-behavior dependency encoder upon the hypergraph neural architecture. Inspired by the flexibility of hypergraphs in connecting multiple nodes through a single edge~\\cite{feng2019hypergraph,xia2022hypergraph}, we leverage the hyperedge structure to capture the tetradic or higher-order multi-behavior dependencies over time. Additionally, given the behavior-aware interaction sequence of different users, we construct different hypergraph structures over the sequence $S_i$ ($u_i\\in \\mathcal{U}$), with the aim of encoding the multi-behavior dependency in a customized manner.\n\n",
                    "subsubsection 2.2.1": {
                        "name": "\\bf Item-wise Hypergraph Construction",
                        "content": "\nIn our hypergraph framework, we generate two types of item-wise hyperedge connections corresponding to i) long-range semantic correlations among items; ii) item-specific multi-behavior dependencies across time.\\\\\\vspace{-0.12in}\n\n\\label{sec:hg_construct}\n\\noindent \\textbf{Item Semantic Dependency Encoding with Metric Learning}.\nTo encode the time-evolving item semantics and the underlying long-range item dependencies based on the same user interest (\\eg food, outdoor activities), we introduce an item semantic encoder based on a metric learning framework. Specifically, we design the learnable metric $\\hat{\\beta}^n_{j,j'}$ between items with a multi-channel weight function $\\tau({\\cdot})$ presented as follows:\n\\begin{align}\n\t\\beta_{j,j'} & = \\frac{1}{N}\\sum_{n=1}^{N} \\hat{\\beta}^n_{j,j'}; \\textbf{v}_j = \\textbf{e}_j \\oplus \\textbf{b}_j \\\\\n\t\\hat{\\beta}^n_{j,j'} & = \\tau(\\boldsymbol{w}^n \\odot \\textbf{v}_j, \\boldsymbol{w}^n \\odot \\textbf{v}_{j'})\n\\end{align}\n\\noindent where $\\hat{\\beta}^n_{j,j'}$ represents the learnable channel-specific dependency weight between item $v_j$ and $v_{j'}$. We define the weight function $\\tau({\\cdot})$ as the cosine similarity estimation based on the trainable $\\boldsymbol{w}^n$ of $n$-th representation channel. ($\\boldsymbol{w}^n \\odot \\textbf{v}_{j}$) represents the embedding projection operation. In our item-wise semantic dependency, we perform metric learning under $N$ representation channels (indexed by $n$). The mean pooling operation is applied to all learned channel-specific item semantic dependency scores (\\eg $\\hat{\\beta}^n_{j,j'}$), to obtain the final relevance $\\beta_{j,j'}$ between item $v_j$ and $v_{j'}$.\\\\\\vspace{-0.12in}\n\n% %------------------------------------------------------\n% \\subsection{Hypergraph Construction}\n% \\label{sec:hg}\n% % \\begin{figure}\n% % \t\\includegraphics[width=0.9\\linewidth]{material/hg_case}\n% % \t\\caption{An illustrative presentation of the constructed hypergraph. Blue parts stands for \\textit{beh-hyperedges} and green parts for \\textit{sim-hyperedges}. Items in color indicate behavior-aware self-gating.}\n% % \\end{figure}\n% The hypergraph tackles the limitation in ordinary graphs that information can only be transferred node-to-node, allowing message aggregation along hyperedges that can include any number of nodes. To formulate the discussed multi-order information passing in the sequence modeling process, we construct hyperedges according to item-item similarities and multi-behavior signals in the sequence. To be specific, we construct two types of hyperedges to model different types of information in the sequence, i.e., \\textit{sim-hyperedges} that models interests towards clustered items and \\textit{beh-hyperedges} that models interests shared across behaviors.\n% %------------------------------------------------------\n\n% %------------------------------------------------------\n% \\subsubsection{\\rm \\textbf{Behavior-aware Self-Gating.}} To construct hyperedges, we firstly apply a self-gating mechanism on item embeddings to avoid gradient conflicts with other tasks and to control the information flow from the raw embeddings. Note that there are also inherent differences between these behaviors, for they separately reflect different levels of the user's interests. Therefore, we leverage behavior embedding to differentiate their information in the self-gating operation as\n% \\begin{align}\n% \t\\boldsymbol{u}^{(0)}_i &= \\boldsymbol{v}^{(0)}_i + \\boldsymbol{r}_i \\\\\n% \t\\boldsymbol{U}^{(0)} &= \\boldsymbol{U}^{(0)} \\odot \\sigma(\\boldsymbol{U}^{(0)}\\boldsymbol{W}_g + \\boldsymbol{b}_g).\n% \\label{eq:mb-gating}\n% \\end{align}\n\n% \\subsubsection{\\rm \\textbf{Similarity Edges.}} As a user may share the same aspect of interests for similar items, connecting similar items using a hyperedge acts as clustering user interests. In addition, the similarity edges allow for multi-order information modeling, since by sharing same similar neighbors, items can have high-order connectivity. Here we adopt the multi-headed weighted cosine similarity as the learnable metric for item similarities:\n% \\begin{align}\n% \ta_{i,j} &= \\frac{1}{N}\\sum_{n=1}^{N}s^n_{i,j}\\\\\n% \ta^n_{i,j} &= \\textsf{cos}(\\boldsymbol{w}^n_s\\odot\\boldsymbol{u}^{(0)}_i, \\boldsymbol{w}^n_s\\odot\\boldsymbol{u}^{(0)}_j),\n% \\end{align}\n% where $\\textbf{w}_s$ is the learnable weight that projects item embeddings to the metric space by applying the Hadamard product. We adopt $N$ metric learning heads to enhance the expression ability and concatenate the heads to obtain the final similarity score $a_{i,i}$ for items $v_i$ and $v_j$.\n% %------------------------------------------------------\n\n\\noindent \\textbf{Item-wise Semantic Dependency Hypergraph}. With the encoded semantic dependencies among different items, we generate the item-wise semantic hypergraph by simultaneously connecting multiple highly dependent items with hyperedges. In particular, we construct a set of hyperedges $\\mathcal{E}^p$, where $|\\mathcal{E}^p|$ corresponds to the number of unique items in sequence $S_i$. In the hypergraph $\\mathcal{G}_p$ of item-wise semantic dependencies, each unique item will be assigned with a hyperedge in $\\epsilon \\in \\mathcal{E}^p$ to connect top-$k$ semantic dependent items according to the learned item-item semantic dependency score $\\beta_{j,j'}$ (encoded from the metric learning component). $A_j$ represents the set of top-$k$ semantic correlated items of a specific item $v_j$. We define the connection matrix between items and hyperedges as $\\mathcal{M}^p \\in \\mathbb{R}^{J\\times |\\mathcal{E}^p|}$ in which each entry $m^{p}({v_j,\\epsilon_{j'}})$ is:\n% The connection strength of hyperedge between highly dependent items is defined as follows:\n\\begin{align}\n\\label{eq:sim_edge}\n\tm^{p}({v_j,\\epsilon_{j'}}) &= \\begin{cases}\n\t\t\\beta_{j,j'} & v_{j'} \\in A_j; \\\\\n\t\t0 & otherwise;\n\t\\end{cases}\n\\end{align}\n\\noindent where $\\epsilon_{j'}$ denotes the hyperedge which is assigned to item $v_{j'}$.\\\\\\vspace{-0.12in}\n\n% %------------------------------------------------------\n% With item-item similarities, for each item in the sequence, we assign a hyperedge to it, and let it connect the most similar $k$ items group in the sequence. Note that, in multi-behavior recommendation scenario, an item may appear multiple times in a sequence with different behavior types, we construct only one hyperedge for these recurring items to avoid redundancy. Hence, we construct $|C|$ hyperedges from the sequence $S_u$, which equals the count for unique items in $S_u$.\n% % Therefore, we build a simple mapping function $f_1: S_u \\rightarrow C$ to construct $|C|$ hyperedges from the sequence. $f_1$ for each item in sequence $S_u$ refers to a hyperedge, and identical items refer to the same hyperedge. In other words, $|C|$ is the count for unique items in $S_u$.\n% By connecting top-$k$ similar items of each item to its unique hyperedge, $\\textbf{G}^{s} \\in \\mathbb{R}^{t^\\prime \\times |C|}$ is construct as\n% \\begin{align}\n% \t\\textbf{G}_{i, k}^{s} &= \\begin{cases}\n% \t\ta_{i,j}, & v_i \\in A_j; \\\\\n% \t\t0, & otherwise;\n% \t\\end{cases},\n% \\end{align}\n% where $k$ is the hyperedge assigned to $v_i$, and $A_j$ is the set of top-$k$ similar items for item $v_j$.\n% %------------------------------------------------------\n\n\\noindent \\textbf{Item-wise Multi-Behavior Dependency Hypergraph}.\nTo capture the personalized item-wise multi-behavior dependency in a time-aware environment, we further generate a hypergraph structure $\\mathcal{G}_q$ based on the observed multi-typed interactions (\\eg page view, add-to-cart) between user $u_i$ and a specific item $v_j$ at different timestamps. Here, we define $\\mathcal{E}^q$ to represent the set of items which have multi-typed interactions with user $u_i$. In hypergraph $\\mathcal{G}_q$, the number of hyperedges is equal to $|\\mathcal{E}^q|$. Given that users have diverse multi-behaviour patterns with their interacted items, the constructed multi-behavior dependency hypergraphs vary by users. To be specific, we generate item-hyperedge connection matrix $\\mathcal{M}^q \\in \\mathbb{R}^{J\\times |\\mathcal{E}^q|}$ ($m^q \\in \\mathcal{M}^q$) for hypergraph $\\mathcal{G}_q$ as follow:\n\\begin{align}\n\\label{eq:mb_edge}\n\tm^{q}(v_j^b,\\epsilon_{j}^{b'}) &= \\begin{cases}\n\t\t1 & v_{j}^b \\in \\mathcal{E}^q_j; \\\\\n\t\t0 & otherwise;\n\t\\end{cases}\n\\end{align}\n\\noindent $v_j^b$ represents that item $v_j$ is interacted with user $u_i$ under the $b$-th behavior type. $\\mathcal{E}^q_j$ denotes the set of multi-typed $u_i$-$v_j$ interactions.\n\n% %------------------------------------------------------\n% \\subsubsection{\\rm \\textbf{Multi-Behavior Edges.}} In an attempt to more precisely model the interest transfer and interest aspects of multiple behaviors for the same item, we explicitly connect these multi-behaviors using a hyperedge. The edges enable multiple behaviors for the same item to share similar information, and join the multi-order message passing. For items that have multi-behavior instances in $S_u$ as a set $M$, we assign a hyperegde to connect these instances, and $|M|$ hyperedges are created in total. Hence, $\\textbf{G}^m$ is constructed as\n% % Likewise, we firstly build a mapping function $f_2:S_u \\rightarrow M$ to refer each multi-behavior item to a hyperedge, where $|M|$ is the count for multi-behavior items in $S$. Then, we construct $\\textbf{G}^{beh} \\in \\mathbb{R}^{t^\\prime \\times |M|}$ as\n% \\begin{equation}\n% \t\\textbf{G}_{i, k}^{m} = \\begin{cases}\n% \t\t1, & v_i \\in M; \\\\\n% \t\t0, & otherwise;\n% \t\\end{cases},\n% \\end{equation}\n% where $k$ is the multi-behavior hyperedge assigned to $v_i$.\n% The use of hypergraphs to model multi-behavior signals offers the perspective that we force multi-behavior signals for the same item to share information, which, interpretably, is the global user interest towards the item.\n% %------------------------------------------------------\n\n% \\noindent \\textbf{Hypergraph Intergration}.  \n\nWe further integrate our constructed hypergraph structures $\\mathcal{G}_p$ and $\\mathcal{G}_q$ by concatenating connection matrices $\\mathcal{M}^p$ and $\\mathcal{M}^q$ along with the column side. As such, the integrated hypergraph $\\mathcal{G}$ is constructed with the concatenated connection matrix $\\mathcal{M} \\in \\mathbb{R}^{J \\times (|\\mathcal{E}^p|+|\\mathcal{E}^q|)}$, \\ie $\\mathcal{M} = \\mathcal{M}^p \\mathbin\\Vert \\mathcal{M}^q$. Different behavior-aware interaction sequences result in different hypergraph structures for different users, which allow our \\model\\ model to encode the personalized multi-behavior dependent patterns in a customized way. \n\n% %------------------------------------------------------\n% Finally, we concatenate $\\textbf{G}^{s}$ and $\\textbf{G}^{m}$ along the columns to obtain the completed hypergraph adjacency matrix $\\textbf{G} \\in \\mathbb{R}^{t^\\prime \\times (|C|+|M|)}$ as\n% \\begin{equation}\n% \t\\textbf{G} = \\textbf{G}^{s} \\mathbin\\Vert \\textbf{G}^{m}.\n% \\end{equation}\n% Note that we omit user subscript $u$ for brevity, indeed the construction of $\\textbf{G}$ is specific to user sequence $S_u$.\n% %------------------------------------------------------\n\n%\\paratitle{Hypergraph Redundancy Elimination.}\n%We construct the hypergraph for each sequence that includes the aforementioned two types of hyperedges. However, for sequences that contain multi-behavior items, redundancy exists between these two hyperedges, and affects the gradient weights of the HGNN to the detriment of the learning process. The first manifestation of the redundancy is the duplication of \\textit{sim-hyperedges}. To be specific, different behavior instances of the same item in the sequence usually share the same similar neighbors. Although metric learning takes into account the differences between behaviors, here we take top-$k$ similar neighbors for these items and obtain almost identical sets, with only difference in relative rank. Therefore, for multi-behavior items that share identical top-$k$ similar neighbors, we leave just one \\textit{sim-hyperedge} for them. From the perspective of the matrix, this operation preserves only one of the rows with identical values in the column, so that\n%\\begin{align}\n%\tH^{sim} = [H^{sim}_0, \\cdots, H^{sim}_n]^\\trans, \\text{where}\\ \\nexists (i,j), \\forall k, H^{sim}_{i, k} = H^{sim}_{j, k}\n%\\end{align}\n%\n%The second type of the redundancy is the overlap between \\textit{sim-hyperedges} and \\textit{beh-hyperedges}. When computing item-item similarities, same items of different behaviors usually have a very high similarity, and such values in the \\textit{sim-hyperedges} play a similar role as the \\textit{beh-hyperedges}. This redundancy makes the multi-behavior signals repeatedly transferred, leading to a biased gradient weight and destroys the hierarchy that multi-behavior signals are prior to similar-item signals in the hypergraph convolution. Therefore, to eliminate such redundancy, we mask similarity values of multi-behavior items using a truncation value $s_\\tau$. This operation is performed in the matrices as\n%\\begin{align}\n%\tH^{sim}_{i,j} = \\begin{cases}\n%\t\ts_\\tau, & H^{beh}_{i,k}=1; \\\\\n%\t\tH^{sim}_{i,j}, & otherwise;\n%\t\\end{cases}\n%\\end{align}\n%where $i$ is the position of multi-behavior items in the sequence and $j, k$ are the rows of corresponding two types of hyperedges.\n%\n%Finally, we concatenate $H^{sim}$ and $H^{beh}$ along the columns to obtain the completed hypergraph adjacency matrix.\n%\\begin{equation}\n%\t\\textbf{H} = H^{sim} \\mathbin\\Vert H^{beh}\n%\\end{equation}\n\n"
                    },
                    "subsubsection 2.2.2": {
                        "name": "\\bf Hypergraph Convolution Module",
                        "content": "\n\\label{sec:conv_and_ro}\nIn this module, we introduce our hypergraph message passing paradigm with the convolutional layer, to capture the global multi-behavior dependencies over time. The hypergraph convolutional layer generally involves two-stage information passing \\cite{feng2019hypergraph}, \\ie node-hyperedge and hyperedge-node embedding propagation along with the hypergraph connection matrix $\\mathcal{M}$ for refining item representations. Particularly, we design our hypergraph convolutional layer as:\n\\begin{align}\n\\label{eq:HGCN}\n\t\\textbf{X}^{(l+1)} = \\textbf{D}_v^{-1} \\cdot \\boldsymbol{\\mathcal{M}} \\cdot \\textbf{D}_e^{-1} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans \\cdot \\textbf{X}^{(l)}\n\\end{align}\n\\noindent where $\\textbf{X}^{(l)}$ represents the item embeddings encoded from the $l$-th layer of hypergraph convolution. Furthermore, $\\textbf{D}_v$ and $\\textbf{D}_e$ are diagonal matrices for normalization based on vertex and edge degrees, respectively. Note that the two-stage message passing by $\\boldsymbol{\\mathcal{M}} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans$ takes $O((|\\mathcal{E}^p|+ |\\mathcal{E}^q|)\\times J^2)$ calculations, which is quite time-consuming. Inspired by the design in \\cite{yu2021self}, we calculate a matrix $\\boldsymbol{\\mathcal{M}}^\\prime$ by leveraging pre-calculated $\\beta_{j,j^\\prime}$ to obtain a close approximation representation of $\\boldsymbol{\\mathcal{M}} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans$ and thus boost the inference. The detailed process can be found in the supplementary material. We also remove the non-linear projection following \\cite{he2020lightgcn} to simplify the message passing process. Each item embedding $\\textbf{x}^{(0)}$ in $\\textbf{X}^{(0)}$ is initialized with the behavior-aware self-gating operation as: $\\textbf{x}^{(0)} = (\\textbf{v}_j \\oplus \\textbf{b}_j) \\odot \\text{sigmoid} ( (\\textbf{v}_j \\oplus \\textbf{b}_j) \\cdot \\textbf{w} + \\textbf{r})$.\n\n%---------------------------------------------------\n% \\subsection{Hypergraph Convolution and Readout}\n% \\label{sec:conv_and_ro}\n% \\subsubsection{\\rm \\textbf{Light HGCN.}}Following the spectral hypergraph convolution in \\cite{feng2019hypergraph}, we define the hypergraph convolution function as:\n% \\begin{equation}\n% \\label{eq:HGCN}\n% \t\\boldsymbol{X}^{(l+1)} = \\boldsymbol{D}_v^{-1}\\boldsymbol{G}\\textbf{D}_e^{-1}\\textbf{G}^\\trans\\textbf{X}^{(l)},\n% \\end{equation}\n% where $\\textbf{D}_v$ and $\\textbf{D}_e$ are diagonal matrices that serve the role of normalization, denoting the edge degrees and the vertex degrees, respectively. $\\textbf{X}^{(l)}$ is item embeddings at the $l$-th layer of the hypergraph convolution. We follow the suggestions in \\cite{he2020lightgcn, yu2021self} to remove non-linear activation and linear projection. As elaborated above, we set $\\textbf{X}^{(0)} = \\bm{U}^{(0)}$ in Eq. \\ref{eq:mb-gating} as the initial hypergraph node embeddings to differentiate the input embeddings by applying behavior-aware self-gating operation.\n\n% Technically, the hypergraph convolution performs the two-stage message passing ‘node-hyperedge-node' as the feature transformation and aggregation based on the hypergraph structure $\\bm{G}$. It firstly passes node information to hyperedges as the transformation by applying $\\bm{G}^\\trans\\bm{X}^{(l)}$, then multiplying $\\bm{G}$ to aggregate information along hyperedges for nodes.\n\n% %Model efficiency improvement with approximate matrix\n% %---------------------------------------------------\n% However, such a message passing operation $\\bm{G}\\bm{G}^\\trans$ takes too much time and greatly lower the training and inference, since it requires $O(ht^{\\prime 2})$ calculations for each input sequence. To accelerate the HGCN, some relevant works \\cite{yu2021self, wang2020next} employ simpler matrix calculations to replace the operation. Here we present a light version of the hypergraph convolution function, which leverages the item-item similarity matrix $\\bm{A}$ to approximate $\\bm{G}\\bm{G}^\\trans$. Recall that for \\textit{similarity edges}, the multiplication $\\bm{G}\\bm{G}^\\trans$ strengthens first-order connection and creates slight second-order connection if applicable. Considering item-item similarity matrix $\\bm{A} \\in \\mathbb{R}^{t^\\prime \\times t^\\prime}$, we can approximate the results by enhancing top-$k$ connectivity, while preserving and weakening the tail connectivity. For \\textit{multi-behavior edges}, the situation is simpler -- they act as strengthening the connectivity between multi-behavior instances for the same item in $\\bm{G}\\bm{G}^\\trans$. Therefore, we have to amplify the connectivity between multi-behavior items. Formally, we construct a weight matrix $\\bm{W}$ to adjust the values in $\\bm{A}$ based on the above discussion:\n% \\begin{equation}\n% \\label{eq:light_weight}\n% \t\\bm{W}_{i,j} = \\begin{cases}\n% \t\tw_1, & v_j = v_i; \\\\\n% \t\tw_2, & j \\in \\textsf{topk}(i); \\\\\n% \t\tw_3, & otherwise;\n% \t\\end{cases},\n% \\end{equation}\n% where $w_1,w_2 > 1$ and $w_3 < 1$ are hyper-parameters for scaling the connectivity. Finally, we simplify the hypergraph convolution to a light version as\n% \\begin{equation}\n% \\label{eq:light_hgcn}\n% \t\\boldsymbol{X}^{(l+1)} = \\boldsymbol{D}^{-1}\\bm{G}^\\prime\\bm{X}^{(l)},\n% \\end{equation}\n% where $\\bm{G}^\\prime = (\\bm{A}\\odot \\bm{W})$ is the approximated hypergraph and $\\bm{D}$ is the diagonal degree matrix. Notice that $\\bm{G}^\\prime$ is not a strict approximation of $\\bm{G}\\bm{G}^\\trans$, but we observe that after normalization, the difference between values of $\\bm{G}^\\prime$ and $\\bm{G}\\bm{G}^\\trans$ is small, because we approximate $\\bm{G}\\bm{G}^\\trans$ closely by the effect of the transformation. We report the performance difference between the two versions of hypergraph convolution in Section \\ref{exp:ab}.\n\n%Hypergraph Readout\n% %---------------------------------------------------\n% \\subsubsection{\\rm \\textbf{Hypergraph Readout.}} In order to adapt the hypergraph convolution to the mask-predict training strategy for the main task, we design a hypergraph readout function so that the prediction in the hypergraph view can make full use of contextual information as the Transformer view. Specifically, for a masked position $m$, we inject the pooled contextual information computed by HGCN to its representation $\\bm{x}_m$, where we adopt the simple mean pooling. Note that the expression ability of mean pooling is weak for encoding long bidirectional contextual information, we introduce a sliding window $[m-q_1, m+q_2]$ around the mask position and only adopt mean pooling for items within the window:\n% \\begin{equation}\n% \t\\bm{x}_m = \\textsf{mean}([\\bm{x}_{m-q_1}, \\cdots, \\bm{x}_{m-1}, \\bm{x}_{m+1}, \\cdots, \\bm{x}_{m+q_2}]),\n% \\end{equation}\n% where $\\bm{x}_i$ is calculated by averaging the embeddings at each HGCN layer to avoid the over-smoothing problem:\n% \\begin{equation}\n% \t\\bm{x}_i = \\frac{1}{L}\\sum_{l=1}^L\\bm{x}^{(l)}.\n% \\end{equation}\n% %---------------------------------------------------\n\n"
                    }
                },
                "subsection 2.3": {
                    "name": "Cross-View Aggregation",
                    "content": "\nIn the forecasting layer of \\model\\ framework, we propose to fuse the learned item representations from different views: 1) multi-scale behavior-aware sequential patterns with Transformer architecture; 2) personalized global multi-behavior dependencies with Hypergraph framework. To enable this cross-view aggregation in an adaptive way, we develop an attention layer to learn explicit importance for view-specific item embeddings. Formally, the aggregation procedure is presented as follows:\n\\begin{align}\n\t\\label{eq:fuse}\n\t\\alpha_i = \\textsf{Attn}(\\bm{e}_i) = \\frac{\\exp(\\bm{a}^\\trans \\cdot \\bm{W}_a\\bm{e}_i)}{\\sum_i \\exp(\\bm{a}^\\trans \\cdot \\bm{W}_a\\bm{e}_i)} \\\\\\nonumber\n\t\\bm{e}_i \\in \\{ \\tilde{\\textbf{h}}_i, \\tilde{\\textbf{x}}_i\\};~~\\textbf{g}_i = \\alpha_1 \\cdot \\tilde{\\textbf{h}}_i \\oplus \\alpha_2 \\cdot \\tilde{\\textbf{x}}_i\n\\end{align}\n\\noindent where $\\tilde{\\textbf{h}}_i$ and $\\tilde{\\textbf{x}}_i$ are embeddings from the two views separately for item at the $i$-th position, and $\\bm{a} \\in \\mathbb{R}^{d}$, $\\bm{W}_a \\in \\mathbb{R}^{d\\times d}$ are trainable parameters. Here, to eliminate the over-smooth effect of graph convolution, $\\tilde{\\textbf{x}}_i$ is the average of $\\textbf{x}^{(l)}$ across all convolutional layers. Finally, the probability of $i$-th item in the sequence being item $v_j$ is estimated as: $\\hat{y}_{i,j}=\\textbf{g}_i^\\trans \\textbf{v}_j$, where $\\textbf{v}_j$ represents item $v_j$'s embedding.\n\n% %---------------------------------------------------\n% \\subsection{Dual-View Fusion and Prediction}\n% In the final layer of \\baby, we fuse the learned information from both Transformer view and hypergraph view, and calculate the prediction scores for target items. To fully leverage the benefits of both views, we introduce an Attention mechanism to selectively incorporate information computed by multi-scale Transformer and hypergraph convolution to form the comprehensive maksed item embeddings. For masked item embeddings $\\bm{h}_m, \\bm{x}_m$ from corresponding views, we learn a tuple $(\\alpha_1, \\alpha_2)$ to weight the contribution of different views to the final information as prediction. The Attention function is defined as\n% \\begin{equation}\n% \t\\alpha_i = \\textsf{Attn}(\\bm{e}_i) = \\frac{\\exp(\\bm{a}^\\trans \\cdot \\bm{W}_a\\bm{e}_i)}{\\sum_i \\exp(\\bm{a}^\\trans \\cdot \\bm{W}_a\\bm{e}_i)},\n% \\end{equation}\n% where $\\bm{a} \\in \\mathbb{R}^{d}$ and $\\bm{W}_a \\in \\mathbb{R}^{d\\times d}$ are learnable weight parameters and $\\bm{e}_i \\in \\{\\bm{h}_m, \\bm{x}_m\\}$. Following this, we obtain the weighted multi-view representation of the mask item:\n% \\begin{equation}\n% \\label{eq:fuse}\n% \t\\bm{e}_m = \\alpha_1\\bm{h}_m + \\alpha_2\\bm{x}_m.\n% \\end{equation}\n% Finally, we calculate the user's preference score for the target item masked at position $m$ as\n% \\begin{equation}\n% \\label{eq:dotpredict}\n% \t\\hat{y}_t = \\bm{e}_t^\\trans \\bm{e}_m,\n% \\end{equation}\n% where $\\bm{e}_t \\in \\mathbb{R}^d$ is the embedding of the target item $v_t$.\n\n"
                },
                "subsection 2.4": {
                    "name": "Model Learning And Analysis",
                    "content": "\n% \\subsubsection{\\bf Optimized Objective}\nTo fit our multi-behavior sequential recommendation scenario, we utilize the Cloze task~\\cite{kang2021entangled,sun2019bert4rec} as our training objective to model the bidirectional information of item sequence. We describe our Cloze task settings as follows: Considering the multi-behavior sequential recommender systems, we mask all items in the sequence with the target behavior type (\\eg purchase). To avoid the label leak issue, we replace the masked items as well as the corresponding behavior type embeddings with the special token \\textsf{[mask]}, and leave out masked items in the hypergraph construction in Section \\ref{sec:hg_construct}. Instead, for masked items, we generate its hypergraph embedding $\\textbf{x}_i$ in Equation \\ref{eq:fuse} by adopting sliding-window average pooling function over hypergraph embeddings of the contextual neighbors surrounding the mask position $(m-q_1, m+q_2)$. The details are presented in Algorithm \\ref{algorithm} in the supplementary material. Hence, our model makes prediction on masked items based on the encoded  surrounding context embeddings in the behavior interaction sequence. Given the probability estimation function $\\hat{y}_{i,j}=\\textbf{g}_i^\\trans \\textbf{v}_j$, we define our optimized objective loss with Cross-Entropy as below:\n\\begin{align}\n\t\\mathcal{L} = \\frac{1}{|T|} \\sum_{t \\in T, m \\in M} -\\log(\\frac{\\exp \\hat{y}_{m,t}}{\\sum_{j \\in V} \\exp \\hat{y}_{m,j}})\n\\end{align}\n\\noindent where $T$ is the set of ground-truth ids for masked items in each batch, $M$ is the set of masked positions corresponding to $T$, and $V$ is the item set. The time complexity is analyzed in supplementary material.\n\n% \\subsection{Model Learning And Analysis}\n% \\subsubsection{\\rm \\textbf{Learning.}} It is of great importance to perform data augmentation on the input sequence to enrich training instances. Some existing works construct $t-1$ samples for each $t$-length training instance to predict next items with the cutout sequences like $([v_1],v_2), ([v_1,v_2], v_3)$. However, this strategy is very time-consuming and fails to leverage bidirectional sequential information. On the other hand, more importantly, this is not suitable for multi-behavior sequences, since predicting target behavior (\\eg purchase) is of higher priority to predicting auxiliary behaviors (\\eg page-view, add-to-cart and add-to-fav). Hence, it ignores behavior semantics and is not consistent with the evaluation goal, \\ie predicting purchase.\n\n% In order to train \\baby efficiently and prioritize different behaviors, we introduce a hierarchal \\textit{Cloze} task for data augmentation. The basic \\textit{Cloze} task in sequential recommendation is that we randomly mask each item in the input sequence with probability $p$, and then predict the original ids of the masked items based on their context. In our case, since the evaluation goal is to predict target behaviors and thus all other behaviors are auxiliary, we firstly mask all target behaviors in the sequence, and for items of other behaviors, we randomly mask them with probability $p$ to enrich training instances. We replace masked items as well as the behavior types with special token \\textsf{[mask]} to avoid label leak. Additionally, in the hypergraph construction process (Section \\ref{sec:hg}), \\textsf{[mask]} is not included to avoid label leak.\n\n% As defined in Eq. \\ref{eq:fuse}-\\ref{eq:dotpredict}, we apply dot product for the final representation of \\textsf{[mask]} and the target item $v_t$ to calculate the score of \\textsf{[mask]} being $v_t$. Eventually, We adopt the CrossEntropy loss to optimize the model parameters as\n% \\begin{equation}\n% \t\\mathcal{L} = \\frac{1}{|T|} \\sum_{t \\in T} -\\log(\\frac{\\exp \\hat{y}_t}{\\sum_{j \\in V} \\exp \\hat{y}_j}),\n% \\end{equation}\n% \\noindent where $T$ is the set of ground-truth ids for masked items in each batch, and $V$ is the set to contain all items.\n\n% To further improve our model efficiency, we propose to learn a item-wise relevance matrix to approximate $\\mathbf{M}\\cdot \\mathbf{M}^T$ with a trainable weight matrix.\n\n% \\subsubsection{\\rm \\textbf{Time Complexity Analysis.}} Here we analyze the time complexity from two views of our \\baby framework. (1) For the multi-scale Transformer component, the calculation of \\textit{multi-scale attention} counts for most of the time consuming. It takes $O(L\\times d\\times ((\\frac{t}{c})^2+(\\frac{t}{p_1})^2+(\\frac{t}{p_2})^2))$ calculations to compute the attention matrix for three stacked attention layers. Referring to the analysis in \\cite{wang2020linformer}, since we choose values for scales that satisfies $c,p_1,p_2 \\ll t$, we achieve an approximately linear time complexity, which is $O(3Ldt)$. (2) For the hypergraph view, the calculation to generate node-hyperedge-node information transformation matrix takes $O(ht^{\\prime 2})$ calculations, where $h$ is the number of hyperedges and $t^\\prime$ is the count of non-padding items in the sequence. To construct hyperedges, we also need $O(dt^2)$ calculations for obtaining the item-item similarity matrix. The following graph convolution, as most GCN models, takes $O(t^\\prime d^2)$ calculations. However, we adopt the proposed light HGCN function to eliminate the $O(ht^{\\prime 2})$ calculations. Also, the additional calculations of light HGCN to construct $\\bm{W}$ can be done at data pre-processing to save time. Therefore, the overall time complexity of \\baby is $O(3Ldt + Lt^\\prime d^2 + dt^2)$, which is competitive to most of the baselines. For instance, the time complexity of BERT4Rec \\cite{sun2019bert4rec} is $O(Ldt^2)$, and empirically, \\baby has 2.8x larger time complexity compared to BERT4Rec, which is totally acceptable considering the greatly fastened convergence speed in Section \\ref{sec:exp:convergence}.\\vspace{-0.05in}\n"
                }
            },
            "section 3": {
                "name": "Experiments",
                "content": "\n\n\n\n\n% In this section, we perform extensive experiments in various research lines to evaluate \\baby by exploiting the following research questions:\n\nThis section aims to answer the following research questions:\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{RQ1}: How does our \\model\\ perform as compared to various state-of-the-art recommendation methods with different settings?\n\\item \\textbf{RQ2}: How effective are the key modules (\\eg multi-scale attention encoder, multi-behavior hypergraph learning) in \\model? \n\\item \\textbf{RQ3}: How does \\model\\ perform to alleviate the data scarcity issue of item sequences when competing with baselines?\n\\item \\textbf{RQ4}: How do different hyperparameters affect the model performance? (Evaluation results are presented in Appendix \\ref{sec:hp}).\n\\end{itemize}\n\n\n% \\begin{itemize}[leftmargin=*]\n% \\item \\textbf{RQ1:} How does \\baby perform in comparison with \tsate-of-the-art and newest sequential recommendation methods?\n% \\item \\textbf{RQ2:} What are the effects and benefits that key components of \\baby bring?\n% \\item \\textbf{RQ3:} How do different key modules in our \\baby framework contribute to the overall performance?\n% \\item \\textbf{RQ4:} How sensitive is \\baby towards different hyperparameter settings?\n% \\end{itemize}\n\n\n\n\\vspace{-0.05in}\n",
                "subsection 3.1": {
                    "name": "Experimental Settings",
                    "content": "\n",
                    "subsubsection 3.1.1": {
                        "name": "Datasets.",
                        "content": "\nWe utilize three recommendation datasets collected from real-world scenarios. i) \\paratitle{Taobao}. This dataset is collected from Taobao which is one of the largest e-commerce platforms in China. Four types of user-item interactions are included in this dataset, \\ie target behaviors-\\textit{purchase}; auxiliary behaviors-\\textit{add-to-favorites, add-to-cart, page view}. ii) \\paratitle{Retailrocket}. This dataset is generated from an online shopping site-Retailrocket over 4 months, to record three types of user behaviors, \\ie  target behaviors-\\textit{purchase}; auxiliary behaviors-\\textit{page view \\& add-to-cart}. iii) \\paratitle{IJCAI}. This dataset is released by IJCAI Contest 2015 for the task of repeat buyers prediction. It shares the same types of interaction behaviors with the Taobao dataset. The detailed statistical information of these experimented datasets are summarized in Table~\\ref{tab:datasets}. Note that different datasets vary by average sequence length and user-item interaction density, which provides diverse evaluation settings.\n\n%\\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=649}\n%\\footnote{https://www.kaggle.com/retailrocket/ecommerce-dataset}\n%\\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=47}\n\n% We utilize three real-world datasets collected from large online platforms for model evaluations.\n\n% \\paratitle{Taobao\\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=649}.} The Taobao dataset is a subset of Taobao User Behavior Dataset \\cite{zhu2018tree} that is collected on Taobao.com during November 25 to December 03, 2017. There are four types of user behavior, \\ie \\textit{buy, add-to-cart, add-to-favorites} and \\textit{page view}.\n\n% \\paratitle{Retailrocket\\footnote{https://www.kaggle.com/retailrocket/ecommerce-dataset}.} It is collected on Retailrocket, a multi-channel shopping recommendation website, over 4.5 months. Three types of behavior are included, separately \\textit{buy, add-to-cart} and \\textit{page view}.\n\n% \\paratitle{IJCAI\\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=47}.} This dataset is released by IJCAI Contest 2015 for repeat buyers prediction. It is collected from Tmall.com during big promotions, and includes the same four behavior types as Taobao Dataset.\n\n% We provide detail statistics for the three datasets in Table \\ref{tab:datasets}. From the table, we can see that the three datasets vary in size and have differentiated average sequence length and sparsity, which allows us to more thoroughly evaluate the performances under different scenarios.\n\\vspace{-0.05in}\n"
                    },
                    "subsubsection 3.1.2": {
                        "name": "Evaluation Protocols.",
                        "content": "\nIn our experiments, closely following the settings in~\\cite{sun2019bert4rec,tang2018caser}, we adopt the \\textit{leave-one-out} strategy for performance evaluation. For each user, we regard the temporally-ordered last purchase as the test samples, and the previous ones as validation samples. Additionally, we pair each positive sample with 100 negative instances based on item popularity~\\cite{sun2019bert4rec}. We utilize three evaluation metrics: \\textit{Hit Ratio (HR@N), Normalized Discounted Cumulative Gain (NDCG@N)} and \\textit{Mean Reciprocal Rank (MRR)}~\\cite{wang2020next,yang2022knowledge,wu2019srgnn}. Note that larger HR, NDCG and MRR scores indicate better recommendation performance.\n\n% \\subsubsection{\\rm \\textbf{Evaluation Protocols.}} To simulate the real-world next-item prediction scenario, we closely follow the settings in \\cite{sun2019bert4rec, kang2018sasrec, tang2018caser} to adopt the \\textit{leave-one-out} strategy for evaluation. Specifically, we randomly select $10,000$ users for Taobao and IJCAI datasets, and $2,000$ for Retailrocket dataset based on the user counts. For each user, we treat the last \\textit{purchase} behavior of the sequence as the test data and the previous \\textit{purchase} behavior as the validation data. Additionally, we sample $100$ negative items for each positive sample in the test and evaluation data. To obtain reliable and representative samples \\cite{sun2019bert4rec, huang2018improving}, these negative items are sampled based on the popularity. Finally, for each user, we rank the collected $101$ samples with $1$ ground-truth to evaluate the recommendation performance. For metrics, we adopt a variety of metrics that are widely used for sequential recommendation, separately \\textit{Hit Ratio (HR@N), Normalized Discounted Cumulative Gain (NDCG@N)} and \\textit{Mean Reciprocal Rank (MRR)}. Since we have single ground-truth for each test user, \\textit{HR@N} equals \\textit{Recall@N}. Here we report results for $N=\\{5, 10\\}$, and \\textit{MRR} is calculated over all $101$ samples.\n\\vspace{-0.05in}\n"
                    },
                    "subsubsection 3.1.3": {
                        "name": "Baselines.",
                        "content": "\nWe compare our \\baby with a variety of recommendation baselines to validate the performance superiority.\n\\paratitle{General Sequential Recommendation Methods.}\n\\vspace{-0.05in}\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{GRU4Rec}~\\cite{hidasi2015session}. It utilizes the gated recurrent unit as sequence encoder to learn dynamic preference with a ranking-based loss.\n% \t\\item \\textbf{GRU4Rec} \\cite{hidasi2015session}. It models sequential information with GRU units and a ranking-based loss for session-based recommendation.\n\n\\item \\textbf{SASRec}~\\cite{kang2018sasrec}. The self-attention mechanism is leveraged in this method to encode the item-wise sequential correlations.\n\n\\item \\textbf{Caser}~\\cite{tang2018caser}. This method integrates the convolutional neural layers from both vertical and horizontal views to encode time-evolving user preference of item sequence.\n\n\\item \\textbf{HPMN}~\\cite{ren2019lifelong}. It employs a hierarchically structured periodic memory network to model multi-scale transitional information of user sequential behaviors. The incremental updating mechanism is introduced to retain behaviour patterns over time.\n% It employs a hierarchical periodic memory network to capture multi-scale information in the sequence.\n\n\\item \\textbf{BERT4Rec}~\\cite{sun2019bert4rec}. It uses a bidirectional encoder for modeling sequential information with Transformer. The model is optimized with the Cloze objective, and has produced state-of-the-art performance among sequence learning-based baselines.\n\n\\end{itemize}\n\n% \\begin{itemize}[leftmargin=*]\n% \t\\item \\textbf{Caser} \\cite{tang2018caser}. It utilizes CNN layers in both vertical and horizontal views to model the sequential information.\n% \t\\item \\textbf{HPMN} \\cite{ren2019lifelong}. It employs a hierarchical periodic memory network to capture multi-scale information in the sequence.\n% \t\\item \\textbf{GRU4Rec} \\cite{hidasi2015session}. It models sequential information with GRU units and a ranking-based loss for session-based recommendation.\n% \t\\item \\textbf{SASRec} \\cite{kang2018sasrec}. It captures attentive representations for sequential items from the left-to-right order by employing the self-attention mechanism and Transformer layer.\n% \t\\item \\textbf{BERT4Rec} \\cite{sun2019bert4rec}. It uses a bidirectional encoder for modeling sequential information with Transformer layer and a \\textit{Cloze} objective loss. It achieves \\textit{state-of-the-art} performance among general sequential methods.\n% \\end{itemize}\n% \\vspace{-0.05in}\n\\paratitle{Graph-based Sequential Recommender Systems.}\n\\vspace{-0.05in}\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{SR-GNN}~\\cite{wu2019srgnn}. It generates graph structures based on item-item transitional relations in sequences, and conducts graph-based message passing to capture local and global user interests.\n% \t\\item \\textbf{SR-GNN} \\cite{wu2019srgnn}. It constructs graphs for sequences to encode sequential items and uses hybrid embedding to consider both local and global interests for users.\n\\item \\textbf{GCSAN}~\\cite{xu2019gcsan}. It empowers the self-attention mechanism with with a front-mounted GNN structure. The attentive aggregation is performed over the encoded graph embeddings.\n% \t\\item \\textbf{GCSAN} \\cite{xu2019gcsan}. It aims to empower self-attention network with a front-mounted GNN, constructing a graph firstly and apply self-attention mechanism on graph embeddings to encode long-term interests.\n\\item \\textbf{HyperRec}~\\cite{wang2020next}. It designs sequential hypergraphs to capture evolving users' interests and regards users as hyperedges to connect interacted items, so as to model dynamic user preferences. \n\\item \\textbf{SURGE}~\\cite{chang2021surge}. It adopts metric learning to build personalized graphs and uses hierarchical attention to capture multi-dimensional user interests in the graph. \n% It achieves state-of-the-art performance among graph-based sequential recommender systems.\n\\end{itemize}\n% \\vspace{-0.05in}\n\\paratitle{Multi-Behavior Recommendation Models.}\n\\vspace{-0.05in}\n\\begin{itemize}[leftmargin=*]\n\t\\item \\textbf{BERT4Rec-MB} \\cite{sun2019bert4rec}. We enhance the BERT4Rec method to handle the dynamic multi-behavior context by injecting behavior type representations into the input embeddings for self-attention.\n\t\\item \\textbf{MB-GCN}~\\cite{jin2020mbgcn}. This model is built upon the graph convolutional layer to refine user/item embeddings through the behavior-aware message passing on the user-item interaction graph.\n% \t\\item \\textbf{MBGCN}~\\cite{jin2020mbgcn}. It constructs a unified multi-behavior graph to model users' multi-behaviors towards items, and perform behavior-aware propagation on the graph.\n\\item \\textbf{NMTR}~\\cite{gao2019nmtr}. It defines the behavior-wise cascading relationships to model the dependency among different types of behaviors under a multi-task learning paradigm.\n% \t\\item \\textbf{NMTR} \\cite{gao2019nmtr}. It uses cascading to capture relationships between users and behaviors, and performs multi-task learning to model multi-behavior signals.\n\t\\item \\textbf{MB-GMN}~\\cite{xia2021graph}. It employs a graph meta network to capture personalized multi-behavior signals and model the diverse multi-behavior dependencies. It generates state-of-the-art performance among different multi-behavior recommendation methods.\n\\end{itemize}\n\n% To ensure fair comparison, we implement our \\baby in the \\textit{Recbole} \\cite{zhao2021recbole} environment and evaluate most of the baselines in the \\textit{Recbole} and \\textit{Microsoft Recommenders}\\footnote{https://github.com/microsoft/recommenders} environment. For baselines not included in the two environments, we either implement them in the \\textit{Recbole} (BERT4Rec-MB, HPMN), or adopt the author-released unified evaluation environment (MB-GCN, NMTR, MBGMN) to minimize the inconsistency of the evaluation environment.\n\n% For ensure fair performance comparison, we either use the released source code or \\textit{Recbole} \\cite{zhao2021recbole} library for evaluating the performance of baseline methods. We follow the parameter settings suggested in their original papers and use grid search for parameter tuning to achieve their best performance. For example, the graph neural network depth is searched from the range of \\{1,2,3,4\\}. The number of multi-head latent spaces is tuned from \\{$2^0$, $2^1$, $2^2$\\}.\n\n% To ensure fair performance comparison, we either use the released source code or \\textit{Recbole} \\cite{zhao2021recbole} library for evaluating the performance of baseline methods.\n\n"
                    },
                    "subsubsection 3.1.4": {
                        "name": "Hyperparameter Settings.",
                        "content": "\nIn \\model\\ model, we search the number of hypergraph propagation layers from \\{1,2,3,4\\}. The number of multi-head channels is set as 2 for both self-attention and metric learning components. The value of $k$ for constructing item-wise semantic dependency hypergraph is tuned from $[4,6,8,10,12,14]$. Considering that datasets vary by average sequence length, the multi-scale setting parameters $(C, p_1, p_2)$ are searched amongst the value range of ([20,4,20],[20,8,40],[40,4,20],[40,8,40]).\n\n% We set all hyper-parameters according to the original papers if the authors adopt the same dataset, or by performing grid-search to obtain the best hyper-parameter settings. For our \\baby, the Transformer layer number and hypergraph convolution layer number are set to 2 and the head number for multi-head attention and multi-head metric learning is set to 2. For simplicity, we heuristically set the adjustment weights $(w_1,w_2,w_3)$ for light HGCN to $[1.5, 1.2, 0.6]$ and for datasets with different average sequence length, we set the graph readout window $(q_1, q_2)$ to $[[10,6],[20,12],[30,18]]$. We tune the $k$ for selecting top-$k$ similar items group amongst $[4,6,8,10,12,14]$ and scales $(c, p_1, p_2)$ amongst $[[20,4,20],[20,8,40],[40,4,20],[40,8,40]]$.\n\n% \\vspace{-0.05in}\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Performance Evaluation (RQ1)",
                    "content": "\n% We summarize the overall performance results across \\baby and all baselines in Table \\ref{tab:results}. Based on the results, we make following observations:\nWe report the detailed performance comparison on different datasets in Table~\\ref{tab:results} and summarize the observations as followed:\n\n\\begin{itemize}[leftmargin=*]\n\\item The proposed \\model\\ consistently outperforms all types of baselines by a significant margin on different datasets. The performance improvements can be attributed from: i) Through the multi-scale behavior-aware Transformer, \\model\\ is able to capture the behavior-aware item transitional patterns from fine-grained to coarse-grained time granularities. ii) With the hypergraph neural network for multi-behavior dependency learning, we endow \\model\\ with the capability of capturing long-range item correlations across behavior types over time. \\\\\\vspace{-0.12in}\n\n    % \\item The proposed \\baby consistently outperforms all baselines over all metrics by a significant margin, which  the effectiveness and superiority of the dual-view learning based on multi-scale Transformer and hypergraph. Since we employ three real-world datasets from different platforms, with differentiated sequence lengths, sparsity and levels of user and item volume, the results demonstrate the generalization ability of our \\baby in multi-behavior sequential recommendation scenarios.\n\n\\item By jointly analyzing the results across different datasets, we can observe that our \\model\\ is robust to different recommendation scenarios with various data characteristics, such as average sequence length and user-item interaction density, reflecting various user behaviour patterns in many online platforms.\\\\\\vspace{-0.12in}\n\n\\item Graph-based sequential recommendation methods (\\eg SR-GNN, GCSAN, SURGE) perform worse than general sequential baselines (\\eg BERT4Rec, HPMN) on IJCAI dataset with longer item sequences. The possible reason is that passing message between items over the generated graph structures based on their directly transitional relations can hardly capture the long-term item dependencies. However, the performance superiority of GNN-based models can be observed on Retailrocket with shorter item sequences. In such cases, modeling of short-term item transitional regularities is sufficient for capturing item dependencies.\\\\\\vspace{-0.12in}\n\n    % \\item We can observe that existing graph-based models are more applicable to short sequences, which implies that these works may ignore long-term interest information on sequences. For Retailrocket dataset with the shortest average sequence length, graph-based models (\\ie SR-GNN, GCSAN, SURGE, MB-GCN and MB-GMN) perform very well, all of which outperform the strongest general sequential baseline, \\ie BERT4Rec. However, most of these models lose their superiority on Taobao dataset, which is of medium sequence length. Specifically, SR-GNN and SURGE show serious performance degradation, merely on the same level with GRU4Rec. Eventually, on IJCAI dataset that has the longest average sequence length, all graph-based models are no better than BERT4Rec. Another significant problem with graph-based sequential models is the performance instability across datasets, \\eg SURGE failing on Taobao and SR-GNN failing on IJCAI, which hints that these models may not be very generalizable.\n\n\\item BERT4Rec-MB outperforms BERT4Rec in most evaluation cases. In addition, it can be found that multi-behavior recommendation models (\\eg MB-GCN, MB-GMN) achieve comparable performance to other baselines. These observations indicate the effectiveness of incorporating multi-behavior context into the learning process of user preference. With the effective modeling of dynamic multi-behaviour patterns from both short-term and long-term perspectives, our \\model\\ is more effective than those stationary multi-behavior recommendation approaches.\n    \n    % \\item Models that incorporate multi-behavior signals show better overall performance, although there were exceptions. This demonstrates the effectiveness of encoding multi-behavior signals in sequences. The most obvious example is that BERT4Rec-MB outperforms BERT4Rec on two of the datasets by simply adding behavior embedding to the inputs. Besides, graph-based multi-behavior models, \\ie MB-GCN and MB-GMN consistently perform better than most of the general sequential models, \\eg SASRec, HPMN and Caser. However, these superior multi-behavior models fail to compete with BERT4Rec on IJCAI dataset,  which indicates that multi-behavior signals can sometimes be noisy to reduce the performance, especially on long sequences.\n\\end{itemize}\n\n\n\n\\vspace{-0.05in}\n"
                },
                "subsection 3.3": {
                    "name": "Ablation Study (RQ2)",
                    "content": "\n\\label{exp:ab}\n",
                    "subsubsection 3.3.1": {
                        "name": "Effects of Key Components.",
                        "content": "\nWe firstly investigate the effectiveness of different components of our \\baby from both Transformer and Hypergraph learning views. Specifically, we generate four variants and make comparison with our \\model\\ method:\n\\vspace{-0.05in}\n\\begin{itemize}[leftmargin=*]\n    % \\item \\textit{(+) Full HGCN.} This variant does not include the light version of HGCN elaborated in Eq. \\ref{eq:light_weight}-\\ref{eq:light_hgcn}. Instead, it employs the original HGCN function in Eq. \\ref{eq:HGCN}.\n\n\\item \\textit{(-) MB-Hyper.} This variant does not include the hypergraph of item-wise multi-behavior dependency to capture the long-range cross-type behavior correlations. \n    \n    % \\item \\textit{(-) MB-Hyperedges.} This variant does not include hyperedges for multi-behavior items. In the light HGCN, it removes the effect of $w_1$ in Eq. \\ref{eq:light_weight}.\n\n\\item \\textit{(-) ML-Hyper.} In this variant, we remove the hypergraph message passing over the hyperedges of item semantic dependence (encoded with the metric learning component).\n    \n    % \\item \\textit{(-) ML-Hyperedges.} This variant removes the effect of $w_2$ in Eq. \\ref{eq:light_weight} to exclude hyperedges learned by item-item similarity based on metric learning.\n\n\\item \\textit{(-) Hypergraph.} This variant disables the entire hypergraph item-wise dependency learning, and only relies on the multi-scale Transformer to model the sequential behavior patterns.\n\n\\item \\textit{(-) MS-Attention.} For this variant, we replace our multi-scale attention layer with the original multi-head attentional operation.\n\n\\end{itemize}\n\n% We also devise \\textit{(-) Hyperedges} and \\textit{(-) Multi-Scale Attention} which separately remove the whole hyperedge view and substitute the multi-scale attention with the original multi-head attention in \\cite{vaswani2017attention}.\n\nFrom the reported results in Table~\\ref{tab:ab}, we summarize the following observations to show the rationality of our model design. 1) With the incorporation of hypergraph-based dependency learning on either item-wise latent semantic ((-) MB-Hyper) or dynamic multi-behavior correlations ((-) ML-Hyper), \\model\\ can further boost the recommendation performance. 2) Comparing with the vanilla multi-head attention ((-) MS-Attention), the effectiveness of our multi-scale low-rank self-attention can be validated.\n\n% We report the results in Table \\ref{tab:ab} and the performances clearly demonstrate the superiority of \\baby in: (i) the excellent approximation capability of light HGCN; (ii) the effectiveness of two types of hyperedges and (iii) the positive contributions of both the Transformer and the hypergraph views.\n\n\n\n"
                    },
                    "subsubsection 3.3.2": {
                        "name": "Contribution of Learning Views.",
                        "content": "\nWe further investigate the contribution of sequential and hypergraph learning views, by presenting the distributions of our learned importance scores of $\\textbf{h}_i$ and $\\textbf{x}_i$ in Figure \\ref{fig:contrib}. In particular, $\\textbf{h}_i$ preserves multi-scale behavior dynamics of diverse user preference and $\\textbf{x}_i$ encodes the global multi-behavior dependencies. We can observe that hypergraph learning view contributes more to the effective modeling of dynamic multi-behavior patterns with longer item sequences (\\eg Taobao and IJCAI dataset). This further confirms the efficacy of our behavior-aware hypergraph learning component in capturing the long-range item dependencies in multi-relational sequential recommendation.\n\n% We further delve into the contribution of the two views to the prediction task on different datasets. We present the attention contributions of sequential view and hypergraph view as the box plots in Figure \\ref{fig:contrib}. From the results, we can conclude that in datasets with larger average sequence length, the contribution of hypergraph view outweighs the sequential view. This may be because the hypergraph view can better encode multi-behavior dependencies and user interests from a long-term perspective. However, note that the contribution of hypergraph view does not increase proportionally to the increase in the average length of sequences. Specifically, IJCAI dataset has a longer average length than Taobao dataset, but the median and mean contribution of hypergraph view are lower. This further indicates that the applicability of the two modeling views, \\ie sequential view and graph view, varies for different datasets. Hence, it is more beneficial to fuse the two views attentively to accommodate various real-world data characteristics.\n\n\n\n% \\vspace{-0.05in}\n"
                    }
                },
                "subsection 3.4": {
                    "name": "Model Benefit Study (RQ3)",
                    "content": "\n",
                    "subsubsection 3.4.1": {
                        "name": "w.r.t",
                        "content": "\nTo further study the robustness of our model, we evaluate \\model\\ on item sequences with different length. Specifically, we split users into five groups in terms of their item sequences and conduct the performance comparison on each group of users. From results presented in Figure~\\ref{fig:group_test}, \\model\\ outperforms several representative baselines not only on the shorter item interaction sequences, but also on the longer item sequences. It indicates that our recommendation framework is enhanced by injecting the behavior-aware short-term and long-term dependencies (from locally to globally) into the sequence embeddings. Such data scarcity issue is hard to be alleviated purely from the general and GNN-based sequential recommender systems.\\vspace{-0.05in}\n\n% \\subsection{Benefits Study (RQ3)}\n% \\subsubsection{\\rm \\textbf{Tests on sequence groups with different lengths.}} In conventional sequential recommendation methods, both shorter and longer sequences encounter difficulties in prediction, the former because the sparse data are not sufficient to accurately predict the next behavior, and the latter because of poor modeling of users' long-term interests. However, \\baby can bridge this gap because the multi-scale attention mechanism captures information at different temporal granularities in the sequence, and the hypergraph view can better encode long-term user interest, regardless of the time span. We group the test data in Taobao and IJCAI by sequence length into five to test the performances of \\baby and baselines on different length groups. We present the evaluation results in Figure \\ref{fig:group_test}, where the larger group ID denotes longer sequence length. Obviously, \\baby consistently outperforms some strongest baselines by a significant gap across most settings (9 of 10), which demonstrates the superiority of \\baby dealing with sequences of different lengths. In particular, the performance gap is larger in short sequence groups (0,1 and 2), which proves \\baby's capability of tackling the sparse data issue.\n\n\n\n"
                    },
                    "subsubsection 3.4.2": {
                        "name": "Model Convergence Study.",
                        "content": "\nWe further investigate the convergence property of our \\model\\ and various sequential and graph-based temporal recommendation methods in Figure~\\ref{fig:convergence}. Along the model training process, \\model\\ achieves faster convergence rate compared with most competitive methods. For example, \\model\\ obtains its best  performance at epoch 2 and 8, while BERT4Rec and BERT4Rec-MB take 10 and 15 epochs to converge on Taobao and IJCAI datasets, respectively. This observation suggests that exploring the augmented multi-behavior information from both sequential and hypergraph views can provide better gradient to guide the model optimization in sequential recommendation.\n\n% \\subsubsection{\\rm \\textbf{Study on training convergence speed.}}\\label{sec:exp:convergence} Above experiments demonstrate that the dual-view learning can encode enriched multi-behavior training information to improve recommendation performance. Here we would like to further study the influence of the dual-view learning on training efficiency, \\ie the model's convergence speed. We compare the training curve of \\baby with BERT4Rec and BERT4Rec-MB, which adopt the same CE loss, and the results are presented in Figure \\ref{fig:convergence}. As the epoch number increases, the upper subfigures draw the training loss curves and the bottoms display \\textit{HR@5} on test sets. From the figures, obviously, \\baby converges much faster than the other two baselines in both loss decreasing and testing performance increasing. In particular, \\baby achieves its best testing performance at epoch 2 and 8, while the two baselines take 10 and 15 epochs for Taobao and IJCAI datasets respectively. This benefit may be caused by: (i) \\baby encoding the multi-behavior sequential information from two views, thus exploiting more useful information to guide the gradient decreasing and optimizing the parameters; (ii) for the hypergraph view, we set sliding windows for graph readout in Section \\ref{sec:conv_and_ro} to reduce possible noise.\n\n\n% \\begin{figure}[t]\n% \\centering\n% \\subfigure[Sensitivity to attention scales]{\n% \\label{fig:HP:scale}\n% \\includegraphics[width=0.48\\linewidth]{material/HP_scales.pdf}}\n% \\subfigure[Sensitivity to sim-group length]{\n% \\label{fig:HP:glen}\n% \\includegraphics[width=0.45\\linewidth]{material/HP_hglen.pdf}}\n% \\vspace{-0.2in}\n% \\caption{Hyperparameter study of \\model\\ framework.}\n% \\label{fig:HP}\n% \\vspace{-0.2in}\n% \\end{figure}\n\n% \\vspace{-0.05in}\n% \\subsection{Hyperparameter Study (RQ4)}\n% We conduct experiments to analyze the influence of key hyperparameters in our \\model\\ framework and report results in Figure~\\ref{fig:HP}.\\\\\\vspace{-0.12in}\n\n% \\noindent \\textbf{Impact of Multi-Scale Settings}. We search multi-scale setting parameters $(C, p_1, p_2)$ among the range $\\{[20,4,20],[20,8,40],[40,4,20]$ $[40,8,40]\\}$. We present the observations as followed:\n% \\begin{itemize}[leftmargin=*]\n% \\item The best performance on Retailrocket and Taobao datasets can be achieved with $(p_1,p_2)=(4,20)$ and $(p_1,p_2)=(8,40)$ given the difference of average sequence length.\n% \\item For the low-rank projection scale parameter $C$, we can notice that projecting original self-attentive sequence embedding space into $\\frac{J}{20}$ channels can bring the best performance on IJCAI and Retailrocket datasets. For Taobao dataset, we can observe that $\\frac{J}{40}$ performs better than $\\frac{J}{20}$, which indicates that dense user-item interaction data may need less low-rank projection channels for the best sequential pattern encoding.\n% \\end{itemize}\n\n% % We plot the performances of \\baby under different hyper-parameter settings of attention scales $c, p_1, p_2$ and similarity group lengths $k$ in Figure \\ref{fig:HP}, to further explore the sensitivity of \\baby to these two key hyper-parameters and to provide suggestions for experiments and model deployment. Figure \\ref{fig:HP:scale} presents the effects of attention scales. In general, the selection of attention scales is independent with the average sequence length of the dataset. In particular, on IJCAI and Retailrocket with the longest and shortest average length respectively, scales [10,4,20] bring both the best results. However, [5,8,40] is more suitable for Taobao. We also notice that the first scale $c$ for low-rank project attention has larger effect on the performances. For example, setting $c$ from 5 to 10 and keeping $p_1,p_2$ unchanged brings obvious performance change on all datasets. Hence, we suggest tuning $c$ more delicately according to the adopted dataset.\n\n% \\noindent \\textbf{Impact of Item-wise Semantic Dependency Set}.\n% Our hypergraph item dependency encoder investigates the latent semantic correlations among different items. We search the top-$k$ semantic dependent items from \\{4,6,8,10,12,14\\} for global message passing through the item-wise semantic hyperedges. Observations are:\n% \\begin{itemize}[leftmargin=*]\n% \\item The best settings of $k$ is proportionally to the average sequence length of different datasets. It indicates that larger hypergraph propagation scope is better for longer item sequences.\n% \\item Increasing the number of connected items through hyperedges may firstly boost the performance at the early stage, and then lead to performance degradation by involving noise during the hypergraph-based embedding propagation.\n% \\end{itemize}\n\n% We also present results on all datasets across different settings of similarity group length $k$ in Figure \\ref{fig:HP:glen}. This parameter determines the number of similar items connected to each item by similarity hyperedges, affecting the extent of information propagation on the graph. Generally, the best setting of $k$ is proportionally to the average sequence length of the dataset. Specifically, as the average sequence length increases from Retailrocket, Taobao and IJCAI, the best value of $k$ changes from 6 to 10 and to 12.\n\n% \\subsection{Case Study}\n\n\n\n\\vspace{-0.05in}\n"
                    }
                },
                "subsection 3.5": {
                    "name": "Case Study",
                    "content": "\nIn this section, we conduct further model analysis with case study to show the model interpretation for multi-behavior dependency modeling. In particular, we show user-specific cross-type behavior dependencies in Figure~\\ref{fig:case_study} (a)-(d). Each $4\\times 4$ dependency matrix is learned from our multi-scale Transformer. We compute the item-item correlations by considering their behavior-aware interactions in the specific sequence $S_i$ of user $u_i$. Figure~\\ref{fig:case_study} (e)-(f) show the scale-specific multi-behavior dependencies with the scales of $p_1$ and $p_2$ in our multi-scale modeling of behavior-aware sequential patterns. In Figure~\\ref{fig:case_study} (g), we show the overall relevance scores among different types of behaviors in making final forecasting on target behaviors. Additionally, our hypergraph-based item dependencies ($\\boldsymbol{\\mathcal{M}} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans$) are shown in Figure~\\ref{fig:case_study} (h), such as hypergraph-based i) behavior-aware item relevance; and ii) item-wise semantic dependence.% \\vspace{-0.2in}\n% \\vspace{-0.1in}\n"
                }
            },
            "section 4": {
                "name": "Related Work",
                "content": "\n\n\\noindent \\textbf{Sequential Recommendation}.\nEarlier studies solve the next-item recommendation using the Markov Chain-based approaches to model item-item transitions~\\cite{rendle2010mc, he2016fusing}. In recent years, many efforts have been devoted to proposing neural network-enhanced sequential recommender systems to encode the complex dependencies among items from different perspectives. For example, recurrent neural network in GRU4Rec~\\cite{hidasi2015session} and convolutional operations in Caser~\\cite{tang2018caser}.\n% For example, GRU4Rec~\\cite{hidasi2015session} leverages the recurrent neural network as the sequence encoder for user preference learning. Caser~\\cite{tang2018caser} models the item-item transitions, through convolutional sequence embedding with point-level and union-level sequential patterns.\nInspired by the strength of Transformer, SASRec~\\cite{kang2018sasrec} and BERT4Rec~\\cite{sun2019bert4rec} are built upon the self-attention mechanism for item-item relation modeling. Furthermore, recently emerged graph neural networks produce state-of-the-art performance by performing graph-based message passing among neighboring items, in order to capture sequential signals, \\eg SR-GNN~\\cite{wu2019srgnn}, MTD~\\cite{huang2021graph}, MA-GNN~\\cite{ma2020memory} and SURGE~\\cite{chang2021surge}. However, most of those methods are specifically designed for singular type of interaction behaviors, and cannot handle diverse user-item relationships. \\\\\\vspace{-0.12in}\n\n\\noindent \\textbf{Hypergraph Learning for Recommendation}. Motivated by expressiveness of hypergraphs~\\cite{feng2019hypergraph,yi2020hypergraph}, hypergraph neural networks are utilized in several recent recommender systems to model high-order relationships, such as multi-order item correlations in HyperRec~\\cite{wang2020next}, global user dependencies in HCCF~\\cite{xia2022hypergraph}, high-order social relationships in MHCN~\\cite{yu2021self}, and item dependencies with multi-modal features in HyperCTR~\\cite{he2021click}. Following this research line, this work integrates the hypergraph neural architecture with Transformer architecture to encode behavior-aware sequential patterns from local to global-level for comprehensive behavior modeling. \\\\\\vspace{-0.12in}\n\n\\noindent \\textbf{Multi-Behavior Recommender Systems}. There exist recently developed multi-behavior recommender systems for modeling user-item relation heterogeneity~\\cite{zhang2020multiplex,gao2019nmtr,jin2020mbgcn,yang2021hyper,wei2022contrastive}. For example, NMTR~\\cite{gao2019nmtr} is a multi-task recommendation framework with the predefined behavior-wise cascading relationships. Motivated by the strength of GNNs, MBGCN~\\cite{jin2020mbgcn}, MBGMN~\\cite{xia2021graph}, MGNN~\\cite{zhang2020multiplex} are developed based on the graph-structured message passing over the generated multi-relational user-item interaction graphs. Nevertheless, none of those approaches considers the time-evolving multi-behaviour user preference. To fill this gap, our \\model\\ model is able to capture both short-term and long-term multi-behavior dependencies with a hypergraph-enhanced transformer architecture.\n\n% \\subsection{Next-item Recommendation}\n% The earliest work on sequence recommendation \\cite{rendle2010mc} began with the modeling of item-item transitions using Markov Chain. Some following works \\cite{he2016vista, he2016fusing} employ similar MC architecture to predict the next purchase on the sequence by modeling the item-item transition probabilities, but fusing high-order information to enhance the performances. With the advances of neural networks in the IR field, researchers \\cite{hidasi2015session} firstly introduce GRU networks to tackle the sequential modeling problem in session-based recommendation. This work, GRU4Rec, was further improved by some researchers, resulting in better-performing variants \\cite{hidasi2016parallel, huang2018improving, quadrana2017personalizing, ren2019repeatnet}. As the success of Attention \\cite{vaswani2017attention} mechanism in the NLP field, some research works leverage the attention-based network \\cite{kang2018sasrec, sun2019bert4rec} to alleviate the problem of insufficient weight on long-term dependencies in RNN networks. Despite the success of bidirectional encoding attention models in the task, some researchers argue that these models lack explicit modeling of users' long-term interests. Hence, some recent works utilize general recommendation models to supplement sequential recommendations to capture more global user interest, for instance, GNN networks \\cite{chang2021surge, xu2019gcsan, wu2019srgnn} and CNN networks \\cite{tang2018caser}.\n\n% However, we argue that in addition to capturing global user interest, it is also critical to capture user interest at different time granularities in the sequence. Compared with these works, our proposed \\baby not only constructs a hypergraph to model global user interest, but also captures multi-scale fine-grained information in the sequence to form two views for prediction.\n\n% \\subsection{Multi-Behavior Recommendation}\n% Researchers in multi-behavior recommendation improve the recommendation performances by encoding multi-behavioral signals in user-item interactions by various approaches. Some researchers  \\cite{gao2019nmtr, chen2020efficient, tang2016empirical} employ a multi-task learning paradigm to constrain the transition relations between behaviors and to obtain more supervision signals that are beneficial to the task. Some works define the auxiliary behavior signals as a kind of weak supervision signals and propose negative-sampling based solutions accordingly \\cite{loni2016bayesian, ding2018improving}. Recently, some researchers \\cite{jin2020mbgcn, xia2021graph, xia2021multi} put multi-behavior signals on the user-item interaction graph and learn behavior dependencies as a graph collaborative filtering paradigm. These methods achieve great performances by leveraging benefits of GNNs for CF and type weight of graph edges. However, few research works pay attention on multi-behavior signals in sequential recommendation scenarios, which is a very important application in real-life online platforms. Hence, we analyze in depth the challenges of this task and propose \\baby, which performs well on three real-world datasets.% \\vspace{-0.05in}\n"
            },
            "section 5": {
                "name": "Conclusion",
                "content": "\nIn this paper, we present a new sequential recommendation framework \\model\\ which explicitly captures both short-term and long-term multi-behavior dependencies. \\model\\ designs a multi-scale Transformer to encode the behavior-aware sequential patterns at both fine-grained and coarse-grained levels. To capture the global cross-type behavior dependencies, we empower \\model\\ with a multi-behavior hypergraph learning component. Empirical results on several real-world datasets validate the strengths of our \\model\\ when competing with state-of-the-art recommendation methods.\n\n% In future, we plan to investigate how to incorporate item external information into multi-behavior sequential recommendation.\n\n"
            },
            "section 6": {
                "name": "Acknowledgments",
                "content": "\n%We thank the anonymous reviewers for their constructive feedback and comments. \nThis research is supported by the research grants from the Department of Computer Science \\& Musketeers Foundation Institute of Data Science at the University of Hong Kong.\n\n\\bibliographystyle{ACM-Reference-Format}\n% \\vspace{-0.05in}\n\\bibliography{reference}\n\n\\clearpage\n\\appendix "
            },
            "section 7": {
                "name": "Supplementary Material",
                "content": "\n\\balance\n\\label{sec:appendix}\nIn our supplemental material, we first summarize the learning process of our \\model\\ framework in Algorithm \\ref{algorithm} and conduct the model time complexity analysis. Then, we present our strategy to simplify the implementation of our hypergraph-based embedding propagation, so as to improve the model efficiency.\n\n",
                "subsection 7.1": {
                    "name": "The Learning Process of \\model",
                    "content": "\n\\begin{algorithm}\n\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n\\caption{The forward propagation flow of \\baby}\n\\label{algorithm}\n\\Input{The behavior-aware interaction sequence $S_i$ for user $u_i$ with mask tokens at positions $M$ and with true labels $T$. $S_i=[(v_{i,1}, b_{i,1}),...,\\textsf{[mask]},...,(v_{i,J}, b_{i,J})]$.}\n\\Output{The estimated probability for user $u_i$ interacting with ground-truth items $T$ at the time step positions $M$.}\n\\emph{\\textbf{Multi-Scale Transformer View}}\\;\nInject positional and behavior signals into Transformer inputs: $\\textbf{H} \\leftarrow [\\textbf{h}_1,...,\\textbf{h}_{J+1}]$, $\\textbf{h}_j \\leftarrow \\textbf{e}_j \\oplus \\textbf{p}_j \\oplus \\textbf{b}_j$\\;\nPerform multi-scale attention under multi-scale settings $C, p_1$ and $p_2$ according to Equation \\ref{eq:lowrank}-\\ref{eq:pool_attn}: $\\widetilde{\\textbf{H}} \\leftarrow f(\\widehat{\\textbf{H}} \\mathbin\\Vert {\\textbf{H}}^{p_1} \\mathbin\\Vert {\\textbf{H}}^{p_2})$\\;\nPerform point-wise feed-forward to inject non-linearity: $\t\\widetilde{\\textbf{H}}^{(l)} \\leftarrow [\\textsf{FFN}(\\tilde{\\textbf{h}}_1^{(l)})^\\trans, \\cdots, \\textsf{FFN}(\\tilde{\\textbf{h}}_t^{(l)})^\\trans]$\\;\n\\emph{\\textbf{Hypergraph View}}\\;\nModel item-wise semantic dependencies with multi-channel metric learning: $\t\\beta_{j,j'} \\leftarrow \\frac{1}{N}\\sum_{n=1}^{N} \\hat{\\beta}^n_{j,j'}$\\;\nConstruct customized hyperedges according to Equation \\ref{eq:sim_edge}-\\ref{eq:mb_edge}: $\\mathcal{M} \\leftarrow \\mathcal{M}^p \\mathbin\\Vert \\mathcal{M}^q$\\;\nApply hypergraph convolutional function to aggregate information from the graph: $\\textbf{X}^{(l+1)} \\leftarrow \\textbf{D}_v^{-1} \\cdot \\boldsymbol{\\mathcal{M}} \\cdot \\textbf{D}_e^{-1} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans \\cdot \\textbf{X}^{(l)}$\\;\n\\emph{\\textbf{Fusion and Prediction}}\\;\n\\ForEach{$m \\in M, t \\in T$}{\n    Generate hypergraph-view embedding for mask position $m$ using sliding-window contextual pooling: $\\tilde{\\textbf{x}}_{m} \\leftarrow \\textsf{mean}(\\tilde{\\textbf{x}}_{m-q_1},...,\\tilde{\\textbf{x}}_{m+q_2})$\\;\n    Apply attentive cross-view aggregation to fuse the information according to Equation \\ref{eq:fuse}: $\\textbf{g}_{m} \\leftarrow \\alpha_1 \\cdot \\tilde{\\textbf{h}}_{m} \\oplus \\alpha_2 \\cdot \\tilde{\\textbf{x}}_{m}$\\;\n    Calculate the probability of item at $m$ being $v_t$: $\\hat{y}_{m,t} \\leftarrow \\textbf{g}_{m}^\\trans \\textbf{v}_{t}$\\;\n}\n\\Return{$[\\hat{y}_{m_1,t_1}, \\hat{y}_{m_2,t_2},...]$}\\;\n\\end{algorithm}\n\n% \\begin{algorithm}\n% \t\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n% \t\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n% \t\\algnewcommand\\algorithmicforeach{\\textbf{for each}}\n%     \\algdef{S}[FOR]{ForEach}[1]{\\algorithmicforeach\\ #1\\ \\algorithmicdo}\n% \t\\caption{The forward propagation flow of MBHT}\n% \t\\label{alg:augmentation}\n% \t\\begin{algorithmic}[1]\n% \t    \\Require The behavior-aware interaction sequence $S_i$ for user $u_i$ with mask tokens at positions $M$ and with true labels $T$. $S_i=[(v_{i,1}, b_{i,1}),...,\\textsf{[mask]},...,(v_{i,J}, b_{i,J})]$.\n% \t\t\\Ensure The estimated probability for user $u_i$ interacting with ground-truth items $T$ at the time step positions $M$.\n% \t\t\\State Inject positional and behavior signals into Transformer inputs: $\\textbf{H} \\leftarrow [\\textbf{h}_1,...,\\textbf{h}_{J+1}]$, $\\textbf{h}_j \\leftarrow \\textbf{e}_j \\oplus \\textbf{p}_j \\oplus \\textbf{b}_j$\n% \t\t\\State Performing multi-scale attention under multi-scale settings $C, p_1$ and $p_2$ according to Equation \\ref{eq:lowrank}-\\ref{eq:pool_attn}: $\\widetilde{\\textbf{H}} \\leftarrow f(\\widehat{\\textbf{H}} \\mathbin\\Vert {\\textbf{H}}^{p_1} \\mathbin\\Vert {\\textbf{H}}^{p_2})$\n%         \\State Perform point-wise feed-forward to inject non-linearity: $\t\\widetilde{\\textbf{H}}^{(l)} = [\\textsf{FFN}(\\tilde{\\textbf{h}}_1^{(l)})^\\trans, \\cdots, \\textsf{FFN}(\\tilde{\\textbf{h}}_t^{(l)})^\\trans]$\n%         \\State Compute item-item semantic similarities with multi-channel metric learning: $\t\\beta_{j,j'} = \\frac{1}{N}\\sum_{n=1}^{N} \\hat{\\beta}^n_{j,j'}$\n%         \\State Construct customized hyperedges according to Equation \\ref{eq:sim_edge}-\\ref{eq:mb_edge}: $\\mathcal{M} \\leftarrow \\mathcal{M}^p \\mathbin\\Vert \\mathcal{M}^q$\n%         \\State Apply hypergraph convolutional function to aggregation information from the graph: $\\textbf{X}^{(l+1)} \\leftarrow \\textbf{D}_v^{-1} \\cdot \\boldsymbol{\\mathcal{M}} \\cdot \\textbf{D}_e^{-1} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans \\cdot \\textbf{X}^{(l)}$\n%         \\ForEach {$m \\in M, t \\in T$}\n%         \\State Generate hypergraph-view embedding for mask position $m$ using sliding-window contextual pooling: $\\tilde{\\textbf{x}}_{m} \\leftarrow \\textsf{mean}(\\tilde{\\textbf{x}}_{m-q_1},...,\\tilde{\\textbf{x}}_{m+q_2})$\n%         \\State Apply attentive cross-view aggregation to fuse the information according to Equation \\ref{eq:fuse}: $\\textbf{g}_{m} \\leftarrow \\alpha_1 \\cdot \\tilde{\\textbf{h}}_{m} \\oplus \\alpha_2 \\cdot \\tilde{\\textbf{x}}_{m}$\n%         Calculate the probability of item at $m$ being $v_t$: $\\hat{y}_{m,t} = \\textbf{g}_{m}^\\trans \\textbf{v}_{t}$\n%         \\EndFor\n% \t\t\\State \\Return $[\\hat{y}_{m_1,t_1}, \\hat{y}_{m_2,t_2},...]$\n% \t\\end{algorithmic}  \n% \\end{algorithm}\n\n"
                },
                "subsection 7.2": {
                    "name": "Time Complexity Analysis",
                    "content": "\n\\label{sec:time_comp}\nThis section conducts the time complexity analysis of our \\model\\ framework as follows. (1) For the multi-scale Transformer view, with our low-rank self-attention layer, we significantly reduce the computational cost from $O(L\\times d\\times ((\\frac{J}{C})^2+(\\frac{J}{p_1})^2+(\\frac{J}{p_2})^2))$ to approximate the linear time complexity $O(3LdJ)$, given that $C$ (low-rank scale), $p_1,p_2$ (resolution scales) $\\ll J$~\\cite{wang2020linformer}. $J$ denotes the length of item sequence. (2) For the hypergraph learning view, the semantic metric learning takes $O(J^2d)$ complexity, and the hypergraph convolutional function takes $O(LJd^2)$. \n% the most time-consuming part comes from message passing between items and hyperedges along with the hypergraph connection matrix $\\mathbf{M}$. To be specific, the hypergraph convolution takes $O((|\\mathcal{E}^p|+ |\\mathcal{E}^q|)\\times J^2)$. \nBased on the above discussion, the overall time complexity of our \\model\\ is $O(3LJd + LJd^2 + J^2d)$, which is comparable to state-of-the-art baselines.\n\n"
                },
                "subsection 7.3": {
                    "name": "Simplifying Hypergraph Message Passing",
                    "content": "\n% \\subsection{A Light Version of Hypergraph Convolutional Function}\n\\label{sec:lighthgcn}\nWith the consideration of high computational cost during the message passing in our hypergraph learning framework, we propose to simplify the propagation scheme with the learnable embedding projection. Formally, we rewrite the two-stage (node-hyperedge-node) message propagation process $\\boldsymbol{\\mathcal{M}} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans$ as follows:\n\\begin{align}\n    (\\boldsymbol{\\mathcal{M}}\\cdot \\boldsymbol{\\mathcal{M}}^\\trans)_{ij} = \\sum_{k=1}^n \\boldsymbol{\\mathcal{M}}_{ik}\\boldsymbol{\\mathcal{M}}^\\trans_{kj} = \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}^{{\\trans}^{(j)}} = \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}_{(j)}\n\\end{align}\nwhere $\\boldsymbol{\\mathcal{M}}_{(i)}$ and $\\boldsymbol{\\mathcal{M}_{(j)}}$ denotes the $i$-th row and $j$-th column vector, respectively. Based on our item-wise semantic dependence and multi-behavior correlations, our hypergraph-guided information propagation can be presented as follows:\n\\begin{align}\n\\label{eq:lhgcn_split}\n    \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}_{(j)} = \\sum_{k=1}^{|\\mathcal{E}^q|} \\boldsymbol{\\mathcal{M}}_{i,k}\\boldsymbol{\\mathcal{M}}_{j,k} + \\sum_{m=1}^{|\\mathcal{E}^p|} \\boldsymbol{\\mathcal{M}}_{i,m}\\boldsymbol{\\mathcal{M}}_{j,m}\n\\end{align}\n\\noindent where $\\boldsymbol{\\mathcal{M}}_{i,k}$ and $\\boldsymbol{\\mathcal{M}}_{i,m}$ denote the value of hypergraph connection matrix with item-wise semantic dependency and multi-behavior correlations of item $v_i$, respectively. Here, we first simplify the multiplication operations with the multi-behavior dependency hyperedges. Specifically, the same item ($v_i=v_j$) interacted with different behavior types over time will be connected to the same hyperedge $\\epsilon^q$, \\ie $\\boldsymbol{\\mathcal{M}}_{i,\\epsilon^q}= \\boldsymbol{\\mathcal{M}}_{j,\\epsilon^q} = 1$. Since each item can only be connected to one hyperedge, for $\\epsilon' \\neq \\epsilon^q$, $\\boldsymbol{\\mathcal{M}}_{i, \\epsilon'}= \\boldsymbol{\\mathcal{M}}_{j, \\epsilon^q} = 0$. This indicates that if $v_i \\neq v_j$, there exists no multi-behavior dependency hyperedge $\\epsilon'$ such that $\\boldsymbol{\\mathcal{M}}_{i, \\epsilon'} \\boldsymbol{\\mathcal{M}}_{j, \\epsilon'}=1$, since different items cannot be connected to the same hyperedge. Hence, we can isolate the influence of the multi-behavior dependency hyperedges as follows:\n\\begin{align}\n    \\sum_{\\epsilon'=1}^{|\\mathcal{E}^q|} \\boldsymbol{\\mathcal{M}}_{i, \\epsilon'}\\boldsymbol{\\mathcal{M}}_{j, \\epsilon'} = \\begin{cases}\n    1 & \\text{if}~~ v_i=v_j \\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n\\end{align}\n\n% is the $i$-th row vector of $\\boldsymbol{\\mathcal{M}}$ and $\\boldsymbol{\\mathcal{M}}^{(j)}$ is the $j$-th column vector.\n\n% Here we present how to build $\\boldsymbol{\\mathcal{M}}^\\prime$ as to closely approximate $\\boldsymbol{\\mathcal{M}} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans$ in Equation \\ref{eq:HGCN}. Specifically, the matrix multiplication of $\\boldsymbol{\\mathcal{M}} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans$ can be explained as\n% \\begin{align}\n%     (\\boldsymbol{\\mathcal{M}}\\cdot \\boldsymbol{\\mathcal{M}}^\\trans)_{ij} = \\sum_{k=1}^n \\boldsymbol{\\mathcal{M}}_{ik}\\boldsymbol{\\mathcal{M}}^\\trans_{kj} = \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}^{{\\trans}^{(j)}} = \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}_{(j)}\n% \\end{align}\n% where $\\boldsymbol{\\mathcal{M}}_{(i)}$ is the $i$-th row vector of $\\boldsymbol{\\mathcal{M}}$ and $\\boldsymbol{\\mathcal{M}}^{(j)}$ is the $j$-th column vector. Recall that, we build 2 types of hyperedges, the semantic dependency hyperedges and the multi-behavior dependency hyperedges. We further rewrite the result as\n% \\begin{align}\n% \\label{eq:lhgcn_split}\n%     \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}_{(j)} = \\sum_{k=1}^{|\\mathcal{E}^q|} \\boldsymbol{\\mathcal{M}}_{ik}\\boldsymbol{\\mathcal{M}}_{jk} + \\sum_{m=1}^{|\\mathcal{E}^p|} \\boldsymbol{\\mathcal{M}}_{im}\\boldsymbol{\\mathcal{M}}_{jm}\n% \\end{align}\n% where $\\boldsymbol{\\mathcal{M}}_{ik}$ and $\\boldsymbol{\\mathcal{M}}_{im}$ denote the feature of multi-behavior dependency hyperedges and semantic dependency hyperedges to items $v_i$ separately. We firstly consider simplifying the multiplication based on the characteristics of the multi-behavior dependency hyperedges.\n\n% For the multi-behavior dependency hyperedges, if $v_i =v_j$, \\ie the items are the same, they belong to one multi-behavior dependency hyperedge $k_q$, and thus $\\boldsymbol{\\mathcal{M}}_{ik_q}= \\boldsymbol{\\mathcal{M}}_{jk_q} = 1$. Since each item can only belong to one multi-behavior dependency hyperedge, for $k \\neq k_q$, $\\boldsymbol{\\mathcal{M}}_{ik}= \\boldsymbol{\\mathcal{M}}_{jk} = 0$. This conclusion also illustrates that for $v_i \\neq v_j$, there exists no multi-behavior dependency hyperedge $k$ such that $\\boldsymbol{\\mathcal{M}}_{ik} \\boldsymbol{\\mathcal{M}}_{jk}=1$, because they are different items and can not belong to the same multi-behavior dependency hyperedge. Based on the above conclusions, we can isolate the influence of the multi-behavior dependency hyperedges as\n% \\begin{align}\n%     \\sum_{k=1}^{|\\mathcal{E}^q|} \\boldsymbol{\\mathcal{M}}_{ik}\\boldsymbol{\\mathcal{M}}_{jk} = \\begin{cases}\n%     1 & \\text{if}~~ v_i=v_j \\\\\n%     0 & \\text{otherwise}\n%     \\end{cases}\n% \\end{align}\n\nWe further investigate the item-wise semantic dependency hyperedges in Equation \\ref{eq:lhgcn_split} for simplifying hypergraph convolutional operations. Note that, $\\boldsymbol{\\mathcal{M}}_{i, m}$ denotes the cosine similarity between item $v_i$ and $v_m$ that belong to the hyperedge $m$. Generally, the computation $\\sum_{m=1}^{|\\mathcal{E}^p|} \\boldsymbol{\\mathcal{M}}_{im}\\boldsymbol{\\mathcal{M}}_{jm}$ can be divided into three cases depending on the hyperedge $m$: i) behavior-aware self-connection, if $v_i = v_j$ and $m$ is the hyperedge assigned to this item; ii) first-order similarity, if $v_i \\neq v_j$ and $m$ is assigned to $v_i$ or $v_j$; iii) second-order similarity, if $v_i \\neq v_j$ and $m$ is not assigned to either $v_i$ or $v_j$.\n\nWe leverage the pre-calculated behavior-aware semantic similarities $\\beta_{i,j}$ between items for the first and second cases. Here $\\beta_{i,j}$ is truncated from the top-$k$ value to be consistent with the semantic dependency hyperedges. We denote the values of the third case by $w_{i,j}$. Therefore, based on the above discussions, for the first case, we have: $\\sum_{m=1}^{|\\mathcal{E}^p|}\\boldsymbol{\\mathcal{M}}_{im}\\boldsymbol{\\mathcal{M}}_{jm} = \\beta_{i,j} + w_{i,j}$; for the second case, we have $\\sum_{m=1}^{|\\mathcal{E}^p|}\\boldsymbol{\\mathcal{M}}_{im}\\boldsymbol{\\mathcal{M}}_{jm} = \\beta_{i,j} + \\beta_{j,i} + w_{i,j}$. Since the computation of the second-order complexity is time-consuming, and a number of such values are slight due to the top-$k$ truncation, we \nreplace $w_{i,j}$ by a hyperparameter $w_0$ to obtain a close approximation $\\boldsymbol{\\mathcal{M}}^\\prime$ for $\\boldsymbol{\\mathcal{M}}\\boldsymbol{\\mathcal{M}}^\\trans$. Formally, $\\boldsymbol{\\mathcal{M}}^\\prime \\in \\mathbb{R}^{J\\times J}$ is generated as follows:\n\\begin{align}\n    \\boldsymbol{\\mathcal{M}}^\\prime = C \\oplus A \\oplus W\n\\end{align}\nwhere $C_{i,j} = 1$ if $v_i=v_j$, otherwise $C_{i,j} = 0$, and $A_{i,j} = \\beta_{i,j}$ if $v_i=v_j$, otherwise $A_{i,j} = \\beta_{i,j} + \\beta_{j,i}$. $W_{i,j} = w_0$. In our experiments, $w_0$ is searched among $[0.05, 0.1, 0.15, 0.2]$. Finally, we express the light version of hypergraph convolution function below:\n\\begin{align}\n\t\\textbf{X}^{(l+1)} = \\textbf{D}^{-1} \\cdot \\boldsymbol{\\mathcal{M}}^\\prime \\cdot \\textbf{X}^{(l)}\n\\end{align}\n\n% We introduce the pre-calculated behavior-aware item-item semantic similarity $\\beta_{i,j}$ to cover the cases i) and ii). \n% Here $\\beta_{i,j}$ is truncated from the top-$k$ value to be consistent with the semantic dependency hyperedges. We denote the values of case iii) by $w_{i,j}$. Therefore, based on the above discussions, for case i) we have: \n% $\\sum_{m=1}^{|\\mathcal{E}^p|}\\boldsymbol{\\mathcal{M}}_{im}\\boldsymbol{\\mathcal{M}}_{jm} = \\beta_{i,j} + w_{i,j}$. \n% And for case ii) we have: $\\sum_{m=1}^{|\\mathcal{E}^p|}\\boldsymbol{\\mathcal{M}}_{im}\\boldsymbol{\\mathcal{M}}_{jm} = \\beta_{i,j} + \\beta_{j,i} + w_{i,j}$.\n\n\n\nWe conduct empirical experiments to further evaluate the model performance of our simplified hypergraph convolution mechanism. We report the evaluation results in Table~\\ref{tab:lhgcn}. The model training is performed on a single GTX3090 GPU for running time evaluation. We can observe that our simplified hypergraph-based message passing scheme only lead to slightly performance degradation and improve the model efficiency with lower computational cost. The potential reasons are: i) a large number of second-order similarity are slight, since scores are truncated from top-$k$ and ii) the convolution process inherently takes into account high-order connectivity. At the same time, simplifying the convolutional function brings a lot enhancement on inference speed, since it eliminates the original $O((|\\mathcal{E}^p|+ |\\mathcal{E}^q|)\\times J^2)$ calculations and $\\boldsymbol{\\mathcal{M}}^\\prime$ can be easily built by leveraging pre-calculated values and data pre-processing.\n\n% and find that the performance loss brought by the light version is quite small, which is averagely 1.23\\% across three datasets. This may be because: i) a large number of second-order similarity are slight, since scores are truncated from top-$k$ and ii) the convolution process inherently takes into account high-order connectivity. At the same time, simplifying the convolutional function brings a lot enhancement on inference speed, since it eliminates the original $O((|\\mathcal{E}^p|+ |\\mathcal{E}^q|)\\times J^2)$ calculations and $\\boldsymbol{\\mathcal{M}}^\\prime$ can be easily built by leveraging pre-calculated values and data pre-processing.\n\n\n\n"
                },
                "subsection 7.4": {
                    "name": "Hyperparameter Study (RQ4)",
                    "content": "\n\\label{sec:hp}\nWe conduct experiments to analyze the influence of key hyperparameters in our \\model\\ framework and report results in Figure~\\ref{fig:HP}.\\\\\\vspace{-0.12in}\n\n\\noindent \\textbf{Impact of Multi-Scale Settings}. We search multi-scale setting parameters $(C, p_1, p_2)$ among the range $\\{[20,4,20],[20,8,40],[40,4,20]$ $[40,8,40]\\}$. We present the observations as followed:\n\\begin{itemize}[leftmargin=*]\n\\item The best performance on Retailrocket and Taobao datasets can be achieved with $(p_1,p_2)=(4,20)$ and $(p_1,p_2)=(8,40)$ given the difference of average sequence length.\\\\\\vspace{-0.1in}\n\\item For the low-rank projection scale parameter $C$, we can notice that projecting original self-attentive sequence embedding space into $\\frac{J}{20}$ channels can bring the best performance on IJCAI and Retailrocket datasets. For Taobao dataset, we can observe that $\\frac{J}{40}$ performs better than $\\frac{J}{20}$, which indicates that dense user-item interaction data may need less low-rank projection channels for better sequential pattern encoding.\n\\end{itemize}\n\n% We plot the performances of \\baby under different hyper-parameter settings of attention scales $c, p_1, p_2$ and similarity group lengths $k$ in Figure \\ref{fig:HP}, to further explore the sensitivity of \\baby to these two key hyper-parameters and to provide suggestions for experiments and model deployment. Figure \\ref{fig:HP:scale} presents the effects of attention scales. In general, the selection of attention scales is independent with the average sequence length of the dataset. In particular, on IJCAI and Retailrocket with the longest and shortest average length respectively, scales [10,4,20] bring both the best results. However, [5,8,40] is more suitable for Taobao. We also notice that the first scale $c$ for low-rank project attention has larger effect on the performances. For example, setting $c$ from 5 to 10 and keeping $p_1,p_2$ unchanged brings obvious performance change on all datasets. Hence, we suggest tuning $c$ more delicately according to the adopted dataset.\n\n\\noindent \\textbf{Impact of Item-wise Semantic Dependency Set}. Our hypergraph item dependency encoder investigates the latent semantic correlations among different items. We search the top-$k$ semantic dependent items from \\{4,6,8,10,12,14\\} for global message passing through the item-wise semantic hyperedges. Observations are:\n\\begin{itemize}[leftmargin=*]\n\\item The best settings of $k$ is proportionally to the average sequence length of different datasets. It indicates that larger hypergraph propagation scope is better for modeling longer item sequences.\\\\\\vspace{-0.1in}\n\\item Increasing the number of connected items through hyperedges may firstly boost the performance at the early stage, and then lead to performance degradation by involving noise during the hypergraph-based embedding propagation.\n\\end{itemize}\n"
                }
            }
        },
        "tables": {
            "tab:datasets": "\\begin{table}[t]\n\\centering\n\\caption{Statistical information of experimented datasets.}\n\\vspace{-0.15in}\n\\label{tab:datasets}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{c|c|c|c}\n\\toprule\nStats.          & Taobao & Retailrocket & IJCAI  \\\\\n\\hline\n\\# Users        & 147, 892 & 11, 649 & 200, 000        \\\\\n\\# Items        & 99, 038 & 36, 223 & 808, 354        \\\\\n\\# Interactions & 7, 092, 362 & 87, 822 & 13, 072, 940 \\\\\n\\# Average Length & 48.23 & 14.55 & 78.58 \\\\\n\\# Density        & $5\\times 10^{-6}$ & $1\\times 10^{-6}$ & $7\\times 10^{-7}$ \\\\\n\\# Behavior Types & [buy, cart, fav, pv] & [buy, cart, pv] & [buy, cart, fav, pv] \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-0.25in}\n}\n\\end{table}",
            "tab:results": "\\begin{table*}[t]\n\t\\centering\n% \t\\caption{Performance evaluation of the baselines across three datasets. The best and the second best performances are in bold and underline respectively. Performances with superscript $\\ast$ are statistically significant at $p<0.01$ level compared with the second best.}\n\t\\caption{The performance of our method and the best performed baseline are presented with bold and underlined, respectively. Superscript $\\ast$ indicates the significant improvement between our \\model\\ and the best performed baseline with $p$ value $<0.01$.}\n\t\\vspace{-0.1in}\n\t\\label{tab:results}\n\t\\resizebox{\\linewidth}{!}{\n\t\\begin{tabular}{c|ccccc|ccccc|ccccc}\n\t\t\\hline\n\t\t\\multirow{2}{*}{Model} & \\multicolumn{5}{c|}{Taobao} & \\multicolumn{5}{c|}{Retailrocket} & \\multicolumn{5}{c}{IJCAI} \\\\\n\t\t~ & HR@5 & NDCG@5 & HR@10 & NDCG@10 & MRR & HR@5 & NDCG@5 & HR@10 & NDCG@10 & MRR & HR@5 & NDCG@5 & HR@10 & NDCG@10 & MRR \\\\\n\t\t\\hline\n\t\t\\multicolumn{7}{l}{\\textit{General Sequential Recommendation Methods}}\\\\\n\t\t\\hline\n\t\tCaser & 0.082 & 0.058 & 0.123 & 0.071 & 0.070 & 0.632 & 0.539 & 0.754 & 0.578 & 0.535 & 0.134 & 0.092 & 0.167 & 0.104 & 0.109 \\\\ \n\t\tHPMN & 0.162 & 0.130 & 0.219 & 0.141 & 0.139 & 0.664 & 0.633 & 0.711 & 0.587 & 0.602 & 0.144 & 0.085 & 0.197 & 0.124 & 0.123\\\\\n\t\tGRU4Rec & 0.147 & 0.105 & 0.209 & 0.125 & 0.118 & 0.640 & 0.575 & 0.708 & 0.597 & 0.572 & 0.141 & 0.100 & 0.200 & 0.119 & 0.113 \\\\\n\t\tSASRec & 0.150 & 0.110 & 0.206 & 0.128 & 0.123 & 0.669 & 0.644 & 0.689 & 0.650 & 0.645 & 0.146 & 0.110 & 0.191 & 0.124 & 0.122 \\\\\n\t\tBERT4Rec & 0.198 & 0.153 & 0.254 & 0.171 & 0.163 & 0.808 & 0.670 & 0.881 & 0.694 & 0.639 & \\underline{0.297} & \\underline{0.220} & \\underline{0.402} & \\underline{0.253} & \\underline{0.227} \\\\\n\t\t\\hline\n\t\t\\multicolumn{7}{l}{\\textit{Graph-based Sequential Recommender Systems}}\\\\\n\t\t\\hline\n\t\tSR-GNN & 0.102 & 0.071 & 0.153 & 0.087 & 0.086 & 0.848 & 0.780 & 0.891 & 0.793 & 0.767 & 0.072 & 0.048 & 0.118 & 0.062 & 0.064 \\\\\n\t\tGCSAN & \\underline{0.217} & 0.160 & 0.305 & \\underline{0.188} & 0.173 & 0.872 & 0.846 & 0.890 & 0.851 & 0.842 & 0.119 & 0.086 & 0.175 & 0.104 & 0.101  \\\\\n\t\tHyperRec & 0.145 & 0.130 & 0.224 & 0.133 & 0.129 & 0.860 & 0.705 & 0.833 & 0.820 & 0.816 & 0.140 & 0.109 & 0.236 & 0.144 & 0.132\\\\\n\t\tSURGE & 0.122 & 0.078 & 0.193 & 0.100 & 0.093 & \\underline{0.878} & \\underline{0.879} & \\underline{0.906} & \\underline{0.887} & \\underline{0.870} & 0.226 & 0.159  & 0.322 & 0.190 & 0.171 \\\\\n\t\t\\hline\n\t\t\\multicolumn{7}{l}{\\textit{Multi-Behavior Recommendation Models}}\\\\\n\t\t\\hline\n\t\tBERT4Rec-MB & 0.211 & \\underline{0.169} & 0.263 & 0.186 & \\underline{0.178} & 0.875 & 0.858 & 0.889 & 0.863 & 0.857 & 0.257 & 0.189 & 0.342 & 0.216 & 0.197 \\\\\n\t\tMB-GCN & 0.185 & 0.103 & 0.309 & 0.143 & 0.149 & 0.844 & 0.735 & 0.878 & 0.752 & 0.739 & 0.218 & 0.145 & 0.335 & 0.182 & 0.177\\\\\n\t\tNMTR & 0.125 & 0.082 & 0.174 & 0.097 & 0.103 & 0.827 & 0.697 & 0.858 & 0.724 & 0.741 & 0.109 & 0.076 & 0.184 & 0.099 & 0.106\\\\\n\t\tMB-GMN & 0.196 & 0.115 & \\underline{0.319} & 0.154 & 0.151 & 0.853 & 0.762 & 0.901 & 0.830 & 0.822 & 0.235 & 0.161 & 0.337 & 0.193 & 0.176\\\\\n\t\t\\hline\n\t\t\\textbf{\\baby} & \\textbf{0.323}$^\\ast$ & \\textbf{0.257}$^\\ast$ & \\textbf{0.405}$^\\ast$ & \\textbf{0.283}$^\\ast$ & \\textbf{0.262}$^\\ast$ & \\textbf{0.931}$^\\ast$ & \\textbf{0.933}$^\\ast$ & \\textbf{0.956}$^\\ast$ & \\textbf{0.950}$^\\ast$ & \\textbf{0.929}$^\\ast$ & \\textbf{0.346}$^\\ast$ & \\textbf{0.268}$^\\ast$ & \\textbf{0.437}$^\\ast$ & \\textbf{0.297}$^\\ast$ & \\textbf{0.272}$^\\ast$ \\\\\n\t\t\\# Improve & 48.84\\% & 52.07\\% & 26.95\\% & 50.53\\% & 47.19\\% & 6.04\\% & 6.14\\% & 5.52\\% & 7.10\\% & 6.78\\% & 16.50\\% & 21.82\\% & 8.71\\% & 17.39\\% & 19.82\\% \\\\\n\t\t\\hline\n\t\\end{tabular}\n\t}\n\t\\vspace{-0.1in}\n\\end{table*}",
            "tab:ab": "\\begin{table}[t]\n\\caption{Ablation study with key modules.}\n\\label{tab:ab}\n\\vspace{-0.12in}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|cc|cc|cc}\n\\hline\n\\multirow{2}{*}{Model Variants} & \\multicolumn{2}{c|}{Taobao} & \\multicolumn{2}{c|}{Retailrocket} & \\multicolumn{2}{c}{IJCAI} \\\\ \\cline{2-7} \n & HR@5 & NDCG@5 & HR@5 & NDCG@5 & HR@5 & NDCG@5 \\\\ \\hline\n\\baby & \\textbf{0.323}  & \\textbf{0.257} & \\textbf{0.956} & \\textbf{0.950} & \\textbf{0.346} & \\textbf{0.268} \\\\\n\\hline\n% \\textit{(+) Full HGCN} & 0.326 & 0.260 & 0.959 & 0.953 & 0.352 & 0.279\\\\\n\\textit{(-) MB-Hyper} & 0.261 & 0.206 & 0.883 & 0.861 & 0.320  & 0.249  \\\\\n\\textit{(-) ML-Hyper} & 0.271 & 0.212 & 0.898 & 0.874 & 0.328 & 0.256   \\\\\n\\textit{(-) Hypergraph} & 0.246 & 0.194 & 0.813 & 0.839 & 0.301 & 0.234  \\\\\n\\textit{(-) MS-Attention} & 0.253 & 0.200 & 0.816 & 0.832 & 0.329 & 0.256    \\\\\n \\hline\n\\end{tabular}\n}\\vspace{-0.25in}\n\\end{table}",
            "tab:lhgcn": "\\begin{table}[t]\n\\caption{Comparison between two implementation of hypergraph message passing schemes, \\ie the original embedding propagation based on hypergraph connection matrices, and the simplified message passing schema. The recommendation accuracy is measured by Recall@5 and model computational cost is measured by the running time of each epoch.}\n% \\caption{Comparison on \\baby adopting two versions of hypergraph message passing function. The epoch times are present in minutes and observed by training the model on a single GTX3090 GPU.}\n\\label{tab:lhgcn}\n% \\vspace{-0.1in}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{c|cc|cc|cc}\n\\hline\n& \\multicolumn{2}{c|}{Taoabo}                   & \\multicolumn{2}{c|}{Retailrocket}             & \\multicolumn{2}{c}{IJCAI}                    \\\\\n& Recall@5 & Epoch Time& Recall@5 & Epoch Time & Recall@5 & Epoch Time\\\\\n\\hline\nOrigin     & 0.326                  & 86.43               & 0.959                  & 4.58                & 0.352                  & 133.28                \\\\\nSimplified & 0.323                  &   38.05             & 0.956                  & 2.11                & 0.346                  & 65.2 \\\\      \n\\hline\n\\end{tabular}\n}\n% \\vspace{-0.1in}\n\\end{table}"
        },
        "figures": {
            "fig:intro": "\\begin{figure}\n\t\\includegraphics[width=0.95\\linewidth]{material/intro.pdf}\n\t\\vspace{-0.1in}\n\t\\caption{(a) Illustration example of sequential recommendation with multi-behavior dynamics. (b) Learned behavior-aware dependency weights for short-term item correlations among neighboring $\\{[4], [5], [6]\\}$ and long-range dependencies among $\\{[2], [13], [15]\\}$ by BERT4Rec and our \\model.}\n% \tThe $1^{st}$ and $2^{nd}$ matrix represent the learned relevance weights by Bert4Rec-MB among the item set \\{[4], [5], [6]\\} (short-term dependencies) and \\{[2], [13], [15]\\} (long-range dependencies). The $3^{rd}$ matrix represents the learned relevance weights by our \\model\\ to capture global dependencies among \\{[2], [13], [15]\\}.}\n\t\\label{fig:intro}\n\t\\vspace{-0.15in}\n\\end{figure}",
            "fig:framework": "\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{material/model_architecture_MBHT.pdf}\n    \\vspace{-0.3in}\n    \\caption{\\model's model flow. (a) We inject the behavior-aware interaction context into item embeddings $\\textbf{h}_j = \\textbf{e}_j \\oplus \\textbf{p}_j \\oplus \\textbf{b}_j$. (b) Multi-scale transformer architecture to capture behavior-aware transitional patterns via low-rank self-attention and  multi-scale sequence aggregation. Scale-specific behavior patterns are fused through the fusion function $\\widetilde{\\textbf{H}} = f(\\widehat{\\textbf{H}} \\mathbin\\Vert {\\textbf{H}}^{p_1} \\mathbin\\Vert {\\textbf{H}}^{p_2})$. (c) We capture the global and personalized multi-behavior dependency learning with our hypergraph neural architecture over $\\mathcal{G}$.}\n    \\label{fig:framework}\n    \\vspace{-0.1in}\n\\end{figure*}",
            "fig:contrib": "\\begin{figure}[t]\n\\centering\n\\subfigure[Taobao]{\n\\label{fig:conv:tmall}\n\\includegraphics[width=0.31\\linewidth]{material/tmall_view_contrib}}\n\\subfigure[IJCAI]{\n\\label{fig:conv:ijcai}\n\\includegraphics[width=0.31\\linewidth]{material/ijcai_view_contrib}}\n\\subfigure[Retailrocket]{\n\\label{fig:conv:ijcai}\n\\includegraphics[width=0.31\\linewidth]{material/retail_view_contrib}}\n\\vspace{-0.2in}\n\\caption{Distributions of the learned attentive view-specific contributions. Green triangles and black line in the showed boxes denote the mean and median values, respectively.}\n\\label{fig:contrib}\n\\vspace{-0.2in}\n\\end{figure}",
            "fig:group_test": "\\begin{figure}[t]\n\\centering\n\\subfigure[Taobao]{\n\\label{fig:group_test:taobao}\n\\includegraphics[width=0.46\\linewidth]{material/tmall_colditem.pdf}}\n\\subfigure[IJCAI]{\n\\label{fig:group_test:ijcai}\n\\includegraphics[width=0.46\\linewidth]{material/ijcai_colditem.pdf}}\n\\vspace{-0.15in}\n% \\caption{Evaluation on different sequence groups with increasing sequence length.}\n\\caption{Performance \\textit{w.r.t} different sequence lengths.}\n\\label{fig:group_test}\n\\vspace{-0.1in}\n\\end{figure}",
            "fig:convergence": "\\begin{figure}[t]\n\\centering\n\\subfigure[Taobao]{\n\\label{fig:conv:tmall}\n\\includegraphics[width=0.48\\linewidth]{material/tmall_convergence.pdf}}\n\\subfigure[IJCAI]{\n\\label{fig:conv:ijcai}\n\\includegraphics[width=0.48\\linewidth]{material/ijcai_convergence.pdf}}\n\\vspace{-0.1in}\n\\caption{Training curves evaluated by testing Hit Rate.}\n\\label{fig:convergence}\n\\vspace{-0.15in}\n\\end{figure}",
            "fig:case_study": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{material/case_study_1.pdf}\n    \\vspace{-0.2in}\n    \\caption{Case studies with cross-type behavior dependencies.}\n    \\vspace{-0.2in}\n    % \\caption{Selected cases and overall visualization of weights in the multi-scale attention and the hypergraph.}\n    \\label{fig:case_study}\n\\end{figure}",
            "fig:HP": "\\begin{figure}[t]\n\\centering\n\\subfigure[Sensitivity to attention scales]{\n\\label{fig:HP:scale}\n\\includegraphics[width=0.48\\linewidth]{material/HP_scales.pdf}}\n\\subfigure[Sensitivity to sim-group length]{\n\\label{fig:HP:glen}\n\\includegraphics[width=0.45\\linewidth]{material/HP_hglen.pdf}}\n\\vspace{-0.1in}\n\\caption{Hyperparameter study of \\model\\ framework.}\n\\label{fig:HP}\n% \\vspace{-0.1in}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\t\\textbf{h}_j = \\textbf{e}_j \\oplus \\textbf{p}_j \\oplus \\textbf{b}_j\n\\end{align}",
            "eq:2": "\\begin{align}\n\\label{eq:lowrank}\n\t\t\\widehat{\\textbf{H}} =\\textsf{softmax}(\\frac{\\textbf{H}  \\cdot {W}^Q(\\textbf{E} \\cdot \\textbf{H}  \\cdot {W}^K)^\\trans}{\\sqrt{d}})\\cdot \\textbf{F} \\cdot \\textbf{H}  \\cdot {W}^V\n\\end{align}",
            "eq:3": "\\begin{align}\n\\label{eq:attn_pool}\n\\boldsymbol{\\Gamma}^p = \\{\\boldsymbol{\\gamma}_1,...,\\boldsymbol{\\gamma}_{\\frac{J}{p}}\\} = [\\eta(\\textbf{h}_1,...,\\textbf{h}_p);...; \\eta(\\textbf{h}_{J-p+1},...,\\textbf{h}_{J})]\n\\end{align}",
            "eq:4": "\\begin{align}\n\\label{eq:pool_attn}\n\t\t{\\textbf{H}}^p =\\textsf{softmax}(\\frac{\\boldsymbol{\\Gamma}^p \\cdot {W}^Q_p(\\boldsymbol{\\Gamma}^p \\cdot {W}^K_p)^\\trans}{\\sqrt{d}}) \\cdot \\boldsymbol{\\Gamma}^p \\cdot {W}^V_p\n\\end{align}",
            "eq:5": "\\begin{align}\n\\widetilde{\\textbf{H}} = f(\\widehat{\\textbf{H}} \\mathbin\\Vert {\\textbf{H}}^{p_1} \\mathbin\\Vert {\\textbf{H}}^{p_2})\n\\end{align}",
            "eq:6": "\\begin{align}\n\t\\widetilde{\\textbf{H}} &= (\\text{head}_1 \\mathbin\\Vert \\text{head}_2 \\mathbin\\Vert  \\cdots \\mathbin\\Vert \\text{head}_N)\\boldsymbol{W}^D \\\\\\nonumber\n\t\\text{head}_n &= f(\\widehat{\\textbf{H}}_n \\mathbin\\Vert {\\textbf{H}}_n^{p_1} \\mathbin\\Vert {\\textbf{H}}_n^{p_2})\n\\end{align}",
            "eq:7": "\\begin{align}\n\t\\textsf{PFFN}(\\widetilde{\\textbf{H}}^{(l)}) &= [\\textsf{FFN}(\\tilde{\\textbf{h}}_1^{(l)})^\\trans, \\cdots, \\textsf{FFN}(\\tilde{\\textbf{h}}_t^{(l)})^\\trans] \\\\\\nonumber\n\t\\textsf{FFN}(\\textbf{x}) &= \\textsf{GELU}(\\textbf{x}\\textbf{W}_1^{(l)} + \\textbf{b}_1^{(l)})\\textbf{W}_2^{(l)}+\\textbf{b}_2^{(l)},\n\\end{align}",
            "eq:8": "\\begin{align}\n\t\\beta_{j,j'} & = \\frac{1}{N}\\sum_{n=1}^{N} \\hat{\\beta}^n_{j,j'}; \\textbf{v}_j = \\textbf{e}_j \\oplus \\textbf{b}_j \\\\\n\t\\hat{\\beta}^n_{j,j'} & = \\tau(\\boldsymbol{w}^n \\odot \\textbf{v}_j, \\boldsymbol{w}^n \\odot \\textbf{v}_{j'})\n\\end{align}",
            "eq:9": "\\begin{align}\n\\label{eq:sim_edge}\n\tm^{p}({v_j,\\epsilon_{j'}}) &= \\begin{cases}\n\t\t\\beta_{j,j'} & v_{j'} \\in A_j; \\\\\n\t\t0 & otherwise;\n\t\\end{cases}\n\\end{align}",
            "eq:10": "\\begin{align}\n\\label{eq:mb_edge}\n\tm^{q}(v_j^b,\\epsilon_{j}^{b'}) &= \\begin{cases}\n\t\t1 & v_{j}^b \\in \\mathcal{E}^q_j; \\\\\n\t\t0 & otherwise;\n\t\\end{cases}\n\\end{align}",
            "eq:11": "\\begin{align}\n\\label{eq:HGCN}\n\t\\textbf{X}^{(l+1)} = \\textbf{D}_v^{-1} \\cdot \\boldsymbol{\\mathcal{M}} \\cdot \\textbf{D}_e^{-1} \\cdot \\boldsymbol{\\mathcal{M}}^\\trans \\cdot \\textbf{X}^{(l)}\n\\end{align}",
            "eq:12": "\\begin{align}\n\t\\label{eq:fuse}\n\t\\alpha_i = \\textsf{Attn}(\\bm{e}_i) = \\frac{\\exp(\\bm{a}^\\trans \\cdot \\bm{W}_a\\bm{e}_i)}{\\sum_i \\exp(\\bm{a}^\\trans \\cdot \\bm{W}_a\\bm{e}_i)} \\\\\\nonumber\n\t\\bm{e}_i \\in \\{ \\tilde{\\textbf{h}}_i, \\tilde{\\textbf{x}}_i\\};~~\\textbf{g}_i = \\alpha_1 \\cdot \\tilde{\\textbf{h}}_i \\oplus \\alpha_2 \\cdot \\tilde{\\textbf{x}}_i\n\\end{align}",
            "eq:13": "\\begin{align}\n\t\\mathcal{L} = \\frac{1}{|T|} \\sum_{t \\in T, m \\in M} -\\log(\\frac{\\exp \\hat{y}_{m,t}}{\\sum_{j \\in V} \\exp \\hat{y}_{m,j}})\n\\end{align}",
            "eq:14": "\\begin{align}\n    (\\boldsymbol{\\mathcal{M}}\\cdot \\boldsymbol{\\mathcal{M}}^\\trans)_{ij} = \\sum_{k=1}^n \\boldsymbol{\\mathcal{M}}_{ik}\\boldsymbol{\\mathcal{M}}^\\trans_{kj} = \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}^{{\\trans}^{(j)}} = \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}_{(j)}\n\\end{align}",
            "eq:15": "\\begin{align}\n\\label{eq:lhgcn_split}\n    \\boldsymbol{\\mathcal{M}}_{(i)}\\boldsymbol{\\mathcal{M}}_{(j)} = \\sum_{k=1}^{|\\mathcal{E}^q|} \\boldsymbol{\\mathcal{M}}_{i,k}\\boldsymbol{\\mathcal{M}}_{j,k} + \\sum_{m=1}^{|\\mathcal{E}^p|} \\boldsymbol{\\mathcal{M}}_{i,m}\\boldsymbol{\\mathcal{M}}_{j,m}\n\\end{align}",
            "eq:16": "\\begin{align}\n    \\sum_{\\epsilon'=1}^{|\\mathcal{E}^q|} \\boldsymbol{\\mathcal{M}}_{i, \\epsilon'}\\boldsymbol{\\mathcal{M}}_{j, \\epsilon'} = \\begin{cases}\n    1 & \\text{if}~~ v_i=v_j \\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n\\end{align}",
            "eq:17": "\\begin{align}\n    \\boldsymbol{\\mathcal{M}}^\\prime = C \\oplus A \\oplus W\n\\end{align}",
            "eq:18": "\\begin{align}\n\t\\textbf{X}^{(l+1)} = \\textbf{D}^{-1} \\cdot \\boldsymbol{\\mathcal{M}}^\\prime \\cdot \\textbf{X}^{(l)}\n\\end{align}"
        },
        "git_link": "https://github.com/yuh-yang/MBHT-KDD22"
    }
}