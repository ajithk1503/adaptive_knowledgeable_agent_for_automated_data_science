{
    "pdf_content": "An Ensemble Model with Multi-Scale Features for Incorrect\nAssignment Detection\nMing Shen\nshenming807@gmail.com\nXiangyuxing Investment\nNingBo, China\nAbstract\nWith the number of the publication increasing, the name ambigu-\nity problem is becoming increasingly complex. To improve this\nresearch, OGA-Challenge Team published a large-scale dataset and\nhosted KDD Cup 2024 Challenge for detecting paper assignment\nerrors based on each author and their paper matadata. This paper\npresents an effective and resource-efficient solution to the afore-\nmentioned challenge. Rather than utilising LLM, we have elected\nto employ an embedding model for the representation of text in-\nformation. Furthermore, we have implemented multi-scale feature\nextraction and a graph neural network for the extraction of re-\nlationships between papers. Finally, with our solution, our team\nLoveFishO won 2nd place in task1(WhoIsWho) among 400+ partic-\nipants.\nCCS Concepts\n•Computing methodologies →Machine learning ;Information\nextraction .\nKeywords\nName Ambiguity, Machine Learning, Text Embedding, Feature Ex-\ntraction,Tree Model, Graph Neural Network\nACM Reference Format:\nMing Shen. 2024. An Ensemble Model with Multi-Scale Features for Incor-\nrect Assignment Detection. In Proceedings of Make sure to enter the correct\nconference title from your rights confirmation emai (KDDCup’24). ACM, New\nYork, NY, USA, 3 pages. https://doi.org/XXXXXXX.XXXXXXX\n1 INTRODUCTION\nThe issue of name disambiguation represents a significant challenge\nfor online scholarly systems, particularly in light of the growing\nvolume of published papers. As the number of published papers\ncontinues to increase, this challenge is likely to become increas-\ningly complex and demanding to address[ 1]. While considerable\nattention has been devoted to name disambiguation, comparatively\nlittle attention has been directed towards the study of incorrect\nassignment detection (IND). Conversely, greater attention is being\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDDCup’24, August 25, 2024, Barcelona, Spain\n©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXXpaid to scratch name disambiguation (SND) and real-time name dis-\nambiguation (RND)[ 5]. In order to stimulate further research in this\narea, the OAG-Challenge Team has organized the WhoIsWho-IND\nKDD Cup 2024.\n2 OVERVIEW\nAs depicted in Figure 1, our solution was composed of three parts:\nfeature extraction, feature combination, and ensemble. In the fea-\nture extraction phase, a variety of features were extracted from\ndisparate perspectives. In the feature combination phase, diverse\nfeatures were integrated as input to the different model. Finally, in\nthe model ensemble phase, multiple models were trained based on\ndifferent features, and these models were integrated by assigning\ndifferent weights.\n3 PREPROCESS\n•Fill null value.\n- Fill null value of the year with 0.\n- Replace None with an empty string.\n•Clean the text.\n- Convert uppercase to lowercase.\n- Remove spaces, stop words, and special symbols.\n4 FEATURE EXTRACTION\nTo fully represent the papers, we extract features from multiple\ndimensions. The first feature dimension is the basic statistical fea-\nture of the paper. Secondly, we use pre-trained embedding model\nto encode the textual information in the paper. These embedding\nvectors could effectively describe the content and theme of the\npaper. Finally, we build amount of cross features using basic statis-\ntical features and text vector features to describe the relationships\nbetween all papers under the same author. Here are few powerful\nfeatures:\n•Basic Statistical Features\n- Keyword Feature: Count of keywords\n- Author Feature: Count of authors\n-Organization Features: Nunique of organization; Nunique\nof organization divided by total number of organizations;\nCount of same organizations; Count of same organizations\ndivided by total number of organizations.\n•Text Embedding Features\n-Embedding Features: Encode the title, abstract, and venue\nof the paper using E5-Instruct1[4] and Voyage2\n•Author-Paper Features\n1multilingual-e5-large-instruct\n2voyage-large-2-instruct\nKDDCup’24, August 25, 2024, Barcelona, Spain Ming Shen\nFigure 1: The overview of architecture\n-Text Similarity Features: This refers to the similarity be-\ntween this paper and the author’s other papers.\n𝐶𝑜𝑠𝑆𝑖𝑚(𝑖,𝑋)=𝐴𝐺𝐺(𝑐𝑜𝑠𝑖𝑛𝑒(𝑣𝑖,𝑣𝑗)|𝑗∈𝑋) (1)\nwhere𝐴𝐺𝐺 is the aggregation function, such as max, mean\nand std.𝑐𝑜𝑠𝑖𝑛𝑒 is the cosine similarity function. 𝑣𝑖and𝑣𝑗\nare embedding vectors for title, abstract, and venue.\n𝐽𝑊𝑆𝑖𝑚(𝑖,𝑋)=𝐴𝐺𝐺(𝑠𝑖𝑚_𝑓𝑢𝑛𝑐(𝑠𝑖,𝑠𝑗)|𝑗∈𝑋) (2)\nwhere𝐴𝐺𝐺 is the aggregation function, such as max, mean\nand std.𝑠𝑖𝑚_𝑓𝑢𝑛𝑐 is the Jaro–Winkler similarity function.\n𝑠𝑖and𝑠𝑗are the text of title and venue.\n𝑆𝑢𝑚𝑆𝑖𝑚(𝑖,𝑋)=𝐽𝑊𝑆𝑖𝑚(𝑖,𝑋)+𝐶𝑜𝑠𝑆𝑖𝑚(𝑖,𝑋) (3)\nwhere𝐽𝑊𝑆𝑖𝑚(𝑖,𝑋)is the Jaro–Winkler similarity, 𝐶𝑜𝑠𝑆𝑖𝑚(𝑖,𝑋)\nis cosine similarity.\n- Basic Statistical Cross Features:\n-Max count of the same keyword, author, and organiza-\ntion.\n-Max count of the same keyword, author, and organi-\nzation divided by the count of keywords, authors, and\norganizations.\n-The absolute gap in publication years between papers\nwith the max count of the same keyword, author, and\norganization.\n-The gap between the year with the max, median, mean,\nand min of year\n-The gap between the year with the average of max of\nyear and min of year\n-The gap between the closest and second closest years\nto the current publication year.\n-Has the author previously published a paper at this\nvenue.\n- Is the publication year within the prescribed range.\n- et al.\n5 TREE MODEL\nThe tree model3is employed for the purpose of identifying misclas-\nsified papers, with the extracted features mentioned above serving\n3Lightgbm[2]as the basis for this identification. Nevertheless, the dimension of\nthe text embedding vectors is excessive and greatly exceeds the\nnumber of other features. This results in the model being unable\nto effectively capture the other features, leading to sub-optimal\nmodelling outcomes. To address this issue, it is necessary to down-\nscale the text vector features. The SVD4method is selected for\nthis purpose as it is capable of identifying nonlinear relationships\nand latent space information, which are not as readily discernible\nthrough PCA5. A trial has demonstrated that the optimal reduction\nof the text embedding vector is to 32 dimensions. The data analysis\nrevealed that the data categories were not balanced. To mitigate\nthe model overfitting to a single category, a 10-fold StratifiedKFold\ncross-validation was employed to train the tree model.\n6 GRAPH NEURAL NETWORK\nIn the context of graph neural networks, it is necessary to construct\nthe graph in accordance with the specific task, and to define the\nnodes and edges within the graph. In this study, each author is\ndefined as a graph, with papers representing nodes and connec-\ntions between papers by the same author other than the original\nauthor represented as edges. The node features are comprised of\ntwo distinct parts. The first part encompasses the features utilized\nin the aforementioned tree model, while The second part incorpo-\nrates the text embedding vector features that have not undergone\ndownscaling. The Edge features are comprised of two key elements:\nthe percentage of identical author names and keywords, and the\nsimilarity of organisational and conference names.Although edge\nfeatures are incorporated into the graph convolution process, they\nare not utilized directly. Instead, they are employed to filter out\nsome of the less significant connections. For this task, it was decided\nthat the GCN[3] model would be used as the base model.\n7 ENSEMBLE\nThe train data were not employed for the purpose of fine-tuning\nthe embedding models. In order to guarantee the relative reliabil-\nity of the embedding models within the context of the thesis, two\nembedding models were therefore utilized for the purpose of repre-\nsenting the paper information. Concurrently, in order to ascertain\n4Singular Value Decomposition\n5Principal Components Analysis\nAn Ensemble Model with Multi-Scale Features for Incorrect Assignment Detection KDDCup’24, August 25, 2024, Barcelona, Spain\nTable 1: Model Weight\nModel Weight\nLGB-Voyage 0.385\nLGB-E5-Instruct 0.315\nGCN-E5-Instruct 0.075\nGCN-Voyage 0.075\nGCN-E5-Instruct-Voyage 0.075\nGCN-Voyage-E5-Instruct 0.075\nTable 2: Overall Performance\nModel Score\nLGB-Voyage 0.81433\nLGB-E5-Instruct 0.81827\nGCN-E5-Instruct 0.78082\nLGB(E5-Instruct/Voyage)x2+GCN(E5-Instruct/Voyage)x4 0.82486\nTable 3: Top 10 score in task. Our team \"LoveFishO\" won 2nd\nin this task of KDD Cup 2024\nRank Team Name Score\n1 BlackPearl 0.83454\n2 LoveFishO 0.82487\n3 AGreat 0.81349\n4 Kozuki Cats 0.80890\n5 M1stic 0.80720\n6 DOCOMOLABS 0.80487\n7 qianlan 0.80137\n8 LGB YYDS 0.79941\n9 DeepMayNotLearn 0.79774\n10 Leo_Lu 0.79738\nthe interrelationships between the papers from disparate perspec-\ntives, we employed not only the tree model but also the graph\nneural network model. In total, six models have been integrated,\ndesignated LGB-E5-Instruct, LGB-Voyage, GCN-E5-Instruct, GCN-\nVoyage, GCN-E5-Instruct-Voyage, and GCN-Voyage-E5-Instruct.\nAll models are integrated with different weights1 assigned to each.\n8 EXPERIMENT\nThe performance of our models is listed in Table 2. The results of the\nanalysis indicate that the tree model outperforms the graph neural\nnetwork. Additionally, the E5-Instruct model demonstrates greater\nefficacy than the Voyage model in characterising papers. Although\nthe single graph neural network is not particularly effective, its\nintegration with the tree model is beneficial.\n9 CONCLUSION\nThis paper presents our solution to the WhoIsWho task for the\nKDD Cup 2024. Our approach utilizes tree model and graph neural\nnetwork to detect paper assignment errors. Instead of using dataset\nto train a traditional language model to vectorise the text, we use apre-trained embedding model based on a large corpus to vectorise\nthe text. This approach not only capitalises on the capabilities of\nlarge models but also reduces the consumption of resources. It was\nalso a key factor which won the 2nd place in Task of KDD Cup 2024\nChallenge.\nReferences\n[1]Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li,\nYuxiao Dong, and Jie Tang. 2023. Web-Scale Academic Name Disambiguation:\nThe WhoIsWho Benchmark, Leaderboard, and Toolkit. In Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach,\nCA, USA) (KDD ’23) . Association for Computing Machinery, New York, NY, USA,\n3817–3828. https://doi.org/10.1145/3580305.3599930\n[2]Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision\ntree. Advances in neural information processing systems 30 (2017).\n[3]Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with\nGraph Convolutional Networks. arXiv:1609.02907 [cs.LG] https://arxiv.org/abs/\n1609.02907\n[4]Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder,\nand Furu Wei. 2024. Multilingual E5 Text Embeddings: A Technical Report.\narXiv:2402.05672 [cs.CL] https://arxiv.org/abs/2402.05672\n[5]Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu\nWang, Qingfei Zhao, Yuqing Cheng, et al .2024. OAG-Bench: A Human-Curated\nBenchmark for Academic Graph Mining. arXiv preprint arXiv:2402.15810 (2024).\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\n",
    "latex_extraction": {
        "1": {
            "name": "Title and Author Information",
            "content": "An Ensemble Model with Multi-Scale Features for Incorrect Assignment Detection\nMing Shen\nshenming807@gmail.com\nXiangyuxing Investment\nNingBo, China"
        },
        "2": {
            "name": "Abstract",
            "content": "With the number of the publication increasing, the name ambiguity problem is becoming increasingly complex. To improve this research, OGA-Challenge Team published a large-scale dataset and hosted KDD Cup 2024 Challenge for detecting paper assignment errors based on each author and their paper metadata. This paper presents an effective and resource-efficient solution to the aforementioned challenge. Rather than utilizing LLM, we have elected to employ an embedding model for the representation of text information. Furthermore, we have implemented multi-scale feature extraction and a graph neural network for the extraction of relationships between papers. Finally, with our solution, our team LoveFishO won 2nd place in task1 (WhoIsWho) among 400+ participants."
        },
        "3": {
            "name": "CCS Concepts and Keywords",
            "content": "CCS Concepts\n•Computing methodologies → Machine learning; Information extraction.\nKeywords\nName Ambiguity, Machine Learning, Text Embedding, Feature Extraction, Tree Model, Graph Neural Network."
        },
        "4": {
            "name": "ACM Reference Format",
            "content": "Ming Shen. 2024. An Ensemble Model with Multi-Scale Features for Incorrect Assignment Detection. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (KDDCup’24). ACM, New York, NY, USA, 3 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        "5": {
            "name": "1 INTRODUCTION",
            "content": "The issue of name disambiguation represents a significant challenge for online scholarly systems, particularly in light of the growing volume of published papers. As the number of published papers continues to increase, this challenge is likely to become increasingly complex and demanding to address. While considerable attention has been devoted to name disambiguation, comparatively little attention has been directed towards the study of incorrect assignment detection (IND). Conversely, greater attention is being paid to scratch name disambiguation (SND) and real-time name disambiguation (RND). In order to stimulate further research in this area, the OAG-Challenge Team has organized the WhoIsWho-IND KDD Cup 2024."
        },
        "6": {
            "name": "2 OVERVIEW",
            "content": "As depicted in Figure 1, our solution was composed of three parts: feature extraction, feature combination, and ensemble. In the feature extraction phase, a variety of features were extracted from disparate perspectives. In the feature combination phase, diverse features were integrated as input to the different model. Finally, in the model ensemble phase, multiple models were trained based on different features, and these models were integrated by assigning different weights."
        },
        "7": {
            "name": "3 PREPROCESS",
            "content": "• Fill null value.\n  - Fill null value of the year with 0.\n  - Replace None with an empty string.\n• Clean the text.\n  - Convert uppercase to lowercase.\n  - Remove spaces, stop words, and special symbols."
        },
        "8": {
            "name": "4 FEATURE EXTRACTION",
            "content": "To fully represent the papers, we extract features from multiple dimensions. The first feature dimension is the basic statistical feature of the paper. Secondly, we use a pre-trained embedding model to encode the textual information in the paper. These embedding vectors could effectively describe the content and theme of the paper. Finally, we build a number of cross features using basic statistical features and text vector features to describe the relationships between all papers under the same author. Here are a few powerful features:\n\n• Basic Statistical Features\n  - Keyword Feature: Count of keywords\n  - Author Feature: Count of authors\n  - Organization Features: Unique count of organizations; Unique count divided by total number of organizations; Count of same organizations; Count of same organizations divided by total number of organizations.\n\n• Text Embedding Features\n  - Embedding Features: Encode the title, abstract, and venue of the paper using E5-Instruct and Voyage.\n\n• Author-Paper Features\n  - Text Similarity Features: This refers to the similarity between this paper and the author’s other papers.\n    * CosSim(i, X) = AGG(cosine(vi, vj) | j∈X)\n      where AGG is the aggregation function, such as max, mean, and std. Cosine is the cosine similarity function. vi and vj are embedding vectors for title, abstract, and venue.\n    * JWSim(i, X) = AGG(sim_func(si, sj) | j∈X)\n      where AGG is the aggregation function, such as max, mean, and std. sim_func is the Jaro–Winkler similarity function. si and sj are the text of title and venue.\n    * SumSim(i, X) = JWSim(i, X) + CosSim(i, X)\n      where JWSim(i, X) is the Jaro–Winkler similarity, CosSim(i, X) is cosine similarity.\n\n  - Basic Statistical Cross Features:\n    * Max count of the same keyword, author, and organization.\n    * Max count of the same keyword, author, and organization divided by the count of keywords, authors, and organizations.\n    * The absolute gap in publication years between papers with the max count of the same keyword, author, and organization.\n    * The gap between the year with the max, median, mean, and min of year\n    * The gap between the year with the average of max of year and min of year\n    * The gap between the closest and second closest years to the current publication year.\n    * Has the author previously published a paper at this venue.\n    * Is the publication year within the prescribed range.\n    * et al."
        },
        "9": {
            "name": "5 TREE MODEL",
            "content": "The tree model (LightGBM) is employed for the purpose of identifying misclassified papers, with the extracted features mentioned above serving as the basis for this identification. Nevertheless, the dimension of the text embedding vectors is excessive and greatly exceeds the number of other features. This results in the model being unable to effectively capture the other features, leading to sub-optimal modeling outcomes. To address this issue, it is necessary to downscale the text vector features. The SVD (Singular Value Decomposition) method is selected for this purpose as it is capable of identifying nonlinear relationships and latent space information, which are not as readily discernible through PCA (Principal Components Analysis). A trial has demonstrated that the optimal reduction of the text embedding vector is to 32 dimensions. The data analysis revealed that the data categories were not balanced. To mitigate the model overfitting to a single category, a 10-fold StratifiedKFold cross-validation was employed to train the tree model."
        },
        "10": {
            "name": "6 GRAPH NEURAL NETWORK",
            "content": "In the context of graph neural networks, it is necessary to construct the graph in accordance with the specific task, and to define the nodes and edges within the graph. In this study, each author is defined as a graph, with papers representing nodes and connections between papers by the same author other than the original author represented as edges. The node features are comprised of two distinct parts. The first part encompasses the features utilized in the aforementioned tree model, while the second part incorporates the text embedding vector features that have not undergone downscaling. The Edge features are comprised of two key elements: the percentage of identical author names and keywords, and the similarity of organizational and conference names. Although edge features are incorporated into the graph convolution process, they are not utilized directly. Instead, they are employed to filter out some of the less significant connections. For this task, it was decided that the GCN (Graph Convolutional Network) model would be used as the base model."
        },
        "11": {
            "name": "7 ENSEMBLE",
            "content": "The train data were not employed for the purpose of fine-tuning the embedding models. In order to guarantee the relative reliability of the embedding models within the context of the thesis, two embedding models were therefore utilized for the purpose of representing the paper information. Concurrently, in order to ascertain the interrelationships between the papers from disparate perspectives, we employed not only the tree model but also the graph neural network model. In total, six models have been integrated, designated as LGB-E5-Instruct, LGB-Voyage, GCN-E5-Instruct, GCN-Voyage, GCN-E5-Instruct-Voyage, and GCN-Voyage-E5-Instruct. All models are integrated with different weights assigned to each. The model weights and performance details are detailed as follows:\n\nTable 1: Model Weight\n| Model | Weight |\n|---------|--------|\n| LGB-Voyage | 0.385 |\n| LGB-E5-Instruct | 0.315 |\n| GCN-E5-Instruct | 0.075 |\n| GCN-Voyage | 0.075 |\n| GCN-E5-Instruct-Voyage | 0.075 |\n| GCN-Voyage-E5-Instruct | 0.075 |\n\nTable 2: Overall Performance\n| Model | Score |\n|---------|--------|\n| LGB-Voyage | 0.81433 |\n| LGB-E5-Instruct | 0.81827 |\n| GCN-E5-Instruct | 0.78082 |\n| LGB(E5-Instruct/Voyage)x2+GCN(E5-Instruct/Voyage)x4 | 0.82486 |"
        },
        "12": {
            "name": "8 EXPERIMENT",
            "content": "The performance of our models is listed in Table 2. The results of the analysis indicate that the tree model outperforms the graph neural network. Additionally, the E5-Instruct model demonstrates greater efficacy than the Voyage model in characterizing papers. Although the single graph neural network is not particularly effective, its integration with the tree model is beneficial.\n\nTable 3: Top 10 score in task. Our team \"LoveFishO\" won 2nd in this task of KDD Cup 2024\n| Rank | Team Name | Score |\n|------|--------------|----------|\n| 1 | BlackPearl | 0.83454 |\n| 2 | LoveFishO | 0.82487 |\n| 3 | AGreat | 0.81349 |\n| 4 | Kozuki Cats | 0.80890 |\n| 5 | M1stic | 0.80720 |\n| 6 | DOCOMOLABS | 0.80487 |\n| 7 | qianlan | 0.80137 |\n| 8 | LGB YYDS | 0.79941 |\n| 9 | DeepMayNotLearn | 0.79774 |\n| 10 | Leo_Lu | 0.79738 |"
        },
        "13": {
            "name": "9 CONCLUSION",
            "content": "This paper presents our solution to the WhoIsWho task for the KDD Cup 2024. Our approach utilizes tree models and graph neural networks to detect paper assignment errors. Instead of using the dataset to train a traditional language model to vectorize the text, we use a pre-trained embedding model based on a large corpus to vectorize the text. This approach not only capitalizes on the capabilities of large models but also reduces the consumption of resources. It was also a key factor which won the 2nd place in Task of KDD Cup 2024 Challenge."
        },
        "14": {
            "name": "References",
            "content": "[1] Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li, Yuxiao Dong, and Jie Tang. 2023. Web-Scale Academic Name Disambiguation: The WhoIsWho Benchmark, Leaderboard, and Toolkit. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD ’23). Association for Computing Machinery, New York, NY, USA, 3817–3828.\nhttps://doi.org/10.1145/3580305.3599930\n\n[2] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. Advances in neural information processing systems 30 (2017).\n\n[3] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. arXiv:1609.02907 [cs.LG] https://arxiv.org/abs/1609.02907\n\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual E5 Text Embeddings: A Technical Report. arXiv:2402.05672 [cs.CL] https://arxiv.org/abs/2402.05672\n\n[5] Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, et al. 2024. OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining. arXiv preprint arXiv:2402.15810 (2024)."
        }
    },
    "LLM_extraction": {
        "meta_data": {
            "title": "An Ensemble Model with Multi-Scale Features for Incorrect Assignment Detection",
            "authors": [
                "Ming Shen"
            ],
            "affiliations": [
                "Xiangyuxing Investment, NingBo, China"
            ],
            "abstract": "With the number of publications increasing, the name ambiguity problem is becoming increasingly complex. To improve this research, OGA-Challenge Team published a large-scale dataset and hosted KDD Cup 2024 Challenge for detecting paper assignment errors based on each author and their paper metadata. This paper presents an effective and resource-efficient solution to the aforementioned challenge. Rather than utilizing LLM, we have elected to employ an embedding model for the representation of text information. Furthermore, we have implemented multi-scale feature extraction and a graph neural network for the extraction of relationships between papers. Finally, with our solution, our team LoveFishO won 2nd place in Task1 (WhoIsWho) among 400+ participants.",
            "keywords": [
                "Name Ambiguity",
                "Machine Learning",
                "Text Embedding",
                "Feature Extraction",
                "Tree Model",
                "Graph Neural Network"
            ],
            "year": "2024",
            "venue": "Proceedings of KDD Cup '24",
            "doi link": "https://doi.org/XXXXXXX.XXXXXXX",
            "method name": null
        },
        "relate work": {
            "related work category": [
                "Name Disambiguation",
                "Incorrect Assignment Detection"
            ],
            "related papers": "[1] For name disambiguation: Bo Chen, Jing Zhang, et al. Web-Scale Academic Name Disambiguation: The WhoIsWho Benchmark, Leaderboard, and Toolkit. KDD ’23",
            "comparisons with related methods": "In this paper, the focus is shifted from the well-studied problem of name disambiguation to the relatively less explored area of incorrect paper assignment detection, introducing multi-scale feature extraction combined with a graph neural network."
        },
        "high_level_summary": {
            "summary of this paper": "The paper introduces a novel approach for detecting incorrect assignment in academic papers through an ensemble of models employing multi-scale feature extraction techniques.",
            "research purpose": "To develop a robust model for detecting paper assignment errors among authors using extracted multi-scale features.",
            "research challenge": "Addressing the complexity in name ambiguity and incorrect assignments in a growing database of publications.",
            "method summary": "The paper employs an ensemble model integrating tree-based models and graph neural networks, leveraging multi-scale feature extraction and pre-trained text embedding models.",
            "conclusion": "The method demonstrated effective performance, securing second place in the KDD Cup 2024's WhoIsWho task."
        },
        "Method": {
            "description": "An ensemble model integrating tree-based models and graph neural networks designed for detecting paper assignment errors, particularly in handling name ambiguity issues.",
            "problem formultaion": "Addressing the incorrect assignment of academic papers in databases due to name ambiguity.",
            "feature processing": "Features are extracted from text embeddings, basic statistical measures, cross-paper and author-paper relationships using cosine and Jaro-Winkler similarity metrics.",
            "model": "LightGBM for the tree model and GCN (Graph Convolutional Neural Network) for graph-based modelling.",
            "tasks": [
                "Incorrect Assignment Detection"
            ],
            "theoretical analysis": null,
            "complexity": "The paper addresses the high dimensionality of text embedding data by dimension reduction techniques using SVD (Singular Value Decomposition).",
            "algorithm step": null
        },
        "Experiments": {
            "datasets": [
                "KDD Cup 2024 dataset; includes metadata and text from authors' papers"
            ],
            "baselines": [
                "Comparisons are made against other participants in the KDD Cup 2024, indicating relative performance."
            ],
            "evaluation metric": null,
            "setup": "The training involved a combination of tree models (LightGBM) and GCN, with features like text embeddings and organization-level data.",
            "hyperparameters": null,
            "results": "The tree models outperformed GCN, with best results obtained through model integration.",
            "performance": "Achieved a score of 0.82487, earning 2nd place in the competition.",
            "analysis": null,
            "ablation study": null
        },
        "conclusion": {
            "summary": "The proposed method integrates tree models with graph neural networks, leveraging multi-scale feature extraction techniques and pre-trained text embeddings to detect incorrect paper assignments effectively.",
            "future work": null
        }
    }
}