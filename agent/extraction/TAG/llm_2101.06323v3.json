{
    "meta_info": {
        "title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored  Search",
        "abstract": "Text encoders based on C-DSSM or transformers have demonstrated strong\nperformance in many Natural Language Processing (NLP) tasks. Low latency\nvariants of these models have also been developed in recent years in order to\napply them in the field of sponsored search which has strict computational\nconstraints. However these models are not the panacea to solve all the Natural\nLanguage Understanding (NLU) challenges as the pure semantic information in the\ndata is not sufficient to fully identify the user intents. We propose the\nTextGNN model that naturally extends the strong twin tower structured encoders\nwith the complementary graph information from user historical behaviors, which\nserves as a natural guide to help us better understand the intents and hence\ngenerate better language representations. The model inherits all the benefits\nof twin tower models such as C-DSSM and TwinBERT so that it can still be used\nin the low latency environment while achieving a significant performance gain\nthan the strong encoder-only counterpart baseline models in both offline\nevaluations and online production system. In offline experiments, the model\nachieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for\nlong-tail low-frequency Ads, and in the online A/B testing, the model shows a\n2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.",
        "author": "Jason Yue Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, Huasha Zhao",
        "link": "http://arxiv.org/abs/2101.06323v3",
        "category": [
            "cs.CL",
            "cs.LG"
        ],
        "additionl_info": "Jason Yue Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus  Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. 2021.  TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search.  In Proceedings of the Web Conference 2021 (WWW 21), April 19-23, 2021,  Ljubljana, Slovenia. ACM, New York, NY, USA, 10 pages. https:  //doi.org/10.1145/3442381.3449842"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\nSponsored search refers to the business model of search engine platforms where third-party sponsored information is shown to targeted users along with other organic search results. This allows the advertisers such as manufacturers or retailers to increase the exposure of their products to more targeted potential buyers, and at the same time gives users a quicker access to solutions for their needs. Hence it has become an indispensable part of our modern web experience. While many of the existing models are very powerful for various tasks in sponsored search, there still remain three main challenges for future developments in this field: 1) while the existing models have strong performances on matching common queries with popular products, they usually still find long-tail low-frequency queries/Ads to be more challenging. The worse embedding representations in rare items are potentially caused by under-training due to naturally scarce data on these low-frequency examples. 2) while many modern models improve in implicit feature engineering on the existing input data, finding new and easily accessible data with complement information is still a promising route to greatly improve the model performance but is rarely explored. 3) the search engine systems generally have very strict constraints on computational resources and latency requirements. Many recently developed large powerful models are simply infeasible to deploy onto the highly constrained online search engine systems.\n\nRepresentation learning for queries, products, or users has been a key research field with many breakthroughs over the last years and has been adopted in many production sponsored search systems \\cite{Huang_2020}\\cite{10.1145/3394486.3403280}\\cite{10.1145/3219819.3219885}\\cite{10.1145/3219819.3219897}. Convolutional Deep Structured Semantic Model (C-DSSM) \\cite{shen2014learning} is among the first powerful solutions to encode text data into low-dimensional representation vectors which can be applied to downstream tasks and have efficient inference performance, but its NLU performance has been surpassed by many recently developed NLP models. The pre-trained language models emerged in recent years, such as transformers \\cite{10.5555/3295222.3295349} and BERT \\cite{devlin-etal-2019-bert}, have demonstrated far superior performance in many NLU tasks and even reach human level performance on many tasks. These models are better at capturing contextual information in the sentences and generate better language representation embeddings, leading to much stronger performance in downstream tasks. However, due to the complexity, these models are unfortunately not feasible to run in low latency systems without modifications. Recently, the transformer model has been modified and trained with special techniques such as knowledge distillation \\cite{hinton2015distilling}, which allows us to use similar transformers structure but much smaller model called TwinBERT \\cite{lu2020twinbert} to run with reasonable computational cost in the production systems while having little or no performance loss compared to the full size BERT models. This breakthrough significantly improves the user Information Retrieval experience when using search engines. However, while both C-DSSM and TwinBERT are specifically designed to be applied to the low latency systems with strong performance, they are not the panacea to fully solve all the problems in sponsored search. Their model \nability is sometimes hindered by the limited information in the original input texts and hence still suffers in understanding many challenging low frequency inputs.\n\nGiven the strong performance of the baseline models in NLU tasks, it would be extremely difficult to further improve them solely based on the structural changes of the model without introducing new complement information. The newly developed NLP models achieve relatively small improvements with exponentially growth in model complexity, and hence reach the margin of diminishing returns making it harder to satisfy all the latency constraints. A real improved model in this field should then be able to take in additional information beyond the tradition semantic text inputs, demonstrate stronger performance over the harder low-frequency inputs, and at the same time should not significantly increase the inference time.\n\nA natural and easily accessible data source that provides information beyond semantic text in the search engine system is users' implicit feedbacks recorded in logs in the form of clicks through the links shown to them. A click signals a connection between a query and an Ad and hence a large behavior graph based on clicks can be easily built. In the recent years, various Graph Neural Network (GNN) structures \\cite{zhou2019graph} have been proposed to deal with the abundant graph-typed data and demonstrated strong performance and breakthroughs in social networks, recommendations, or natural science tasks. Motivated by the recent developments in GNN community, we are aiming to identify ways to include complementary and abundant graph-type data into the text model in a natural way. Most existing GNN models focus only on the aggregation of pre-existing neighbor features that are fixed throughout training. Instead of training the language model and the graph model separately, we want the two models to work in conjunction with each \nother to generate better query/Ad representations that can help understanding users' needs in a deeper way. \n% Such graph data can be naturally constructed from user historical behaviors in the logs of the search engine systems. While the transformer model is already very strong in NLU, the bottleneck to further improve such model is largely on the limited semantic information that the model can possibly extract from the inputs. \n\nThe main contributions of this work are three-folds:\n\\begin{enumerate}\n    \\item We propose TextGNN\\footnote{The BERT version implementation of the model may be found at: https://github.com/microsoft/TextGNN}, a general end-to-end framework for NLU that combines the strong language modeling text encoders with graph information processed by Graph Neural Networks to achieve stronger performance than each of its individual components.\n    \\item We find a systematical way to leverage graph information that greatly improves the robustness and performance by 1\\% on hard examples. These samples are very challenging when only using semantic information.\n    \\item We trained TextGNN with knowledge distillation to get a compact model. The model has been adopted in the production system that has strict computational and latency constraints while achieving a 2.03\\% increase in Revenue Per Mille with a 2.32\\% decrease in Ad defect rate in the online A/B testing.\n\\end{enumerate}\n\nThe rest of this paper is organized as follows. Section 2 is a brief introduction of sponsored search and Ad relevance task. Section 3 reviews related literature. Section 4 discusses the details of the model, including the architecture, the construction of graph-type data, and the training methodology. Section 5 reports the experimental results of TextGNN in comparison to the baseline model under both offline and online settings with a few illustrative case study examples. Section 6 concludes the paper and briefly discusses the future directions of this work.\n\n",
                "category": [
                    "introduction"
                ]
            },
            "section 2": {
                "name": "Sponsored Search and Ad Relevance",
                "content": "\nThe TextGNN model is developed to improve the existing Ad Relevance model at a major Sponsored Search platform. In a typical sponsored search ecosystem, there are often three parties: user, advertiser and search engine platform. When the user types a query into the search engine, the goal of the platform is to understand the underlying intent of the user behind the semantic meanings of the query, and then try to best match it with a short list of Ads submitted by the advertisers alongside other organic search results. \n\nIn the back-end when a query is received by the platform, the system will first conduct a quick but crude recall step using highly efficient Information Retrieval algorithms (such as TF-IDF \\cite{Jones72astatistical} or BM25 \\cite{robertson1995okapi}) to retrieve an initial list of matched candidates. The relatively long list is then passed to the downstream components for a finer filtering and final ranking using much more sophisticated but slightly less efficient models to serve the users. In both of the later steps, Deep Learning based Ad Relevance models play a key role in delivering high quality contents to the user and match advertisers' products with the potential customers.\nFor the Ad Relevance task, our model usually relies only on the query from a user and keywords provided by the advertiser. A \\textbf{query} refers to a short text that a user typed into the search engine when he/she is looking for relevant information or product, and the model needs to identify the user's intent based on the short query. A \\textbf{keyword} is a short text submitted by an advertiser that is chosen to express their intent about potential customers. The keyword is in general not visible from end users, but it is crucial for the search engine platform to match user intents.\n\nWhen an Ad is displayed to a user, we call this an \\textbf{impression}. The platform does not receive anything from an impression but earns revenue only when the displayed Ad is \\textbf{click}ed by the user. Because of this mechanism, the search engine platform has an incentive to display the Ads that best match user intents, which directly affects the revenue. Lastly, given the scale of the traffic of the search engine, Ad Relevance models are such an indispensable component of the system and any improvement of the performance of the model can lead to huge impact on the business side of the search engine.\n\n",
                "category": [
                    "introduction"
                ]
            },
            "section 3": {
                "name": "Related Work",
                "content": "\n\\textbf{Text Encoders} including C-DSSM and Pre-trained Transformer-based Language Models (such as BERT) have achieved impressive state-of-the-art performance in many NLP tasks for their effective language or contextual word representations, hence have become one of the most important and most active research areas. \n\nC-DSSM is developed specifically for extracting semantic information into a low-dimension representation vector by combining convolutional layers that extract local contextual semantic information in the string with max-pooling layers that helps identifying globally important features. It is still a workhorse model used extensively in the stacks of many production search engine systems.\n\nThe large and expensive BERT model has recently become very popular. The model is usually learned in two steps. First the model is trained on extremely large corpus with unsupervised tasks such as masked language model (MLM) and next sentence prediction (NSP) to learn the general language, and then in a second step fine-tuned on the task-specific labelled data to be used in downstream tasks. Despite the strong performance of the BERT models on language representations, they are in general too expensive to be deployed in the real-time search engine systems where there are strict constraints on computation costs and latency.\n\n\n  \n\\textbf{Distilled TwinBERT} is one successful model that adapts the Transformer family models to the sponsored search applications and achieves comparable performance at reasonable inference time cost compared with heavy stacked transformer layers. The TwinBERT model as demonstrated in Figure \\ref{fig:twinbert} benefits from two important techniques: 1) given two input texts, a query and a keyword, a vanilla transformer encoder would concatenate them into one input sequence, while TwinBERT has a twin tower structure to decouple the two-sentence input. Such twin tower structure is first proposed in the DSSM model \\cite{huang2013learning} for web document ranking. Given that the keywords are already known to the platform, the encoded outputs of the keyword-side tower could then be pre-generated offline and fetched efficiently during inference time. Without concatenating the keyword strings, the input to the query-side tower can also be set with a low maximum length, and hence greatly reduce the inference time complexity compared to a large BERT model. 2) knowledge distillation technique is used to transfer the knowledge learnt by a teacher model to a much smaller student model. Our teacher model can be seen as a stronger version of the BM25 signal in the previous weak supervision method \\cite{10.1145/3077136.3080832}. While the teacher model has strong performance, it is usually too costly and infeasible to be directly used in a production system. Knowledge distillation enables us to train a smaller model that is much faster when inference with only little or no significant loss in performance \\cite{10.1145/3308558.3313466}\\cite{DBLP:journals/corr/abs-1910-01108}. When a TwinBERT model with only 3 layers of encoders is used, with all the optimizations it is possible to be deployed in the real-world production systems that satisfies the strict limit from computational resources and latency requirement. \n\nHowever, as a pure language model, TwinBERT can only rely on the semantic meanings of the query-keyword pairs to infer the relationships, and in many cases when we encounter uncommon words it is still very challenging to correctly infer relevance for our main applications based on the limited input information.\n\n\\textbf{Graph Neural Network} has also become a hot research area in recent years due to its efficacy in dealing with complex graph data. Graph Convolutional Networks (GCN) \\cite{Kipf:2016tc}, GraphSage \\cite{NIPS2017_6703}, and Graph Attention Networks (GAT) \\cite{velickovic2018graph} are among the most popular GNN models that can effectively propagate neighbor information in a graph through connected edges and hence are able to generate convincing and highly interpretable results on many graph specific tasks such as node/edge/graph property predictions. Recently there are also attempts to bring GNN to the sponsored search area such as click-through rate (CTR, ratio of the number of clicks to the number of impressions) prediction \\cite{10.1145/3357384.3357951}\\cite{10.1145/3357384.3357833}, but so far these attempts have only focused on using GNN to generalize the interactions among the existing fixed features. There is no strong convincing story why these features naturally form a graph and the GNN itself has no impact on the generation of the features. Alternatively people have also proposed to utilize the graph information implicitly through label-propagation to unlabeled examples\\cite{10.1145/1645953.1646090}, but explicitly using the neighbor features in the model structure will be more efficient in aggregating complementary information as demonstrated in the experiments.\n\nTo the best of our knowledge, we are the first to extend various text encoders with a graph in a natural way, and co-train both text encoders and GNN parameters at the same time to achieve stronger performance in our downstream tasks.\n\n",
                "category": [
                    "related work"
                ]
            },
            "section 4": {
                "name": "TextGNN",
                "content": "\n\n  \nIn this section we will discuss the architecture of the proposed TextGNN model in Section 4.1. Then we describe the graph we used to naturally augment the semantic information of the input query-keyword sentence pairs in Section 4.2. Lastly in Section 4.3 we briefly recap knowledge distillation and its application in our model.\n",
                "subsection 4.1": {
                    "name": "Model Architecture",
                    "content": "\nThe architecture of the TextGNN model is discussed in detail in this subsection and also illustrated in Figure \\ref{fig:architecture}. The proposed model is a natural extension of the high-performance C-DSSM/TwinBERT baseline model with additional information from graph structured data. In sponsored search scenario, we have tens of millions candidate Ads. It is infeasible to use a complex text encoder to compute the similarity between a search query and each Ad one-by-one. Twin tower structure is a good choice for us where we could compute Ads representation vectors in advance and when a query comes, we then compute the representation vector of the query online. Notice that we only need to run the complex text encoder once for each incoming search query, compared with vanilla BERT which requires this for each unique pair. For transformer encoders, the computation cost in self-attention is also quadratic to the length of the input string. Hence, splitting the query and keyword strings for separate calculation is also much less costly than calculating the concatenated string. With these benefits in mind, our model also follows the twin tower structure of the baseline models with small encoder structure layers so that all the benefits of the twin tower structured model are inherited and hence can be deployed in the production system. Taking the query-side tower as an example, given a query and its three neighbors (defined later in the graph construction section) will all go through any general Text Encoder blocks to each generate a vector representation for the short sentence. The information from the four representation vectors is then aggregated by a GNN Aggregator to generate a single output vector. This output vector is then connected with the direct output of the text encoder of the query sentence through either concatenation or addition, similar to the idea of a Residual Connection Network \\cite{he2015residual}. The combined output vector is considered as the final output of the query-side tower and can then be interacted with keyword-side output (generated from the very similar structured keyword-side tower) in the crossing layer to get the final output similar to a C-DSSM/TwinBERT model. \n",
                    "subsubsection 4.1.1": {
                        "name": "Text Encoder Block",
                        "content": "\nThe Text Encoder block is very similar to a single tower in the C-DSSM/TwinBERT model. For example, for a transformer type text encoder, a sentence is first tokenized using the BERT WordPiece tokenizer. Trainable token embedding vectors is combined with BERT style positional embedding through addition before it go through three BERT encoder layers. The only difference with a BERT-style model is that the segment embeddings in the BERT are no longer needed as all inputs will be from the same sentence. With this structure so similar to a BERT-type one, we can conveniently load the weights from the first three layers of the pre-trained large BERT model to get a good starting point that leads to much better performance, faster model convergence, and requires significantly less training data compared to a random initialization. After the text encoder layers, we get a sequence of vectors corresponding to each token in the sentence. The vectors are then combined using a weighted-average pooling layer similar to the TwinBERT model which has demonstrated better performance in generating a single vector representation for a sentence.\n\nThe four Text Encoder blocks within a single tower are set to share the same parameters. However, the model is flexible enough to allow the two towers to have all different Text Encoder blocks, but as the TwinBERT paper shows that shared encoder blocks generally lead to slightly better performance we use that approach.\n\n"
                    },
                    "subsubsection 4.1.2": {
                        "name": "GNN Aggregator",
                        "content": "\nIn one tower of our TextGNN, the four text encoder blocks generate four vector representations, one for the center node (query/keyword) and the other three for its three one-hop neighbors. To aggregate the information from four vectors into one, we adopt a GNN aggregation layer, where we take the query/keyword as the central node and perform one-hop aggregation using the three neighbor nodes. The aggregation itself can be very general and use most existing GNN aggregators such as GCN, GraphSAGE, and GAT. In our experiments we found that GAT, which assigns learnable weights to the neighbors to generated a weighted average, demonstrates the strongest performance and is used in our experiments.\n\n"
                    },
                    "subsubsection 4.1.3": {
                        "name": "Skip Layer",
                        "content": "\nThe output vector of the query/keyword encoder is connected to the output of GNN Aggregator as the final output of the query-/keyword-side tower. This layer can be thought as a skip layer \\cite{he2015residual} so that the additional GNN outputs serve as a complementary information to the text semantic representation vector. In this sense the encoder-only-models can also be considered as a special case of the TextGNN model when the GNN output is completely skipped. The two vectors are combined using either concatenation or addition. In case they have different dimensions an additional dense layer is applied after the GNN Aggregator to up/downscale the GNN output dimension to match the Text Encoder output.\n\n"
                    },
                    "subsubsection 4.1.4": {
                        "name": "Crossing Layer",
                        "content": "\nGiven the final outputs of the query-/keyword-side tower, the two vectors are first combined through concatenation, and then compute the similarity score using the Residual network proposed in the TwinBERT model. Formally, the residual function is defined as:\n\\begin{equation}\n\\textbf{y} = \\mathcal{F}(\\textbf{x}, W, b) + \\textbf{x},\n\\end{equation}\nwhere \\textbf{x} is the concatenation of the query-side vector \\textbf{q} and keyword-side vector \\textbf{k} and $\\mathcal{F}$ is the mapping function from \\textbf{x} to the residual with parameters $W$ and $b$. A logistic regression layer is then applied to the output vector \\textbf{y} to predict the binary relevance label.\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Graph Construction",
                    "content": "\nOn top of the powerful structure of the model, it is also crucial to get access to high quality graph-type data. Such data should satisfy the following properties: \n\\begin{enumerate}\n    \\item \\textbf{Relevant:} since the graph neural networks propagate information along the edges, we are looking for neighbors that are highly relevant to the intent of the center node (query/keyword).\n    \\item \\textbf{Complementary:} we expect the GNN to excel the most in situations where the language modeling part struggles to infer the intention only from the semantic meanings of the sentence, but the additional neighbors might be extremely valuable to provide complementary information that help the model to better understand the inputs. This situation happens most frequently on rare and low frequency items where the language models usually struggles on these long-tail inputs.\n    \\item \\textbf{Accessible:} in sponsored search system, there are large amount of user input queries and candidate keywords. We try to find their neighbors in a graph. As a large graph is preferred, the neighbors need to be found with little effort and constructing the graph data should be feasible without heavy manual work, strong assumptions, or complicated structures.\n\\end{enumerate}\n\nGiven the requirements, we find that the user behavior graph generated from historical user clicking logs is a great candidate for our purpose. It is based on the insight that when a user inputs a query $a$ and then clicks the Ad $b$, then $b$ has to sufficiently fit the user's intent from $a$ to trigger the click. In the next two subsections, we discuss such behavior graph and its extension to address the sparse coverage issue of the behavior graph.\n\n\n  \n",
                    "subsubsection 4.2.1": {
                        "name": "User Click Graph",
                        "content": "\nThe eligible neighbors of a query are the keyword of Ads that have been shown to be relevant to the query and received explicit positive feedback by a click. One general assumption to sort all the candidates is that the empirically observable CTR is highly correlated to the relevance between the query and the keyword. Based on this assumption, as illustrated in Figure \\ref{fig:neigh}(a), we take all clicked Ads that have been shown to users at least 50 times in the past year (to partially address the issue of noisy estimates of CTR on Ads with small number of impressions) and take the top three as the neighbors.\n\nTable \\ref{tab:click_graph} shows an illustrative example, where the search query is \"usps com careers login\". Its top three neighbors, which are the keywords of the corresponding Ads, are listed with their historical total number of impressions and clicks. Although the first keyword \"united state postal service jobs\" is only shown 59 times which is significantly fewer than the third keyword \"postal service hiring\" with 1,721 impressions, it has a much higher CTR of 30.5\\% compared to 22.3\\%, indicating that users who searched for this query are more likely to find the first keyword useful, which is a strong indication of higher relevance.\n\n\n\n"
                    },
                    "subsubsection 4.2.2": {
                        "name": "User Click Graph with Semantic ANN",
                        "content": "\nFor rare and low frequency queries/keywords, we observe by construction substantially less feedback from clicks logs. Furthermore, to avoid the noise of selecting neighbors with high CTR, we have criteria to exclude neighbors that are shown less than 50 times in the past year and this unfortunately eliminates a number of neighbors and makes the situation even worse for long-tail inputs. To address this issue, we propose a neighbor completion technique based on Approximate Nearest Neighbor (ANN) \\cite{Indyk98approximatenearest} using Neighborhood Graph Search (NGS) \\cite{10.1145/2393347.2393378}. As illustrated in Figure \\ref{fig:neigh}(b), first we infer vector representations by a powerful C-DSSM (which is used extensively in a major sponsored search system) for all nodes in user click graph. Next, for a query that we could not identify any eligible clicked keywords, we infer its vector representation by the same C-DSSM. Then, we leverage the ANN search tool to find another query that is supposed to be semantically close enough to the original query and has the click neighbors and use its clicked keywords as approximate neighbors for the original query. This has the same spirit as the common technique of query rewriting in search engine systems but does so in a more implicit way. For keywords without any clicked queries, we find neighbors for them in a similar way.\n\nIn Table \\ref{tab:click_graph_ann} we show another example that we are not able to find any eligible neighbors for the query \"video games computers free\", but its ANN query \"no internet games\" has user behavior feedback and the three approximate neighbors are obviously relevant to the original query.\n\n\n\nFor both types of graphs, we only take at most the top three neighbors. The number of neighbors can be set as a hyper-parameter of the model framework. We choose three for following reasons: \n\\begin{enumerate}\n    \\item More than one neighbor to provides additional complementary information while also adds robustness.\n    \\item Each additional neighbor means an extra run of the text encoder. Even though the encoder blocks can be run in parallel a large number of neighbors can still be computationally challenging for the system.\n    \\item We do not want to include more neighbors that are less relevant and introduce additional noisy information to \"pollute\" the encoded representation.\n\\end{enumerate}\nTherefore, choosing three neighbors balances all the requirements and concerns.\n\n"
                    }
                },
                "subsection 4.3": {
                    "name": "Knowledge Distillation",
                    "content": "\nIn order to have a high performance but compact model that satisfies the computation and latency constraints, the teacher-student training framework via knowledge distillation is used. We use an expensive but high-performance RoBERTa model \\cite{liu2019roberta} as the teacher model to label a very large query-keyword pair dataset, the label scores are between 0 and 1. Our model is relatively data-hungry and without this teacher model to automatically label the huge dataset, our existing human-labelled data is not sufficient to train a strong model that gets close to teacher model level performance. Since the model target, the RoBERTa score, is a continuous value, it provides more fine-grained information than the traditional binary labels. For example, a score of 0.99 indicates a stronger relevance than a score of 0.51, although both will be categorized as relevant pairs. We use mean squared error to measure the difference between the model output and the RoBERTa teacher scores.\n\nWith such a strong teacher model, we train the student TwinBERT/TextGNN model with small encoder blocks (only 3 transformer layers). Hence the student models are much more feasible in inference time but are able to achieve close to teacher model performance with only very minor performance loss. We could even further finetune the student model on a smaller human-labelled dataset with binary labels and achieve a performance surpassing the much larger teacher model. Hence, the performance of our model is not capped/limited by the teacher model.\n\n"
                },
                "category": [
                    "method"
                ]
            },
            "section 5": {
                "name": "Experiments",
                "content": "\nIn this section we present experiment results of TextGNN on various tasks. We also show the comparison with the strong baseline models to show the superiority of the proposed new model and the efficacy of introducing graph information. In Section 5.1 we discuss some key statistics of the complementary graph data, and some related details of our training methods. Section 5.2 compares the performance with the baseline encoder-only models. Section 5.3 shows a more detailed sub-group analysis. Section 5.4 presents case studies of typical examples with false positive and false negative examples for TwinBERT which are correctly classified by the new TextGNN model and provide intuitive insights why the additional graph information can be valuable. Lastly in Section 5.5 we present an initial effort to apply our model to online production system and show the significant improvement over the baseline in online A/B testings.\n\n",
                "subsection 5.1": {
                    "name": "Data and Training Details",
                    "content": "\nFor our knowledge distillation training, 397 million query-keyword pairs are scored by the teacher RoBERTa model. The student models are initialized using the parameters of the first three transformer layers of the 12-layer uncased BERT-base checkpoint \\cite{Wolf2019HuggingFacesTS}. The models are evaluated on a small evaluation dataset consisting of 243 thousand human labelled samples. The query and keyword pairs were given labels with five different levels of relevance: excellent, perfect, good, fair, and bad. In the evaluation stage the first four levels excellent, perfect, good, and fair are mapped as positive samples (label 1) where the bad category is kept as negative category (label 0). The model ROC-AUC is our main metric for evaluation.\n\nWe construct the behavior click graph based on the historical search engine click logs from July 2019 to June 2020. Here in Table \\ref{tab:coverage} we present some statistics on the neighbor coverage comparing the two ways of graph constructions. Here are some key observations:\n\\begin{enumerate}\n    \\item Without the added ANN neighbors, almost 2/3 of the queries miss neighbors from the user click graph. The situation is significantly better for keywords as the majority of the Ads have been shown and clicked by users.\n    \\item With the ANN search, we essentially increase the neighbor coverage to almost 100\\%.\n    \\item Among all nodes, the majority of them have at least three eligible neighbors. For the examples with less than 3 neighbors, dummy padding are added.\n\\end{enumerate}\n\n% \\begin{figure}[h]\n% \\includegraphics[width=0.5\\textwidth]{coverage}\n%   \\caption{Neighbor Graph Coverage Summary}\n%   \\label{fig:coverage}\n%   \\end{figure}\n  \n  \n\n"
                },
                "subsection 5.2": {
                    "name": "Model Performance Results",
                    "content": "\nIn the experiment we train the baseline TwinBERT model and the new TextGNN model with the same common hyper-parameters for a fair comparison. The same training dataset files were used by both models, but the additional neighbor information is not read by the baseline TwinBERT model as it does not have the mechanism to process the additional information.\n\nTabel \\ref{tab:auc} presents the ROC-AUC values of the baseline model and TextGNN based on two different types of graphs. We see that the addition of GNN has significantly improves the performance of the baseline model and the performance increase of this magnitude will lead to a huge difference in revenue for large scale systems.\n\n  \n\n"
                },
                "subsection 5.3": {
                    "name": "Sub-group Analysis",
                    "content": "\nIn addition to showing the stronger overall performance of the TextGNN models over the baseline, we also conduct a more detailed sub-group analysis on inference results to confirm that the TextGNN models indeed improve on the tail examples just as expected.\n\nWe split the validation data into three bins by the Ads frequency in the dataset (as a proxy for their population frequency of impressions). 43\\% of the samples are Ads that have been shown only once (among 243k samples) which are the rare examples, and 12\\% of the samples have been shown twice. Even though the tail Ads individually are rarely recalled and shown to users, they consist of the majority portion of the total traffic and the improvements on these long-tail examples can lead to significant benefits.\n\nWe see the results in Figure \\ref{fig:perf_by_bins} that the TextGNN model based on vanilla click graph shows an extremely large improvement in the most rare Ads, but the performance downgrades in common ones. Our hypothesis is that in the more common examples the semantic information is already good, and the limited additional information from a sparse graph is not enough to offset the potential under-fitting from a more complex model. Once we adopt ANN to generate a more complete graph, we see the TextGNN model demonstrates stronger performance than baseline across the board.\n\nLastly, we note that the non-ANN version is still much stronger than the ANN version in the bin of the most rare Ads, potentially because the ANN proxy neighbors are on average having lower quality than the native neighbors, and hence introduce noise to the model. This analysis also reveals a future direction to further improve the model where we can potentially use the sample frequency as a simple indicator to switch between various candidate models based on their strength within different sub-groups.\n\n\n\n  \n\n"
                },
                "subsection 5.4": {
                    "name": "Case Studies",
                    "content": "\n\n\nWe expect the introduction of graph data to improve the model performance especially on tail inputs that are often seen as \"hard\" samples for the baseline models. In table \\ref{tab:case_study}, we present some \"hard\" cases to demonstrate the value that graph data could bring.\n\n",
                    "subsubsection 5.4.1": {
                        "name": "False-positive Examples of TwinBERT",
                        "content": "\nThe first example shows that the user searched for the Greek methology \"achilles heel\", which was incorrectly determined by TwinBERT as relevant to plantar fasciitis shoes. From the semantic meaning, heel is very close to shoes and the achilles ankle is highly related to the pain of tendon. However, the neighbors strongly indicate that people who search for this query are actually looking for the story from Greek mythology and not the foot injury.\n\n\nThe second example shows that TwinBERT determines that \"animal repellent products\" is highly relevant to animal cleaning product. From the semantic meaning it is true that repellent is close in meaning to the word \"remove\" but the two products are used for completely different purposes. When averaging over the neighbors it is very clear that this is a negative example.\n\n\n"
                    },
                    "subsubsection 5.4.2": {
                        "name": "False-negative Examples of TwinBERT",
                        "content": "\nThe query \"sharding\" is a very specific concept in database systems on how large data are split and stored. Without the domain knowledge it is very hard to understand such an uncommon word. Furthermore, the word is tokenized to: [CLS], sha, \\#\\#rdi, \\#\\#ng, [SEP] by the BERT WordPiece tokenizer, making it essentially an impossible task for TwinBERT to identify the relevance. However, from the historical user behaviors we clearly see both sides taking the very important common words \"database\", hence allowing the TextGNN model to leverage on the user behavior to identify domain specific connections and find the hidden relevance.\n\n\nThe second false-negative one is an example of two video editing softwares on the Mac platform. Without the domain knowledge is it impossible to conclude from the semantic meaning that adobe premier mac is a video editing software. However, since the query string is identified as a neighbor of the keyword, our graph model can use this information to find the correct connection.\n\n\n"
                    }
                },
                "subsection 5.5": {
                    "name": "Online A/B Test",
                    "content": "\nA slightly simplified version of our TextGNN model has already been successfully deployed in a major sponsored search platform and demonstrated significant performance gains. We have evaluated the performance of the models on the sponsored product advertising system where user search queries are matched with products with rich information provided by advertisers. In this initial effort we choose C-DSSM as the text encoder for its much faster inference time in the application of large-scale Ads corpus and use graph aggregators only on the product side of the tower. Note again that the product side representations can be generated offline in advance and hence at online service stage the latency is identical to a traditional C-DSSM model. We use the TextGNN model outputs as features to be feed into a downstream online product advertising system and evaluated the efficacy of this simple model in both offline and online settings.\n\nFor evaluation, we randomly sampled examples from online logs and labeled the data manually by human experts and observe on average 1.3\\% (we only show normalized relative numbers due to business confidentiality) PR-AUC lift across different validation sets when comparing the simplified TextGNN model with the baseline C-DSSM model.\n\nThe online A/B testing results of the TextGNN model are summarized in Table \\ref{tab:ab} as we applied the model to both recall and relevance stage of the Ads serving in the system, where we observe significant gains in several normalized key online metrics numbers that are crucial for our sponsored search system. The two most important metrics are:\n\\begin{enumerate}\n    \\item \\textbf{Revenue Per Mille (RPM):} the revenue gained for every thousand search requests, which is one of the most important online metrics for sponsored search.\n    \\item \\textbf{Ad Defect Rate:} the ratio of irrelevant Ad impressions with respect to total number of Ad impressions. In online A/B test, this ratio is approximated by sampling Ad impressions and submitting them for human-evaluated labels. This is highly correlated to user satisfaction and hence is considered as a very crucial metric.\n\\end{enumerate}\n\n\nAs shown in the table, the TextGNN model yields very impressive results as it can greatly boost the RPM and reduce the Ad Defect Rate, which is a strong sign that model could help to improve revenue and user experience simultaneously. It's worthy pointing out that current production model already contains many advanced sub-models and features so the magnitude of the improvement in the online KPI here is considered as a significant gain for our system at the large scale.\n\n\n\n"
                },
                "category": [
                    "experiments"
                ]
            },
            "section 6": {
                "name": "Conclusion",
                "content": "\nWe present a powerful NLP model TextGNN that combines two strong model structures, text encoders and GNN, into a single end-to-end framework and shows strong performance in the task of Ad relevance. The model retains the strong natural language understanding ability from the existing powerful text encoders, while complements text encoders with additional information from graph-type data to achieve stronger performance than what could be achieved from only pure semantic information. We demonstrate with experiments that the TextGNN model show overall much stronger performance than a great baseline model based only on text encoders, and that the new model demonstrates the big gains in the most difficult task of low-frequency Ads. In our next step, the ensemble model idea could be explored to automatically mix different representation model outputs based on Ads frequency to achieve even better performance. \n\n\n\n%%\n%% The next two lines define the bibliography style to be used, and\n%% the bibliography file.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\n\n",
                "category": [
                    "conclusion"
                ]
            }
        },
        "tables": {
            "tab:click_graph": "\\begin{table}\n  \\caption{Example of neighbors of a query from the Click Graph}\n  \\label{tab:click_graph}\n  \\begin{tabular}{c|c|c|c}\n    \\toprule\n    &Clicked Neigh&Neigh&Neigh\\\\\n    Query&Keyword&\\# Impress&\\# Click\\\\\n    \\midrule\n     & united state & &\\\\\n    usps com & postal service jobs & 59 & 18\\\\\n    \\cline{2-4}\n    careers login & usps com employment & 344 & 92\\\\\n    \\cline{2-4}\n    & postal service hiring & 1721 & 384\\\\\n  \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:click_graph_ann": "\\begin{table}\n  \\caption{Example of a query from with Semantic ANN: proxy neighbor are quite relevant to the original query}\n  \\label{tab:click_graph_ann}\n  \\begin{tabular}{c|c|c|c|c}\n    \\toprule\n    &ANN&Clicked Neigh&Neigh&Neigh\\\\\n    Query&Query&Keyword&\\# Impress&\\# Click\\\\\n    \\midrule\n    video& & free games & 58&1\\\\\n    \\cline{3-5}\n    games & no & online games & 260 & 4\\\\\n    \\cline{3-5}\n    computers & internet &online &  & \\\\\n    free  & games & computer games & 67 & 1\\\\\n  \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:coverage": "\\begin{table}\n  \\caption{Coverage Summary of Two Graph Construction Methods: almost full coverage after adopting ANN Neighbors}\n  \\label{tab:coverage}\n  \\begin{tabular}{l|cc|cc}\n    \\toprule\n    & \\multicolumn{2}{c|}{Click Only}&\\multicolumn{2}{c}{ANN}\\\\\n    \\cline{2-3}\\cline{4-5}\n    &Q&K&Q&K\\\\\n    \\midrule\n    1 Neighbor & 4\\% & 7\\% & 5\\% & 7\\%\\\\\n    2 Neighbors & 3\\% & 4\\% & 3\\% & 4\\%\\\\\n    3 Neighbors & 30\\% & 76\\% & 92\\% & 88\\%\\\\\n    \\midrule\n    \\textbf{Coverage} & \\textbf{37\\%} & \\textbf{87\\%}  & \\textbf{100\\%}  & \\textbf{99\\%} \\\\\n  \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:auc": "\\begin{table}[h]\n  \\caption{ROC-AUC Comparison: TextGNN with ANN Neighbor Graph significantly outperform baseline TwinBERT}\n  \\label{tab:auc}\n  \\begin{tabular}{c|c}\n    \\toprule\n    Model & AUC\\\\\n    \\midrule\n    TwinBERT & 0.8459 \\\\\n    TextGNN & 0.8461  \\\\\n    TextGNN with ANN Neighbor & \\textbf{0.8471}\\\\\n  \\bottomrule\n\\end{tabular}\n\\end{table}",
            "tab:case_study": "\\begin{table*}[!htb]\n\\caption{Case study Examples: neighbors provide crucial complementary information}\n  \\label{tab:case_study}\n\\begin{tabular}{llll}\n\\hline\n\\hline\n\\multicolumn{4}{l}{\\textbf{False Positive Examples}}                                                                                                                                    \\\\ \\hline\nQuery                                        & \\multicolumn{1}{l|}{Query Neighbors}                  & Keyword                                   & Keyword Neighbors                    \\\\ \\hline\n\\multirow{3}{*}{achilles heel}               & \\multicolumn{1}{l|}{what is an achilles heel}         & \\multirow{3}{*}{plantar fasciitis shoes}  & shoes plantar fasciitis heel pain    \\\\\n                                             & \\multicolumn{1}{l|}{what is achilles heel}            &                                           & work shoes plantar fasciitis         \\\\\n                                             & \\multicolumn{1}{l|}{causes heel spurs}                &                                           & tennis shoes good plantar fasciitis  \\\\ \\hline\\hline\n\\multirow{3}{*}{animal repellent   products} & \\multicolumn{1}{l|}{animal repeller}                  & \\multirow{3}{*}{animal odor}              & best cleaning remove \\& product home \\\\\n                                             & \\multicolumn{1}{l|}{keep squirrel out attic}          &                                           & air fresheners home                  \\\\\n                                             & \\multicolumn{1}{l|}{animal repellent}                 &                                           & best air fresheners                  \\\\ \\hline\\hline\n% \\multirow{3}{*}{grillworks}                  & \\multicolumn{1}{l|}{floor metal grille}               & \\multirow{3}{*}{floor metal grille}       & decorative vents                     \\\\\n%                                              & \\multicolumn{1}{l|}{grillworks grills}                &                                           & fancy vents                          \\\\\n%                                              & \\multicolumn{1}{l|}{gas grills ducane parts}          &                                           & louvered vents                       \\\\ \\hline\\hline\n\\multicolumn{4}{l}{\\textbf{False Negative Examples}}                                                                                                                                    \\\\ \\hline\nQuery                                        & \\multicolumn{1}{l|}{Query Neighbors}                  & Keyword                                   & Keyword Neighbors                    \\\\ \\hline\n\\multirow{3}{*}{sharding}                    & \\multicolumn{1}{l|}{mongodb cluster}                  & \\multirow{3}{*}{sql server}               & sql server download windows 10       \\\\\n                                             & \\multicolumn{1}{l|}{database sharding}                &                                           & sql server hosting                   \\\\\n                                             & \\multicolumn{1}{l|}{N/A}                              &                                           & sequel server database               \\\\ \\hline\\hline\n\\multirow{3}{*}{use imovie}                  & \\multicolumn{1}{l|}{imovies}                          & \\multirow{3}{*}{adobe premiere}           & adobe premiere pro mac               \\\\\n                                             & \\multicolumn{1}{l|}{imovie 11 tutorials}              &                                           & adobe premier mac                    \\\\\n                                             & \\multicolumn{1}{l|}{imovie video editor}              &                                           & use imovie                           \\\\ \\hline\n% \\multirow{3}{*}{best productivity   apps}    & \\multicolumn{1}{l|}{free task management application} & \\multirow{3}{*}{task management software} & planer online                        \\\\\n%                                              & \\multicolumn{1}{l|}{task management software}         &                                           & online planner                       \\\\\n%                                              & \\multicolumn{1}{l|}{apps scheduling employees}        &                                           & online agenda                        \\\\ \\hline\n\\end{tabular}\n\\end{table*}",
            "tab:ab": "\\begin{table}[h]\n  \\caption{Online A/B Testing: significant improvements in production product advertising systems}\n  \\label{tab:ab}\n  \\begin{tabular}{c|c|c}\n    \\toprule\n    Tasks & Relative RPM & Relative Ad Defect Rate\\\\\n    \\midrule\n    TextGNN Relevance & +2.03\\% & -2.32\\% \\\\\n    TextGNN Selection & +1.21\\% & -0.34\\% \\\\\n  \\bottomrule\n\\end{tabular}\n\\end{table}"
        },
        "figures": {
            "fig:twinbert": "\\begin{figure}[tb]\n\\includegraphics[width=0.46\\textwidth]{twinbert}\n  \\caption{Architecture of the twin tower TwinBERT model}\n  \\label{fig:twinbert}\n  \\end{figure}",
            "fig:architecture": "\\begin{figure*}\n  \\includegraphics[width=\\textwidth]{architecture}\n  \\caption{TextGNN Architecture: twin tower structure for decoupled generation of query/keyword embeddings}\n%   \\Description{}\n  \\label{fig:architecture}\n\\end{figure*}",
            "fig:neigh": "\\begin{figure}[htb]\n\\includegraphics[width=0.5\\textwidth]{neigh}\n  \\caption{Click Graph Construction: use ANN proxy neighbor if no native neighbor available}\n  \\label{fig:neigh}\n  \\end{figure}",
            "fig:perf_by_bins": "\\begin{figure}[tb]\n\\includegraphics[width=0.5\\textwidth]{perf_by_bins.pdf}\n  \\caption{Performance on Different Subgroups of Data by Ads Frequency: TextGNN with vanilla click neighbor achieves extremely large gain in low frequency Ads, while the ANN version outperforms the baseline across the board}\n  \\label{fig:perf_by_bins}\n  \\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{equation}\n\\textbf{y} = \\mathcal{F}(\\textbf{x}, W, b) + \\textbf{x},\n\\end{equation}"
        },
        "git_link": "https://github.com/microsoft/TextGNN"
    },
    "llm_extraction": {
        "dataset": [
            {
                "name": "Query-Keyword Pairs",
                "description": "A dataset consisting of 397 million pairs of queries and keywords scored by the teacher RoBERTa model for knowledge distillation training. This dataset serves as the foundational input for training the TextGNN model.",
                "proposed_in_paper": true,
                "url": null,
                "size": "397 million pairs",
                "usage": [
                    "training"
                ],
                "splitting": null,
                "sampling": "Knowledge distillation from RoBERTa model",
                "preprocessing": null
            },
            {
                "name": "Evaluation Dataset",
                "description": "A small evaluation dataset consisting of 243 thousand human-labeled samples used to evaluate the model's effectiveness. The queries and keyword pairs were labeled with five different levels of relevance: excellent, perfect, good, fair, and bad.",
                "proposed_in_paper": false,
                "url": null,
                "size": "243 thousand samples",
                "usage": [
                    "validation"
                ],
                "splitting": "First four levels are positive samples (label 1), while the bad category is negative (label 0).",
                "sampling": null,
                "preprocessing": null
            },
            {
                "name": "Behavior Click Graph",
                "description": "Constructed from historical search engine click logs from July 2019 to June 2020, providing crucial neighborhood information to enhance model performance by capturing user behavior patterns.",
                "proposed_in_paper": true,
                "url": null,
                "size": "Log data covering the period from July 2019 to June 2020",
                "usage": [
                    "training"
                ],
                "splitting": null,
                "sampling": "Graph constructed from user click logs, includes neighborhood coverage considerations.",
                "preprocessing": "Neighbor coverage enhanced with ANN methods to increase relevance."
            }
        ]
    }
}