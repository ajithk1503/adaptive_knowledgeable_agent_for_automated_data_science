{
    "meta_info": {
        "title": "Impatient Bandits: Optimizing Recommendations for the Long-Term Without  Delay",
        "abstract": "Recommender systems are a ubiquitous feature of online platforms.\nIncreasingly, they are explicitly tasked with increasing users' long-term\nsatisfaction. In this context, we study a content exploration task, which we\nformalize as a multi-armed bandit problem with delayed rewards. We observe that\nthere is an apparent trade-off in choosing the learning signal: Waiting for the\nfull reward to become available might take several weeks, hurting the rate at\nwhich learning happens, whereas measuring short-term proxy rewards reflects the\nactual long-term goal only imperfectly. We address this challenge in two steps.\nFirst, we develop a predictive model of delayed rewards that incorporates all\ninformation obtained to date. Full observations as well as partial (short or\nmedium-term) outcomes are combined through a Bayesian filter to obtain a\nprobabilistic belief. Second, we devise a bandit algorithm that takes advantage\nof this new predictive model. The algorithm quickly learns to identify content\naligned with long-term success by carefully balancing exploration and\nexploitation. We apply our approach to a podcast recommendation problem, where\nwe seek to identify shows that users engage with repeatedly over two months. We\nempirically validate that our approach results in substantially better\nperformance compared to approaches that either optimize for short-term proxies,\nor wait for the long-term outcome to be fully realized.",
        "author": "Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek",
        "link": "http://arxiv.org/abs/2307.09943v2",
        "category": [
            "cs.LG",
            "stat.ML"
        ],
        "additionl_info": "Presented at the 29th ACM SIGKDD Conference on Knowledge Discovery  and Data Mining (KDD '23)"
    },
    "latex_extraction": {
        "content": {
            "section 1": {
                "name": "Introduction",
                "content": "\n\\label{sec:intro}\n\n% Rec. sys. and the role they play.\nMany online platforms rely on recommender systems to assist users in finding relevant items among vast collections of content~\\citep{ricci2015recommender}.\nApplications are wide-ranging: recommender systems help individuals find books, movies or audio content~\\citep{bennett2007netflix, mcinerney2018explore};\nthey help doctors find medical treatments for their patients~\\citep{tran2021recommender}, and students find learning resources~\\citep{verbert2012context}, among many others.\n% Move towards long-term outcomes.\nA key question underpins the design of any recommender system: What is a successful recommendation?\nAcross many applications, there is an ongoing shift towards defining success at longer time-horizons~\\citep{zheng2018drn, zou2019reinforcement}, as long-term metrics are often better suited to capture users' satisfaction and platforms' goals~\\citep{hohnhold2015focusing}.\nFor example, e-commerce platforms may want to maximize long-term revenue, subscription-based services may want to increase retention, and social platforms may want to encourage habitual engagement measured over several weeks or months.\nIn the context of podcast recommendations on an online audio streaming platform, recent work has shown that explicitly optimizing for long-term engagement (measured over a 60-day window post-recommendation) can significantly improve the user experience \\citep{maystre2023optimizing}.\nMost of the literature, however, implicitly assumes that there is sufficient data to estimate the long-term impact of recommendations.\n\n",
                "subsection 1.1": {
                    "name": "Content Exploration Problem",
                    "content": "\n\n% Discussion of the podcast setting which motivated the work\nIn this paper, we focus on a specific aspect of recommender systems and seek to address a content exploration problem.\n% encountered during the design of a real-world podcast recommender system. \nOn most online platforms, new content is released regularly.\nIn order to learn about that content's appeal, we must first recommend it to users.\nThis is known as the \\emph{cold-start problem}.\nAfter ensuring an adequate amount of information has been gathered, an effective system should rapidly shift recommendations away from poor content.\n\n\n\nWe formalize this task as a \\emph{multi-armed bandit} problem, where we seek to identify promising content through successive interactions with users \\citep{li2010contextual}.\nOptimizing for long-term definitions of success in the bandit setting is challenging, as long-term metrics are---by construction---delayed~\\citep{hong2019tutorial}.\nThis gives rise to an apparent tradeoff, illustrated in Figure~\\ref{fig:tradeoff}, between using short-term proxies that are observable quickly (top-left) and ensuring that actions selected are aligned with long-term success (bottom-right).\nWe propose a means of circumventing this tradeoff by exploiting the insight that \\emph{most long-term outcomes become increasingly predictable over time}.\n\nDriven by practical applications, we assume that intermediate outcomes are progressively revealed over time, from the moment the action is selected up to the moment the full reward is observed.\nWe call this the \\emph{progressive feedback} setting.\nWe develop a probabilistic model that forms beliefs about the delayed rewards an arm generates on the basis of outcomes observed so far.\nAs time passes, uncertainty diminishes and the model is able to make increasingly precise predictions.\nTo facilitate this, we contend that historical data from distinct but similar applications (e.g., previous content releases) can be used to learn the association between intermediate and long-term outcomes.\nIn effect, we propose a meta-learning approach that learns to infer long-term outcomes of interest from intermediate observations, revealed progressively over time.\nWe then take advantage of this reward model to address our sequential decision-making problem, by combining the predictive model with a bandit algorithm.\nThe bandit uses probabilistic predictions from the model to efficiently balance exploration and exploitation.\nEven if the first few intermediate outcomes are insufficient to perfectly infer the average delayed reward an arm generates, they might be sufficient to reveal that this arm is outperformed by others.\nIn such cases, our bandit algorithm will shift effort away from the arm.\nNote that, in contrast to well-studied bandit settings where feedback is observed at once, either immediately or after a given delay, the progressive feedback setting presents distinctive challenges:\nInformation can be obtained actively, by selecting an action, or passively by letting time unfold and incrementally receiving new data about the outcomes of actions taken in the past.\n\nOur methodology is very general, and can be applied to a wide range of problems.\nIn this work, we consider a recently-studied podcast recommendation application~\\citep{maystre2023optimizing}.\nIn this application, actions correspond to podcast shows, and the reward is defined as the number of days a user engages with a show in the 59 days that follow a successful recommendation.\nIntermediate outcomes consist of binary activity indicators for each of the 59 days, observed with the corresponding delay.\nWe evaluate our approach using data from the Spotify audio streaming platform, and show that \\begin{enuminline}\n\\item the full reward can be accurately predicted after only a few days of observation, and that\n\\item the content-exploration problem can be solved much quicker than approaches that rely on short-term proxies or wait for the full reward to become available.\n\\end{enuminline}\n\n\\paragraph{Summary of Contributions}\nIn this work, we make the following contributions.\n\\begin{itemize}\n    \\item A Bayesian filtering approach to reward estimation, which enables us to incorporate all available information in order to predict delayed outcomes and quantify uncertainty (Section~\\ref{sec:model}).\n    \\item A meta-learning approach, where the prior and noise covariance structures that power Bayesian filtering are themselves learned from data. The method learns across items how to make rapid inferences about a new item (Section~\\ref{sec:training}).\n    \\item The \\emph{impatient bandit algorithm}, a novel algorithm for the progressive feedback setting, which uses intermediate information received at each round to iteratively update the Bayesian filter, and enables us to efficiently balance exploration and exploitation whilst providing recommendations that optimize for long-term engagement (Section~\\ref{sec:bandit}).\n    \\item An application of our impatient bandit algorithm to a real-world podcast recommendation problem, presented alongside empirical results, that show that our proposed method considerably outperforms approaches based on fully-delayed feedback or short-term proxy metrics (Section~\\ref{sec:podcasts}).\n\\end{itemize}\n\n% We emphasize that, even though our empirical results cover a specific application, we believe the ideas developed in this paper could be applied to a wide range of domains and applications, possibly with different definitions of success and different timeframes.\n"
                }
            },
            "section 2": {
                "name": "Related Work",
                "content": "\n\\label{sec:relwork}\n\nWe start by briefly discussing relevant related work on multi-armed bandits and applications to recommender systems.\n\n\\paragraph{Multi-Armed Bandits}\nOften used to model online platforms~\\citep{mattos2019multi}, multi-armed bandits (MAB) formalize a simple sequential decision-making problem, where at each round $t$ an agent selects one of several possible actions and receives a corresponding reward $r_t$.\nThe goal is usually to maximize the sum of rewards received over a given time horizon.\nThe simplest and most widely studied bandit setting is the \\emph{strictly sequential feedback} scenario, where $r_t$ is immediately observed~\\citep{srinivas2009gaussian, agrawal2012analysis}.\nHowever many extensions have been proposed. \n\nOne such extension is the case of \\emph{parallelized actions}.\nRather than simply selecting a single action at each round and receiving a single corresponding reward, we can also consider a scenario in which multiple actions can be taken at each round in parallel~\\citep{desautels2014parallelizing, kandasamy2018parallelised}.\nThis extension is also referred to as the \\emph{batched} bandit setting.\nThe challenge arises from having to concurrently select several actions without knowing the reward associated with the other actions in the batch.\n\nAn additional variant of the MAB relevant to our work is the \\emph{delayed feedback} setting, which can be viewed as a generalization of the batch feedback setting \\citep{kandasamy2018parallelised}.\nIn this setting, the reward $r_t$ is only revealed after a delay of $\\Delta$ rounds, i.e., at round $t + \\Delta$.\nIn this case, the agent is forced to make a series of decisions without knowledge of the results of all previous actions taken.\nPrior work in this area has utilized Thompson sampling~\\citep{chapelle2011empirical, kandasamy2018parallelised} and upper-confidence bound (UCB)~\\citep{joulani2013online, desautels2014parallelizing} algorithms to address this setting.\n% TODO cite chapelle2014modeling and vernade2017stochastic? They describe models where conversion are observed with delay, and a separate model is trained to predict conversions before they realize.\n\n\\paragraph{Thompson Sampling}\nThompson sampling is a class of algorithms used for sequential decision-making in bandit settings, that efficiently balances \\emph{exploration} of the action space with \\emph{exploitation} of actions that are believed to be associated with comparatively large rewards~\\citep{russo2018tutorial}.\nWhilst the technique was introduced almost a century ago~\\citep{thompson1933likelihood}, it has become increasingly popular over the course of the last decade due to its strong empirical performance when applied to modern, large-scale online learning problems~\\citep{scott2010modern, graepel2010web}.\n\nIn the face of uncertainty, Thompson sampling randomizes among all actions that are plausibly optimal.\nThis allows for greater robustness to delayed feedback compared to algorithms based on upper-confidence bounds, which are deterministic and rely on rapidly changing beliefs to adjust which arm is sampled.\nThere is both theoretical and empirical evidence that, due to its randomized nature, Thompson sampling is resilient to delayed feedback~\\citep{chapelle2011empirical, kandasamy2018parallelised, qin2022adaptivity, wu2022thompson}.\nThis characteristic of the algorithm is crucial in our problem.\n\nIn our work, we use Thompson sampling in combination with a belief model that predicts a long-term metric using progressively revealed intermediate observations.\nWe meta-learn this model on historical data.\nThis aspect of our work connects to a series of recent papers that develop provable bounds on the loss in performance due to fitting the prior used in Bayesian bandit algorithms from past data~\\citep{bastani2022meta, simchowitz2021bayesian, basu2021no}.\n\n\n\\paragraph{Intermediate Feedback}\nSeveral recent papers have employed Thompson sampling in settings with delayed outcomes, but useful intermediate observations~\\citep{yang2020targeting, caria2020adaptive, wu2022partial}.\nUCB algorithms have also been applied to this setting by~\\citet{grover2018best}, who consider a scenario where noisy observations of the true feedback are received at intermediate rounds between $t$ and $t+\\Delta$.\nKey differences between their work and ours include the fact that \\citeauthor{grover2018best} consider the problem of top-$k$ best-arm identification with a stochastic delay, in contrast to our objective of cumulative regret minimization with a fixed delay.\nAnother differentiating factor of greater consequence is the fact that they assume that the intermediate feedback consists of independent random variables, whereas our progressive feedback is crucially not i.i.d., which is how our model is able to effectively generalize.\nAdditionally, our work employs Bayesian filtering to seamlessly perform inference, an approach explicitly motivated by a real use-case, where historical data allows us to fit an informed prior.\n\nOutside of the literature on MABs, \\citet{prentice1989surrogate} and \\citet{athey2019surrogate} formalize conditions under which intermediate feedback can be used to estimate long-term outcomes.\nThey also find empirically that intermediate feedback can lead to both increased accuracy and precision in estimates of long-term outcomes.\n\n\\paragraph{Recommender Systems \\& Long-Term Goals}\n%We present an application of our framework that aims to foster long-term user engagement on an audio streaming platform by making recommendations that explicitly optimize for cumulative activity over an extended period of time.\nBandits are a popular approach for addressing many types of recommendation problem.\nIn a seminal paper, \\citet{li2010contextual} use a contextual variant of the bandit problem to personalize recommendations on a news platform, and more recently, \\citet{aziz2022identifying} use MABs to recommend podcasts by maximizing the impression-to-stream rate.\n\nOptimizing recommendations for long-term user engagement is a problem that is of great practical interest in industry, and bandits have also been used previously to address this specific scenario.\n\\citet{wu2017returning} address this problem using a UCB algorithm that models the temporal return behaviour of users to maximize the cumulative number of clicks from a group of users over a period of time.\n%The authors model the probability of individual users returning to the system with exponential distributions whose parameters are obtained via maximum likelihood estimation.\n%This differs from our approach of modeling the overall propensity of users to return to a given piece of content, using an iteratively updated Bayesian filter.\n\nBeyond bandits, more general reinforcement learning (RL) approaches have also been applied to the problem of maximizing long-term user engagement~\\citep{zheng2018drn, zou2019reinforcement}.\n``Full'' RL enables principled reasoning about inter-temporal tradeoffs and delayed rewards, at the expense of increased complexity.\nImplementing effective RL algorithms that address realistic recommender system problems is non-trivial due to the challenging nature of off-policy learning and evaluation~\\citep{zhao2019deep}.\n"
            },
            "section 3": {
                "name": "Methodology",
                "content": "\n\\label{sec:method}\n\nWe present our approach to solving the content exploration problem outlined in the introduction.\nWe adopt the terminology of multi-armed bandits.\nWe consider a set of $N$ actions, $\\mathcal{A} = \\{a_1, \\ldots, a_N\\}$, corresponding, e.g., to different recommendation candidates.\nAt each round $t = 1, 2, \\ldots$, we select one or more actions.\nFor every action we select, we observe a reward $r_a$ after a delay of $\\Delta$ rounds, i.e., at round $t + \\Delta$.\nInformally, we seek to develop a methodology that helps us quickly identify and exploit actions with high mean reward $\\bar{r}_a = \\mathbb{E}[r_a]$.\nWe assume that the reward $r_a$ is a function of intermediate observations $z_{a,1}, \\ldots, z_{a,K}$, that become available progressively during the interval $[t, t + \\Delta]$ after selecting the action.\nWe call this the \\emph{progressive feedback} setting.\n\nIn Section~\\ref{sec:model}, we consider a fixed action $a$ and develop a Bayesian reward model that takes advantage of intermediate observations to estimate the mean reward $\\bar{r}_a$.\nIn Section~\\ref{sec:training}, we take advantage of historical data to estimate the parameters of the reward model, effectively instantiating a \\emph{meta-learning} approach. \nBuilding on this model, in Section~\\ref{sec:bandit}, we develop a bandit algorithm that efficiently balances exploration and exploitation in the progressive feedback setting.\n\n\\paragraph{Concrete Example}\nWhile this section introduces the methodology in a generic way, it is helpful to keep a concrete application in mind.\nIn Section~\\ref{sec:podcasts} we consider a podcast recommendation problem, where the actions $\\mathcal{A}$ correspond to podcast shows.\nThe reward $r$ is the cumulative engagement with a podcast show over a period of $\\Delta$ days: $r_a = \\sum_{i=i}^\\Delta z_{a,i}$.\n\n",
                "subsection 3.1": {
                    "name": "Bayesian Reward Model",
                    "content": "\n\\label{sec:model}\n\nWe consider a fixed action $a$ and, for conciseness, we omit $a$ from all subscripts.\nLet $r$ be the sample reward and $\\bar{r} = \\mathbb{E}[r]$ be the mean reward associated to selecting the action.\nDefine the sample trace, $\\bm{z} = (z_1, \\ldots, z_K) \\in \\mathbf{R}^K$, as a vector containing intermediate outcomes.\nWe assume that $z_k$ is observed after $\\Delta_k \\le \\Delta$ rounds, and, \nwithout loss of generality, that $\\Delta_1 \\le \\cdots \\le \\Delta_K$.\nCorrespondingly, we define the average trace as $\\bar{\\bm{z}} = \\mathbb{E}[\\bm{z}]$.\nWe postulate the following generative model of sample traces $\\{\\bm{z}_m\\}$:\n\\begin{align}\n\\label{eq:genmodel}\n\\bar{\\bm{z}} &\\sim \\mathcal{N}(\\bm{\\mu}, \\bm{\\Sigma}),\n&\\bm{z}_m  &= \\bar{\\bm{z}} + \\bm{\\varepsilon}_m,\n&\\bm{\\varepsilon}_m &\\sim \\mathcal{N}(\\bm{0}, \\bm{V}) \\ \\text{i.i.d.}\n\\end{align}\nThat is, we assume a priori that the average trace $\\bar{\\bm{z}}$ corresponding to the action is sampled from a multivariate Gaussian distribution with mean $\\bm{\\mu}$ and covariance matrix $\\bm{\\Sigma}$, and that\na sample trace $\\bm{z}_m$ is a noisy copy of $\\bar{\\bm{z}}$, corrupted by additive zero-mean Gaussian noise with covariance matrix $\\bm{V}$, independently for each $m$.\nFurthermore, we assume that we can reconstruct the reward from all intermediate observations as\n\\begin{align*}\nr = \\bm{w}^\\top \\bm{z},\n\\end{align*}\nwhere $\\bm{w} \\in \\mathbf{R}^K$ is a vector of weights.\nBy the linearity of expectation, it follows that $\\bar{r} = \\bm{w}^\\top \\bar{\\bm{z}}$.\nWe treat $\\bm{w}$ as given, and $\\{\\bm{\\mu}, \\bm{\\Sigma}, \\bm{V} \\}$ as model parameters.\nWe discuss how to learn them from data in Section~\\ref{sec:training}.\n\nAssume that we are at round $t$ and that we have selected the action $M$ times so far, at rounds $t_1 \\le \\cdots \\le t_m \\le t$.\nWe represent the observations collected at round $t$ as a dataset of $M$ independent traces, $\\mathcal{D} = \\{(\\bm{z}_m, \\ell_m) : m = 1, \\ldots, M\\}$.\nSome traces might only be partially observed, and we use $\\ell_m \\doteq \\max \\{ k : \\Delta_k \\le t - t_m \\}$ to index the last element of $\\bm{z}_m$ that is observed at round $t$.\n\n",
                    "subsubsection 3.1.1": {
                        "name": "Iterative Belief Updates",
                        "content": "\n\nWe consider the problem of estimating $\\bar{r}$ given $\\mathcal{D}$.\nInstead of reasoning about $\\bar{r}$ directly, we begin by addressing the problem of estimating $\\bar{\\bm{z}}$.\nWe take a Bayesian approach and seek to compute the posterior distribution\n\\begin{align*}\np(\\bar{\\bm{z}} \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\bar{\\bm{z}}) \\mathcal{N}(\\bar{\\bm{z}} \\mid \\bm{\\mu}, \\bm{\\Sigma}).\n\\end{align*}\nGiven our generative model~\\eqref{eq:genmodel}, we will show that the posterior remains Gaussian, even in the presence of partially observed traces.\nFinally, writing $p(\\bar{\\bm{z}} \\mid \\mathcal{D}) \\doteq \\mathcal{N}(\\bar{\\bm{z}} \\mid \\bm{\\mu}', \\bm{\\Sigma}')$ and given that $\\bar{r}$ is a linear function of the mean trace $\\bar{\\bm{z}}$, we have that\n\\begin{align*}\n\\bar{r} \\mid \\mathcal{D} \\sim \\mathcal{N}(\\mu, \\sigma^2),\n\\end{align*}\nwhere $\\mu = \\bm{w}^\\top \\bm{\\mu}'$ and $\\sigma^2 = \\bm{w}^\\top \\bm{\\Sigma}' \\bm{w}$.\n\nWe describe the process by which we fold in a single trace into the belief. \nThe full posterior can be obtained by repeating this procedure iteratively, $M$ times.\nFor conciseness, we drop the subscript $m$ and denote the trace and cutoff index as $(z, \\ell)$, respectively.\nWe denote by $\\bm{A}_{:i, :j}$ the submatrix obtained by taking the $i$ first rows and the $j$ first columns of a matrix $\\bm{A}$.\nSimilarly, we denote by $\\bm{a}_{:i}$ the first $i$ elements of the vector $\\bm{a}$.\nThanks to the self-conjugacy property of the Gaussian distribution, we can write the posterior distribution of $\\bar{\\bm{z}}$ after observing the $\\ell$ first elements of the trace $\\bm{z}$ as a multivariate Gaussian with mean vector and covariance matrix\n\\begin{align*}\n\\bm{\\mu}' &=\n    \\bm{\\mu} + \\bm{\\Sigma}_{:K, :\\ell} \\left(\\bm{\\Sigma}_{:\\ell, :\\ell} + \\bm{V}_{:\\ell, :\\ell} \\right)^{-1} \\left(\\bm{z}_{:\\ell} - \\bm{\\mu}_{:\\ell} \\right), \\\\\n\\bm{\\Sigma}' &=\n    \\bm{\\Sigma} + \\bm{\\Sigma}'_{:K, :\\ell} \\left(\\bm{\\Sigma}_{:\\ell, :\\ell} + \\bm{V}_{:\\ell, :\\ell} \\right)^{-1} \\bm{\\Sigma}_{:\\ell, :K},\n\\end{align*}\nrespectively.\nWe refer the reader to~\\citet[Section A.2]{rasmussen2006gaussian} for more details on these update equations.\nThe complete iterative procedure is provided in Algorithm~\\ref{alg:inference}.\n\n\\begin{algorithm}[t]\n\\caption{Computing the posterior of $\\bar{z}$.}\n\\label{alg:inference}\n\\begin{algorithmic}[1]\n\\Require Parameters $\\bm{\\mu}, \\bm{\\Sigma}, \\bm{V}$, dataset $\\mathcal{D}$\n\\State $\\bm{\\mu}' \\gets \\bm{\\mu}$\n\\State $\\bm{\\Sigma}' \\gets \\bm{\\Sigma}$\n\\For{$(\\bm{z}, \\ell) \\in \\mathcal{D}$}\n\\State $\\bm{A} \\gets \\bm{\\Sigma}'_{:K, :\\ell} (\\bm{\\Sigma}'_{:\\ell, :\\ell} + \\bm{V}_{:\\ell, :\\ell} )^{-1}$\n\\State $\\bm{\\mu}' \\gets\n    \\bm{\\mu} + \\bm{A} \\left(\\bm{z}_{:\\ell} - \\bm{\\mu}'_{:\\ell} \\right)$\n\\State $\\bm{\\Sigma}' \\gets\n    \\bm{\\Sigma}' + \\bm{A} \\bm{\\Sigma}'_{:\\ell, :K}$\n\\EndFor\n%\\Return posterior parameters $\\bm{\\mu}', \\bm{\\Sigma}'$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\paragraph{A Note on Gaussian Noise}\nThe assumption in~\\eqref{eq:genmodel} that each trace $\\bm{z}$ is Gaussian with mean $\\bar{\\bm{z}}$ might seem restrictive at first sight.\nFor example, in Section~\\ref{sec:podcasts}, we consider binary observation vectors $\\bm{z} \\in \\{0, 1\\}^\\Delta$, for which a Gaussian is arguably a poor model.\nIn fact, given that our ultimate goal is to infer $\\bar{\\bm{z}}$ from several traces, the impact of this assumption is relatively benign.\nTo see this, assume that we are given $M$ full traces $\\bm{z}_1, \\ldots, \\bm{z}_M$ such that $z_m = \\bar{\\bm{z}} + \\bm{\\varepsilon}_m$, where $\\{\\bm{\\varepsilon}_m\\}$ are independently and identically distributed but not necessarily Gaussian.\nIt can be shown that the empirical average $\\hat{\\bm{z}} = M^{-1} \\sum_m \\bm{z}_m$ is a sufficient statistic for $\\bar{\\bm{z}}$ given $\\{ \\bm{z}_m \\}$.\nFor $M$ large, we can invoke the central limit theorem to argue that a Gaussian approximation for $\\hat{\\bm{z}}$ (and, correspondingly, a Gaussian approximation for the individual traces $\\bm{z}_1, \\ldots, \\bm{z}_M$) is accurate for the purpose of estimating $\\bar{\\bm{z}}$. \n\n\\paragraph{Optimizing the Implementation}\nFor simplicity, we have described Bayesian inference in our model as a sequential procedure.\nIn practice, there are several ways in which Algorithm~\\ref{alg:inference} can be made more computationally efficient.\nThese include \\begin{enuminline}\n\\item updating the posterior using multiple traces in a single batch, instead of processing each trace independently;\n\\item performing incremental updates by reusing beliefs from previous rounds; and\n\\item only updating beliefs for actions that have received new observations.\n\\end{enuminline}\n\n\n"
                    }
                },
                "subsection 3.2": {
                    "name": "Training the Reward Model",
                    "content": "\n\\label{sec:training}\n\nA crucial aspect of our method is the ability to take advantage of past data to learn the model parameters $\\{\\bm{\\mu}, \\bm{\\Sigma}, \\bm{V}\\}$.\nSpecifically, we assume access to historical data about a different set of actions $\\mathcal{A}'$.\nIn the context of a recommender system, for example, this could be existing content for which we already have a sufficient amount of interaction data.\nFor each $a \\in \\mathcal{A}'$, denote by $\\mathcal{H}_a = \\{ (\\bm{z}_{am}, r_{am}) : m = 1, \\ldots, M_a \\}$ the data corresponding to action $a$.\n\nFor each action $a \\in \\mathcal{A}'$, we  begin by computing the empirical mean trace vector and noise covariance matrix\n\\begin{align*}\n\\hat{\\bm{z}}_a &= M_a^{-1} \\sum_{\\bm{z} \\in \\mathcal{H}_a} \\bm{z},\n&\\hat{\\bm{V}}_a &= M_a^{-1} \\sum_{\\bm{z} \\in \\mathcal{H}_a} (\\bm{z} - \\hat{\\bm{z}}_a)(\\bm{z} - \\hat{\\bm{z}}_a)^\\top,\n\\end{align*}\nrespectively.\nWe then estimate the model parameters $\\bm{\\mu}, \\bm{\\Sigma}, \\bm{V}$ by using empirical averages, as\n\\begin{align*}\n\\bm{\\mu}\n    &= \\lvert \\mathcal{A}' \\rvert^{-1} \\sum_{a \\in \\mathcal{A}'} \\hat{\\bm{z}}_a, \\\\\n\\bm{\\Sigma}\n    &= \\lvert \\mathcal{A}' \\rvert^{-1} \\sum_{a \\in \\mathcal{A}'} (\\bm{\\mu} - \\hat{\\bm{z}}_a)(\\bm{\\mu} - \\hat{\\bm{z}}_a)^\\top, \\\\\n\\bm{V}\n    &= \\lvert \\mathcal{A}' \\rvert^{-1} \\sum_{a \\in \\mathcal{A}'} \\hat{\\bm{V}}_a.\n\\end{align*}\nIn principle, more advanced estimation methods might be used, such as type-II maximum likelihood, also known as \\emph{empirical Bayes} \\citep{rasmussen2006gaussian}.\nIn practice, however, we have found that the simple empirical averages described above are very effective.\n\nIntuitively, the covariance matrices $\\bm{\\Sigma}$ and $\\bm{V}$ play a critical role in our approach.\nThey encode the correlations between outcomes observed at different points in time.\nIf intermediate outcomes observed early on are highly predictive of later outcomes, we expect that we can accurately estimate $\\bar{\\bm{z}}$ (and thus $\\bar{r}$) without waiting for the full $\\Delta$ rounds required to observe $r$.\nWe will revisit this from an empirical perspective in Section~\\ref{sec:evalmodel}.\n\n\\paragraph{A Note on the Weights}\nOur approach assumes that the reward is a given linear function of the trace.\nFor example, in Section~\\ref{sec:podcasts}, we consider a problem where the reward is defined as $r = \\sum_k z_k$, corresponding to $\\bm{w} \\doteq \\bm{1}$.\nIn practice, one might try to fit long-term objectives to a linear model, by solving a regression problem\n\\begin{align*}\n\\textstyle\n\\Argmin_{w} \\sum_{a \\in \\mathcal{A}'} \\sum_{(\\bm{z}, y) \\in \\mathcal{H}_a'} (y - \\bm{w}^\\top \\bm{z})^2,\n\\end{align*}\nwhere $y$ is a target that is not exactly a linear function of $\\bm{z}$.\nIn this case, it is important to note that the reward $r = \\bm{w}^\\top \\bm{z}$ is an approximation of the true objective $y$.\nWe briefly elaborate on this in Appendix~\\ref{app:model}.\n\n"
                },
                "subsection 3.3": {
                    "name": "Bandit Algorithm",
                    "content": "\n\\label{sec:bandit}\n\nEquipped with a model capable of making inferences about the arms' mean rewards given intermediate observations, we can now develop a bandit algorithm that works effectively in the progressive-feedback setting, where information about the reward is revealed progressively over multiple rounds.\n\nAlthough several different objectives for the bandit problem exist in the literature, in this work we focus on the goal of minimizing the \\emph{cumulative expected regret}.\nIn the case of a single action being selected at each round, we define the cumulative expected regret at round $T$ as\n\\begin{align*}\n\\mathbb{E} \\left[ R_T \\right] = \\mathbb{E}\\left[\\sum_{t=1}^T (\\bar{r}^\\star - \\bar{r}_t)\\right],\n\\end{align*}\nwhere $\\bar{r}^\\star$ is the mean reward obtained by selecting the best action, $r_t$ is the mean reward corresponding to the action selected at round $t$, and the expectation is taken over the algorithm's internal randomization over actions \\citep{slivkins2019introduction}.\nWe extend this definition to the case where we select multiple actions in parallel at each round, as\n\\begin{align*}\n\\mathbb{E} \\left[ R_T \\right] = \\mathbb{E} \\left[ \\sum_{t=1}^T \\left(\\bar{r}^\\star - B^{-1} \\sum_{i = 1}^B \\bar{r}_{t, i}\\right) \\right],\n\\end{align*}\nwhere $B$ is the number of actions per round, and $\\bar{r}_{t, i}$ is the mean reward associated to the $i$th action performed at round $t$.\n\nBefore describing our algorithm, we first present a brief overview of Thompson sampling~\\citep{russo2016information,russo2018tutorial}. \n%Thompson sampling was introduced over a century ago but it is only recently that its performance has been analysed for the vanilla multi-armed bandit problem.\n\\citet{slivkins2019introduction} give a generalized formulation of Thompson sampling for bandits with immediately observable rewards, which we simplify here for ease of exposition.\nIn a strictly sequential multi-armed bandit, when an agent takes an action $a_t \\in \\mathcal{A}$, a corresponding reward $r_t \\sim q_{\\theta}(\\cdot \\mid a_t)$ is observed.\nWe place a prior distribution $p$ over the model parameters $\\theta$.\nThe action to be taken at each round is chosen by computing $a_t \\gets \\Argmax_{a \\in \\mathcal{A}} \\mathbb{E}_{q_{\\hat{\\theta}}} [r_t \\mid a_t = a]$, yielding a realized observation, which we then condition on to update $p$.\nRather than taking a \\emph{greedy} approach, whereby $\\hat{\\theta}$ is the expectation of $\\theta$ with respect to $p$, Thompson sampling instead samples the parameters from $p$ (i.e. $\\hat{\\theta} \\sim p$).\nThis is a subtle, but powerful difference, as it ensures that the algorithm does not purely exploit actions that yield large rewards in the first few rounds of feedback, ignoring other, possibly better actions.\nDue to the non-zero variance of the belief on the mean reward associated with each action, Thompson sampling may select an action other than that which the greedy algorithm would deem optimal.\nThis mechanism trades off exploration and exploration effectively, and is known to achieve low cumulative regret~\\citep{russo2018tutorial}.\n\n",
                    "subsubsection 3.3.1": {
                        "name": "Impatient Bandit Algorithm",
                        "content": "\nOur approach builds on the Thompson sampling algorithm, applying it to the progressive feedback setting.\nIn our case, the parameters $\\theta$ simply correspond to the average rewards $\\{ \\bar{r}_a : a \\in \\mathcal{A} \\}$.\nThe key to our approach is to make use of the reward model developed in Section~\\ref{sec:model} to infer beliefs $p(\\bar{r}_a)$.\nBy updating beliefs based intermediate outcomes, we enable the sampling step in the Thompson sampling to take full advantage of \\emph{all} information collected up to round $t$, and not only of fully observed rewards.\nWe call the resulting procedure the \\emph{impatient bandit} and describe it in Algorithm~\\ref{alg:bandit}.\n\n\\begin{algorithm}[t]\n\\caption{Impatient Bandit Algorithm}\\label{alg:bandit}\n\\begin{algorithmic}[1]\n\\Require Actions $\\mathcal{A}$, number of actions per round $B$\n%, parameters $\\{\\bm{w}, \\bm{\\mu}, \\bm{\\Sigma}, \\bm{V}\\}$\n\\For{$t = 1, \\dots, T$}\n  \\For{$a \\in \\mathcal{A}$}\n    \\State Update $\\mathcal{D}_a$ with new observations\n    \\State $p(\\bar{\\bm{z}}_a) \\gets \\mathcal{N}(\\bar{\\bm{z}}_a \\mid \\bm{\\mu}_a, \\bm{\\Sigma}_a)$ via Algorithm~\\ref{alg:inference} on $\\mathcal{D}_a$\n    \\State $p(\\bar{r}_a) \\gets \\mathcal{N}(\\bar{r}_a \\mid \\bm{w}^\\top \\bm{\\mu}_a, \\bm{w}^\\top \\bm{\\Sigma}_a \\bm{w})$\n  \\EndFor\n  \\For{$i = 1, \\dots, B$}\n    \\For{$a \\in \\mathcal{A}$}\n      \\State Sample mean reward $\\hat{r}_a \\sim p(\\bar{r}_a)$ \n    \\EndFor\n    \\State Take action $a_{t,i} \\gets \\Argmax_{a \\in \\mathcal{A}} \\{ \\hat{r}_a \\}$\n    %\\State Select action $a^\\star = \\Argmax_a \\{ \\hat{r}_a \\}$\n  \\EndFor \n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n"
                    }
                }
            },
            "section 4": {
                "name": "Application to Podcasts",
                "content": "\n\\label{sec:podcasts}\n\n\n\nWe consider a concrete application of our content-exploration problem to podcast recommendations on Spotify, a leading online audio streaming platform.\\footnote{%\nSee: \\url{https://newsroom.spotify.com/company-info/}.}\nIn Section~\\ref{sec:setup}, we begin by describing how our generic methodology can be applied to optimizing long-term user engagement with podcasts, and present a real-world dataset of podcast consumption traces.\nIn Section~\\ref{sec:evalmodel}, we study the reward model in isolation, and evaluate its predictive accuracy.\nIn Section~\\ref{sec:evalbandit}, we consider a sequential decision-making task in the progressive-feedback setting, and compare the empirical performance of our impatient bandit against competing approaches.\n\nTo complement the experiments presented in this section, we provide a companion software package with a reference implementation of our algorithm.\\footnote{%\nSee: \\url{https://github.com/spotify-research/impatient-bandits}.}\nWhile we are unable to publicly release the data due to confidentiality reasons, our package includes a synthetic dataset that leads to comparable findings.\n\n",
                "subsection 4.1": {
                    "name": "Problem Formulation \\& Data",
                    "content": "\n\\label{sec:setup}\n\nTraditionally, podcast recommender systems optimize for short-term rewards, such as the click-through-rate~\\citep{aziz2022identifying}.\nRecently, \\citet{maystre2023optimizing} show that explicitly optimizing podcast recommendations for long-term outcomes can lead to substantial impact on a real-world, large-scale recommendation problem.\nThey propose a system that reasons simultaneously about the \\emph{clickiness} (i.e., the click-through-rate) and the \\emph{stickiness} of a recommendation.\nStickiness is defined in terms of the downstream consequences of a successful recommendation. In particular, the authors suggest counting the number of days users engage with a podcast show discovered through a recommendation over the \\num{59} days that follows a first listen.\nIn this work, we adopt their definitions and optimization metrics, but consider a specific subset of the overall recommendation problem.\nWe focus on estimating stickiness (i.e., we do not model the click-through rate), and seek to quickly identify new podcast shows that have high average stickiness.\nThis lets us investigate the challenging problem of estimating long-term rewards for new content \\emph{in isolation}, without being confounded by other aspects of the overall recommendation problem.\n\\citet{maystre2023optimizing} discuss how to estimate the click-through rate, and how to personalize models  to take into account users' preferences, but they do not address the content exploration problem we study here.\n\n\n% Formal definition of the problem.\nFormally, we instantiate the methodology described in Section~\\ref{sec:method} as follows.\nThe set of actions $\\mathcal{A}$ corresponds to $N$ candidate podcast shows that are new and that we need to explore.\nWe define the reward $r \\in \\{0, \\ldots, 59 \\}$ as the number of days a user engages with a show in the 59 days that follow a successful recommendation.\\footnote{%\n%The choice of a \\num{60}-day window reflects business goals of the streaming service.\nFor the purposes of this paper, note that such a long horizon crystallizes the challenges of optimizing for the long-term, and forces us to develop methods that explicitly address these challenges.}\nThis reward is observed with a delay of $\\Delta = 60$ days.\nWe refer to the mean reward $\\bar{r}_a$ corresponding to show $a$ as the \\emph{stickiness} of the show.\nWe collect intermediate outcomes $z_k = \\mathbf{1}\\{\\text{the user engaged on day $k$}\\}$ into an activity trace $\\bm{z} \\in \\{0, 1\\}^{59}$.\nNaturally, each activity indicator $z_k$ is observed with delay $\\Delta_k = k+1$.\nFrom these definitions, it follows that $r = \\sum_k z_k = \\bm{w}^\\top \\bm{z}$, where $\\bm{w} = \\bm{1}$ is the all-ones vector.\nThe distinct set $\\mathcal{A}'$ and historical data $\\mathcal{H}_a$, $a \\in \\mathcal{A}'$ correspond to a set of established shows and the corresponding historical consumption traces, respectively.\nWe seek to develop a bandit algorithm that learns to maximize the long-term engagement attributable to each recommendation.\nThis is a clear instance of the progressive feedback setting;\nEvery day, actions must be taken with only partial knowledge about the outcome of decisions made in the previous 59 days.\n\n",
                    "subsubsection 4.1.1": {
                        "name": "Dataset",
                        "content": "\n\\label{sec:datadesc}\n\nWe consider a dataset of podcast consumption traces collected on the Spotify audio streaming platform between September 2021 and May 2022.\nThe data is divided into a training set and an independent validation set.\n%a training set spanning the last three months of 2021 and a validation set covering the first three months of 2022, respectively.\nEach subset consists of a sample of \\num{200} podcast shows first published on the platform during a given three-month period.\nFor each of these shows, the data contains a representative sample of users that discover the show during the same three-month period.\nFor each user, we obtain a longitudinal trace that captures their engagement with the show on each day starting from the day of discovery,\\footnote{%\nWe define a discovery as the first stream that happens on the platform.} in the form of a $59$-dimensional binary vector.\nThe training and validation sets cover podcast shows appearing during the periods September--December 2021 and January--March 2022, respectively.\nEach subset covers a distinct set of shows.\n\nThe podcast shows included in the dataset span a wide range of categories, from \\emph{Arts} to \\emph{True Crime}.\nFigure~\\ref{fig:spotifydata} (left) shows that the distribution of shows over categories is comparable across the two periods.\\footnote{%\nFor confidentiality reasons, we obfuscate the names of the categories.}\nIn total, the dataset consists of \\num{8.77}M activity traces, corresponding to a total of \\num{26}M cumulative active-days.\nThe number of traces per show ranges between \\num{2.4}K and \\num{295}K, with a median of \\num{5.8}K (Figure~\\ref{fig:spotifydata}, center-left).\nFor each show, we define the ground-truth stickiness by means of the empirical average (across users) of the cumulative active-days.\nFigure~\\ref{fig:spotifydata} (center-right) shows that there is substantial heterogeneity in stickiness across shows, with the lower quartile, median, and upper quartile at \\num{2.6}, \\num{3.4}, and \\num{4.6} days, respectively.\nThis suggests that the downstream impact of a discovery can be very different across shows.\nWe note that the stickiness histogram is comparable across the two subsets.\nFinally, some categories appear to be somewhat stickier than others, but within-category variability is significantly larger than between-category variability (Figure~\\ref{fig:spotifydata}, right).\n\n\n"
                    }
                },
                "subsection 4.2": {
                    "name": "Evaluating the Reward Model",
                    "content": "\n\\label{sec:evalmodel}\n\nWe focus first on evaluating our Bayesian reward model in isolation.\nWe estimate $\\bm{\\mu}$, $\\bm{\\Sigma}$ and $\\bm{V}$ by using the shows and consumption traces contained in the training dataset.\nFor each show in the validation dataset, we randomly sample 2000 user traces.\nFrom this subset, we use $M$ traces to infer the stickiness of each show (via Algorithm~\\ref{alg:inference}), and we use the remaining ($2000 - M$) traces for computing the ground truth empirical stickiness.\nIn Figure~\\ref{fig:prederror}, we visualize how the predictive accuracy of our stickiness model varies as a function of both number of days observed, and number of user traces observed.\nWe see that stickiness predictions can be relatively accurate after observing only 10 days of data. The predictions improve as time passes, and having access to more user traces further increases predictive accuracy.\n\n\n\nWe now study the noise and prior covariance matrices $\\bm{V}$ and $\\bm{\\Sigma}$, respectively.\nWe investigate how the variance of the sample reward, $\\mathbb{V}[r \\mid \\bm{z}_{:t}, \\bar{\\bm{z}}]$, and the variance of the mean reward, $\\mathbb{V}[\\bar{r} \\mid \\bar{\\bm{z}}_{:t}]$, are progressively explained away as $t$ increases, i.e., as we condition on more and more days observed.\nNormalizing the $t$th conditional variance by the total (unconditional) variance, we obtain the fraction of total variance explained by the first $t$ intermediate outcomes.\nTechnical details are provided in Appendix~\\ref{app:covar}, alongside visualizations of the covariance matrices as heatmaps.\n\nIn Figure \\ref{fig:varexplained} (left), we look at the noise covariance $\\bm{V}$.\nThe diagonal straight line represents a hypothetical scenario where daily activity indicators $\\bm{z}$ are distributed independently and identically around $\\bar{\\bm{z}}$, resulting in us gaining a constant amount of information about $r$ for each additional day of observed data.\nThe \\emph{empirical} line corresponds to the actual covariance matrix learned by our approach.\nWe can see that around 10 days worth of data is sufficient to capture over 50\\% of the aleatoric uncertainty in the reward $r$.\nThere are two factors that account for this.\nThe first is that, as time progresses, user activity reduces, so the variance is larger early on in the 60-day window;\nThis would be the case even if activity was entirely independent across days.\nThe second and more interesting factor, is that activity is correlated across days, therefore knowledge of activity up to a given day allows us to predict future activity.\nThe \\emph{uncorrelated} line corresponds to the hypothetical case where the diagonal of the covariance matrix matches that of $\\bm{V}$, but there is no correlation (i.e., off-diagonal elements of the matrix are set to zero).\nThe gap between the empirical and uncorrelated curves illustrates how much information we gain by exploiting the fact that past activity is predictive of future activity.\n\n\n\nSimilarly, in Figure~\\ref{fig:varexplained} (right), we look at the prior covariance $\\bm{\\Sigma}$.\nIntuitively, this lets us explore how much of the variance of $\\bar{r}$ would be explained if we were to observe the first $t$ elements of each of a set of $M$ independent sample traces, as $M \\to \\infty$.\n%If daily activity indicator variables were independently and identically distributed, this would immediately tend to 1 after just a single day of observed activity (not pictured).\nWe can draw similar conclusions from this plot as to those mentioned in the context of $\\bm{V}$.\nHowever the trend is even more stark here, as 50\\% of variance is explained by just eight days of data, and 95\\% of the variance is explained within a month.\n\n"
                },
                "subsection 4.3": {
                    "name": "Sequential Decision-Making Task",
                    "content": "\n\\label{sec:evalbandit}\n\n\n\nWe now turn our focus to the evaluation of the impatient bandit algorithm, and to the comparison of its empirical performance with competing approaches.\nThe way observed feedback is used is one of the main points of differentiation between the approaches we consider.\nAs such, we refer to our approach as \\emph{progressive}, since we makes use of all observations as they are revealed over time.\nWe contrast the performance of our approach to three baselines.\n\\begin{description}\n\\item[Delayed.] The case in which we solely receive full observations, $\\Delta$ days after an action is taken is referred to as the \\emph{delayed} feedback baseline.\nThis naive approach does not attempt to take advantage of intermediate outcomes.\n\n\\item[Day-two proxy.] We treat the second day of activity as a proxy for stickiness and discard all subsequent information.\nThis baseline captures an intuitive outcome that is clearly related to the goal of maximizing habitual engagement: Does the user return to the show the day after discovering it? \nThis baseline is representative of short-term proxies widely used in recommender systems, such as the click-through-rate, the dwell time, or the conversion rate \\citep{bogina2017incorporating, dedieu2018hierarchical, lalmas2014measuring}.\n\n\\item[Oracle.] Finally, we include an \\emph{oracle} baseline, which assumes that the full 60-day activity trace is received immediately after an action is taken.\nThis is clearly unrealistic, but it is useful to include as it provides an upper-bound on the performance of any model.\n\\end{description}\nThese baselines have been chosen to illustrate the benefits of incorporating progressive feedback into a bandit algorithm, and the effectiveness of our approach in making use of this intermediate information.\nWe use Thompson sampling for all of our baselines to ensure that any performance differences are due to the manner in which feedback is being considered, rather than to the relative strengths and weaknesses of different families of bandit algorithms.\nFor similar reasons, we do not compare to any works that study other aspects of recommendation unrelated to this study, such as personalization.\n\nTo mimic a realistic deployment setting in which the prior would be computed using data from the past, we compute our prior using the training data, and then run our algorithm on the unseen evaluation dataset.\nA single prior is computed using all available traces from all 200 shows in our training set, this is then used for all of the experiments in this section.\nWe run the bandit for 180 rounds (corresponding to approximately 6 months), repeating each experiment 10 times to generate confidence intervals for the average regret.\nThree different experimental setups are considered, with varying numbers of actions taken per day.\n\n",
                    "subsubsection 4.3.1": {
                        "name": "Results",
                        "content": "\n\nFigure~\\ref{fig:bandit200} (top row) visualizes the average per-step regret for each of these experimental settings, which ideally should tend to zero as $t \\to \\infty$.\nAcross all of the experiments, the performance of the delayed approach is poor as it is forced to make uninformed decisions for the first $\\Delta$ rounds of evaluation due to the inherent delay in feedback being received.\nAdditionally, the oracle, as expected, outperforms the other approaches due to the unrealistic amount of information it has access to.\nThe day-two proxy approach performs well at first, comparably to our approach across the initial month of evaluation, but past this stage the limitations of optimizing for this proxy become clear.\nThe proxy is not well aligned, and the per-step regret rapidly plateaus.\n\nOur progressive approach exhibits superior performance compared to the competing delayed and day-two proxy approaches; in fact, the performance of our approach is closer to that of the oracle.\nAs we increase the number of actions per round, we see a slight reduction in per-step regret across all approaches.\n\nFigure~\\ref{fig:bandit200} (bottom row) provides an alternative perspective on the outcome of these experiments, visualizing the entropy of the set of actions taken at each round.\nShould a bandit converge on recommending a single show repeatedly at each round, the entropy would tend to zero.\nThe entropy plots show that, early on in the evaluation phase, our progressive algorithm tends to diversify across actions more than the oracle and day-two proxy.\nThe interpretation of this is that our approach is performing a broader exploration of the action space, a characteristic that can be very useful in a realistic, deployment setting, which we discuss below. \nNot only does Figure~\\ref{fig:bandit200} let us compare the empirical performance of all four approaches, it also enables us to differentiate the effects of observational noise from the effects of delayed feedback.\nFor example, the large gap in per-step regret between the oracle and delayed approaches is entirely due to the delay in feedback, as both approaches receive full user traces of length $\\Delta$.\nOn the other hand, the gap in per-step regret between the oracle and day-two proxy approaches is due to the fact that the second day of activity is a noisy proxy for the true stickiness, thus the day-two proxy approach tends to rapidly converge on a small subset of sub-optimal shows (this can be seen from its entropy, which quickly approaches zero).\n\n\n\nIn Figure \\ref{fig:bandit50}, we present additional results for a scenario in which we have a smaller action space, consisting of a subset of \\num{50} shows sampled from the original evaluation dataset discussed previously.\nThis is clearly a simpler problem setting, as evidenced by the fact that all of the approaches tend more quickly to lower values of average regret in this case except for the day-two proxy feedback.\nBesides this observation, the results follow largely similar trends to those seen in Figure \\ref{fig:bandit200}.\n\n\\paragraph{Changing Show Set}\nIn addition to considering a static library of shows, we also briefly consider a setting where we have a library of shows that is constantly evolving over time.\nSpecifically, at each round, one randomly selected show is removed from the library and is replaced with a new show.\nFrom the results shown in Figure~\\ref{fig:changing}, we can see that our algorithm once again considerably outperforms the delayed and day-two proxy feedback schemes, even in this challenging setting where new content is constantly entering the system and exploration is always necessary.\n\n"
                    }
                }
            },
            "section 5": {
                "name": "Conclusion \\& Future Work",
                "content": "\n\\label{sec:conclusion}\n\nIn this work we have introduced a new type of bandit algorithm that efficiently optimizes for delayed rewards, assuming that intermediate outcomes correlated with the final reward are revealed progressively over time.\nThis is achieved by way of a meta-learning approach.\nWe begin by learning the parameters of a Bayesian filter by using historical data from a related but distinct problem.\nThen, we combine this probabilistic reward model with Thompson sampling, effectively balancing exploration and exploitation.\nThe key to our success is that the Bayesian filter is able to make accurate inferences on delayed rewards using intermediate outcomes.\n\nWe have evaluated our framework empirically on a podcast content exploration problem.\nUsing real-world platform data, experimental results show that our approach, which utilizes all available intermediate information to estimate a long-term reward, significantly outperforms approaches that only use short-term proxies or wait until the reward is available.\n\nWe have presented a non-personalized methodology for optimizing recommendations over an extended period of time.\nA natural avenue for future work is extending this to a personalized setting.\nConceptually, we do not foresee any major difficulty.\nIn Appendix~\\ref{app:contextual}, we sketch an contextual extension of our Bayesian filter that conditions beliefs on user embeddings.\nAnother avenue of research would be to build a theoretical understanding of the favorable empirical performance observed in practical applications.\nCan we formally characterize the benefits of progressive feedback over delayed rewards in terms of the average regret?\n\nFinally, we would like to emphasize that the general framework we present can also benefit other application domains, beyond recommendations on online content platforms.\nFor example, we believe our algorithm could be used to allocate resources in hyperparameter optimisation problems~\\citep{li2017hyperband} by identifying more or less promising hyperparameter configurations in the early stages of training from an array of intermediate validation metrics.\nThis could significantly reduce the computational cost of training large models and its environmental impact, which has become a major concern in the ML community in recent years~\\citep{lacoste2019quantifying}.\n\n%%\n%% The acknowledgments section is defined using the \"acks\" environment\n%% (and NOT an unnumbered section). This ensures the proper\n%% identification of the section in the article metadata, and the\n%% consistent spelling of the heading.\n\\begin{acks}\nWe thank the anonymous reviewers for their constructive feedback, which greatly contributed to improving this paper.\n\\end{acks}\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{paper}\n\n\\clearpage\n\\appendix\n"
            },
            "section 6": {
                "name": "Comments on Reward Model",
                "content": "\n\\label{app:model}\n\nIn Section~\\ref{sec:training}, we mention that if the definition of the problem at hand does not directly imply a linear relation between a given set of intermediate observations and a long-term reward of interest, one might try to learn a model of target outcomes $y$ by solving a regression problem\n\\begin{align*}\n\\textstyle\n\\Argmin_{w} \\sum_{(\\bm{z}, y) \\in \\mathcal{D}} (y - \\bm{w}^\\top \\bm{z})^2\n\\end{align*}\non some historical data $\\mathcal{D}$.\nIn general, the reward $r = \\bm{w}^\\top \\bm{z}$ will no longer be identical to the true long-term target $y$.\n\nIn this case, the reward can be thought of as a \\emph{surrogate index}, as defined in \\citet{athey2019surrogate}.\nProvided that several assumptions hold, this approach is principled.\nAmong others, $y$ needs to be independent of the selected action $a$ given $r$ (a.k.a. the surrogacy assumption).\nThis assumption requires $\\bm{z}$ to contain sufficient information on $a$ as it relates to $y$.\nFurthermore, the reward $r = \\bm{w}^\\top \\bm{z}$ learned on historical training data should generalize to data coming in during evaluation (a.k.a. the comparability assumption).\nIn practice, it might be important to test these assumptions empirically.\n\n\n",
                "subsection 6.1": {
                    "name": "Non-Linear Extension",
                    "content": "\n\nThe assumption that the reward $r$ is linear in the trace $\\bm{z}$ is not as restrictive as it might appear at first sight.\nIt is easy to extend the model to capture non-linear relationships between $\\bm{z}$ and $r$, while staying in the same linear-Gaussian framework that we rely on throughout Section~\\ref{sec:method}.\n\nAs a concrete example, consider a reward $r$ that depends on $\\bm{z}\\in \\mathbb{R}^2$ in a non-linear way, for example $r = z_1^2 - 3z_2 + 6z_1 z_2$.\nWe can augment the trace into a new vector $\\bm{z}' = (z_1', z_2', z_3', z_4', z_5') = (z_1, z_2, z_1^2, z_2^2, z_1 \\cdot z_2)$.\nNow, we can represent any quadratic relationship between  $\\bm{z}$ and $r$ as a linear relationship between $\\bm{z}'$ and $r$.\nIn particular, our example yields $r = \\bm{w}^\\top \\bm{z}'$ with $\\bm{w} = (0, -3, 1, 0, 6)$.\nBy instantiating the reward model over $\\bm{z}'$ instead of $\\bm{z}$, we can thus model non-linear (quadratic) relations between intermediate outcomes and long-term reward.\nThis idea can be extended to higher-order polynomials or (perhaps better) to regression splines~\\citep{hastie2009elements}, and capture non-linear relationships in a flexible way.\n"
                }
            },
            "section 7": {
                "name": "Covariance Visualizations",
                "content": "\n\\label{app:covar}\n\nIn Figure \\ref{fig:covmats}, we show visualizations of the prior and noise covariance matrices $\\bm{V}$ and $\\bm{\\Sigma}$ obtained by training the model on the data described in Section \\ref{sec:datadesc}.\nIn the prior covariance matrix we see a clear weekly trend, and whilst the entries around the first few days of activity dominate, there is still a rich covariance structure across the whole 60-day period.\nFrom the noise covariance, we can conclude that the daily observations are clearly not independent, but there is still a significant degree of day-to-day variability which is not explained.\n\n\n\n\\paragraph{Technical Details on Figure~\\ref{fig:varexplained}}\n\nThis figure provides an additional perspective on the matrices $\\bm{V}$ and $\\bm{\\Sigma}$.\nOn Figure~\\ref{fig:varexplained} (left) we consider the noise covariance matrix $\\bm{V}$.\nWe have\n\\begin{align*}\n\\mathbb{V}[r \\mid \\bm{z}_{:t}, \\bar{\\bm{z}}]\n    &= \\mathbb{V}[\\bm{1}^\\top \\bm{z} \\mid \\bm{z}_{:t}, \\bar{\\bm{z}}]\n    = \\bm{1}^\\top \\tilde{\\bm{V}}_t \\bm{1} \\doteq \\tilde{v}^2_t, \\\\\n\\tilde{\\bm{V}}_t\n    &= \\bm{V}_{t+1:,t+1:} - \\bm{V}_{t+1:,:t} \\bm{V}_{:t,:t}^{-1}\\bm{V}_{:t,t+1:},\n\\end{align*}\nwhere the last equality makes use of standard Gaussian identities~\\citep[Section A.2]{rasmussen2006gaussian}.\nWe normalize the conditional variance by the total (unconditional) variance to obtain the fraction of total variance explained by the first $t$ intermediate outcomes, i.e., we report $\\tilde{v}^2_t / \\tilde{v}^2_0$.\n\nOn Figure~\\ref{fig:varexplained} (right), we proceed similarly for the prior covariance matrix $\\bm{\\Sigma}$, computing\n\\begin{align*}\n\\mathbb{V}[\\bar{r} \\mid \\bar{\\bm{z}}_{:t}]\n    &= \\mathbb{V}[\\bm{1}^\\top \\bar{\\bm{z}} \\mid \\bar{\\bm{z}}_{:t}]\n    = \\bm{1}^\\top \\tilde{\\bm{\\Sigma}}_t \\bm{1} \\doteq \\tilde{\\sigma}^2_t, \\\\\n\\tilde{\\bm{\\Sigma}}_t\n    &= \\bm{\\Sigma}_{t+1:,t+1:} - \\bm{\\Sigma}_{t+1:,:t} \\bm{\\Sigma}_{:t,:t}^{-1}\\bm{\\Sigma}_{:t,t+1:},\n\\end{align*}\nand we report the normalized value $\\tilde{\\sigma}^2_t / \\tilde{\\sigma}^2_0$.\n"
            },
            "section 8": {
                "name": "Contextual Extension",
                "content": "\n\\label{app:contextual}\n\nOur methodology can be extended to the contextual setting, and we briefly sketch this extension here.\nFor conciseness, let us consider the case of disjoint linear payoffs~\\citep{li2010contextual}, and let us fix a single action and omit the subscript $a$.\nInstead of modeling the $K$-dimensional average trace $\\bar{\\bm{z}}$, we now model a $(d \\times K)$-dimensional matrix $\\bm{\\Theta}$.\nWe assume that the expected reward for selecting the action is $\\bm{x}^\\top \\bm{\\Theta} \\bm{w}$, where $\\bm{x}$ is a context vector (e.g., describing a user's preferences) that can change across rounds.\nIntuitively, the $k$th column of $\\bm{\\Theta}$ describes coefficients of the $k$th context-dependent average intermediate outcome.\n\nWe can extend the Bayesian filter we describe in Section~\\ref{sec:model} to model a belief over the random matrix $\\bm{\\Theta}$ instead of the random vector $\\bar{\\bm{z}}$.\nThis is achieved simply by vectorizing the matrix, that is, $\\mathrm{vec}(\\bm{\\Theta}) \\sim \\mathcal{N}(\\bm{\\mu}, \\bm{\\Sigma})$, with $\\bm{\\mu}$ of dimension $dK$ and $\\bm{\\Sigma}$ of dimension $dK \\times dK$.\nWe can condition the belief updates on the context using standard closed-form formulas.\nFor simplicity, consider a full trace $\\bm{z}$ observed in context $\\bm{x}$;\nThe posterior belief update is given by\n\\begin{align*}\n\\bm{A} &\\gets \\bm{\\Sigma}(\\bm{\\Sigma} + \\bm{x} \\bm{x}^\\top \\otimes \\bm{V})^{-1} \\\\\n\\bm{\\mu}' &\\gets \\bm{\\mu} + \\bm{A}(\\bm{x} \\otimes \\bm{z} - \\bm{\\mu}) \\\\\n\\bm{\\Sigma}' &\\gets \\bm{\\Sigma} + \\bm{A}\\bm{\\Sigma},\n\\end{align*}\nwhere $\\otimes$ denotes the Kronecker product.\n\nThe simple training procedure described in Section~\\ref{sec:training} cannot be easily extended to the contextual case, since the quantities involved in the averages are context-dependent.\nInstead, we suggest using type-II maximum likelihood, a standard hyperparameter selection procedure.\nWe leave a detailed development of a contextual version of our approach for future work.\n\n"
            }
        },
        "figures": {
            "fig:tradeoff": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.8\\linewidth]{fig/tradeoff}\n  \\caption{Short-term proxies enable a rapid feedback loop, but might be poorly aligned with long-term success metrics, which take longer to realize.\n  Our method finds the optimal tradeoff by adaptively making use of all available information at a given time.}\n  \\label{fig:tradeoff}\n\\end{figure}",
            "fig:spotifydata": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics{fig/spotifydata}\n  \\caption{Summary statistics of a dataset of podcast shows and corresponding consumption traces.\n  There is large heterogeneity in show-stickiness (center-right), even when controlling for category (boxplot, right).}\n  \\Description{Summary statistics of a dataset of podcast shows and corresponding consumption traces.\n  There is large heterogeneity in show-stickiness (center-right), even when controlling for category (boxplot, right).}\n  \\label{fig:spotifydata}\n\\end{figure*}",
            "fig:prederror": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics{fig/prederror}\n  \\caption{Mean absolute error ($\\pm$ standard error) of stickiness estimates as a function of days of data observed.}\n  \\Description{A plot showing how the mean absolute error achieved by our Bayesian stickiness model varies as a function of how many days of activity data we have access to, visualized for scenarios in which we have access to 10, 100 and 1000 user traces.}\n  \\label{fig:prederror}\n\\end{figure}",
            "fig:varexplained": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics{fig/varexplained}\n  \\caption{Explained variance as a function of days of activity data observed.}\n  \\Description{Plots visualizing the fraction of variance in user activity that is explained by the first $t$ elements of a covariance matrix.\n  Left: noise covariance matrix $\\bm{V}$. Right: prior covariance matrix $\\bm{\\Sigma}$.}\n  \\label{fig:varexplained}\n\\end{figure}",
            "fig:bandit200": "\\begin{figure*}[t]\n  \\centering\n  \\includegraphics{fig/bandit200}\n  \\caption{Average per-step regret and entropy of set of actions taken at each round, for $N = 200$ podcast shows.}\n  \\Description{Top row: plot of how the per-step regret incurred by the impatient bandit algorithm progresses across 180 rounds of bandit evaluation, along with three baselines (delayed, day-one and oracle feedback), in the scenario where we have a library of 200 podcast shows.\n  Bottom row: plot of how the entropy of the actions taken by the impatient bandit algorithm progresses across 180 rounds of bandit evaluation, along with three baselines (delayed, day-two and oracle feedback), in the scenario where we have a library of 200 podcast shows.}\n  \\label{fig:bandit200}\n\\end{figure*}",
            "fig:bandit50": "\\begin{figure*}\n  \\centering\n  \\includegraphics{fig/bandit50}\n  \\caption{Average per-step regret and entropy of set of actions taken at each round, for $N=50$ podcast shows.}\n  \\Description{Top row: plot of how the per-step regret incurred by the impatient bandit algorithm progresses across 180 rounds of bandit evaluation, along with three baselines (delayed, day-one and oracle feedback), in the scenario where we have a library of 50 podcast shows..\n  Bottom row: plot of how the entropy of the actions taken by the impatient bandit algorithm progresses across 180 rounds of bandit evaluation, along with three baselines (delayed, day-one and oracle feedback), in the scenario where we have a library of 50 podcast shows..}\n  \\label{fig:bandit50}\n\\end{figure*}",
            "fig:changing": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics{fig/changing}\n  \\caption{Average per-step regret in a scenario where we have a \\emph{changing} set of 60 podcast shows.}\n  \\label{fig:changing}\n  \\Description{Plot of how the per-step regret incurred by the impatient bandit algorithm progresses across 180 rounds of bandit evaluation, along with three baselines (delayed, day-one and oracle feedback), in the scenario where we have a changing library of 60 podcast shows.}\n\\end{figure}",
            "fig:covmats": "\\begin{figure}[t]\n  \\centering\n  \\includegraphics{fig/covmats.pdf}\n  \\caption{A visualization of the prior and noise covariance matrices.}\n  \\Description{A visualization of the prior and noise covariance matrices computed using our training dataset.}\n  \\label{fig:covmats}\n\\end{figure}"
        },
        "equations": {
            "eq:1": "\\begin{align}\n\\label{eq:genmodel}\n\\bar{\\bm{z}} &\\sim \\mathcal{N}(\\bm{\\mu}, \\bm{\\Sigma}),\n&\\bm{z}_m  &= \\bar{\\bm{z}} + \\bm{\\varepsilon}_m,\n&\\bm{\\varepsilon}_m &\\sim \\mathcal{N}(\\bm{0}, \\bm{V}) \\ \\text{i.i.d.}\n\\end{align}",
            "eq:2": "\\begin{align*}\nr = \\bm{w}^\\top \\bm{z},\n\\end{align*}",
            "eq:3": "\\begin{align*}\np(\\bar{\\bm{z}} \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\bar{\\bm{z}}) \\mathcal{N}(\\bar{\\bm{z}} \\mid \\bm{\\mu}, \\bm{\\Sigma}).\n\\end{align*}",
            "eq:4": "\\begin{align*}\n\\bar{r} \\mid \\mathcal{D} \\sim \\mathcal{N}(\\mu, \\sigma^2),\n\\end{align*}",
            "eq:5": "\\begin{align*}\n\\bm{\\mu}' &=\n    \\bm{\\mu} + \\bm{\\Sigma}_{:K, :\\ell} \\left(\\bm{\\Sigma}_{:\\ell, :\\ell} + \\bm{V}_{:\\ell, :\\ell} \\right)^{-1} \\left(\\bm{z}_{:\\ell} - \\bm{\\mu}_{:\\ell} \\right), \\\\\n\\bm{\\Sigma}' &=\n    \\bm{\\Sigma} + \\bm{\\Sigma}'_{:K, :\\ell} \\left(\\bm{\\Sigma}_{:\\ell, :\\ell} + \\bm{V}_{:\\ell, :\\ell} \\right)^{-1} \\bm{\\Sigma}_{:\\ell, :K},\n\\end{align*}",
            "eq:6": "\\begin{align*}\n\\hat{\\bm{z}}_a &= M_a^{-1} \\sum_{\\bm{z} \\in \\mathcal{H}_a} \\bm{z},\n&\\hat{\\bm{V}}_a &= M_a^{-1} \\sum_{\\bm{z} \\in \\mathcal{H}_a} (\\bm{z} - \\hat{\\bm{z}}_a)(\\bm{z} - \\hat{\\bm{z}}_a)^\\top,\n\\end{align*}",
            "eq:7": "\\begin{align*}\n\\bm{\\mu}\n    &= \\lvert \\mathcal{A}' \\rvert^{-1} \\sum_{a \\in \\mathcal{A}'} \\hat{\\bm{z}}_a, \\\\\n\\bm{\\Sigma}\n    &= \\lvert \\mathcal{A}' \\rvert^{-1} \\sum_{a \\in \\mathcal{A}'} (\\bm{\\mu} - \\hat{\\bm{z}}_a)(\\bm{\\mu} - \\hat{\\bm{z}}_a)^\\top, \\\\\n\\bm{V}\n    &= \\lvert \\mathcal{A}' \\rvert^{-1} \\sum_{a \\in \\mathcal{A}'} \\hat{\\bm{V}}_a.\n\\end{align*}",
            "eq:8": "\\begin{align*}\n\\textstyle\n\\Argmin_{w} \\sum_{a \\in \\mathcal{A}'} \\sum_{(\\bm{z}, y) \\in \\mathcal{H}_a'} (y - \\bm{w}^\\top \\bm{z})^2,\n\\end{align*}",
            "eq:9": "\\begin{align*}\n\\mathbb{E} \\left[ R_T \\right] = \\mathbb{E}\\left[\\sum_{t=1}^T (\\bar{r}^\\star - \\bar{r}_t)\\right],\n\\end{align*}",
            "eq:10": "\\begin{align*}\n\\mathbb{E} \\left[ R_T \\right] = \\mathbb{E} \\left[ \\sum_{t=1}^T \\left(\\bar{r}^\\star - B^{-1} \\sum_{i = 1}^B \\bar{r}_{t, i}\\right) \\right],\n\\end{align*}",
            "eq:11": "\\begin{align*}\n\\textstyle\n\\Argmin_{w} \\sum_{(\\bm{z}, y) \\in \\mathcal{D}} (y - \\bm{w}^\\top \\bm{z})^2\n\\end{align*}",
            "eq:12": "\\begin{align*}\n\\mathbb{V}[r \\mid \\bm{z}_{:t}, \\bar{\\bm{z}}]\n    &= \\mathbb{V}[\\bm{1}^\\top \\bm{z} \\mid \\bm{z}_{:t}, \\bar{\\bm{z}}]\n    = \\bm{1}^\\top \\tilde{\\bm{V}}_t \\bm{1} \\doteq \\tilde{v}^2_t, \\\\\n\\tilde{\\bm{V}}_t\n    &= \\bm{V}_{t+1:,t+1:} - \\bm{V}_{t+1:,:t} \\bm{V}_{:t,:t}^{-1}\\bm{V}_{:t,t+1:},\n\\end{align*}",
            "eq:13": "\\begin{align*}\n\\mathbb{V}[\\bar{r} \\mid \\bar{\\bm{z}}_{:t}]\n    &= \\mathbb{V}[\\bm{1}^\\top \\bar{\\bm{z}} \\mid \\bar{\\bm{z}}_{:t}]\n    = \\bm{1}^\\top \\tilde{\\bm{\\Sigma}}_t \\bm{1} \\doteq \\tilde{\\sigma}^2_t, \\\\\n\\tilde{\\bm{\\Sigma}}_t\n    &= \\bm{\\Sigma}_{t+1:,t+1:} - \\bm{\\Sigma}_{t+1:,:t} \\bm{\\Sigma}_{:t,:t}^{-1}\\bm{\\Sigma}_{:t,t+1:},\n\\end{align*}",
            "eq:14": "\\begin{align*}\n\\bm{A} &\\gets \\bm{\\Sigma}(\\bm{\\Sigma} + \\bm{x} \\bm{x}^\\top \\otimes \\bm{V})^{-1} \\\\\n\\bm{\\mu}' &\\gets \\bm{\\mu} + \\bm{A}(\\bm{x} \\otimes \\bm{z} - \\bm{\\mu}) \\\\\n\\bm{\\Sigma}' &\\gets \\bm{\\Sigma} + \\bm{A}\\bm{\\Sigma},\n\\end{align*}"
        },
        "git_link": "https://github.com/spotify-research/impatient-bandits"
    }
}